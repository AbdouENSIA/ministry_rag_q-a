{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":1,"page_label":"i","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Fundamentals of Machine Learning for Predictive Data Analytics","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":3,"page_label":"iii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Fundamentals of Machine Learning for Predictive Data Analytics\nAlgorithms, Worked Examples, and Case Studies\nSecond Edition\nJohn D. Kelleher, Brian Mac Namee, and Aoife D’Arcy\nThe MIT Press\nCambridge, Massachusetts\nLondon, England","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":4,"page_label":"iv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"c⃝ 2020 Massachusetts Institute of Technology\nAll rights reserved. No part of this book may be reproduced in any form by any electronic or me-\nchanical means (including photocopying, recording, or information storage and retrieval) without \npermission in writing from the publisher.\nThis book  was set in Times New Roman by the authors. \nLibrary of Congress Cataloging-in-Publication Data\nNames: Kelleher, John D., 1974- author. |\nMac Namee, Brian, author. |\nD’Arcy, Aoife, 1978- author.\nTitle: Fundamentals of machine learning for predictive data analytics : algorithms, worked exam-\nples, and case studies / John D. Kelleher, Brian Mac Namee and Aoife D’Arcy.\nDescription: Second edition. | Cambridge, Massachusetts : The MIT Press, 2020. | Includes biblio-\ngraphical references and index.\nIdentifiers: LCCN 2020002998 | ISBN 9780262044691 (hardcover)\nSubjects: LCSH: Machine learning. | Data mining. | Prediction theory.\nClassification: LCC Q325.5 .K455 2020  | DDC 519.2/870285631–dc23\nLC record available at https://lccn.loc.gov/2020002998","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":5,"page_label":"v","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"To my wife and family,\nthank you for your love, support, and patience.\nJohn\nTo my family.\nBrian\nTo Grandad D’Arcy, for the inspiration.\nAoife","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":7,"page_label":"vii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Contents\nPreface xv\nNotation xxiii\nList of Figures xxxi\nList of Tables xlvii\nI INTRODUCTION TO MACHINE LEARNING AND\nDATA ANALYTICS 1\n1 Machine Learning for Predictive Data Analytics 3\n1.1 What Is Predictive Data Analytics? 3\n1.2 What Is Machine Learning? 5\n1.3 How Does Machine Learning Work? 7\n1.4 Inductive Bias Versus Sample Bias 12\n1.5 What Can Go Wrong with Machine Learning? 13\n1.6 The Predictive Data Analytics Project Lifecycle: CRISP-DM 15\n1.7 Predictive Data Analytics Tools 17\n1.8 The Road Ahead 19\n1.9 Exercises 21\n2 Data to Insights to Decisions 23\n2.1 Converting Business Problems into Analytics Solutions 23\n2.1.1 Case Study: Motor Insurance Fraud 25\n2.2 Assessing Feasibility 26\n2.2.1 Case Study: Motor Insurance Fraud 27\n2.3 Designing the Analytics Base Table 28\n2.3.1 Case Study: Motor Insurance Fraud 31\n2.4 Designing and Implementing Features 32\n2.4.1 Different Types of Data 34","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":8,"page_label":"viii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"viii Contents\n2.4.2 Different Types of Features 34\n2.4.3 Handling Time 36\n2.4.4 Legal Issues 39\n2.4.5 Implementing Features 41\n2.4.6 Case Study: Motor Insurance Fraud 42\n2.5 Summary 44\n2.6 Further Reading 47\n2.7 Exercises 48\n3 Data Exploration 53\n3.1 The Data Quality Report 54\n3.1.1 Case Study: Motor Insurance Fraud 55\n3.2 Getting to Know the Data 55\n3.2.1 The Normal Distribution 61\n3.2.2 Case Study: Motor Insurance Fraud 62\n3.3 Identifying Data Quality Issues 63\n3.3.1 Missing Values 64\n3.3.2 Irregular Cardinality 64\n3.3.3 Outliers 65\n3.3.4 Case Study: Motor Insurance Fraud 66\n3.4 Handling Data Quality Issues 69\n3.4.1 Handling Missing Values 69\n3.4.2 Handling Outliers 70\n3.4.3 Case Study: Motor Insurance Fraud 71\n3.5 Advanced Data Exploration 72\n3.5.1 Visualizing Relationships between Features 72\n3.5.2 Measuring Covariance and Correlation 81\n3.6 Data Preparation 87\n3.6.1 Normalization 87\n3.6.2 Binning 89\n3.6.3 Sampling 91\n3.7 Summary 94\n3.8 Further Reading 95\n3.9 Exercises 96\nII PREDICTIVE DATA ANALYTICS 115\n4 Information-Based Learning 117\n4.1 Big Idea 117\n4.2 Fundamentals 120","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":9,"page_label":"ix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Contents ix\n4.2.1 Decision Trees 121\n4.2.2 Shannon’s Entropy Model 123\n4.2.3 Information Gain 127\n4.3 Standard Approach: The ID3 Algorithm 132\n4.3.1 A Worked Example: Predicting Vegetation Distributions 135\n4.4 Extensions and Variations 141\n4.4.1 Alternative Feature Selection and Impurity Metrics 142\n4.4.2 Handling Continuous Descriptive Features 146\n4.4.3 Predicting Continuous Targets 149\n4.4.4 Tree Pruning 153\n4.4.5 Model Ensembles 158\n4.5 Summary 169\n4.6 Further Reading 170\n4.7 Exercises 172\n5 Similarity-Based Learning 181\n5.1 Big Idea 181\n5.2 Fundamentals 182\n5.2.1 Feature Space 183\n5.2.2 Measuring Similarity Using Distance Metrics 184\n5.3 Standard Approach: The Nearest Neighbor Algorithm 187\n5.3.1 A Worked Example 188\n5.4 Extensions and Variations 191\n5.4.1 Handling Noisy Data 191\n5.4.2 Efﬁcient Memory Search 196\n5.4.3 Data Normalization 204\n5.4.4 Predicting Continuous Targets 208\n5.4.5 Other Measures of Similarity 211\n5.4.6 Feature Selection 223\n5.5 Summary 230\n5.6 Further Reading 233\n5.7 Epilogue 234\n5.8 Exercises 236\n6 Probability-Based Learning 243\n6.1 Big Idea 243\n6.2 Fundamentals 245\n6.2.1 Bayes’ Theorem 248\n6.2.2 Bayesian Prediction 251\n6.2.3 Conditional Independence and Factorization 256","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":10,"page_label":"x","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"x Contents\n6.3 Standard Approach: The Naive Bayes Model 261\n6.3.1 A Worked Example 262\n6.4 Extensions and Variations 265\n6.4.1 Smoothing 265\n6.4.2 Continuous Features: Probability Density Functions 269\n6.4.3 Continuous Features: Binning 280\n6.4.4 Bayesian Networks 284\n6.5 Summary 300\n6.6 Further Reading 303\n6.7 Exercises 305\n7 Error-Based Learning 311\n7.1 Big Idea 311\n7.2 Fundamentals 312\n7.2.1 Simple Linear Regression 312\n7.2.2 Measuring Error 315\n7.2.3 Error Surfaces 317\n7.3 Standard Approach: Multivariable Linear Regression with Gradient\nDescent 319\n7.3.1 Multivariable Linear Regression 319\n7.3.2 Gradient Descent 321\n7.3.3 Choosing Learning Rates and Initial Weights 328\n7.3.4 A Worked Example 330\n7.4 Extensions and Variations 332\n7.4.1 Interpreting Multivariable Linear Regression Models 332\n7.4.2 Setting the Learning Rate Using Weight Decay 334\n7.4.3 Handling Categorical Descriptive Features 336\n7.4.4 Handling Categorical Target Features: Logistic Regression 338\n7.4.5 Modeling Non-Linear Relationships 351\n7.4.6 Multinomial Logistic Regression 357\n7.4.7 Support Vector Machines 361\n7.5 Summary 367\n7.6 Further Reading 370\n7.7 Exercises 371\n8 Deep Learning 381\n8.1 Big Idea 382\n8.2 Fundamentals 383\n8.2.1 Artiﬁcial Neurons 384\n8.2.2 Artiﬁcial Neural Networks 388","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":11,"page_label":"xi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Contents xi\n8.2.3 Neural Networks as Matrix Operations 390\n8.2.4 Why Are Non-Linear Activation Functions Necessary? 394\n8.2.5 Why Is Network Depth Important? 395\n8.3 Standard Approach: Backpropagation and Gradient Descent 403\n8.3.1 Backpropagation: The General Structure of the Algorithm 404\n8.3.2 Backpropagation: Backpropagating the Error Gradients 407\n8.3.3 Backpropagation: Updating the Weights in a Network 413\n8.3.4 Backpropagation: The Algorithm 418\n8.3.5 A Worked Example: Using Backpropagation to Train a\nFeedforward Network for a Regression Task 421\n8.4 Extensions and Variations 434\n8.4.1 Vanishing Gradients and ReLUs 434\n8.4.2 Weight Initialization and Unstable Gradients 447\n8.4.3 Handling Categorical Target Features: Softmax Output\nLayers and Cross-Entropy Loss Functions 463\n8.4.4 Early Stopping and Dropout: Preventing Overﬁtting 472\n8.4.5 Convolutional Neural Networks 477\n8.4.6 Sequential Models: Recurrent Neural Networks and Long\nShort-Term Memory Networks 499\n8.5 Summary 521\n8.6 Further Reading 523\n8.7 Exercises 524\n9 Evaluation 533\n9.1 Big Idea 533\n9.2 Fundamentals 534\n9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set 535\n9.4 Extensions and Variations 540\n9.4.1 Designing Evaluation Experiments 540\n9.4.2 Performance Measures: Categorical Targets 547\n9.4.3 Performance Measures: Prediction Scores 556\n9.4.4 Performance Measures: Multinomial Targets 572\n9.4.5 Performance Measures: Continuous Targets 574\n9.4.6 Evaluating Models after Deployment 578\n9.5 Summary 585\n9.6 Further Reading 586\n9.7 Exercises 588","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":12,"page_label":"xii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xii Contents\nIII BEYOND PREDICTION 595\n10 Beyond Prediction: Unsupervised Learning 597\n10.1 Big Idea 597\n10.2 Fundamentals 598\n10.3 Standard Approach: The k-Means Clustering Algorithm 600\n10.3.1 A Worked Example 601\n10.4 Extensions and Variations 605\n10.4.1 Choosing Initial Cluster Centroids 605\n10.4.2 Evaluating Clustering 607\n10.4.3 Choosing the Number of Clusters 612\n10.4.4 Understanding Clustering Results 613\n10.4.5 Agglomerative Hierarchical Clustering 616\n10.4.6 Representation Learning with Auto-Encoders 624\n10.5 Summary 628\n10.6 Further Reading 629\n10.7 Exercises 631\n11 Beyond Prediction: Reinforcement Learning 637\n11.1 Big Idea 637\n11.2 Fundamentals 638\n11.2.1 Intelligent Agents 639\n11.2.2 Fundamentals of Reinforcement Learning 640\n11.2.3 Markov Decision Processes 643\n11.2.4 The Bellman Equations 651\n11.2.5 Temporal-Difference Learning 654\n11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference\nLearning 657\n11.3.1 A Worked Example 659\n11.4 Extensions and Variations 664\n11.4.1 SARSA, On-Policy Temporal-Difference Learning 664\n11.4.2 Deep Q Networks 668\n11.5 Summary 674\n11.6 Further Reading 677\n11.7 Exercises 679","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":13,"page_label":"xiii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Contents xiii\nIV CASE STUDIES AND CONCLUSIONS 683\n12 Case Study: Customer Churn 685\n12.1 Business Understanding 685\n12.2 Data Understanding 688\n12.3 Data Preparation 691\n12.4 Modeling 697\n12.5 Evaluation 698\n12.6 Deployment 702\n13 Case Study: Galaxy Classiﬁcation 703\n13.1 Business Understanding 704\n13.1.1 Situational Fluency 706\n13.2 Data Understanding 707\n13.3 Data Preparation 713\n13.4 Modeling 719\n13.4.1 Baseline Models 719\n13.4.2 Feature Selection 722\n13.4.3 The 5-Level Model 722\n13.5 Evaluation 725\n13.6 Deployment 727\n14 The Art of Machine Learning for Predictive Data Analytics 729\n14.1 Different Perspectives on Prediction Models 731\n14.2 Choosing a Machine Learning Approach 735\n14.2.1 Matching Machine Learning Approaches to Projects 738\n14.2.2 Matching Machine Learning Approaches to Data 739\n14.3 Beyond Prediction 740\n14.4 Your Next Steps 741\nV APPENDICES 743\nA Descriptive Statistics and Data Visualization for Machine Learning 745\nA.1 Descriptive Statistics for Continuous Features 745\nA.1.1 Central Tendency 745\nA.1.2 Variation 746\nA.2 Descriptive Statistics for Categorical Features 749\nA.3 Populations and Samples 750\nA.4 Data Visualization 752\nA.4.1 Bar Plots 752\nA.4.2 Histograms 752","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":14,"page_label":"xiv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xiv Contents\nA.4.3 Box Plots 755\nB Introduction to Probability for Machine Learning 757\nB.1 Probability Basics 757\nB.2 Probability Distributions and Summing Out 761\nB.3 Some Useful Probability Rules 762\nB.4 Summary 763\nC Differentiation Techniques for Machine Learning 765\nC.1 Derivatives of Continuous Functions 766\nC.2 The Chain Rule 768\nC.3 Partial Derivatives 768\nD Introduction to Linear Algebra 771\nD.1 Basic Types 771\nD.2 Transpose 772\nD.3 Multiplication 772\nD.4 Summary 774\nBibliography 775\nIndex 787","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":15,"page_label":"xv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Preface\nPreface to the 1stEdition\nIn writing this book our target was to deliver an accessible, introductory text on the funda-\nmentals of machine learning and the ways that machine learning is used in practice to solve\npredictive data analytics problems in business, science, and other organizational contexts.\nAs such, the book goes beyond the standard topics covered in machine learning books and\nalso covers the lifecycle of a predictive analytics project, data preparation, feature design,\nand model deployment.\nThe book is intended for use in machine learning, data mining, data analytics, or artiﬁcial\nintelligence modules of undergraduate and postgraduate computer science, natural and so-\ncial science, engineering, and business courses. The book provides case studies illustrating\nthe application of machine learning within the industry context of data analytics, which\nalso makes it a suitable text for practitioners looking for an introduction to the ﬁeld and a\ntextbook for industry training courses in these areas.\nThe design of the book is informed by our many years of experience in teaching machine\nlearning, and the approach and material in the book has been developed and “road-tested”\nin the classroom. In writing this book, we have adopted the following guiding principles\nto make the material accessible:\n1.Explain the most important and popular algorithms clearly, rather than overview the\nfull breadth of machine learning. As teachers we believe that giving students a deep\nknowledge of the core concepts underpinning a ﬁeld provides them with a solid ba-\nsis from which they can explore the ﬁeld themselves. This sharper focus allows us\nto spend more time introducing, explaining, illustrating, and contextualizing the algo-\nrithms that are fundamental to the ﬁeld and their uses.\n2.Informally explain what an algorithm has been designed to do before presenting the\ntechnical formal description of how it does it. Providing this informal introduction to\neach topic gives students a solid basis from which to attack the more technical mate-\nrial. Our experience with teaching this material to mixed audiences of undergraduates,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":16,"page_label":"xvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xvi Preface\npostgraduates, and professionals has shown that these informal introductions enable\nstudents to easily access the topic.\n3.Provide complete worked examples. In this book we have presented complete work-\nings for all examples, because this enables readers to check their understanding in\ndetail.\nPreface to the 2ndEdition\nSince the ﬁrst edition of this book was released in 2015, things have been moving very\nfast in the world of machine learning. Machine learning models are being used for a more\ndiverse range of applications than ever before, and new developments in machine learning\nmethods are opening up even more opportunities. This has led to the ﬁeld receiving signif-\nicant attention from the media (both positive and negative) and a greater interest than ever\nbefore from people who want to become machine learning practitioners. For these reasons\nwe felt the time was right for a second edition of the book. In this edition we have sought\nto expand the topics covered in the ﬁrst edition to bring them up to date with modern de-\nvelopments, while at the same time staying true to the approach used in the ﬁrst edition\nof covering the core concepts in the ﬁeld in depth with fully worked examples. The main\nadditions in the second edition are the following:\n‚Due to its expanded size, this edition of the book has been organized into ﬁve parts:\nPart I: Introduction to Machine Learning and Data Analytics; Part II: Predictive Data\nAnalytics; Part III: Beyond Prediction; Part IV: Case Studies and Conclusions; and Part\nV: Appendices.\n‚Chapter 8 is a new chapter on deep learning that covers the fundamentals of artiﬁcial\nneural networks as well as the most important network architectures used in modern\nmachine learning applications for images, language, and more. This brings the book\nright up to date with the most recent developments in machine learning.\n‚Chapter 10 is a new chapter covering the most important ideas and techniques in unsu-\npervised learning . This chapter is one-half of a new part of the book, Beyond Predic-\ntion, that expands beyond the focus of the ﬁrst edition on supervised learning to allow\nbroader coverage of machine learning.\n‚Chapter 11, in the second half of the new Beyond Prediction part of the book, describes\nreinforcement learning from the fundamental ideas that underpin it to the use of deep\nneural networks in modern reinforcement learning systems.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":16,"page_label":"xvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"machine learning applications for images, language, and more. This brings the book\nright up to date with the most recent developments in machine learning.\n‚Chapter 10 is a new chapter covering the most important ideas and techniques in unsu-\npervised learning . This chapter is one-half of a new part of the book, Beyond Predic-\ntion, that expands beyond the focus of the ﬁrst edition on supervised learning to allow\nbroader coverage of machine learning.\n‚Chapter 11, in the second half of the new Beyond Prediction part of the book, describes\nreinforcement learning from the fundamental ideas that underpin it to the use of deep\nneural networks in modern reinforcement learning systems.\n‚Section 4.4.5 has been revised to cover ensemble models in more detail than in the ﬁrst\nedition and includes a new description of gradient boosting methods.\n‚Appendix D is a new section covering the fundamentals of linear algebra that underpin\nthe new chapter on deep learning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":17,"page_label":"xvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Preface xvii\n‚The question sets at the end of each chapter have been revised and expanded—this in-\ncludes over 50 new questions .\nWe hope that these additions and revisions will make the second edition of the book more\nuseful and relevant than the ﬁrst. We are indebted to all those people—readers, students,\ninstructors, reviewers, colleagues, friends, and family—who gave us feedback on the ﬁrst\nedition, which has helped us massively in designing this new edition.\nStructure of the Book\nIn teaching a technical topic, it is important to show the application of the concepts dis-\ncussed to real-life problems. For this reason, we present machine learning within the con-\ntext of predictive data analytics, an important industry application of machine learning.\nThe link between machine learning and data analytics runs through every chapter in the\nbook but is especially strong in Part I, which includes Chapters 1 to 3. In Chapter 1 we\nintroduce machine learning and explain its role within a standard data analytics project\nlifecycle. In Chapter 2 we provide a framework for designing and constructing a predic-\ntive analytics solution based on machine learning that meets a business need. All machine\nlearning algorithms assume that a dataset is available for training; and in Chapter 3 we\nexplain how to design, construct, and quality check a dataset before using it to build a\nprediction model.\nPart II[117]of the book includes Chapters 4 to 9 and covers the main supervised machine\nlearning material. Each of this part’s ﬁrst ﬁve chapters presents a different approach to\nmachine learning: Chapter 4, learning through information gathering; Chapter 5, learning\nthrough analogy; Chapter 6, learning by predicting probable outcomes; Chapter 7, learning\nby searching for solutions that minimize error; and Chapter 8, learning new representations\nusing deep neural networks. All these chapters follow the same two-part structure:\n‚The ﬁrst part of each chapter presents an informal introduction to the material presented\nin the chapter, followed by a detailed explanation of the fundamental technical concepts\nrequired to understand the material. Then it presents a standard machine learning algo-\nrithm used in that learning approach, along with a detailed worked example.\n‚The second part of each chapter explains different ways that the standard algorithm can\nbe extended and well-known variations on the algorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":17,"page_label":"xvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"by searching for solutions that minimize error; and Chapter 8, learning new representations\nusing deep neural networks. All these chapters follow the same two-part structure:\n‚The ﬁrst part of each chapter presents an informal introduction to the material presented\nin the chapter, followed by a detailed explanation of the fundamental technical concepts\nrequired to understand the material. Then it presents a standard machine learning algo-\nrithm used in that learning approach, along with a detailed worked example.\n‚The second part of each chapter explains different ways that the standard algorithm can\nbe extended and well-known variations on the algorithm.\nThe motivation for structuring these technical chapters in two parts is that it provides a\nnatural break in the chapter material. As a result, a topic can be included in a course by\ncovering just the ﬁrst part of a chapter (Big Idea, fundamentals, standard algorithm, and\nworked example); and then—time permitting—the coverage of the topic can be extended\nto some or all of the material in the second part. Chapter 9 explains how to evaluate the\nperformance of prediction models and presents a range of different evaluation metrics. This\nchapter also adopts the two-part structure of standard approach followed by extensions","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":18,"page_label":"xviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xviii Preface\nand variations. Throughout these technical chapters, the link to the broader predictive\nanalytics context is maintained through detailed and complete real-world examples, along\nwith references to the datasets and/or papers on which the examples are based.\nPart III covers fundamental techniques in machine learning beyond the supervised ma-\nchine learning approaches described in Part II. Chapter 10 describes unsupervised machine\nlearning, and Chapter 11 describes reinforcement learning. These chapters follow the same\ntwo-part structure as the chapters in Part II.\nIn Part IV the link between the broader business context and machine learning is shown\nvery clearly in the case studies presented in Chapters 12 (predicting customer churn) and 13\n(galaxy classiﬁcation). In particular, these case studies highlight how a range of issues and\ntasks beyond model building—such as business understanding, problem deﬁnition, data\ngathering and preparation, and communication of insight—are crucial to the success of a\npredictive analytics project. Finally, Chapter 14 discusses a range of fundamental topics\nin machine learning and highlights that the selection of an appropriate machine learning\napproach for a given task involves factors beyond model accuracy—we must also match\nthe characteristics of the model to the needs of the business.\nPart V of the book contains appendices covering background material required to support\nthe content of the other chapters of the book. This includes descriptive statistics and data\nvisualization (Appendix A), probability (Appendix B), differentiation (Appendix C), and\nlinear algebra (Appendix D).\nHow to Use This Book\nThrough our years of teaching this material, we have developed an understanding of what\nis a reasonable amount of material to cover in a one-semester introductory module and in\na two-semester more advanced module. To facilitate its use in these different contexts, the\nbook has been designed to be modular—with very few dependencies between chapters. As\na result, instructors using this book can plan their courses by simply selecting the sections\nof the book they wish to cover without worrying about dependencies between the sections.\nWhen presented in class, the material in Chapters 1, 2, 12, 13, and 14 typically takes two\nto three lecture hours per chapter to cover; and the material in Chapters 3, 4, 5, 6, 7, 8, 9,\n10, and 11 normally takes four to six lecture hours per chapter to cover.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":18,"page_label":"xviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is a reasonable amount of material to cover in a one-semester introductory module and in\na two-semester more advanced module. To facilitate its use in these different contexts, the\nbook has been designed to be modular—with very few dependencies between chapters. As\na result, instructors using this book can plan their courses by simply selecting the sections\nof the book they wish to cover without worrying about dependencies between the sections.\nWhen presented in class, the material in Chapters 1, 2, 12, 13, and 14 typically takes two\nto three lecture hours per chapter to cover; and the material in Chapters 3, 4, 5, 6, 7, 8, 9,\n10, and 11 normally takes four to six lecture hours per chapter to cover.\nIn Table 0.1 we have listed a number of suggested course plans targeting different con-\ntexts. All these courses include Chapter 1 (Machine Learning for Predictive Data Ana-\nlytics) and Chapter 14 (The Art of Machine Learning for Predictive Data Analytics). The\nﬁrst course listed (column “M.L. (short) (deep)”) is designed to be a one-semester machine\nlearning course with a focus on giving students a deep understanding of two approaches\nto machine learning, along with an understanding of the correct methodology to use when\nevaluating a machine learning model. In our suggested course we have chosen to cover\nall of Chapters 4 (Information-Based Learning) and 7 (Error-Based Learning). However,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":19,"page_label":"xix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Preface xix\nTable 0.1\nSuggested course syllabi.\nM.L. M.L. M.L. P.D.A. P.D.A.\n(short) (short) (long) (short) (long)\nChapter Section (deep) (broad)\n1 •••••\n2 ••\n3 3.1, 3.2, 3.3, 3.4, 3.5 ••\n3.6 ••• •\n4 4.1, 4.2, 4.3 •••••\n4.4.1, 4.4.2, 4.4.3 • •\n4.4.4 •••\n4.4.5 ••• •\n5 5.1, 5.2, 5.3, 5.4.1, 5.4.3, 5.4.6 •• •\n5.4.2, 5.4.4, 5.4.5 •\n6 6.1, 6.2, 6.3, 6.4.1 •• •\n6.4.2, 6.4.3, 6.4.4 •\n7 7.1, 7.2, 7.3 •••••\n7.4.1, 7.4.2, 7.4.3 • • •\n7.4.4, 7.4.5, 7.4.6, 7.4.7 ••• •\n8 8.1, 8.2, 8.3 •• •\n8.4.1, 8.4.2, 8.4.3, 8.4.4, 8.4.5, 8.4.6, •\n9 9.1, 9.2, 9.3 •••••\n9.4.1, 9.4.2, 9.4.3,9.4.4, 9.4.5 ••• •\n9.4.6 •\n10 10.1, 10.2, 10.3 ••••\n10.4.1, 10.4.2, 10.4.4, 10.4.5, 10.4.6\n11 11.1, 11.2, 11.3 ••\n11.4.1, 11.4.2\n12 ••\n13 •\n14 •••••\nChapter 5 (Similarity-Based Learning) and/or 6 (Probability-Based Learning) could be\nused instead. The “M.L. (short) (deep)” plan is also an ideal course plan for a short (one-\nweek) professional training course. The second course (“M.L. (short) (broad)” is another\none-semester machine learning course. Here, however, the focus is on covering a range of\nmachine learning approaches, and again, evaluation is covered in detail. For a longer two-\nsemester machine learning course (“M.L. (long)”) we suggest covering data preparation\n(Section 3.6), all the machine learning chapters, and the evaluation chapter.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":20,"page_label":"xx","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xx Preface\nThere are contexts, however, in which the focus of a course is not primarily on machine\nlearning. We also present two course paths that focus on the context of predictive data\nanalytics. The course “P.D.A. (short)” deﬁnes a one-semester course. This course gives\nstudents an introduction to predictive data analytics, a solid understanding of how machine\nlearning solutions should be designed to meet a business need, insight into how prediction\nmodels work and should be evaluated, and one of the case studies. The “P.D.A. (short)”\nplan is also an ideal course plan for a short (one-week) professional training course. If there\nis more time available, then “P.D.A. (long)” expands on the “P.D.A. (short)” course so that\nstudents gain a deeper and broader understanding of machine learning, and it includes the\nsecond case study.\nOnline Resources\nThe following website:\nwww.machinelearningbook.com\nprovides access to a wide range of material that supports the book. Worked solutions for\nall end-of-chapter exercises are available. For questions that are not marked with an ˚, a\nsolutions manual is available from the book website. Solutions for those questions that are\nmarked with an ˚are contained in an instructors’ manual available on request from MIT\nPress.\nAcknowledgments\nWe knew when we began writing this book that it would take a huge amount of work\nto complete. We underestimated, however, the amount of support we would need and\nreceive from other people. We are delighted to take this opportunity to acknowledge these\ncontributions to the book. We would like to thank our colleagues and students for the help\nand patience they extended to us over the last few years. We would also like to thank\nthe staff at MIT Press, particularly Marie Lufkin Lee and our editors Melanie Mallon ( 1st\nEdition), as well as Theresa Carcaldi of Westchester Publishing Services ( 2ndEdition).\nWe are also very grateful to the anonymous reviewers who provided insightful and helpful\ncomments on early drafts of the manuscript. Each of us has also been fortunate to have the\nsupport of close friends and family, which was invaluable in completing the book.\nJohn would like to thank Lorraine Byrne, Simon Dobnik, Dietmar Frey, Ionella Longo,\nAlan Mc Donnell, Josef van Genabith, Mary van Genabith, his colleagues and students\nat Technological University Dublin and in the ADAPT Centre, and all his friends from","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":20,"page_label":"xx","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the staff at MIT Press, particularly Marie Lufkin Lee and our editors Melanie Mallon ( 1st\nEdition), as well as Theresa Carcaldi of Westchester Publishing Services ( 2ndEdition).\nWe are also very grateful to the anonymous reviewers who provided insightful and helpful\ncomments on early drafts of the manuscript. Each of us has also been fortunate to have the\nsupport of close friends and family, which was invaluable in completing the book.\nJohn would like to thank Lorraine Byrne, Simon Dobnik, Dietmar Frey, Ionella Longo,\nAlan Mc Donnell, Josef van Genabith, Mary van Genabith, his colleagues and students\nat Technological University Dublin and in the ADAPT Centre, and all his friends from\nbasketball for their help and encouragement. He would also like to thank his parents (John\nand Betty) and his sisters (Elizabeth and Marianne), without whose support he would not\nhave gotten past long division and basic spelling. Finally, he would like to acknowledge","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":21,"page_label":"xxi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Preface xxi\nand thank Aphra; this book would not have been started without her inspiration and would\nnot have been completed without her support and patience.\nBrian would like to thank his parents (Liam and Rois ´ın) and family for all of their\nsupport on this book (and everything else). He would also like to thank all his col-\nleagues and students at University College Dublin (and previously at the Dublin Institute\nof Technology)—especially P ´adraig Cunningham and Sarah Jane Delany, who opened his\neyes to machine learning.\nAoife would like to thank her parents (Michael and Mairead) and family, and all the peo-\nple who have supported her throughout her career—especially the much-valued customers\nof Krisolis who give her data to play with!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":23,"page_label":"xxiii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Notation\nIn this section we provide a short overview of the technical notation used throughout this\nbook.\nNotational Conventions\nThroughout this book we discuss the use of machine learning algorithms to train prediction\nmodels based on datasets. The following list explains the notation used to refer to different\nelements in a dataset. Figure 0.1[xxiii]illustrates the key notation using a simple sample\ndataset.\nID  Name Age Country  Rating \n1 Brian 24 Ireland B \n2 Mary 57 France AA \n3 Sinead 45 Ireland AA \n4 Paul 38 USA A \n5 Donald 62 Canada B \n6 Agnes 35 Sweden C \n7 Tim 32 USA B DRating =AA\nD\nd7d5[3]d[1]\nt4\nFigure 0.1\nHow the notation used in the book relates to the elements of a dataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":24,"page_label":"xxiv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxiv Notation\nDatasets\n‚Ddenotes a dataset.\n‚A dataset is composed of ninstances,pd1,t1qtopdn,tnq, where dis a set of mdescriptive\nfeatures, and tis a target feature.\n‚A subset of a dataset is denoted by Dwith a subscript to indicate the deﬁnition of the\nsubset. For example, Df“lrepresents the subset of instances from the dataset Dwhere\nthe feature fhas the value l.\nVectors of Features\n‚Lowercase boldface letters refer to a vector of features. For example, ddenotes a vector\nof descriptive features for an instance in a dataset, and qdenotes a vector of descriptive\nfeatures in a query.\nInstances\n‚Subscripts are used to index into a list of instances.\n‚xirefers to the ithinstance in a dataset.\n‚direfers to the descriptive features of the ithinstance in a dataset.\nIndividual Features\n‚Lowercase letters represent a single feature (e.g., f,a,b,c...).\n‚Square brackets rsare used to index into a vector of features (e.g., drjsdenotes the value\nof the jthfeature in the vector d).\n‚trepresents the target feature.\nIndividual Features in a Particular Instance\n‚dirjsdenotes the value of the jthdescriptive feature of the ithinstance in a dataset.\n‚airefers to the value for feature aof the ithinstance in a dataset.\n‚tirefers to the value of the target feature of the ithinstance in a dataset\nIndexes\n‚Typically, iis used to index instances in a dataset, and jis used to index features in a\nvector.\nModels\n‚We use Mto refer to a model.\n‚Mwrefers to a model Mparameterized by a parameter vector w.\n‚Mwpdqrefers to the output of a model Mparameterized by parameters wfor descriptive\nfeatures d.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":25,"page_label":"xxv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Notation xxv\nSet Size\n‚Vertical bars| |refer to counts of occurrences (e.g., |a“l|represents the number of\ntimes that a“loccurs in a dataset).\nFeature Names and Feature Values\n‚We use a speciﬁc typography when referring to a feature by name in the text (e.g., P O-\nSITION ,CREDIT RATING , and C LAIM AMOUNT ).\n‚For categorical features, we use a speciﬁc typography to indicate the levels in the domain\nof the feature when referring to a feature by name in the text (e.g., center ,aa, and soft\ntissue ).\nNotational Conventions for Probabilities\nFor clarity there are some extra notational conventions used in Chapter 6[243]on probability.\nGeneric Events\n‚Uppercase letters denote generic events where an unspeciﬁed feature (or set of features)\nis assigned a value (or set of values). Typically, we use letters from the end of the\nalphabet—e.g., X,Y,Z—for this purpose.\n‚We use subscripts on uppercase letters to iterate over events. So,ř\niPpXiqshould be\ninterpreted as summing over the set of events that are a complete assignment to the\nfeatures in X(i.e., all the possible combinations of value assignments to the features in\nX).\nNamed Features\n‚Features explicitly named in the text are denoted by the uppercase initial letters of their\nnames. For example, a feature named M ENINGITIS is denoted by M.\nEvents Involving Binary Features\n‚Where a named feature is binary, we use the lowercase initial letter of the name of the\nfeature to denote the event where the feature is true and the lowercase initial letter pre-\nceded by the␣symbol to denote the event where it is false. So, mwill represent the\nevent M ENINGITIS“true, and␣mwill denote M ENINGITIS“false .\nEvents Involving Non-Binary Features\n‚We use lowercase letters with subscripts to iterate across values in the domain of a fea-\nture.\n‚Soř\niPpmiq“Ppmq`Pp␣mq.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":26,"page_label":"xxvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxvi Notation\n‚In situations where a letter, for example, X, denotes a joint event, thenř\niPpXiqshould\nbe interpreted as summing over all the possible combinations of value assignments to the\nfeatures in X.\nProbability of an Event\n‚The probability that the feature fis equal to the value vis written Ppf“vq.\nProbability Distributions\n‚We use bold notation Ppqto distinguish a probability distribution from a probability mass\nfunction Ppq.\n‚We use the convention that the ﬁrst element in a probability distribution vector is the\nprobability for a true value. For example, the probability distribution for a binary feature,\nA, with a probability of 0.4of being true would be written PpAq“ă 0.4,0.6ą.\nNotational Conventions for Deep Learning\nFor clarity, some additional notational conventions are used in Chapter 8[381]on deep learn-\ning.\nActivations\n‚The activation (or output) of single neuron iis denoted by ai\n‚The vector of activations for a layer of neurons is denoted by apkqwhere kidentiﬁes the\nlayer.\n‚A matrix of activations for a layer of neurons processing a batch of examples is denoted\nbyApkqwhere kidentiﬁes the layer.\nActivation Functions\n‚We use the symbol ϕto generically represent activation functions. In some cases we\nuse a subscript to indicate the use of a particular activation function. For example, ϕS M\nindicates the use of a softmax function activation function, whereas ϕReLU indicates the\nuse of a rectiﬁed linear activation function.\nCategorical Targets In the context of handling categorical target features (see Section\n8.4.3[463]) using a softmax function, we use the following symbols:\n‚We use the‹symbol to indicate the index of the true category in the distribution.\n‚We use Pto write the true probability distribution over the categories of the target; ˆPto\nwrite the distribution over the target categories that the model has predicted; and ˆP‹to\nindicate the predicted probability for the true category.\n‚We write tto indicate the one-hot encoding vector of a categorical target.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":27,"page_label":"xxvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Notation xxvii\n‚In some of the literature on neural networks, the term logit is used to refer to the result of\na weighted sum calculation in a neuron (i.e., the value we normally denote z). In partic-\nular, this terminology is often used in the explanation of softmax functions; therefore, in\nthe section on handling categorical features, we switch from our normal znotation, and\ninstead follow this logit nomenclature, using the notation lto denote a vector of logits\nfor a layer of neurons, and lito indicate the logit for the ithneuron in the layer.\nElementwise Product\n‚We usedto denote an elementwise product. This operation is sometimes called the\nHadamard product .\nError Gradients (Deltas) δ\n‚We use the symbol δto indicate the rate of change of the error of the network with\nrespect to changes in the weighted sum calculated in a neuron. These δvalues are the\nerror gradients that are backpropagated during the backward pass of the backpropagation\nalgorithm. We use a subscript to identify the particular neuron that the δis associated\nwith; for example, δiis theδfor neuron iand is equivalent to the termBE\nBzi. In some cases\nwe wish to refer to the vector of δs for the neurons in a layer l; in these cases we write\nδplq\nNetwork Error\n‚We use the symbol Eto denote the error of the network at the output layer.\nWeights\n‚We use a lowercase wto indicate a single weight on a connection between two neurons.\nWe use a double subscript to indicate the neurons that are connected, with the convention\nthat the ﬁrst subscript is the neuron the connection goes to, and the second subscript is\nthe neuron the connection is from. For example, wi,kis the weight on the connection\nfrom neuron kto neuron i.\n‚We use ∆wi,kto write the sum of error gradients calculated for the weight wi,k. We sum\nerrors in this way during batch gradient descent with which we sum over the examples\nin the batch; see Equation (8.30)[416]and also in cases in which the weight is shared by a\nnumber of neurons, whether in a convolutional neural network or during backpropagation\nthrough time.\n‚We use a bold capital Wto indicate a weight matrix, and we use a superscript in brackets\nto indicate the layer of the network the matrix is associated with. For example, Wpkqis\nthe weight matrix for the neurons in layer k. In an LSTM network we treat the neurons\nin the sigmoid and tanh layers within each gate as a separate layer of neurons, and so","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":27,"page_label":"xxvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"from neuron kto neuron i.\n‚We use ∆wi,kto write the sum of error gradients calculated for the weight wi,k. We sum\nerrors in this way during batch gradient descent with which we sum over the examples\nin the batch; see Equation (8.30)[416]and also in cases in which the weight is shared by a\nnumber of neurons, whether in a convolutional neural network or during backpropagation\nthrough time.\n‚We use a bold capital Wto indicate a weight matrix, and we use a superscript in brackets\nto indicate the layer of the network the matrix is associated with. For example, Wpkqis\nthe weight matrix for the neurons in layer k. In an LSTM network we treat the neurons\nin the sigmoid and tanh layers within each gate as a separate layer of neurons, and so\nwe write Wpfqfor the weight matrix for the neurons in the forget gate, and so on for\nthe weight matrices of the other neuron layers in the other gates. However, in a simple\nrecurrent network we distinguish the weight matrices on the basis of whether the matrix","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":28,"page_label":"xxviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxviii Notation\nis on the connections between the input and the hidden layer, the hidden layer and the\noutput, or the activation memory buffer and the hidden layer. Consequently, for these\nmatrices it is important to highlight the end of the connections the weights are applied\nto; we use a double subscript (similar to the subscript for a single weight), writing Whx\nfor the weight matrix on the connections between the input ( x) and the hidden layer ( h).\nWeighted Sums and Logits\n‚We use a lowercase zto represent the result of the weighted sum of the inputs in a neuron.\nWe indicate the identity of the neuron in which the calculation occurred using a subscript.\nFor example, ziis the result of the weighted sum calculation carried out in neuron i. Note,\nhowever, that in the section on Handling Categorical Target features, we switch to the\nterm logit to refer to the output of the weight sum in a neuron and update the notation\nto reﬂect this switch; see the previous notation section on Categorical Targets for more\ndetails.\n‚The vector of weighted sums for a layer of neurons is denoted by zpkqwhere kidentiﬁes\nthe layer.\n‚A matrix of weighted sums calculations for a layer of neurons processing a batch of\nexamples is denoted by Zpkqwhere kidentiﬁes the layer.\nNotational Conventions for Reinforcement Learning\nFor clarity there are some extra notational conventions used in Chapter 11[637]on reinforce-\nment learning (this chapter also heavily uses the notation from the probability chapter).\nAgents, States, and Actions\n‚In reinforcement learning we often describe an agent at time ttaking an action, at, to\nmove from its current state, st, to the next state, st`1.\n‚An agent’s current state is often modeled as a random variable, St. We therefore often\ndescribe the probability that an agent is in a speciﬁc state, s, at time tasPpSt“sq.\n‚Often states and actions are explicitly named, in which case we use the following for-\nmatting: S TATE andaction .\nTransition Probabilities\n‚We use theÑnotation to represent an agent transitioning from one state to another.\nTherefore, the probability of an agent moving from state s1to state s2can be written\nPps1Ñs2q“PpSt`1“s2|St“s1q","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":29,"page_label":"xxix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Notation xxix\n‚Often we condition the probability of an agent transitioning from one state, s1, to another,\ns2, on the agent taking a speciﬁc action, a. We write this\nPps1aÝ Ñs2q“PpSt`1“s2|St“s1,At“aq\n‚The dynamics of an environment in which an agent transitions between states, a Markov\nprocess, can be captured in a transition matrix\nP“»\n————–Pps1Ñs1qPps1Ñs2q... Pps1Ñsnq\nPps2Ñs1qPps2Ñs2q... Pps2Ñsnq\n............\nPpsnÑs1qPpsnÑs2q... PpsnÑsnqﬁ\nﬃﬃﬃﬃﬂ\n‚When agent decisions are allowed, leading to a Markov decision process (MDP), then\nthe dynamics of an environment can be captured in a set of transition matrices, one for\neach action. For example\nPa“»\n—–Pps1aÝ Ñs1qPps1aÝ Ñs2q... Pps1aÝ Ñsnq\nPps2aÝ Ñs1qPps2aÝ Ñs2q... Pps2aÝ Ñsnq\n............\nPpsnaÝ Ñs1qPpsnaÝ Ñs2q... PpsnaÝ Ñsnqﬁ\nﬃﬂ","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":31,"page_label":"xxxi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures\n0.1 How the notation used in the book relates to the elements of a dataset. xxiii\n1.1 Predictive data analytics moving from data toinsight todecision . 4\n1.2 The two steps in supervised machine learning: (a) learning and (b)\npredicting. 5\n1.3 (a)–(d) Striking a balance between overﬁtting and underﬁtting in trying to\npredict income from age. 15\n1.4 A diagram of the CRISP-DM process that shows the six key phases and\nindicates the important relationships between them. 16\n2.1 The different data sources typically combined to create an analytics base\ntable. 29\n2.2 The hierarchical relationship between an analytics solution, domain concepts,\nand descriptive features. 30\n2.3 Example domain concepts for a motor insurance fraud prediction analytics\nsolution. 32\n2.4 Sample descriptive feature data illustrating numeric, binary, ordinal, interval,\ncategorical, and textual types. 35\n2.5 Modeling points in time using an observation period and an outcome\nperiod. 38\n2.6 Observation and outcome periods deﬁned by an event rather than by a ﬁxed\npoint in time (each line represents a prediction subject, and stars signify\nevents). (a) shows the actual data, and (b) shows the event-aligned data. 39\n2.7 Modeling points in time for a scenario with no real outcome period (each line\nrepresents a customer, and stars signify events). (a) shows the actual data, and\n(b) shows the event-aligned data. 39\n2.8 Modeling points in time for a scenario with no real observation period (each\nline represents a customer, and stars signify events). (a) shows the actual data,\nand (b) shows the event-aligned data. 40","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":32,"page_label":"xxxii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxxii List of Figures\n2.9 A subset of the domain concepts and related features for a motor insurance\nfraud prediction analytics solution. 43\n2.10 A subset of the domain concepts and related features for a motor insurance\nfraud prediction analytics solution. 44\n2.11 A subset of the domain concepts and related features for a motor insurance\nfraud prediction analytics solution. 45\n2.12 A summary of the tasks in the Business Understanding, Data Understanding,\nand Data Preparation phases of the CRISP-DM process. 47\n3.1 Visualizations of the continuous and categorical features in the motor\ninsurance claims fraud detection ABT in Table 3.2[56]. 58\n3.2 Histograms for six different sets of data, each of which exhibit well-known,\ncommon characteristics. 60\n3.3 (a) Three normal distributions with different means but identical standard\ndeviations; and (b) three normal distributions with identical means but\ndifferent standard deviations. 62\n3.4 An illustration of the 68´95´99.7rule. The gray region deﬁnes the area where\n95% of values in a sample are expected. 63\n3.5 Example scatter plots for pairs of features from the dataset in Table 3.7[73],\nshowing (a) the strong positive covariance between HEIGHT and WEIGHT ; (b)\nthe strong negative covariance between SPONSORSHIP EARNINGS and AGE; and\n(c) the lack of strong covariance between HEIGHT and AGE. 74\n3.6 A scatter plot matrix showing scatter plots of the continuous features from the\nprofessional basketball team dataset in Table 3.7[73]. 75\n3.7 Examples of using small multiple bar plot visualizations to illustrate the\nrelationship between two categorical features: (a) the CAREER STAGE and\nSHOE SPONSOR features; and (b) the POSITION and SHOE SPONSOR features.\nAll data comes from Table 3.7[73]. 76\n3.8 Examples of using stacked bar plot visualizations to illustrate the relationship\nbetween two categorical features: (a) CAREER STAGE and SHOE SPONSOR\nfeatures; and (b) POSITION and SHOE SPONSOR features, all from Table\n3.7[73]. 78\n3.9 Example of using small multiple histograms to visualize the relationship\nbetween a categorical feature and a continuous feature. All examples use data\nfrom the professional basketball team dataset in Table 3.7[73]: (a) a histogram\nof the AGEfeature; (b) a histogram of the HEIGHT feature; (c) histograms of\ntheAGEfeature for instances displaying each level of the POSITION feature;\nand (d) histograms of the HEIGHT feature for instances displaying each level\nof the POSITION feature. 79","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":33,"page_label":"xxxiii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xxxiii\n3.10 Using box plots to visualize the relationships between categorical and\ncontinuous features from Table 3.7[73]: (a) and (b) show the relationship\nbetween the POSITION feature and the AGEfeature; and (c) and (d) show the\nrelationship between the POSITION feature and the HEIGHT feature. 80\n3.11 A scatter plot matrix showing scatter plots of the continuous features from\nthe professional basketball team dataset in Table 3.7[73]with correlation\ncoefﬁcients included. 85\n3.12 Anscombe’s quartet. For all four samples, the correlation measure returns the\nsame value ( 0.816) even though the relationship between the features is very\ndifferent in each case. 86\n3.13 The effect of using different numbers of bins when using binning to convert a\ncontinuous feature into a categorical feature. 90\n3.14 (a)–(c) Equal-frequency binning of normally distributed data with different\nnumbers of bins; and (d)–(f) the same data binned into the same number of\nbins using equal-width binning. The dashed lines illustrate the distribution\nof the original continuous feature values, and the gray boxes represent the\nbins. 92\n4.1 Cards showing character faces and names for the Guess Who game. 118\n4.2 The different question sequences that can follow in a game of Guess Who\nbeginning with the question Does the person wear glasses? 119\n4.3 The different question sequences that can follow in a game of Guess Who\nbeginning with the question Is it a man? 120\n4.4 Two decision trees, (a) and (b), that are consistent with the instances in the\nspam dataset; and (c) the path taken through the tree shown in (a) to make\na prediction for the query instance SUSPICIOUS WORDS =true,UNKNOWN\nSENDER =true, and CONTAINS IMAGES =true. 122\n4.5 The entropy of different sets of playing cards measured in bits. 124\n4.6 (a) A graph illustrating how the value of a binary log (the log to the base 2)\nof a probability changes across the range of probability values; and (b) the\nimpact of multiplying these values by ´1. 125\n4.7 How the instances in the spam dataset split when we partition using each of\nthe different descriptive features from the spam dataset in Table 4.2[121]. 128\n4.8 The decision tree after the data has been split using ELEVATION . 138\n4.9 The state of the decision tree after the D7partition has been split using\nSTREAM . 139\n4.10 The state of the decision tree after the D8partition has been split using\nSLOPE . 140\n4.11 The ﬁnal vegetation classiﬁcation decision tree. 141","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":34,"page_label":"xxxiv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxxiv List of Figures\n4.12 The vegetation classiﬁcation decision tree generated using information gain\nratio. 144\n4.13 The vegetation classiﬁcation decision tree after the dataset has been split using\nELEVATIONě4,175. 149\n4.14 The decision tree that would be generated for the vegetation classiﬁcation\ndataset listed in Table 4.9[147]using information gain. 150\n4.15 (a) A set of instances on a continuous number line; (b), (c), and (d) depict\nsome of the potential groupings that could be applied to these instances. 151\n4.16 The decision tree resulting from splitting the data in Table 4.11[152]using the\nfeature SEASON . 153\n4.17 The ﬁnal decision tree induced from the dataset in Table 4.11[152]. To illustrate\nhow the tree generates predictions, this tree lists the instances that ended up at\neach leaf node and the prediction (PRED.) made by each leaf node. 154\n4.18 The decision tree for the post-operative patient routing task. 156\n4.19 The iterations of reduced error pruning for the decision tree in Figure 4.18[156]\nusing the validation set in Table 4.13[157]. The subtree that is being considered\nfor pruning in each iteration is highlighted in black. The prediction returned\nby each non-leaf node is listed in square brackets. The error rate for each node\nis given in parantheses. 158\n4.20 The process of creating a model ensemble using bagging and subspace\nsampling. 160\n4.21 (a) A plot of the bike rental dataset from Table 4.14[162]. (b) An illustration\nof the ﬁnal ensemble model trained using the boosting algorithm. (c)–(e) A\nrepresentation of the changing weights used to generate sample datasets for\nthe ﬁrst iterations of the boosting process. 163\n4.22 (a) A plot of the bike rental dataset from Table 4.15[166]. (b)–(e) Visualizations\nof the prediction models trained in the early iterations of the gradient boosting\nprocess. (f) The ﬁnal ensemble model trained after 20 iterations of gradient\nboosting. 167\n5.1 A feature space plot of the college athlete data in Table 5.2[183]. 184\n5.2 (a) A generalized illustration of the Manhattan and Euclidean distances\nbetween two points; and (b) a plot of the Manhattan and Euclidean distances\nbetween instances d12andd5, and between d12andd17from Table 5.2[183]. 186\n5.3 A feature space plot of the data in Table 5.2[183], with the position in the feature\nspace of the query represented by the ? marker. 188\n5.4 (a) The V oronoi tessellation of the feature space for the dataset in Table","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":34,"page_label":"xxxiv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of the prediction models trained in the early iterations of the gradient boosting\nprocess. (f) The ﬁnal ensemble model trained after 20 iterations of gradient\nboosting. 167\n5.1 A feature space plot of the college athlete data in Table 5.2[183]. 184\n5.2 (a) A generalized illustration of the Manhattan and Euclidean distances\nbetween two points; and (b) a plot of the Manhattan and Euclidean distances\nbetween instances d12andd5, and between d12andd17from Table 5.2[183]. 186\n5.3 A feature space plot of the data in Table 5.2[183], with the position in the feature\nspace of the query represented by the ? marker. 188\n5.4 (a) The V oronoi tessellation of the feature space for the dataset in Table\n5.2[183], with the position of the query represented by the ? marker; and (b) the\ndecision boundary created by aggregating the neighboring V oronoi regions\nthat belong to the same target level. 190","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":35,"page_label":"xxxv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xxxv\n5.5 (a) The V oronoi tessellation of the feature space when the dataset has been\nupdated to include the query instance; and (b) the updated decision boundary\nreﬂecting the addition of the query instance in the training set. 192\n5.6 The decision boundary using majority vote of the nearest 3 and 5\ninstances. 193\n5.7 (a) The decision boundary using majority vote of the nearest 15 neighbors;\nand (b) the weighted knearest neighbor model decision boundary (with\nk“21). 194\n5.8 (a) The k-dtree generated for the dataset in Table 5.4[191]after the initial split;\n(b) the partitioning of the feature space by the k-dtree in (a); (c) the k-d\ntree after the dataset at the left child of the root has been split; and (d) the\npartitioning of the feature space by the k-dtree in (c). 198\n5.9 (a) The ﬁnal k-dtree generated for the dataset in Table 5.4[191]; and (b) the\npartitioning of the feature space deﬁned by this k-dtree. 199\n5.10 (a) The path taken from the root node to a leaf node when we search the tree\nwith a query; and (b) the ? marks the location of the query, and the dashed\ncircle plots the extent of the target. 201\n5.11 (a) The target hypersphere after instance d21has been stored as best, and\nbest-distance has been updated; and (b) the extent of the search process. 203\n5.12 (a) The feature space deﬁned by the SALARY and AGEfeatures in Table 5.5[204];\nand (b) the normalized SALARY andAGEfeature space based on the normalized\ndata in Table 5.7[208]. 205\n5.13 The AGEand RATING feature space for the whiskey dataset. The location\nof the query instance is indicated by the ? symbol. The circle plotted with\na dashed line demarcates the border of the neighborhood around the query\nwhen k“3. The three nearest neighbors to the query are labeled with their ID\nvalues. 211\n5.14 (a) Theθrepresents the inner angle between the vector emanating from the\norigin to instance d1and the vector emanating from the origin to instance d2;\nand (b) shows d1andd2normalized to the unit circle. 218\n5.15 Scatter plots of three bivariate datasets with the same center point Aand two\nqueries BandCboth equidistant from A; (a) a dataset uniformly spread around\nthe center point; (b) a dataset with negative covariance; and (c) a dataset with\npositive covariance. 219\n5.16 The coordinate systems deﬁned by the Mahalanobis distance using the\ncovariance matrix for the dataset in Figure 5.15(c)[219]using three different","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":35,"page_label":"xxxv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"when k“3. The three nearest neighbors to the query are labeled with their ID\nvalues. 211\n5.14 (a) Theθrepresents the inner angle between the vector emanating from the\norigin to instance d1and the vector emanating from the origin to instance d2;\nand (b) shows d1andd2normalized to the unit circle. 218\n5.15 Scatter plots of three bivariate datasets with the same center point Aand two\nqueries BandCboth equidistant from A; (a) a dataset uniformly spread around\nthe center point; (b) a dataset with negative covariance; and (c) a dataset with\npositive covariance. 219\n5.16 The coordinate systems deﬁned by the Mahalanobis distance using the\ncovariance matrix for the dataset in Figure 5.15(c)[219]using three different\norigins: (a)p50,50q; (b)p63,71q; and (c)p42,35q. The ellipses in each ﬁgure\nplot the 1,3, and 5unit distance contours. 221","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":36,"page_label":"xxxvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxxvi List of Figures\n5.17 The effect of using a Mahalanobis versus Euclidean distance. Amarks the\ncentral tendency of the dataset in Figure 5.15(c)[219]. The ellipses plot the\nMahalanobis distance contours from AthatBandClie on. In Euclidean terms,\nBandCare equidistant from A; however, using the Mahalanobis distance, Cis\nmuch closer to Athan B. 222\n5.18 A set of scatter plots illustrating the curse of dimensionality. 226\n5.19 Feature subset space for a dataset with three features X,Y, and Z. 228\n5.20 The process of model induction with feature selection. 230\n5.21 A duck-billed platypus. 234\n6.1 A game of ﬁnd the lady : (a) the cards used; (b) the cards dealt facedown on a\ntable; (c) the initial likelihoods of the queen ending up in each position; and\n(d) a revised set of likelihoods for the position of the queen based on evidence\ncollected. 244\n6.2 (a) The set of cards after the wind blows over the one on the right; (b) the\nrevised likelihoods for the position of the queen based on this new evidence;\nand (c) the ﬁnal positions of the cards in the game. 245\n6.3 Plots of some well-known probability distributions. 270\n6.4 Histograms of two unimodal datasets: (a) the distribution has light tails; and\n(b) the distribution has fat tails. 273\n6.5 Illustration of the robustness of the student- tdistribution to outliers: (a) a\ndensity histogram of a unimodal dataset overlaid with the density curves of a\nnormal and a student- tdistribution that have been ﬁtted to the data; and (b) a\ndensity histogram of the same dataset with outliers added, overlaid with the\ndensity curves of a normal and a student- tdistribution that have been ﬁtted to\nthe data. 273\n6.6 Illustration of how a mixture of Gaussians model is composed of a number of\nnormal distributions. The curve plotted using a solid line is the mixture of\nGaussians density curve, created using an appropriately weighted summation\nof the three normal curves, plotted using dashed and dotted lines. 275\n6.7 (a) The area under a density curve between the limits x´ϵ\n2and x`ϵ\n2; (b)\nthe approximation of this area computed by PDFpxqˆϵ; and (c) the error in\nthe approximation is equal to the difference between area A, the area under\nthe curve omitted from the approximation, and area B, the area above the\ncurve erroneously included in the approximation. Both of these areas will get\nsmaller as the width of the interval gets smaller, resulting in a smaller error in\nthe approximation. 277","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":36,"page_label":"xxxvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"normal distributions. The curve plotted using a solid line is the mixture of\nGaussians density curve, created using an appropriately weighted summation\nof the three normal curves, plotted using dashed and dotted lines. 275\n6.7 (a) The area under a density curve between the limits x´ϵ\n2and x`ϵ\n2; (b)\nthe approximation of this area computed by PDFpxqˆϵ; and (c) the error in\nthe approximation is equal to the difference between area A, the area under\nthe curve omitted from the approximation, and area B, the area above the\ncurve erroneously included in the approximation. Both of these areas will get\nsmaller as the width of the interval gets smaller, resulting in a smaller error in\nthe approximation. 277\n6.8 Histograms, using a bin size of 250 units, and density curves for the ACCOUNT\nBALANCE feature: (a) the fraudulent instances overlaid with a ﬁtted exponential\ndistribution; and (b) the non-fraudulent instances overlaid with a ﬁtted normal\ndistribution. 279","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":37,"page_label":"xxxvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xxxvii\n6.9 (a) A Bayesian network for a domain consisting of two binary features. The\nstructure of the network states that the value of feature A directly inﬂuences\nthe value of feature B. (b) A Bayesian network consisting of four binary\nfeatures with a path containing three generations of nodes: D, C, and B. 287\n6.10 A depiction of the Markov blanket of a node. The gray nodes deﬁne\nthe Markov blanket of the black node. The black node is conditionally\nindependent of the white nodes given the state of the gray nodes. 288\n6.11 (a) A Bayesian network representation of the conditional independence\nasserted by a naive Bayes model between the descriptive features given\nknowledge of the target feature; and (b) a Bayesian network representation\nof the conditional independence assumption for the naive Bayes model in the\nfraud example. 290\n6.12 Two different Bayesian networks, each deﬁning the same full joint probability\ndistribution. 291\n6.13 A Bayesian network that encodes the causal relationships between the features\nin the corruption domain. The CPT entries have been calculated using the\nbinned data from Table 6.18[295]. 296\n7.1 (a) A scatter plot of the SIZEandRENTAL PRICE features from the ofﬁce rentals\ndataset; and (b) the scatter plot from (a) with a linear model relating RENTAL\nPRICE toSIZEoverlaid. 314\n7.2 (a) A scatter plot of the SIZEandRENTAL PRICE features from the ofﬁce rentals\ndataset. A collection of possible simple linear regression models capturing the\nrelationship between these two features are also shown. (b) A scatter plot of\ntheSIZEand RENTAL PRICE features from the ofﬁce rentals dataset showing a\ncandidate prediction model and the resulting errors. 316\n7.3 (a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error\nsurface generated by plotting the sum of squared errors for the ofﬁce rentals\ntraining set for each possible combination of values for wr0s(from the range\nr´10,20s) and wr1s(from the range r´2,3s). 318\n7.4 (a) A 3D plot of an error surface and (b) a bird’s-eye view contour plot of\nthe same error surface. The lines indicate the path that the gradient descent\nalgorithm would take across this error surface from four different starting\npositions to the global minimum—marked as the white dot in the center. 323\n7.5 (a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error\nsurface for the ofﬁce rentals dataset showing the path that the gradient descent","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":37,"page_label":"xxxvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 (a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error\nsurface generated by plotting the sum of squared errors for the ofﬁce rentals\ntraining set for each possible combination of values for wr0s(from the range\nr´10,20s) and wr1s(from the range r´2,3s). 318\n7.4 (a) A 3D plot of an error surface and (b) a bird’s-eye view contour plot of\nthe same error surface. The lines indicate the path that the gradient descent\nalgorithm would take across this error surface from four different starting\npositions to the global minimum—marked as the white dot in the center. 323\n7.5 (a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error\nsurface for the ofﬁce rentals dataset showing the path that the gradient descent\nalgorithm takes toward the best-ﬁt model. 324\n7.6 A selection of the simple linear regression models developed during the\ngradient descent process for the ofﬁce rentals dataset. The bottom-right\npanel shows the sums of squared errors generated during the gradient descent\nprocess. 325","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":38,"page_label":"xxxviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xxxviii List of Figures\n7.7 Plots of the journeys made across the error surface for the simple ofﬁce rentals\nprediction problem for different learning rates: (a) a very small learning rate\n(0.002); (b) a medium learning rate ( 0.08); and (c) a very large learning rate\n(0.18). The changing sum of squared errors are also shown. 329\n7.8 (a) The journey across the error surface for the ofﬁce rentals prediction\nproblem when learning rate decay is used ( α0“0.18,c“10); and (b) a plot\nof the changing sum of squared errors during this journey. 335\n7.9 (a) The journey across the error surface for the ofﬁce rentals prediction\nproblem when learning rate decay is used ( α0“0.25,c“100); (b) a plot of\nthe changing sum of squared errors during this journey. 336\n7.10 (a) A scatter plot of the RPM and VIBRATION descriptive features from\nthe generators dataset shown in Table 7.6[339], where good generators are\nshown as crosses, and faulty generators are shown as triangles; and (b) as\ndecision boundary separating good generators (crosses) from faulty generators\n(triangles). 340\n7.11 (a) A surface showing the value of Equation (7.23)[339]for all values of RPM\nand VIBRATION , with the decision boundary given in Equation (7.23)[339]\nhighlighted; and (b) the same surface linearly thresholded at zero to operate as\na predictor. 341\n7.12 (a) A plot of the logistic function (Equation (7.25)[342]) for the range of values\nr´10,10s; and (b) the logistic decision surface that results from training a\nmodel to represent the generators dataset given in Table 7.6[339](note that the\ndata has been normalized to the range r´1,1s). 343\n7.13 A selection of the logistic regression models developed during the gradient\ndescent process for the machinery dataset from Table 7.6[339]. The bottom-right\npanel shows the sums of squared errors generated during the gradient descent\nprocess. 344\n7.14 A scatter plot of the extended generators dataset given in Table 7.7[347],\nwhich results in instances with the different target levels overlapping each\nother. Instances representing good generators are shown as crosses, and those\nrepresenting faulty generators as triangles. 349\n7.15 A selection of the logistic regression models developed during the gradient\ndescent process for the extended generators dataset in Table 7.7[347]. The\nbottom-right panel shows the sums of squared errors generated during the\ngradient descent process. 350\n7.16 (a) A scatter plot of the RAINand GROWTH feature from the grass growth","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":38,"page_label":"xxxviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"panel shows the sums of squared errors generated during the gradient descent\nprocess. 344\n7.14 A scatter plot of the extended generators dataset given in Table 7.7[347],\nwhich results in instances with the different target levels overlapping each\nother. Instances representing good generators are shown as crosses, and those\nrepresenting faulty generators as triangles. 349\n7.15 A selection of the logistic regression models developed during the gradient\ndescent process for the extended generators dataset in Table 7.7[347]. The\nbottom-right panel shows the sums of squared errors generated during the\ngradient descent process. 350\n7.16 (a) A scatter plot of the RAINand GROWTH feature from the grass growth\ndataset; and (b) the same plot with a simple linear regression model trained to\ncapture the relationship between the grass growth and rainfall. 352\n7.17 A selection of the models developed during the gradient descent process for\nthe grass growth dataset from Table 7.9[351]. 354","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":39,"page_label":"xxxix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xxxix\n7.18 A scatter plot of the P20and P45features from the EEG dataset. Instances\nrepresenting positive images are shown as crosses, and those representing\nnegative images as triangles. 355\n7.19 A selection of the models developed during the gradient descent process\nfor the EEG dataset from Table 7.10[355]. The ﬁnal panel shows the decision\nsurface generated. 356\n7.20 An illustration of three different one-versus-all prediction models for the\ncustomer type dataset in Table 7.11[359], with three target levels: (a) single\n(squares), (b) business (triangles), and (c) family (crosses). 358\n7.21 A selection of the models developed during the gradient descent process for\nthe customer group dataset from Table 7.11[359]. Squares represent instances\nwith the single target level, triangles the business level, and crosses the family\nlevel. The bottom-right panel illustrates the overall decision boundaries\nbetween the three target levels. 360\n7.22 A small sample of the generators dataset with two features, RPM and\nVIBRATION , and two target levels, good (shown as crosses) and faulty (shown\nas triangles): (a) a decision boundary with a very small margin; and (b) a\ndecision boundary with a much larger margin. In both cases, the instances\nalong the margins are highlighted. 361\n7.23 Different margins that satisfy the constraint in Equation (7.44)[364], the instances\nthat deﬁne the margin are highlighted in each case; (b) shows the maximum\nmargin and also shows two query instances represented as black dots. 364\n7.24 (a) The journey across an error surface; and (b) the changing sums of squared\nerrors during this journey. 368\n8.1 A high-level schematic of the structure of a neuron. 383\n8.2 Plots for activation functions that have been popular in the history of neural\nnetworks. 387\n8.3 A schematic of an artiﬁcial neuron. 388\n8.4 A schematic of a feedforward artiﬁcial neural network. 390\n8.5 An illustration of the correspondence between graphical and matrix\nrepresentations of a neural network. 392\n8.6 An illustration of how a batch of examples can be processed in parallel using\nmatrix operations. 393\n8.7 A single-layer network. 397\n8.8 The logical AND and ORfunctions are linearly separable, but the XOR is\nnot. 398\n8.9 (left) The XOR function implemented as a two-layer neural network. (right)\nThe network processing the four possible input combinations. 399","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":40,"page_label":"xl","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xl List of Figures\n8.10 An illustration of how the representational capacity of a network increases as\nmore layers are added to the network. 402\n8.11 The calculation of the zvalues and activations of each neuron during the\nforward pass of the backpropagation algorithm. 406\n8.12 The backpropagation of the δvalues during the backward pass of the\nbackpropagation algorithm. 407\n8.13 Plots of the logistic function and its derivative. 410\n8.14 The forward pass of the examples listed in Table 8.3[423]through the network\nin Figure 8.4[390]. 425\n8.15 An illustration of the forward propagation of d2through the network showing\nthe weights on each connection, and the weighted sum zand activation avalue\nfor each neuron in the network. 427\n8.16 Theδs for each of the neurons in the network for Example 2. 430\n8.17 A plot showing how the sum of squared errors of the network changed during\ntraining. 434\n8.18 The forward pass of the examples listed in Table 8.3[423]through the network\nin Figure 8.4[390]when all the neurons are ReLUs. 440\n8.19 An illustration of the forward propagation of d2through the ReLU network\nshowing the weights on each connection, and the weighted sum zand\nactivation avalue for each neuron in the network. 441\n8.20 A plot showing how the sum of squared errors of the ReLU network changed\nduring training when α“0.2. 443\n8.21 A plot showing how the sum of squared errors of the ReLU network changed\nduring training when α“0.1. 444\n8.22 The architecture of the neural network used in the weight initialization\nexperiments. Note that the neurons in this network use a linear activation\nfunction: ai“zi. 450\n8.23 The internal dynamics of the network in Figure 8.22[450]during the ﬁrst training\niteration when the weights were initialized using a normal distribution with\nµ“0.0,σ“0.01. 453\n8.24 The internal dynamics of the network in Figure 8.22[450]during the ﬁrst training\niteration when the weights were initialized using a normal distribution with\nµ“0.0andσ“0.2. 454\n8.25 The internal dynamics of the network in Figure 8.22[450]during the ﬁrst training\niteration when the weights were initialized using Xavier initialization. 460\n8.26 The internal dynamics of the network in Figure 8.22[450], using ReLUs,\nduring the ﬁrst training iteration when the weights were initialized using He\ninitialization. 462","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":41,"page_label":"xli","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xli\n8.27 A schematic of a feedforward artiﬁcial neural network with a three-neuron\nsoftmax output layer. 465\n8.28 The forward pass of the mini-batch of examples listed in Table 8.13[464]through\nthe network in Figure 8.27[465]. 470\n8.29 An illustration of how different small networks are generated for different\ntraining examples by applying dropout to the original large network. 475\n8.30 Samples of the handwritten digit images from the MNIST dataset. 478\n8.31 A 6-by-6 matrix representation of a grayscale image of a 4, and a neuron with\na receptive ﬁeld that covers the top-left corner of the image. 480\n8.32 A 6-by-6 matrix representation of a grayscale image of a 4, and a neuron with\na different receptive ﬁeld from the neuron in Figure 8.31[480]. 481\n8.33 Illustration of the organization of a set of neurons that share weights (use the\nsame ﬁlter) and their local receptive ﬁelds such that together the receptive\nﬁelds cover the entirety of the input image. 484\n8.34 A grayscale image of a 4 after padding has been applied to the original 6-by-6\nmatrix representation, and the local receptive ﬁeld of a neuron that includes\nboth valid and padded pixels. 488\n8.35 Schematic of the typical sequences of layers found in a convolutional neural\nnetwork. 497\n8.36 Worked example illustrating the dataﬂow through a multilayer, multiﬁlter\nCNN. 498\n8.37 Schematic of the simple recurrent neural architecture. 502\n8.38 A simple RNN model unrolled through time (in this instance, three\ntime-steps). 504\n8.39 An illustration of the different iterations of backpropagation during\nbackpropagation through time. 506\n8.40 A schematic of the internal structure of a long short-term memory unit. 509\n8.41 The ﬂow of activations through a long short-term memory unit during forward\npropagation when ct´1“r0.3,0.6s,ht“r0.1,0.8s, and xt“r0.9s. 514\n8.42 The ﬂow of error gradients through a long short-term memory unit during\nbackpropagation. 516\n9.1 The process of building and evaluating a model using a hold-out test set. 536\n9.2 Hold-out sampling can divide the full data into training, validation, and test\nsets. 541\n9.3 Using a validation set to avoid overﬁtting in iterative machine learning\nalgorithms. 542\n9.4 The division of data during the k-fold cross validation process. Black\nrectangles indicate test data, and white spaces indicate training data. 543","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":42,"page_label":"xlii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xlii List of Figures\n9.5 The division of data during the leave-one-out cross validation process. Black\nrectangles indicate instances in the test set, and white spaces indicate training\ndata. 546\n9.6 The division of data during the ϵ0bootstrap process. Black rectangles indicate\ntest data, and white spaces indicate training data. 547\n9.7 The out-of-time sampling process. 547\n9.8 Surfaces generated by calculating (a) the arithmetic mean and (b) the harmonic\nmean of all combinations of features A and B that range from 0to100. 553\n9.9 Prediction score distributions for two different prediction models. The\ndistributions in (a) are much better separated than those in (b). 557\n9.10 Prediction score distributions for the (a) spam and (b) hamtarget levels based\non the data in Table 9.11[557]. 558\n9.11 (a) The changing values of TPR and TNR for the test data shown in Table\n9.13[560]as the threshold is altered; and (b) points in ROC space for thresholds\nof0.25,0.5, and 0.75. 561\n9.12 (a) A complete ROC curve for the email classiﬁcation example; and (b) a\nselection of ROC curves for different models trained on the same prediction\ntask. 562\n9.13 The K-S chart for the email classiﬁcation predictions shown in Table\n9.11[557]. 564\n9.14 A series of charts for different model performance on the same large email\nclassiﬁcation test set used to generate the ROC curves in Figure 9.12(b)[562].\nEach column from top to bottom: a histogram of the hamscores predicted by\nthe model, a histogram of the spam scores predicted by the model, and the K-S\nchart. 566\n9.15 The (a) gain and (b) cumulative gain at each decile for the email predictions\ngiven in Table 9.11[557]. 569\n9.16 The (a) lift and (b) cumulative lift at each decile for the email predictions\ngiven in Table 9.11[557]. 570\n9.17 Cumulative gain, lift, and cumulative lift charts for four different models for\nthe extended email classiﬁcation test set. 571\n9.18 The distributions of predictions made by a model trained for the bacterial\nspecies identiﬁcation problem for (a) the original evaluation test set, and for\n(b) and (c) two periods of time after model deployment; (d) shows how the\nstability index can be tracked over time to monitor for concept drift. 582\n10.1 The three different arrangements of the magnetic letters made by the Murphy\nchildren on the Murphy family refrigerator. 598\n10.2 Unsupervised machine learning as a single-step process. 599","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":43,"page_label":"xliii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xliii\n10.3 (a) A plot of the mobile phone customer dataset given in Table 10.1[604]. (b)–(f)\nThe progress of the k-means clustering algorithm, working on the simple\ncustomer segmentation dataset. The large symbols represent cluster centroids,\nand the smaller symbols represent cluster assignments. 602\n10.4 (a)–(h) Different clusterings (all with k“3) that can be found for the mobile\nphone customer dataset given in Table 10.1[604]when different initial cluster\ncentroids are used. 606\n10.5 (a)–(d) Initial centroids chosen using the k-means++ approach (all with k“3)\nfor the mobile phone customer dataset given in Table 10.1[604]. 607\n10.6 (a) Intra-cluster distance; (b) inter-cluster distance; (c) a good clustering; and\n(d) a badclustering. 608\n10.7 The silhouette plot for the ﬁnal clustering of the mobile phone customer\ndataset (Table 10.1[604]) found using the k-means algorithm (with k“3). 612\n10.8 (a)–(f) Different clusterings found for the mobile phone customer dataset\nin Table 10.1[604]for values of kinp2,9q. (g) shows the silhouette for each\nclustering. 613\n10.9 (a)–(b) Visualizations of the distributions of the descriptive features in the\nmobile phone customer dataset in Table 10.1[604]across the complete dataset,\nand divided by the clustering found using k-means clustering ( k“3). 615\n10.10 (a)–(i) A plot of the blobs ,circles , and half-moons datasets and the clusterings\nachieved by the k-means clustering and agglomerative hierarchical clustering\nalgorithms (where kis set to 3,2, and 2, respectively). 617\n10.11 (a)–(d) Different linkage methods that can be used to compare the distances\nbetween clusters in agglomerative hierarchical clustering. (Arrows for only\nsome indicative distances are shown in the average linkage diagram (d).) 619\n10.12 (a) A plot of a reduced version of the mobile phone customer dataset given\nin Table 10.1[604]. (b)–(d) show details of several iterations of the AHC\nalgorithm. 621\n10.13 (a) A plot of the hierarchical grouping of the instances in the mobile phone\ncustomer dataset from Table 10.1[604]found by the AHC algorithm (using\nEuclidean distance and single linkage). (b) The clustering returned when\nthe tree is cut at k“3. (c) The clustering returned when the tree is cut at\nk“6. 623\n10.14 The architecture of an auto-encoder network made up of an encoder and a\ndecoder connected by a bottleneck layer. 625\n10.15 (a) A selection of images from the handwritten digits dataset; (b)–(d) image","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":43,"page_label":"xliii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.12 (a) A plot of a reduced version of the mobile phone customer dataset given\nin Table 10.1[604]. (b)–(d) show details of several iterations of the AHC\nalgorithm. 621\n10.13 (a) A plot of the hierarchical grouping of the instances in the mobile phone\ncustomer dataset from Table 10.1[604]found by the AHC algorithm (using\nEuclidean distance and single linkage). (b) The clustering returned when\nthe tree is cut at k“3. (c) The clustering returned when the tree is cut at\nk“6. 623\n10.14 The architecture of an auto-encoder network made up of an encoder and a\ndecoder connected by a bottleneck layer. 625\n10.15 (a) A selection of images from the handwritten digits dataset; (b)–(d) image\nreconstructions generated by the auto-encoder network after 0,10, and 1,000\ntraining epochs. 626","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":44,"page_label":"xliv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xliv List of Figures\n10.16 An image of the digit 2and reconstructions of this image by the auto-\nencoder after various amounts of network training. The pixel values of the\nreconstructed images are shown alongside the images, as is the reconstruction\nerror calculated by comparing these to the pixel values of the original\nimage. 627\n10.17 The process of using an unsupervised auto-encoder network to generate a\nfeature representation used to train a supervised model. 628\n11.1 An agent behaving in an environment and the observation, reward, action\ncycle. The transition from observations of the environment to a state is shown\nby the state generation function, φ. 639\n11.2 A simple Markov process to model the evolution of an infectious disease in\nindividuals during an epidemic using the SUSCEPTIBLE -INFECTED -RECOVERED\n(S-I-R) model. 644\n11.3 A Markov decision process representation for TwentyTwos, a simpliﬁed\nversion of the card game Blackjack. 648\n11.4 A simple grid world. The start position is annotated with an Sand the goal\nwith a G. The squares marked fdenote ﬁre, which is very damaging to an\nagent. 660\n11.5 (a)–(c) The evolution of the entries in the action-value table over episodes of\nQ-learning off-policy temporal-difference learning across the grid world. (d)\nThe cumulative reward earned from each episode. (e) An illustration of the\ntarget policy learned by the agent after 350episodes. (f) The path the agent\nwill take from the start state to the goal state when greedily following the\ntarget policy. 663\n11.6 (a) A visualization of the ﬁnal action-value table for an agent trained using\nSARSA on-policy temporal-difference learning across the grid world after\n350episodes. (b) The cumulative reward earned from each episode. (c) An\nillustration of the target policy learned by the agent after 350episodes. (d)\nThe path the agent will take from the start state to the goal state when greedily\nfollowing the target policy. 667\n11.7 The Lunar Lander environment. The aim of the game is to control the\nspaceship starting from the top of the world and attempting to land on the\nlanding pad. 669\n11.8 Framing the action-value function as a prediction problem. 669\n11.9 An illustration of the DQN algorithm including experience replay and target\nnetwork freezing. 672\n11.10 (a) Frames from an episode early in the training process in which the agent\nperforms poorly. (b) Frames from an episode near the end of the learning","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":44,"page_label":"xliv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"illustration of the target policy learned by the agent after 350episodes. (d)\nThe path the agent will take from the start state to the goal state when greedily\nfollowing the target policy. 667\n11.7 The Lunar Lander environment. The aim of the game is to control the\nspaceship starting from the top of the world and attempting to land on the\nlanding pad. 669\n11.8 Framing the action-value function as a prediction problem. 669\n11.9 An illustration of the DQN algorithm including experience replay and target\nnetwork freezing. 672\n11.10 (a) Frames from an episode early in the training process in which the agent\nperforms poorly. (b) Frames from an episode near the end of the learning\nprocess where the agent is starting to be very effective. (c) Changing episode\nreturns during DQN training. The gray line shows a 50-episode moving\naverage to better highlight the trend. 675","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":45,"page_label":"xlv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Figures xlv\n12.1 The set of domain concepts for the Acme Telephonica customer churn\nprediction problem. 690\n12.2 (a)–(c) Histograms for the features from the AT ABT with irregular cardinality;\n(d)–(g) histograms for the features from the AT ABT that are potentially\nsuffering from outliers. 695\n12.3 (a) A stacked bar plot for the REGION TYPEfeature; and (b) histograms for the\nAVGOVERBUNDLE MINSfeature by target feature value. 697\n12.4 An unpruned decision tree built for the AT churn prediction problem (shown\nonly to indicate its size and complexity). The excessive complexity and depth\nof the tree are evidence that overﬁtting has probably occurred. 698\n12.5 A pruned decision tree built for the AT churn prediction problem. Gray leaf\nnodes indicate a churn prediction, and clear leaf nodes indicate a non-churn\nprediction. For space reasons, we show only the features tested at the top-level\nnodes. 699\n12.6 (a) Cumulative gain, (b) lift, and (c) cumulative lift charts for the predictions\nmade on the large test data sample. 700\n12.7 A pruned and stunted decision tree built for the Acme Telephonica churn\nprediction problem. 701\n13.1 Examples of the different galaxy morphology categories into which SDSS\nscientists categorize galaxy objects. 705\n13.2 The ﬁrst draft of the domain concepts diagram developed by Jocelyn for the\ngalaxy classiﬁcation task. 708\n13.3 Bar plots of the different galaxy types present in the full SDSS dataset for the\n3-level and 5-level target features. 710\n13.4 The revised domain concepts diagram for the galaxy classiﬁcation task. 712\n13.5 SPLOM diagrams of (a) the EXPRAD; and (b) DEVRADmeasurements from the\nraw SDSS dataset. Each SPLOM shows the measure across the ﬁve different\nphotometric bands captured by the SDSS telescope ( u,g,r,i, and z). 714\n13.6 Histograms of a selection of features from the SDSS dataset. 717\n13.7 Histograms of the EXPRAD Rfeature split by target feature level. 718\n13.8 (a)–(c) Small multiple box plots (split by the target feature) of some of the\nfeatures from the SDSS ABT. 718\n14.1 (a) The class conditional densities for two classes ( l1,l2) with a single\ndescriptive feature d. (b) The class posterior probabilities plotted for each\nclass for different values of d. 734\n14.2 An illustration of the decision boundaries learned by different machine\nlearning algorithms for three artiﬁcial datasets. 737","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":46,"page_label":"xlvi","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xlvi List of Figures\nA.1 The members of a school basketball team. The height of each player is listed\nbelow the player. The dashed gray line shows the arithmetic mean of the\nplayers’ heights. 746\nA.2 The members of the school basketball team from Figure A.1[746]with one very\ntallringer added: (a) the dashed gray line shows the mean of the players’\nheights; and (b) the dashed gray line shows the median of the players’ heights,\nwith the players ordered by height. 746\nA.3 The members of a rival school basketball team. Player heights are listed\nbelow each player. The dashed gray line shows the arithmetic mean of the\nplayers’ heights. 747\nA.4 The members of the rival school basketball team from Figure A.3[747]ordered\nby height. 749\nA.5 Example bar plots for the POSITION feature in Table A.1[750]: (a) frequency bar\nplot, (b) density bar plot, and (c) order density bar plot. 753\nA.6 Bar plot of the continuous TRAINING EXPENSES feature from Table\nA.1[750]. 753\nA.7 (a) and (b) frequency histograms and (c) and (d) density histograms for the\ncontinuous TRAINING EXPENSES feature from Table A.1[750], illustrating how\nusing intervals overcomes the problem seen in Figure A.6[753]and the effect of\nvarying interval sizes. 754\nA.8 (a) The structure of a box plot; and (b) a box plot for the TRAINING EXPENSES\nfeature from the basketball team dataset in Table A.1[750]. 756\nB.1 The sample space for the domain of two dice. 757\nC.1 (a) The speed of a car during a journey along a minor road before joining a\nhighway and ﬁnally coming to a sudden halt; and (b) the acceleration, the\nderivative of speed with respect to time, for this journey. 765\nC.2 Examples of continuous functions (shown as solid lines) and their derivatives\n(shown as dashed lines). 767\nC.3 (a) A continuous function in two variables, xandy; (b) the partial derivative\nof this function with respect to x; and (c) the partial derivative of this function\nwith respect to y. 769","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":47,"page_label":"xlvii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Tables\n0.1 Suggested course syllabi. xix\n1.1 A credit scoring dataset. 6\n1.2 A more complex credit scoring dataset. 8\n1.3 A simple retail dataset. 9\n1.4 Potential prediction models (a) before and (b) after training data becomes\navailable. 10\n1.5 The age-income dataset. 14\n2.1 The basic structure of an analytics base table—descriptive features and a\ntarget feature. 29\n2.2 The ABT for the motor insurance claims fraud detection solution. 46\n3.1 The structures of the tables included in a data quality report to describe (a)\ncontinuous features and (b) categorical features. 55\n3.2 Portions of the ABT for the motor insurance claims fraud detection problem\ndiscussed in Section 2.4.6[42]. 56\n3.3 A data quality report for the motor insurance claims fraud detection ABT\ndisplayed in Table 3.2[56]. 57\n3.4 The structure of a data quality plan. 64\n3.5 The data quality plan for the motor insurance fraud prediction ABT. 68\n3.6 The data quality plan with potential handling strategies for the motor insurance\nfraud prediction ABT. 72\n3.7 The details of a professional basketball team. 73\n3.8 Calculating covariance. 82\n3.9 A small sample of the HEIGHT and SPONSORSHIP EARNINGS features from\nthe professional basketball team dataset in Table 3.7[73], showing the result of\nrange normalization and standardization. 88","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":48,"page_label":"xlviii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"xlviii List of Tables\n4.1 A dataset that represents the characters in the Guess Who game. 118\n4.2 An email spam prediction dataset. 121\n4.3 The vegetation classiﬁcation dataset. 136\n4.4 Partition sets (Part.), entropy, remainder (Rem.), and information gain (Info.\nGain) by feature for the dataset in Table 4.3[136]. 137\n4.5 Partition sets (Part.), entropy, remainder (Rem.), and information gain (Info.\nGain) by feature for the dataset D7in Figure 4.8[138]. 138\n4.6 Partition sets (Part.), entropy, remainder (Rem.), and information gain (Info.\nGain) by feature for the dataset D8in Figure 4.9[139]. 140\n4.7 Partition sets (Part.), entropy, Gini index, remainder (Rem.), and information\ngain (Info. Gain) by feature for the dataset in Table 4.3[136]. 146\n4.8 Dataset for predicting the vegetation in an area with a continuous ELEVATION\nfeature (measured in feet). 147\n4.9 Dataset for predicting the vegetation in an area sorted by the continuous\nELEVATION feature. 147\n4.10 Partition sets (Part.), entropy, remainder (Rem.), and information gain (Info.\nGain) for the candidate ELEVATION thresholds:ě750,ě1,350,ě2,250and\ně4,175. 148\n4.11 A dataset listing the number of bike rentals per day. 152\n4.12 The partitioning of the dataset in Table 4.11[152]based on SEASON and\nWORK DAYfeatures and the computation of the weighted variance for each\npartitioning. 153\n4.13 An example validation set for the post-operative patient routing task. 157\n4.14 A simple bicycle demand predictions dataset and the workings of the ﬁrst\nthree iterations of training an ensemble model using boosting to predict\nRENTALS given TEMP. 162\n4.15 A simple bicycle demand predictions dataset and the workings of the ﬁrst\niterations of training a gradient boosting model. 166\n5.1 Matching animals you remember to the features of the unknown animal\ndescribed by the sailor. 182\n5.2 The SPEED and AGILITY ratings for 20 college athletes and whether they were\ndrafted by a professional team. 183\n5.3 The distances (Dist.) between the query instance with SPEED“6.75and\nAGILITY“3.00and each instance in Table 5.2[183]. 189\n5.4 The extended version of the college athletes dataset. 191\n5.5 A dataset listing salary and age information for customers and whether they\npurchased a product. 204","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":49,"page_label":"xlix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Tables xlix\n5.6 The dataset from Table 5.5[204]with the Euclidean distance between each\ninstance and the query SALARY = 56,000, AGE= 35 when we use both\ntheSALARY and AGEfeatures, just the SALARY feature, and just the AGE\nfeature. 206\n5.7 The updated version of Table 5.6[206]once we have applied range normalization\nto the SALARY and AGEfeatures in the dataset and to the query instance. 208\n5.8 A dataset of whiskeys listing the age (in years), the rating (between 1 and 5,\nwith 5 being the best), and the bottle price of each whiskey. 209\n5.9 The whiskey dataset after the descriptive features have been normalized. 210\n5.10 The calculations for the weighted knearest neighbor prediction. 212\n5.11 A binary dataset listing the behavior of two individuals on a website during a\ntrial period and whether they subsequently signed up for the website. 213\n5.12 The similarity between the current trial user, q, and the two users in the dataset,\nd1andd2, in terms of co-presence (CP), co-absence (CA), presence-absence\n(PA), and absence-presence (AP). 214\n6.1 A simple dataset for a MENINGITIS diagnosis with descriptive features that\ndescribe the presence or absence of three common symptoms of the disease:\nHEADACHE ,FEVER , and VOMITING . 246\n6.2 A dataset from a loan application fraud detection domain. 263\n6.3 The probabilities needed by a naive Bayes prediction model, calculated from\nthe data in Table 6.2[263]. 264\n6.4 The relevant probabilities, from Table 6.3[264], needed by the naive Bayes\nprediction model to make a prediction for a query with CH=paid,GC=none,\nand ACC =rent, and the calculation of the scores for each target level. 265\n6.5 The relevant probabilities, from Table 6.3[264], needed by the naive Bayes\nprediction model to make a prediction for the query with CH=paid,GC=\nguarantor , and ACC =free, and the calculation of the scores for each possible\ntarget level. 266\n6.6 The posterior probability distribution for the GUARANTOR /COAPPLICANT\nfeature under the condition that FRAUD =false. 267\n6.7 Smoothing the posterior probabilities for the GUARANTOR /COAPPLICANT\nfeature conditioned on FRAUD =false. 268\n6.8 The Laplace smoothed (with k“3) probabilities needed by a naive Bayes\nprediction model, calculated from the dataset in Table 6.2[263]. 269\n6.9 The relevant smoothed probabilities, from Table 6.8[269], needed by the naive\nBayes prediction model to make a prediction for the query with CH=paid,GC","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":49,"page_label":"xlix","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"prediction model to make a prediction for the query with CH=paid,GC=\nguarantor , and ACC =free, and the calculation of the scores for each possible\ntarget level. 266\n6.6 The posterior probability distribution for the GUARANTOR /COAPPLICANT\nfeature under the condition that FRAUD =false. 267\n6.7 Smoothing the posterior probabilities for the GUARANTOR /COAPPLICANT\nfeature conditioned on FRAUD =false. 268\n6.8 The Laplace smoothed (with k“3) probabilities needed by a naive Bayes\nprediction model, calculated from the dataset in Table 6.2[263]. 269\n6.9 The relevant smoothed probabilities, from Table 6.8[269], needed by the naive\nBayes prediction model to make a prediction for the query with CH=paid,GC\n=guarantor , and ACC =free, and the calculation of the scores for each target\nlevels. 270\n6.10 Deﬁnitions of some standard probability distributions. 271","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":50,"page_label":"l","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"l List of Tables\n6.11 The dataset from the loan application fraud detection domain (from Table\n6.2[263]) with two continuous descriptive features added: ACCOUNT BALANCE\nand LOAN AMOUNT . 278\n6.12 Partitioning the dataset based on the value of the target feature and ﬁtting\nthe parameters of a statistical distribution to model the ACCOUNT BALANCE\nfeature in each partition. 280\n6.13 The Laplace smoothed (with k“3) probabilities needed by a naive Bayes\nprediction model, calculated from the dataset in Table 6.11[278], extended to\ninclude the conditional probabilities for the new ACCOUNT BALANCE feature,\nwhich are deﬁned in terms of PDFs. 281\n6.14 The probabilities, from Table 6.13[281], needed by the naive Bayes prediction\nmodel to make a prediction for the query with CH=paid,GC=guarantor ,ACC\n=free, and AB=759.07, and the calculation of the scores for each candidate\nprediction. 282\n6.15 The LOAN AMOUNT continuous feature discretized into four equal-frequency\nbins. 283\n6.16 The Laplace smoothed (with k“3) probabilities needed by a naive Bayes\nprediction model, calculated from the data in Tables 6.11[278]and 6.15[283]. 284\n6.17 The relevant smoothed probabilities, from Table 6.16[284], needed by the\nnaive Bayes model to make a prediction for the query with CH=paid,GC=\nguarantor ,ACC =free,AB=759.07, and LA=8,000, and the calculation of the\nscores for each candidate prediction. 285\n6.18 Some socioeconomic data for a set of countries, and a version of the data after\nequal-frequency binning has been applied. 295\n6.19 Examples of the samples generated using Gibbs sampling. 301\n7.1 A dataset that includes ofﬁce rental prices and a number of descriptive features\nfor 10 Dublin city-center ofﬁces. 313\n7.2 Calculating the sum of squared errors for the candidate model (with\nwr0s“6.47andwr1s“0.62) to make predictions for the ofﬁce rentals\ndataset. 317\n7.3 Details of the ﬁrst two iterations when the gradient descent algorithm is used\nto train a multivariable linear regression model for the ofﬁce rentals dataset\n(using only the continuous descriptive features). 331\n7.4 Weights and standard errors for each feature in the ofﬁce rentals model. 333\n7.5 The ofﬁce rentals dataset from Table 7.1[313]adjusted to handle the categorical\nENERGY RATING descriptive feature in linear regression models. 337\n7.6 A dataset listing features for a number of generators. 339\n7.7 An extended version of the generators dataset from Table 7.6[339]. 347","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":51,"page_label":"li","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Tables li\n7.8 Details of the ﬁrst two iterations when the gradient descent algorithm is used\nto train a logistic regression model for the extended generators dataset given\nin Table 7.7[347]. 348\n7.9 A dataset describing grass growth on Irish farms in July 2012. 351\n7.10 A dataset showing participants’ responses to viewing positive and negative\nimages measured on the EEG P20and P45potentials. 355\n7.11 A dataset of customers of a large national retail chain. 359\n8.1 Hourly samples of ambient factors and full load electrical power output of a\ncombined cycle power plant. 422\n8.2 The minimum and maximum values for the AMBIENT TEMPERATURE ,\nRELATIVE HUMIDITY , and ELECTRICAL OUTPUT features in the power plant\ndataset. 423\n8.3 The range-normalized hourly samples of ambient factors and full load electrical\npower output of a combined cycle power plant, rounded to two decimal\nplaces. 423\n8.4 The per example error after the forward pass illustrated in Figure 8.14[425],\nthe per example BE{Ba8, and the sum of squared errors for the model over the\ndataset of four examples. 426\n8.5 TheBa{Bzfor each neuron for Example 2 rounded to four decimal places. 428\n8.6 TheBE{Bwi,kcalculations for d2for every weight in the network. We use the\nneuron index 0 to denote the bias input for each neuron. 431\n8.7 The calculation of ∆w7,5across our four examples. 432\n8.8 The per example error after each weight has been updated once, the per\nexampleBE{Ba8, and the sum of squared errors for the model. 433\n8.9 The per example prediction, error, and the sum of squared errors after training\nhas converged to an S S Eă0.0001. 433\n8.10 The per example error of the ReLU network after the forward pass illustrated\nin Figure 8.18[440], the per example BE{Ba8, and the sum of squared errors for\nthe ReLU model. 441\n8.11 TheBa{Bzfor each neuron for d2rounded to four decimal places. 442\n8.12 The ReLU network’s per example prediction, error, and the sum of squared\nerrors after training has converged to an S S Eă0.0001. 443\n8.13 The range-normalized hourly samples of ambient factors and full load electrical\npower output of a combined cycle power plant, rounded to two decimal places,\nand with the (binned) target feature represented using one-hot encoding. 464\n8.14 The calculation of the softmax activation function ϕsmover a vector of three\nlogits l. 464","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":52,"page_label":"lii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"lii List of Tables\n8.15 The calculation of the softmax activations for each of the neurons in the output\nlayer for each example in the mini-batch, and the calculation of the δfor each\nneuron in the output layer for each example in the mini-batch. 471\n9.1 A sample test set with model predictions. 537\n9.2 The structure of a confusion matrix. 538\n9.3 A confusion matrix for the set of predictions shown in Table 9.1[537]. 539\n9.4 The performance measures from the ﬁve individual evaluation experiments\nand an overall aggregate from the 5-fold cross validation performed on the\nchest X-ray classiﬁcation dataset. 544\n9.5 A confusion matrix for a k-NN model trained on a churn prediction\nproblem. 551\n9.6 A confusion matrix for a naive Bayes model trained on a churn prediction\nproblem. 551\n9.7 The structure of a proﬁt matrix. 553\n9.8 The proﬁt matrix for the payday loan credit scoring problem. 554\n9.9 (a) The confusion matrix for a k-NN model trained on the payday loan credit\nscoring problem ( average class accuracy HM“83.824% ); and (b) the confusion\nmatrix for a decision tree model trained on the payday loan credit scoring\nproblem ( average class accuracy HM“80.761% ). 555\n9.10 (a) Overall proﬁt for the k-NN model using the proﬁt matrix in Table 9.8[554]\nand the confusion matrix in Table 9.9(a)[555]; and (b) overall proﬁt for the\ndecision tree model using the proﬁt matrix in Table 9.8[554]and the confusion\nmatrix in Table 9.9(b)[555]. 555\n9.11 A sample test set with model predictions and scores. 557\n9.12 Confusion matrices for the set of predictions shown in Table 9.11[557]using\n(a) a prediction score threshold of 0.75and (b) a prediction score threshold of\n0.25. 559\n9.13 A sample test set with prediction scores and resulting predictions based on\ndifferent threshold values. 560\n9.14 Tabulating the workings required to generate a K-S statistic. 565\n9.15 The test set with model predictions and scores from Table 9.11[557]extended to\ninclude deciles. 568\n9.16 Tabulating the workings required to calculate gain, cumulative gain, lift, and\ncumulative lift for the data given in Table 9.11[557]. 568\n9.17 The structure of a confusion matrix for a multinomial prediction problem with\nltarget levels. 572\n9.18 A sample test set with model predictions for a bacterial species identiﬁcation\nproblem. 573","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":53,"page_label":"liii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"List of Tables liii\n9.19 A confusion matrix for a model trained on the bacterial species identiﬁcation\nproblem. 574\n9.20 The expected target values for a test set, the predictions made by a model,\nand the resulting errors based on these predictions for a blood-thinning drug\ndosage prediction problem. 576\n9.21 Calculating the stability index for the bacterial species identiﬁcation problem\ngiven new test data for two periods after model deployment. 581\n9.22 The number of customers who left the mobile phone network operator each\nweek during the comparative experiment from both the control group (random\nselection) and the treatment group (model selection). 584\n10.1 A dataset of mobile phone customers described by their average monthly\ndata ( DATA USAGE ) and call ( CALL VOLUME ) usage. Details of the ﬁrst\ntwo iterations of the k-means clustering algorithm are also shown. The\nclustering in the second iteration is actually the ﬁnal clustering in this simple\nexample. 604\n10.2 Calculating the silhouette for the ﬁnal clustering of the mobile phone customer\ndataset (Table 10.1[604]) found using the k-means algorithm (with k“3). The\noverall silhouette index value is 0.66. 611\n10.3 Summary statistics for the three clusters found in the mobile phone customer\ndataset in Table 10.1[604]using k-means clustering ( k“3). Note, that the %\nmissing and cardinality columns usually used are omitted here for legibility as\nthese data quality issues will not arise in this simple example. They could be\nincluded when this approach is used on realdatasets. 614\n10.4 Information gain for each descriptive feature as a predictor of membership of\neach cluster based on the clustering of the mobile phone customer dataset in\nTable 10.1[604]found using k-means clustering ( k“3). 616\n10.5 Distance matrices that detail the ﬁrst three iterations of the AHC algorithm\napplied to the reduced version of the mobile phone customer dataset in Table\n10.1[604]. 620\n11.1 Some episodes of games played by the TwentyTwos agent showing the cards\ndealt, as well as the states, actions, and rewards. Note that rewards are shown\non the row indicating the action that led to them, not the state that followed\nthat action. 647\n11.2 An action-value table for an agent trained to play the card game TwentyTwos\n(the simpliﬁed version of Blackjack described in Section 11.2.3[643]). 654\n11.3 A portion of the action-value table for the grid world example at its ﬁrst\ninitialization. 661","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":53,"page_label":"liii","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 10.1[604]found using k-means clustering ( k“3). 616\n10.5 Distance matrices that detail the ﬁrst three iterations of the AHC algorithm\napplied to the reduced version of the mobile phone customer dataset in Table\n10.1[604]. 620\n11.1 Some episodes of games played by the TwentyTwos agent showing the cards\ndealt, as well as the states, actions, and rewards. Note that rewards are shown\non the row indicating the action that led to them, not the state that followed\nthat action. 647\n11.2 An action-value table for an agent trained to play the card game TwentyTwos\n(the simpliﬁed version of Blackjack described in Section 11.2.3[643]). 654\n11.3 A portion of the action-value table for the grid world example at its ﬁrst\ninitialization. 661\n11.4 A portion of the action-value table for the grid world example after 350\nepisodes of Q-learning have elapsed. 665\n12.1 The descriptive features in the ABT developed for the Acme Telephonica\nchurn prediction task. 692","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":54,"page_label":"liv","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"liv List of Tables\n12.2 A data quality report for the Acme Telephonica ABT. 694\n12.3 The confusion matrix from the test of the AT churn prediction stratiﬁed\nhold-out test set using the pruned decision tree in Figure 12.5[699]. 699\n12.4 The confusion matrix from the test of the AT churn prediction non-stratiﬁed\nhold-out test set. 700\n13.1 The structure of the SDSS and Galaxy Zoo combined dataset. 709\n13.2 Analysis of a subset of the features in the SDSS dataset. 711\n13.3 Features from the ABT for the SDSS galaxy classiﬁcation problem. 715\n13.4 A data quality report for a subset of the features in the SDSS ABT. 716\n13.5 The confusion matrices for the baseline models. 720\n13.6 The confusion matrices showing the performance of models on the\nunder-sampled training set. 721\n13.7 The confusion matrices for the models after feature selection. 723\n13.8 The confusion matrix for the 5-level logistic regression model (classiﬁcation\naccuracy: 77.528%, average class accuracy: 43.018%). 724\n13.9 The confusion matrix for the logistic regression model that distinguished\nbetween only the spiral galaxy types (classiﬁcation accuracy: 68.225%,\naverage class accuracy: 56.621%). 724\n13.10 The confusion matrix for the 5-level two-stage model (classiﬁcation accuracy:\n79.410%, average class accuracy: 53.118%). 725\n13.11 The confusion matrix for the ﬁnal logistic regression model on the large\nhold-out test set (classiﬁcation accuracy: 87.979%, average class accuracy:\n67.305%). 726\n14.1 The alignment between the phases of CRISP-DM, key questions for analytics\nprojects, and the chapters and sections of this book. 730\n14.2 A taxonomy of models based on the parametric versus non-parametric and\ngenerative versus discriminative distinctions. 735\nA.1 A dataset showing the positions and monthly training expenses of a school\nbasketball team. 750\nA.2 A frequency table for the POSITION feature from the school basketball team\ndataset in Table A.1[750]. 750\nA.3 Poll results from the run-up to the 2012 U.S. presidential election. 751\nA.4 The density calculation for the TRAINING EXPENSES feature from Table A.1[750]\nusing (a) ten 200-unit intervals and (b) four 500-unit intervals. 755\nB.1 A dataset of instances from the sample space in Figure B.1[757]. 758\nB.2 A simple dataset for MENINGITIS with three common symptoms of the disease\nlisted as descriptive features: HEADACHE ,FEVER , and VOMITING . 760","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":55,"page_label":"1","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"I INTRODUCTION TO MACHINE LEARNING AND DATA ANALYTICS","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":57,"page_label":"3","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1 Machine Learning for Predictive Data Analytics\n“Study the past if you would deﬁne the future. ”\n—Confucius\nModern organizations collect massive amounts of data. To be of value to an organization,\nthis data must be analyzed to extract insights that can be used to make better decisions.\nThe progression from data toinsights todecisions is illustrated in Figure 1.1[4]. Extracting\ninsights from data is the job of data analytics . This book focuses on predictive data\nanalytics , which is an important subﬁeld of data analytics.\n1.1 What Is Predictive Data Analytics?\nPredictive data analytics is the art of building and using models that make predictions\nbased on patterns extracted from historical data. Applications of predictive data analytics\ninclude\n‚Price Prediction: Businesses such as hotel chains, airlines, and online retailers need\nto constantly adjust their prices in order to maximize returns based on factors such as\nseasonal changes, shifting customer demand, and the occurrence of special events. Pre-\ndictive analytics models can be trained to predict optimal prices on the basis of historical\nsales records. Businesses can then use these predictions as an input into their pricing\nstrategy decisions.\n‚Dosage Prediction: Doctors and scientists frequently decide how much of a medicine or\nother chemical to include in a treatment. Predictive analytics models can be used to assist\nthis decision making by predicting optimal dosages based on data about past dosages and\nassociated outcomes.\n‚Risk Assessment: Risk is one of the key inﬂuencers in almost every decision an or-\nganization makes. Predictive analytics models can be used to predict the risk associated\nwith decisions such as issuing a loan or underwriting an insurance policy. These mod-\nels are trained using historical data from which they extract the key indicators of risk.\nThe output from risk prediction models can be used by organizations to make better risk\njudgments.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":58,"page_label":"4","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4 Chapter 1 Machine Learning for Predictive Data Analytics\nFigure 1.1\nPredictive data analytics moving from data toinsight todecision .\n‚Propensity Modeling: Most business decision making would be much easier if we could\npredict the likelihood, or propensity , of individual customers to take particular actions.\nPredictive data analytics can be used to build models that predict future customer actions\non the basis of historical behavior. Successful applications of propensity modeling\ninclude predicting the likelihood of customers to leave one mobile phone operator for\nanother, to respond to particular marketing efforts, or to buy different products.\n‚Diagnosis: Doctors, engineers, and scientists regularly make diagnoses as part of their\nwork. Typically, these diagnoses are based on their extensive training, expertise, and\nexperience. Predictive analytics models can help professionals make better diagnoses\nby leveraging large collections of historical examples at a scale beyond anything one\nindividual would see over his or her career. The diagnoses made by predictive analytics\nmodels usually become an input into the professional’s existing diagnosis process.\n‚Document Classiﬁcation: Predictive data analytics can be used to automatically classify\ndocuments into different categories. Examples include email spam ﬁltering, news sen-\ntiment analysis, customer complaint redirection, and medical decision making. In fact,\nthe deﬁnition of a document can be expanded to include images, sounds, and videos, all\nof which can be classiﬁed using predictive data analytics models.\nAll these examples have two things in common. First, in each case a model is used to\nmake a prediction to help a person or organization make a decision. In predictive data\nanalytics we use a broad deﬁnition of the word prediction . In everyday usage, the word\nprediction has a temporal aspect—we predict what will happen in the future. However, in\ndata analytics a prediction is the assignment of a value to any unknown variable. This could\nbe predicting the price that something will be sold for in the future; alternatively, it could\nmean predicting the type of document. So, in some cases prediction has a temporal aspect\nbut not in all. The second thing that the examples listed have in common is that a model\nis trained to make predictions based on a set of historical examples. We use machine\nlearning to train these models.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":59,"page_label":"5","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.2 What Is Machine Learning? 5\n1.2 What Is Machine Learning?\nMachine learning is deﬁned as an automated process that extracts patterns from data. To\nbuild the models used in predictive data analytics applications, we use supervised machine\nlearning . Supervised machine learning1techniques automatically learn a model of the\nrelationship between a set of descriptive features and a target feature based on a set of\nhistorical examples, or instances . We can then use this model to make predictions for new\ninstances. These two separate steps are shown in Figure 1.2[5].\n(a) Learning a model from a set of historical instances\n(b) Using a model to make predictions\nFigure 1.2\nThe two steps in supervised machine learning: (a) learning and (b) predicting.\nTable 1.1[6]lists a set of historical instances, or dataset , of mortgages that a bank has\ngranted in the past.2This dataset includes descriptive features that describe the mortgage,\n1. Other types of machine learning include unsupervised learning ,semi-supervised learning , and reinforce-\nment learning . In this book, however, we focus mainly on supervised machine learning and in most of the book\nuse the terms supervised machine learning and machine learning interchangeably. Chapters 10[597]and 11[637]\nprovide overviews of unsupervised learning and reinforcement learning, respectively.\n2. This dataset has been artiﬁcially generated for this example. Siddiqi (2005) gives an excellent overview of\nbuilding predictive data analytics models for ﬁnancial credit scoring.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":60,"page_label":"6","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6 Chapter 1 Machine Learning for Predictive Data Analytics\nTable 1.1\nA credit scoring dataset.\nLOAN-SALARY\nID O CCUPATION AGE RATIO OUTCOME\n1 industrial 34 2.96 repay\n2 professional 41 4.64 default\n3 professional 36 3.22 default\n4 professional 41 3.11 default\n5 industrial 48 3.80 default\n6 industrial 61 2.52 repay\n7 professional 37 1.50 repay\n8 professional 40 1.93 repay\n9 industrial 33 5.25 default\n10 industrial 32 4.15 default\nand a target feature that indicates whether the mortgage applicant ultimately defaulted on\nthe loan or paid it back in full. The descriptive features tell us three pieces of information\nabout the mortgage: the O CCUPATION (which can be professional orindustrial ) and A GE\nof the applicant and the ratio between the applicant’s salary and the amount borrowed\n(LOAN-SALARY RATIO ). The target feature, O UTCOME , is set to either default orrepay .\nIn machine learning terms, each row in the dataset is referred to as a training instance ,\nand the overall dataset is referred to as a training dataset .\nAn example of a very simple prediction model for this domain is\nifLOAN-SALARY RATIOą3then\nOUTCOME =default\nelse\nOUTCOME =repay\nend if\nWe can say that this model is consistent with the dataset because there are no instances in\nthe dataset for which the model does not make a correct prediction. When new mortgage\napplications are made, we can use this model to predict whether the applicant will repay\nthe mortgage or default on it and make lending decisions on the basis of this prediction.\nMachine learning algorithms automate the process of learning a model that captures\nthe relationship between the descriptive features and the target feature in a dataset. For\nsimple datasets like the one presented in Table 1.1[6], we may be able to manually create a\nprediction model; in an example of this scale, machine learning has little to offer us.\nConsider, however, the dataset presented in Table 1.2[8], which shows a more complete\nrepresentation of the same problem. This dataset lists more instances, and there are extra","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":61,"page_label":"7","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.3 How Does Machine Learning Work? 7\ndescriptive features describing the A MOUNT that a mortgage holder borrows, the mortgage\nholder’s S ALARY , the type of P ROPERTY that the mortgage relates to (which can be farm ,\nhouse , orapartment ), and the T YPE of mortgage (which can be ftbfor ﬁrst-time buyers or\nstbfor second-time buyers).\nThe simple prediction model using only the loan-salary ratio feature is no longer consis-\ntent with the dataset. It turns out, however, that there is at least one prediction model that\nis consistent with the dataset; it is just a little harder to ﬁnd than the previous one:\nifLOAN-SALARY RATIOă1.5then\nOUTCOME =repay\nelse if LOAN-SALARY RATIOą4then\nOUTCOME =default\nelse if AGEă40and OCCUPATION“industrial then\nOUTCOME =default\nelse\nOUTCOME =repay\nend if\nTo manually learn this model by examining the data is almost impossible. For a machine\nlearning algorithm, however, this is simple. When we want to build prediction models from\nlarge datasets with multiple features, machine learning is the solution.\n1.3 How Does Machine Learning Work?\nMachine learning algorithms work by searching through a set of possible prediction models\nfor the model that best captures the relationship between the descriptive features and target\nfeature in a dataset. An obvious criteria for driving this search is to look for models that\nare consistent with the data. There are, however, at least two reasons why simply searching\nfor consistent models is not sufﬁcient for learning useful prediction models. First, when\nwe are dealing with large datasets, it is likely that there is noise3in the data, and prediction\nmodels that are consistent with noisy data make incorrect predictions. Second, in the vast\nmajority of machine learning projects, the training set represents only a small sample of\nthe possible set of instances in the domain. As a result, machine learning is an ill-posed\nproblem , that is, a problem for which a unique solution cannot be determined using only\nthe information that is available.\nWe can illustrate how machine learning is an ill-posed problem using an example in\nwhich the analytics team at a supermarket chain wants to be able to classify customer\n3. For example, some of the feature values will be mislabeled.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":62,"page_label":"8","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8 Chapter 1 Machine Learning for Predictive Data Analytics\nTable 1.2\nA more complex credit scoring dataset.\nLOAN-\nSALARY\nID A MOUNT SALARY RATIO AGE OCCUPATION PROPERTY TYPE OUTCOME\n1 245,100 66,400 3.69 44 industrial farm stb repay\n2 90,600 75,300 1.20 41 industrial farm stb repay\n3 195,600 52,100 3.75 37 industrial farm ftb default\n4 157,800 67,600 2.33 44 industrial apartment ftb repay\n5 150,800 35,800 4.21 39 professional apartment stb default\n6 133,000 45,300 2.94 29 industrial farm ftb default\n7 193,100 73,200 2.64 38 professional house ftb repay\n8 215,000 77,600 2.77 17 professional farm ftb repay\n9 83,000 62,500 1.33 30 professional house ftb repay\n10 186,100 49,200 3.78 30 industrial house ftb default\n11 161,500 53,300 3.03 28 professional apartment stb repay\n12 157,400 63,900 2.46 30 professional farm stb repay\n13 210,000 54,200 3.87 43 professional apartment ftb repay\n14 209,700 53,000 3.96 39 industrial farm ftb default\n15 143,200 65,300 2.19 32 industrial apartment ftb default\n16 203,000 64,400 3.15 44 industrial farm ftb repay\n17 247,800 63,800 3.88 46 industrial house stb repay\n18 162,700 77,400 2.10 37 professional house ftb repay\n19 213,300 61,100 3.49 21 industrial apartment ftb default\n20 284,100 32,300 8.80 51 industrial farm ftb default\n21 154,000 48,900 3.15 49 professional house stb repay\n22 112,800 79,700 1.42 41 professional house ftb repay\n23 252,000 59,700 4.22 27 professional house stb default\n24 175,200 39,900 4.39 37 professional apartment stb default\n25 149,700 58,600 2.55 35 industrial farm stb default\nhouseholds into the demographic groups single ,couple , orfamily , solely on the basis of\ntheir shopping habits.4The dataset given in Table 1.3[9]contains descriptive features de-\nscribing the shopping habits of ﬁve customers. The descriptive features measure whether a\ncustomer buys baby food, B BY; alcohol, A LC; or organic vegetable products, O RG. Each\nfeature takes one of two values, yesorno. Alongside these descriptive features is a target\n4. This kind of classiﬁcation is not unusual because supermarket chains can collect huge amounts of data about\ncustomers’ shopping habits through a loyalty card scheme but ﬁnd it expensive and time consuming to collect\nmore personal data, such as demographic classiﬁcations. Demographic classiﬁcations, however, are extremely\nuseful to marketing departments in designing special offers and other customer incentives.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":63,"page_label":"9","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.3 How Does Machine Learning Work? 9\nTable 1.3\nA simple retail dataset.\nID B BY ALC ORG GRP\n1 no no no couple\n2 yes no yes family\n3 yes yes no family\n4 no no yes couple\n5 no yes yes single\nfeature, G RP, that describes the demographic group for each customer ( single ,couple , or\nfamily ). The dataset presented in Table 1.3[9]is referred to as a labeled dataset because it\nincludes values for the target feature.\nImagine that we attempt to learn a prediction model for this retail scenario by searching\nfor a model that is consistent with the dataset. The ﬁrst thing we need to do is ﬁgure out\nhow many different possible models actually exist for the scenario. This step deﬁnes the\nset of prediction models the machine learning algorithm will search. From the perspective\nof searching for a consistent model, the most important property of a prediction model is\nthat it deﬁnes a mapping from every possible combination of descriptive feature values to\na prediction for the target feature. For the retail scenario, there are only three binary de-\nscriptive features, so there are 23“8possible combinations of descriptive feature values.\nHowever, for each of these 8 possible combinations of descriptive feature values, there are\n3 possible target feature values, so this means that there are 38“6,561possible prediction\nmodels that could be used. Table 1.4(a)[10]illustrates the relationship between combinations\nof descriptive feature values and prediction models for the retail scenario. The descriptive\nfeature combinations are listed on the left-hand side of the table, and the set of potential\nmodels for this domain are shown as M1toM6,561on the right-hand side of the table. Using\nthe training dataset from Table 1.3[9], a machine learning algorithm will reduce the full set\nof6,561possible prediction models for this scenario to only those that are consistent with\nthe training instances. Table 1.4(b)[10]illustrates this; the blanked-out columns in the table\nindicate the models that are not consistent with the training data.\nTable 1.4(b)[10]also illustrates that the training dataset does not contain an instance for\nevery possible combination of descriptive feature values and that there are still a large num-\nber of potential prediction models that remain consistent with the training dataset after the\ninconsistent models have been excluded.5Speciﬁcally, there are three remaining descrip-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":63,"page_label":"9","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the training dataset from Table 1.3[9], a machine learning algorithm will reduce the full set\nof6,561possible prediction models for this scenario to only those that are consistent with\nthe training instances. Table 1.4(b)[10]illustrates this; the blanked-out columns in the table\nindicate the models that are not consistent with the training data.\nTable 1.4(b)[10]also illustrates that the training dataset does not contain an instance for\nevery possible combination of descriptive feature values and that there are still a large num-\nber of potential prediction models that remain consistent with the training dataset after the\ninconsistent models have been excluded.5Speciﬁcally, there are three remaining descrip-\n5. In this simple example it is easy to imagine collecting a training instance to match every possible combination\nof descriptive features; because there are only three binary descriptive features, there are only 23“8combina-\ntions. In more realistic scenarios, however, there are usually many more descriptive features, which means many\nmore possible combinations. In the credit scoring dataset given in Table 1.2[8], for example, a conservative esti-\nmate of the number of possible combinations of descriptive features is over 3.6 billion!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":64,"page_label":"10","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10 Chapter 1 Machine Learning for Predictive Data Analytics\nTable 1.4\nPotential prediction models (a) before and (b) after training data becomes available.\n(a) Before training data becomes available.\nBBY ALC ORG GRP M1 M2 M3 M4 M5 . . .M6 561\nno no no ? couple couple single couple couple . . . couple\nno no yes ? single couple single couple couple . . . single\nno yes no ? family family single single single . . . family\nno yes yes ? single single single single single . . . couple\nyes no no ? couple couple family family family . . . family\nyes no yes ? couple family family family family . . . couple\nyes yes no ? single family family family family . . . single\nyes yes yes ? single single family family couple . . . family\n(b) After training data becomes available.\nBBY ALC ORG GRP M1 M2 M3 M4 M5 . . . M6 561\nno no no couple couple couple single couple couple . . . couple\nno no yes couple single couple single couple couple . . . single\nno yes no ? family family single single single . . . family\nno yes yes single single single single single single . . . couple\nyes no no ? couple couple family family family . . . family\nyes no yes family couple family family family family . . . couple\nyes yes no family single family family family family . . . single\nyes yes yes ? single single family family couple . . . family\ntive feature value combinations for which the correct target feature value is not known,\nand therefore there are 33“27potential models that remain consistent with the training\ndata. Three of these— M2,M4, andM5—are shown in Table 1.4(b)[10]. Because a single\nconsistent model cannot be found on the basis of the sample training dataset alone, we say\nthat machine learning is fundamentally an ill-posed problem .\nWe might be tempted to think that having multiple models that are consistent with the\ndata is a good thing. The problem is, however, that although these models agree on which\npredictions should be made for the instances in the training dataset, they disagree with\nregard to which predictions should be returned for instances that are not in the training\ndataset. For example, if a new customer starts shopping at the supermarket and buys baby\nfood, alcohol, and organic vegetables, our set of consistent models will contradict each\nother with respect to the prediction that should be returned for this customer; for example,\nM2will return G RP=single ,M4will return G RP=family , andM5will return G RP=\ncouple .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":64,"page_label":"10","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"We might be tempted to think that having multiple models that are consistent with the\ndata is a good thing. The problem is, however, that although these models agree on which\npredictions should be made for the instances in the training dataset, they disagree with\nregard to which predictions should be returned for instances that are not in the training\ndataset. For example, if a new customer starts shopping at the supermarket and buys baby\nfood, alcohol, and organic vegetables, our set of consistent models will contradict each\nother with respect to the prediction that should be returned for this customer; for example,\nM2will return G RP=single ,M4will return G RP=family , andM5will return G RP=\ncouple .\nThe criterion of consistency with the training data doesn’t provide any guidance with\nregard to which of the consistent models to prefer in dealing with queries that are out-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":65,"page_label":"11","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.3 How Does Machine Learning Work? 11\nside the training dataset. As a result, we cannot use the set of consistent models to make\npredictions for these queries. In fact, searching for predictive models that are consistent\nwith the dataset is equivalent to just memorizing the dataset. As a result, no learning is\ntaking place because the set of consistent models tells us nothing about the underlying re-\nlationship between the descriptive and target features beyond what a simple look-up of the\ntraining dataset would provide.\nIf a predictive model is to be useful, it must be able to make predictions for queries that\nare not present in the data. A prediction model that makes the correct predictions for these\nqueries captures the underlying relationship between the descriptive and target features and\nis said to generalize well. Indeed, the goal of machine learning is to ﬁnd the predictive\nmodel that generalizes best. In order to ﬁnd this single best model, a machine learning\nalgorithm must use some criteria for choosing among the candidate models it considers\nduring its search.\nGiven that consistency with the dataset is not an adequate criterion to select the best\nprediction model, which criteria should we use? There are a lot of potential answers to this\nquestion, and that is why there are a lot of different machine learning algorithms. Each\nmachine learning algorithm uses different model selection criteria to drive its search for\nthe best predictive model. So, when we choose to use one machine learning algorithm\ninstead of another, we are, in effect, choosing to use one model selection criterion instead\nof another.\nAll the different model selection criteria consist of a set of assumptions about the char-\nacteristics of the model that we would like the algorithm to induce. The set of assump-\ntions that deﬁnes the model selection criteria of a machine learning algorithm is known as\ntheinductive bias6of the machine learning algorithm. There are two types of inductive\nbias that a machine learning algorithm can use, a restriction bias and a preference bias.\nArestriction bias constrains the set of models that the algorithm will consider during\nthe learning process. A preference bias guides the learning algorithm to prefer certain\nmodels over others. For example, in Chapter 7[311]we introduce a machine learning algo-\nrithm called multivariable linear regression with gradient descent , which implements","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":65,"page_label":"11","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"acteristics of the model that we would like the algorithm to induce. The set of assump-\ntions that deﬁnes the model selection criteria of a machine learning algorithm is known as\ntheinductive bias6of the machine learning algorithm. There are two types of inductive\nbias that a machine learning algorithm can use, a restriction bias and a preference bias.\nArestriction bias constrains the set of models that the algorithm will consider during\nthe learning process. A preference bias guides the learning algorithm to prefer certain\nmodels over others. For example, in Chapter 7[311]we introduce a machine learning algo-\nrithm called multivariable linear regression with gradient descent , which implements\nthe restriction bias of considering only prediction models that produce predictions on the\nbasis of a linear combination of the descriptive feature values and applies a preference bias\nover the order of the linear models it considers in terms of a gradient descent approach\nthrough a weight space. As a second example, in Chapter 4[117]we introduce the Iterative\nDichotomizer 3 (ID3) machine learning algorithm, which uses a restriction bias of con-\nsidering only tree prediction models in which each branch encodes a sequence of checks\n6. Learning a general rule from a ﬁnite set of examples is called inductive learning . This is why machine\nlearning is sometimes described as inductive learning, and the set of assumptions used by the machine algorithm\nthat biases it toward selecting a single model is called the inductive bias of the algorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":66,"page_label":"12","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12 Chapter 1 Machine Learning for Predictive Data Analytics\non individual descriptive features but also uses a preference bias by considering shallower\n(less complex) trees over larger trees. It is important to recognize that using an inductive\nbias is a necessary prerequisite for learning to occur; without inductive bias, a machine\nlearning algorithm cannot learn anything beyond what is in the data.\nIn summary, machine learning works by searching through a set of potential models\nto ﬁnd the prediction model that best generalizes beyond the dataset. Machine learning\nalgorithms use two sources of information to guide this search, the training dataset and the\ninductive bias assumed by the algorithm.\n1.4 Inductive Bias Versus Sample Bias\nInductive bias is not the only type of bias that affects machine learning. An in-depth review\nof the range of biases that affect machine learning and the social harms that they can cause\nare beyond the scope of this book.7However, we highlight sampling bias8as a particular\nform of bias that a data analyst should be aware of and should proactively guard against in\nany data analytics project.\nSampling bias arises when the sample of data used within a data-driven process is col-\nlected in such a way that the sample is not representative of the population the sample is\nused to represent. One of the most famous examples of sampling bias was in the 1936 U.S.\npresidential election, which pitted Franklin D. Roosevelt, the incumbent president, against\nAlfred Landon, the Republican governor of Kansas. At the time Literary Digest was a\nwell-known magazine that had accurately predicted the outcomes of previous presidential\nelections. For the 1936 election Literary Digest ran one of the largest pre-election polls in\nthe U.S. To run the poll Literary Digest created a list of 10 million names by integrating\nevery telephone directory in the U.S., and a number of other sources, and then mailing\neveryone on the list a mock ballot and asking them to return the ballot to the magazine.\nNearly 2.4 million people responded to the survey, and on the basis of this sample, Lit-\nerary Digest very publicly and conﬁdently predicted that Alfred Landon would win by a\nlandslide. However, the actual result was a massive win for Roosevelt, with 62% of the\nvotes. Literary Digest never recovered from the reputational damage this error in predic-\ntion caused and went out of business soon afterward.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":66,"page_label":"12","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the U.S. To run the poll Literary Digest created a list of 10 million names by integrating\nevery telephone directory in the U.S., and a number of other sources, and then mailing\neveryone on the list a mock ballot and asking them to return the ballot to the magazine.\nNearly 2.4 million people responded to the survey, and on the basis of this sample, Lit-\nerary Digest very publicly and conﬁdently predicted that Alfred Landon would win by a\nlandslide. However, the actual result was a massive win for Roosevelt, with 62% of the\nvotes. Literary Digest never recovered from the reputational damage this error in predic-\ntion caused and went out of business soon afterward.\n7. For an excellent overview of the different types of biases that affect machine learning and the potential harms\nthese can cause, we recommend Kate Crawford’s The Trouble with Bias keynote from the NeurIPS 2017 confer-\nence (Crawford, 2017). Videos of this talk are freely available online.\n8. Sampling bias is closely related to the problem of selection bias , and the terms are often treated as synonyms.\nSometimes, however, a distinction is made that sampling bias affects the ability of a trained model to generalize\nappropriately beyond the sample training data to the rest of a population, whereas selection bias is more focused\non the validity of similarities and differences found within the sample. For this reason we use the term sampling\nbias here; however, for the purposes of this book, the distinction is not particularly important and in general we\nuse the terms as synonyms.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":67,"page_label":"13","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.5 What Can Go Wrong with Machine Learning? 13\nThere were two fundamental problems with how Literary Digest collected its sample,\nand the result of both was that the sample used for prediction (the survey responses) was\nnot representative of the overall population. First, only about a quarter of the people who\nwere surveyed responded to the survey (2.4 million out of 10 million). It is likely that\npeople who respond to a survey are systematically different from people who don’t, and so\nwhen the response rate within a population is very low, it is likely that the resulting sample\nunderrepresents particular groups in the population. Second, the selection process used to\ngenerate the original list of 10 million people was based on telephone directories. At the\ntime not every household in U.S. had telephones, and of those that did, a disproportionate\nnumber (relative to the total voting population of the U.S.) were Republican voters. Con-\nsequently, the sample of the population surveyed was skewed toward Republican voters,\nand so the predictions based on these surveys were also skewed. Importantly, the resulting\ndata was skewed even though the surveys were large.\nThe key point to remember here is that if a sample of data is not representative of a pop-\nulation, then inferences based on that sample will not generalize to the larger population.\nThis is true no matter how large the sample is. That said, sampling bias is a difﬁcult prob-\nlem to tackle. One challenge is that bias in a sample can arise in indirect and non-obvious\nways. For example, in 1936 it was not necessarily obvious that using telephone directories\nto create an initial list of names would skew the resulting sample toward a particular group\nin the population (in this instance, Republicans). Consequently, data analysts need to think\nabout the sources of the data they are using and understand how the data was collected and\nwhether the collection processes introduced a bias relative to the population. They also\nneed to reﬂect on the processes they use to preprocess and manage the data, and whether\nany of these processes introduce bias into the sample.9So, in summary, although inductive\nbias is necessary for machine learning, and in a sense, a key goal of a data analyst is to ﬁnd\nthe correct inductive bias, sample bias is something that a data analyst should proactively\nwork hard to remove from the data used in any data analytics project.\n1.5 What Can Go Wrong with Machine Learning?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":67,"page_label":"13","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"about the sources of the data they are using and understand how the data was collected and\nwhether the collection processes introduced a bias relative to the population. They also\nneed to reﬂect on the processes they use to preprocess and manage the data, and whether\nany of these processes introduce bias into the sample.9So, in summary, although inductive\nbias is necessary for machine learning, and in a sense, a key goal of a data analyst is to ﬁnd\nthe correct inductive bias, sample bias is something that a data analyst should proactively\nwork hard to remove from the data used in any data analytics project.\n1.5 What Can Go Wrong with Machine Learning?\nDifferent machine learning algorithms encode different inductive biases. Because a ma-\nchine learning algorithm encodes an inductive bias, it can induce models that generalize\nbeyond the instances in a training dataset. An inappropriate inductive bias, however, can\nlead to mistakes. It has been shown that there is no particular inductive bias that on average\nis the best one to use.10Also, in general, there is no way of knowing for a given predic-\n9. In this section we have primarily considered the population we are sampling from to be a population of people,\nbut sampling bias can also arise for any population of predictive subjects, be they insurance policies, holidays,\ncars, or anything else.\n10. This is known as the No Free Lunch Theorem (Wolpert, 1996).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":68,"page_label":"14","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14 Chapter 1 Machine Learning for Predictive Data Analytics\nTable 1.5\nThe age-income dataset.\nID A GE INCOME\n1 21 24,000\n2 32 48,000\n3 62 83,000\n4 72 61,000\n5 84 52,000\ntive task which inductive bias will work best. Indeed, the ability to select the appropriate\nmachine learning algorithm (and hence inductive bias) to use for a given predictive task is\none of the core skills that a data analyst must develop.\nThere are two kinds of mistakes that an inappropriate inductive bias can lead to: un-\nderﬁtting andoverﬁtting . Underﬁtting occurs when the prediction model selected by the\nalgorithm is too simplistic to represent the underlying relationship in the dataset between\nthe descriptive features and the target feature. Overﬁtting, by contrast, occurs when the\nprediction model selected by the algorithm is so complex that the model ﬁts the dataset too\nclosely and becomes sensitive to noise in the data.\nTo understand underﬁtting and overﬁtting, consider the task of inducing a model to pre-\ndict a person’s I NCOME (the target feature) based on A GE(a single descriptive feature).\nTable 1.5[14]lists a simple dataset that gives ages and salaries for ﬁve people. A visualiza-\ntion11of this dataset is shown in Figure 1.3(a)[15].\nThe line in Figure 1.3(b)[15]represents one model of the relationship between the A GE\nand I NCOME features. This line illustrates a very simple linear function that maps A GE\nto I NCOME . Although this simple model goes some way toward capturing the general\ntrend of the relationship between A GEand I NCOME , it does not manage to capture any\nof the subtlety of the relationship. This model is said to underﬁt the data as it is not\ncomplex enough to fully capture the relationship between the descriptive feature and the\ntarget feature. By contrast, the model shown in Figure 1.3(c)[15], although consistent with\nthe training instances, seems much more complicated than necessary. This model is said\nto overﬁt the training data.\nModels that either underﬁt or overﬁt do not generalize well and so will not be able to\nmake good predictions for query instances beyond the content of the training dataset. The\nprediction model shown in Figure 1.3(d)[15], however, is a Goldilocks model : it is just right ,\nstriking a good balance between underﬁtting and overﬁtting. We ﬁnd these Goldilocks\n11. We discuss exactly this type of visualization, a scatter plot, in detail in Chapter 3[53]. For this example it","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":68,"page_label":"14","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"target feature. By contrast, the model shown in Figure 1.3(c)[15], although consistent with\nthe training instances, seems much more complicated than necessary. This model is said\nto overﬁt the training data.\nModels that either underﬁt or overﬁt do not generalize well and so will not be able to\nmake good predictions for query instances beyond the content of the training dataset. The\nprediction model shown in Figure 1.3(d)[15], however, is a Goldilocks model : it is just right ,\nstriking a good balance between underﬁtting and overﬁtting. We ﬁnd these Goldilocks\n11. We discuss exactly this type of visualization, a scatter plot, in detail in Chapter 3[53]. For this example it\nis sufﬁcient to say that a point is shown for each person in the dataset, placed to represent the person’s age\n(horizontally) and salary (vertically).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":69,"page_label":"15","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.6 The Predictive Data Analytics Project Lifecycle: CRISP-DM 15\nmodels by using machine learning algorithms with appropriate inductive biases. This is\none of the great arts of machine learning and something that we return to throughout this\nbook.\n/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n0 20 40 60 80 10020000 40000 60000 80000\nAgeIncome\n(a) Dataset\n/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n0 20 40 60 80 10020000 40000 60000 80000\nAgeIncome (b) Underﬁtting\n/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n0 20 40 60 80 10020000 40000 60000 80000\nAgeIncome\n(c) Overﬁtting\n/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n0 20 40 60 80 10020000 40000 60000 80000\nAgeIncome (d) Just right\nFigure 1.3\n(a)–(d) Striking a balance between overﬁtting and underﬁtting in trying to predict income from age.\n1.6 The Predictive Data Analytics Project Lifecycle: CRISP-DM\nBuilding predictive data analytics solutions for the kinds of applications described in Sec-\ntion 1.1[3]involves a lot more than just choosing the right machine learning algorithm.\nAs with any other signiﬁcant project, the chances of success for a predictive data analytics","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":70,"page_label":"16","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"16 Chapter 1 Machine Learning for Predictive Data Analytics\nData\t\r  \nBusiness\t\r  \nUnderstanding\t\r  Data\t\r  \nUnderstanding\t\r  \nData\t\r  \nPrepara1on\t\r  \nModeling\t\r  \nEvalua1on\t\r  Deployment\t\r  \nFigure 1.4\nA diagram of the CRISP-DM process that shows the six key phases and indicates the important\nrelationships between them. This ﬁgure is based on Figure 2 of Wirth and Hipp (2000).\nproject are greatly increased if a standard process is used to manage the project through the\nproject lifecycle. One of the most commonly used processes for predictive data analytics\nprojects is the Cross Industry Standard Process for Data Mining (CRISP-DM ).12Key\nfeatures of the CRISP-DM process that make it attractive to data analytics practitioners are\nthat it is non-proprietary; it is application, industry, and tool neutral; and it explicitly views\nthe data analytics process from both an application-focused and a technical perspective.\nFigure 1.4[16]shows six key phases of the predictive data analytics project lifecycle that\nare deﬁned by the CRISP-DM:\n‚Business Understanding: Predictive data analytics projects never start out with the goal\nof building a prediction model. Instead, they focus on things like gaining new customers,\nselling more products, and adding efﬁciencies to a process. So, during the ﬁrst phase in\nany analytics project, the primary goal of the data analyst is to fully understand the\nbusiness (or organizational) problem that is being addressed and then to design a data\nanalytics solution for it.\n12. While the name CRISP-DM refers to data mining (a ﬁeld that overlaps signiﬁcantly with predictive data\nanalytics), it is equally applicable to predictive analytics projects.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":71,"page_label":"17","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.7 Predictive Data Analytics Tools 17\n‚Data Understanding: Once the manner in which predictive data analytics will be used\nto address a business problem has been decided, it is important that the data analyst fully\nunderstand the different data sources available within an organization and the different\nkinds of data that are contained in these sources.\n‚Data Preparation: Building predictive data analytics models requires speciﬁc kinds of\ndata, organized in a speciﬁc kind of structure known as an analytics base table (ABT ).13\nThis phase of CRISP-DM includes all the activities required to convert the disparate data\nsources that are available in an organization to a well-formed ABT from which machine\nlearning models can be induced.\n‚Modeling: In the Modeling phase of the CRISP-DM process, the machine learning work\noccurs. Different machine learning algorithms are used to build a range of prediction\nmodels from which the best model will be selected for deployment.\n‚Evaluation: Before models can be deployed for use within an organization, it is im-\nportant that they are fully evaluated and proved to be ﬁt for the purpose. This phase of\nCRISP-DM covers all the evaluation tasks required to show that a prediction model will\nbe able to make accurate predictions after being deployed and that it does not suffer from\noverﬁtting or underﬁtting.\n‚Deployment: Machine learning models are built to serve a purpose within an organiza-\ntion, and the last phase of CRISP-DM covers all the work that must be done to success-\nfully integrate a machine learning model into the processes within an organization.\nFigure 1.4[16]also illustrates the ﬂow between each of these phases and emphasizes that\ndata is at the heart of the process. Certain phases in CRISP-DM are more closely linked\ntogether than others. For example, Business Understanding and Data Understanding are\ntightly coupled, and projects typically spend some time moving back and forth between\nthese phases. Similarly, the Data Preparation and Modeling phases are closely linked, and\nanalytics projects often spend some time alternating between these two phases. Using the\nCRISP-DM process improves the likelihood that predictive data analytics projects will be\nsuccessful, and we recommend its use.\n1.7 Predictive Data Analytics Tools\nThroughout this book we discuss the many different ways we can use machine learning\ntechniques to build predictive data analytics models. In these discussions we do not refer to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":71,"page_label":"17","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"together than others. For example, Business Understanding and Data Understanding are\ntightly coupled, and projects typically spend some time moving back and forth between\nthese phases. Similarly, the Data Preparation and Modeling phases are closely linked, and\nanalytics projects often spend some time alternating between these two phases. Using the\nCRISP-DM process improves the likelihood that predictive data analytics projects will be\nsuccessful, and we recommend its use.\n1.7 Predictive Data Analytics Tools\nThroughout this book we discuss the many different ways we can use machine learning\ntechniques to build predictive data analytics models. In these discussions we do not refer to\nspeciﬁc tools or implementations of these techniques. There are, however, many different,\neasy-to-use options for implementing machine learning models that interested readers can\nuse to follow along with the examples in this book.\n13. All datasets presented in this chapter have been structured as ABTs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":72,"page_label":"18","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"18 Chapter 1 Machine Learning for Predictive Data Analytics\nThe ﬁrst decision that must be made in choosing a machine learning platform is whether\nto use an application-based solution or to use a programming language. We will look\nat application-based solutions ﬁrst. Well-designed application-based, or point-and-click ,\ntools make it very quick and easy to develop and evaluate models and to perform associ-\nated data manipulation tasks. Using one of these tools, it is possible to train, evaluate, and\ndeploy a predictive data analytics model in less than an hour! Important application-based\nsolutions for building predictive data analytics models include IBM SPSS, Knime Analyt-\nics Platform, RapidMiner Studio, SAS Enterprise Miner, and Weka.14The tools by IBM\nand SAS are enterprise-wide solutions that integrate with the other offerings by these com-\npanies. Knime, RapidMiner, and Weka are interesting because they are all open-source,\nfreely available solutions that readers can begin to use without any ﬁnancial investment.\nAn interesting alternative to using an application-based solution for building predictive\ndata analytics models is to use a programming language. Two of the most commonly used\nprogramming languages for predictive data analytics are R and Python.15Building predic-\ntive data analytics models using a language like R or Python is not especially difﬁcult. For\nexample, the following simple lines of code use the R language to build a predictive model\nfor a simple task:\ncreditscoring.train <- read.csv(\"creditScoringTrain.csv\")\nglm.mod <- glm(Outcome „Amount+Salary+Age+LoanSalaryRatio,\nfamily=binomial(link=\"logit\"), data=creditscoring.train)\ncreditscoring.test <- read.csv(\"creditScoringTest.csv\")\npredicted.values <- predict(glm.mod, creditscoring.test)\nThe advantage of using a programming language for predictive data analytics projects is\nthat it gives the data analyst huge ﬂexibility. Anything that the analyst can imagine can be\nimplemented. This is in contrast to application-based solutions, in which the analyst can\nreally achieve only what the tool developers had in mind when they designed the tool. The\nother main advantage of using a programming language is that, in most cases, the newest\nadvanced analytics techniques become available in programming languages long before\nthey are implemented in application-based solutions.\nObviously, however, using programming languages also has its disadvantages. The main","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":72,"page_label":"18","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"predicted.values <- predict(glm.mod, creditscoring.test)\nThe advantage of using a programming language for predictive data analytics projects is\nthat it gives the data analyst huge ﬂexibility. Anything that the analyst can imagine can be\nimplemented. This is in contrast to application-based solutions, in which the analyst can\nreally achieve only what the tool developers had in mind when they designed the tool. The\nother main advantage of using a programming language is that, in most cases, the newest\nadvanced analytics techniques become available in programming languages long before\nthey are implemented in application-based solutions.\nObviously, however, using programming languages also has its disadvantages. The main\ndisadvantage is that programming is a skill that takes time and effort to learn. Using a pro-\ngramming language for advanced analytics has a signiﬁcantly steeper learning curve than\nusing an application-based solution. The second disadvantage is that in using a program-\nming language, we have very little of the infrastructural support, such as data management,\n14. For further details, see www.ibm.com/software/ie/analytics/spss, www.knime.org, www.rapidminer.com,\nwww.sas.com, and www.cs.waikato.ac.nz/ml/weka.\n15. The website kdnuggets.com runs a regular poll on the most popular programming languages for predic-\ntive data analytics, which R and Python regularly top, www.kdnuggets.com/polls/2013/languages-analytics-data-\nmining-data-science.html. For further details about R and Python, see www.r-project.org and www.python.org.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":73,"page_label":"19","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.8 The Road Ahead 19\noffered with the application-based solutions available to us. This puts an extra burden on\ndevelopers to implement these supports themselves.\n1.8 The Road Ahead\nPredictive data analytics projects use machine learning algorithms to induce prediction\nmodels from historical data. The insights that these prediction models produce are used\nto help organizations make data-driven decisions. Machine learning algorithms learn pre-\ndiction models by inducing a generalized model of the relationship between a set of de-\nscriptive features and a target feature from a set of speciﬁc training instances. Machine\nlearning, however, is made difﬁcult because there is usually more than one model that is\nconsistent with the training dataset—because of this, machine learning is often described\nas an ill-posed problem. Machine learning algorithms address this issue by encoding an\ninductive bias—or set of assumptions—that guide the algorithm to prefer certain models\nover others. We will see as we proceed through this book that the selection of a machine\nlearning algorithm is not the only way that we can bias the predictive data analytics pro-\ncess. All the other choices that we make, such as the data to use, the descriptive features\nto use, and the way in which we deploy a model, bias the outcome of the overall process,\nand this is something that we need to be keenly aware of.\nThe purpose of this book is to give readers a solid grounding in the theoretical under-\npinnings of the most commonly used machine learning techniques and a clear view of the\nways machine learning techniques are used in practice in predictive data analytics projects.\nWith this in mind, readers can view the book as four parts that are mapped to the phases of\nthe CRISP-DM process.\nThe ﬁrst part—Chapters 2[23]and 3[53]—covers the Business Understanding, Data Under-\nstanding, and Data Preparation phases of the process. In this part we discuss how a business\nproblem is converted into a data analytics solution, how data can be prepared for this task,\nand the data exploration tasks that should be performed during these phases.\nThe second part of the book covers the Modeling and Evaluation phase of CRISP-DM.\nWe consider ﬁve main families of machine learning algorithm:\n‚Information-based learning (Chapter 4[117])\n‚Similarity-based learning (Chapter 5[181])\n‚Probability-based learning (Chapter 6[243])\n‚Error-based learning (Chapter 7[311])\n‚Deep learning (Chapter 8[381])","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":73,"page_label":"19","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the CRISP-DM process.\nThe ﬁrst part—Chapters 2[23]and 3[53]—covers the Business Understanding, Data Under-\nstanding, and Data Preparation phases of the process. In this part we discuss how a business\nproblem is converted into a data analytics solution, how data can be prepared for this task,\nand the data exploration tasks that should be performed during these phases.\nThe second part of the book covers the Modeling and Evaluation phase of CRISP-DM.\nWe consider ﬁve main families of machine learning algorithm:\n‚Information-based learning (Chapter 4[117])\n‚Similarity-based learning (Chapter 5[181])\n‚Probability-based learning (Chapter 6[243])\n‚Error-based learning (Chapter 7[311])\n‚Deep learning (Chapter 8[381])\nBy looking at these ﬁve key families, we cover the most commonly used approaches to\ninductive machine learning that can be used to build most predictive data analytics solu-\ntions. The second part of the book concludes with Chapter 9[533], which describes a range\nof approaches to evaluating prediction models.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":74,"page_label":"20","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"20 Chapter 1 Machine Learning for Predictive Data Analytics\nThe third part of the book also deals with Modeling, but in this case it looks at modeling\napproaches beyond prediction. As described previously in this chapter, supervised machine\nlearning is one of three main machine learning paradigms. The other two are unsupervised\nlearning andreinforcement learning . Chapters 10[597]and 11[637]describe these two other\napproaches as a counterpoint to the descriptions of supervised learning in the rest of the\nbook.\nThe fourth part of the book covers the Deployment phases of CRISP-DM. Chapters 12[685]\nand 13[703]present case studies describing speciﬁc predictive analytics projects from Busi-\nness Understanding up to Deployment. These case studies demonstrate how everything\ndescribed in the preceding chapters comes together in a successful predictive data analyt-\nics project.\nFinally, Chapter 14[729]provides some overarching perspectives on machine learning for\npredictive data analytics and summarizes some of the key differences between the different\napproaches covered in this book.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":75,"page_label":"21","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.9 Exercises 21\n1.9 Exercises\n1.What is predictive data analytics ?\n2.What is supervised machine learning ?\n3.Machine learning is often referred to as an ill-posed problem . What does this mean?\n4.The following table lists a dataset from the credit scoring domain that we discussed in\nthe chapter. Underneath the table we list two prediction models consistent with this\ndataset, Model 1 andModel 2 .\nLOAN-SALARY\nID O CCUPATION AGE RATIO OUTCOME\n1 industrial 39 3.40 default\n2 industrial 22 4.02 default\n3 professional 30 2.7 0 repay\n4 professional 27 3.32 default\n5 professional 40 2.04 repay\n6 professional 50 6.95 default\n7 industrial 27 3.00 repay\n8 industrial 33 2.60 repay\n9 industrial 30 4.5 0 default\n10 professional 45 2.78 repay\nModel 1\nifLOAN-SALARY RATIOą3.00then\nOUTCOME =default\nelse\nOUTCOME =repay\nend if\nModel 2\nifAGE“50then\nOUTCOME =default\nelse if AGE“39then\nOUTCOME =default\nelse if AGE“30and OCCUPATION =industrial then\nOUTCOME =default\nelse if AGE“27and OCCUPATION =professional then\nOUTCOME =default\nelse\nOUTCOME =repay\nend if\n(a)Which of these two models do you think will generalize better to instances not\ncontained in the dataset?\n(b)Propose an inductive bias that would enable a machine learning algorithm to make\nthe same preference choice that you made in Part (a).\n(c)Do you think that the model that you rejected in Part (a) of this question is over-\nﬁtting or underﬁtting the data?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":76,"page_label":"22","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"22 Chapter 1 Machine Learning for Predictive Data Analytics\n˚5.What is meant by the term inductive bias ?\n˚6.How do machine learning algorithms deal with the fact that machine learning is an\nill-posed problem ?\n˚7.What can go wrong when an inappropriate inductive bias is used?\n˚8.It is often said that 80% of the work done on predictive data analytics projects is done\nin the Business Understanding, Data Understanding, and Data Preparation phases of\nCRISP-DM , and just 20% is spent on the Modeling, Evaluation, and Deployment\nphases. Why do you think this would be the case?\n˚9.The following table lists a dataset of ﬁve individuals described via a set of stroke risk\nfactors and their probability of suffering a stroke in the next ﬁve years. This dataset has\nbeen prepared by an analytics team who are developing a model as a decision support\ntool for doctors.16The goal of the model is to classify individuals into groups on the\nbasis of their risk of suffering a stroke S TROKE RISK. In this dataset there are three\ncategories of risk: low,medium , and high. All the descriptive features are Boolean,\ntaking two levels: trueorfalse .\nHIGHBLOOD HEART STROKE\nID P RESSURE SMOKER DIABETES DISEASE RISK\n1 true false true true high\n2 true true true true high\n3 true false false true medium\n4 false false false false low\n5 true true true false high\n(a)How many possible models exist for the scenario described by the features in this\ndataset?\n(b)How many of these potential models would be consistent with this sample of data?\n˚10.You are using U.S. census data to build a prediction model. On inspecting the data\nyou notice that the RACE feature has a higher proportion of the category White than\nyou expected. Why do you think this might be?\n˚11.Why might a prediction model that has very high accuracy on a dataset not generalize\nwell after it is deployed?\n16. Due to space limitations, this dataset covers only a sample of the risk factors for stroke. There are, for\nexample, a number of non-modiﬁable risk factors such as age and gender. There is also an array of modiﬁable\nrisk factors, including alcohol and drug use, unhealthy diet, stress and depression, and lack of physical exercise.\nFurthermore, this dataset is not based on precise measurements of stroke risk. For more information on stroke\nand risk factors related to stroke, please see the National Heart, Lung, and Blood Institute on Stroke: https:\n//www.nhlbi.nih.gov/health-topics/stroke.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":77,"page_label":"23","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2 Data to Insights to Decisions\n“We cannot solve our problems with the same thinking we used when we created them. ”\n—Albert Einstein\nPredictive data analytics projects are not handed to data analytics practitioners fully formed.\nRather, analytics projects are initiated in response to a business problem, and it is our job—\nas analytics practitioners—to decide how to address this business problem using analytics\ntechniques. In the ﬁrst part of this chapter we present an approach to developing analytics\nsolutions that address speciﬁc business problems . This involves an analysis of the needs\nof the business, the data we have available for use, and the capacity of the business to use\nanalytics. Taking these factors into account helps to ensure that we develop analytics so-\nlutions that are effective and ﬁt for purpose. In the second part of this chapter we move\nour attention to the data structures that are required to build predictive analytics models,\nand in particular the analytics base table (ABT ). Designing ABTs that properly represent\nthe characteristics of a prediction subject is a key skill for analytics practitioners. We\npresent an approach in which we ﬁrst develop a set of domain concepts that describe the\nprediction subject, and then expand these into concrete descriptive features . Throughout\nthe chapter we return to a case study that demonstrates how these approaches are used in\npractice.\n2.1 Converting Business Problems into Analytics Solutions\nOrganizations don’t exist to do predictive data analytics. Organizations exist to do things\nlike make more money, gain new customers, sell more products, or reduce losses from\nfraud. Unfortunately, the predictive analytics models that we can build do not do any of\nthese things. The models that analytics practitioners build simply make predictions based\non patterns extracted from historical datasets. These predictions do not solve business\nproblems; rather, they provide insights that help the organization make better decisions to\nsolve their business problems.\nA key step, then, in any data analytics project is to understand the business problem\nthat the organization wants to solve and, based on this, to determine the kind of insight","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":78,"page_label":"24","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"24 Chapter 2 Data to Insights to Decisions\nthat a predictive analytics model can provide to help the organization address this problem.\nThis deﬁnes the analytics solution that the analytics practitioner will set out to build using\nmachine learning. Deﬁning the analytics solution is the most important task in the Business\nUnderstanding phase of the CRISP-DM process.\nIn general, converting a business problem into an analytics solution involves answering\nthe following key questions:\n1.What is the business problem? What are the goals that the business wants to\nachieve? These ﬁrst two questions are not always easy to answer. In many cases\norganizations begin analytics projects because they have a clear issue that they want to\naddress. Sometimes, however, organizations begin analytics projects simply because\nsomebody in the organization feels that this is an important new technique that they\nshould be using. Unless a project is focused on clearly stated goals, it is unlikely to be\nsuccessful. The business problem and goals should always be expressed in business\nterms and not yet be concerned with the actual analytics work at this stage.\n2.How does the business currently work? It is not feasible for an analytics prac-\ntitioner to learn everything about the businesses with which they work as they will\nprobably move quickly between different areas of an organization, or even different\nindustries. Analytics practitioners must, however, possess what is referred to as situa-\ntional ﬂuency . This means that they understand enough about a business so that they\ncan converse with partners in the business in a way that these business partners un-\nderstand. For example, in the insurance industry, insurance policyholders are usually\nreferred to as members rather than customers . Although from an analytics perspective,\nthere is really little difference, using the correct terminology makes it much easier for\nbusiness partners to engage with the analytics project. Beyond knowing the correct\nterminology to use, an analytics practitioner who is situationally ﬂuent will have suf-\nﬁcient knowledge of the quirks of a particular domain to be able to competently build\nanalytics solutions for that domain.\n3.In what ways could a predictive analytics model help to address the business\nproblem? For any business problem, there are a number of different analytics solu-\ntions that we could build to address it. It is important to explore these possibilities","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":78,"page_label":"24","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"referred to as members rather than customers . Although from an analytics perspective,\nthere is really little difference, using the correct terminology makes it much easier for\nbusiness partners to engage with the analytics project. Beyond knowing the correct\nterminology to use, an analytics practitioner who is situationally ﬂuent will have suf-\nﬁcient knowledge of the quirks of a particular domain to be able to competently build\nanalytics solutions for that domain.\n3.In what ways could a predictive analytics model help to address the business\nproblem? For any business problem, there are a number of different analytics solu-\ntions that we could build to address it. It is important to explore these possibilities\nand, in conjunction with the business, to agree on the most suitable solution for the\nbusiness. For each proposed solution, the following points should be described: (1)\nthe predictive model that will be built; (2) how the predictive model will be used by\nthe business; and (3) how using the predictive model will help address the original\nbusiness problem. The next section provides a case study of the process for converting\na business problem into a set of candidate analytics solutions.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":79,"page_label":"25","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.1 Converting Business Problems into Analytics Solutions 25\n2.1.1 Case Study: Motor Insurance Fraud\nConsider the following business problem: in spite of having a fraud investigation team that\ninvestigates up to 30% of all claims made, a motor insurance company is still losing too\nmuch money due to fraudulent claims. The following predictive analytics solutions could\nbe proposed to help address this business problem:\n‚[Claim prediction] A model could be built to predict the likelihood that an insurance\nclaim is fraudulent. This model could be used to assign every newly arising claim a fraud\nlikelihood, and those that are most likely to be fraudulent could be ﬂagged for investi-\ngation by the insurance company’s claims investigators. In this way the limited claims\ninvestigation time could be targeted at the claims that are most likely to be fraudulent,\nthereby increasing the number of fraudulent claims detected and reducing the amount of\nmoney lost to fraud.\n‚[Member prediction] A model could be built to predict the propensity of a member1\nto commit fraud in the near future. This model could be run every quarter to identify\nthose members most likely to commit fraud, and the insurance company could take a\nrisk-mitigation action ranging from contacting the member with some kind of warning\nto canceling the member’s policies. By identifying members likely to make fraudulent\nclaims before they make them, the company could save signiﬁcant amounts of money.\n‚[Application prediction] A model could be built to predict, at the point of application,\nthe likelihood that a policy someone has applied for will ultimately result in a fraudulent\nclaim. The company could run this model every time a new application is made and reject\nthose applications that are predicted likely to result in a fraudulent claim. The company\nwould therefore reduce the number of fraudulent claims and reduce the amount of money\nthey would lose to these claims.\n‚[Payment prediction] Many fraudulent insurance claims simply over-exaggerate the\namount that should actually be paid out. In these cases the insurance company goes\nthrough an expensive investigation process but still must make a reduced payment in re-\nlation to a claim. A model could be built to predict the amount most likely to be paid\nout by an insurance company after having investigated a claim. This model could be run\nwhenever new claims arise, and the policyholder could be offered the amount predicted","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":79,"page_label":"25","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"those applications that are predicted likely to result in a fraudulent claim. The company\nwould therefore reduce the number of fraudulent claims and reduce the amount of money\nthey would lose to these claims.\n‚[Payment prediction] Many fraudulent insurance claims simply over-exaggerate the\namount that should actually be paid out. In these cases the insurance company goes\nthrough an expensive investigation process but still must make a reduced payment in re-\nlation to a claim. A model could be built to predict the amount most likely to be paid\nout by an insurance company after having investigated a claim. This model could be run\nwhenever new claims arise, and the policyholder could be offered the amount predicted\nby the model as settlement as an alternative to going through a claims investigation pro-\ncess. Using this model, the company could save on claims investigations and reduce the\namount of money paid out on fraudulent claims.\n1. Remember that in insurance we don’t refer to customers!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":80,"page_label":"26","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"26 Chapter 2 Data to Insights to Decisions\n2.2 Assessing Feasibility\nOnce a set of candidate analytics solutions that address a business problem have been de-\nﬁned, the next task is to evaluate the feasibility of each solution. This involves considering\nthe following questions:\n‚Is the data required by the solution available, or could it be made available?\n‚What is the capacity of the business to utilize the insights that the analytics solution will\nprovide?\nThe ﬁrst question addresses data availability. Every analytics solution will have its own\nset of data requirements, and it is useful, as early as possible, to determine if the business\nhas sufﬁcient data available to meet these requirements. In some cases a lack of appropriate\ndata will simply rule out proposed analytics solutions to a business problem. More likely,\nthe easy availability of data for some solutions might favor them over others. In general,\nevaluating the feasibility of an analytics solution in terms of its data requirements involves\naligning the following issues with the requirements of the analytics solution:\n‚The key objects in the company’s data model and the data available regarding them.\nFor example, in a bricks-and-mortar retail scenario, the key objects are likely to be cus-\ntomers, products, sales, suppliers, stores, and staff. In an insurance scenario, the key\nobjects are likely to be policyholders, policies, claims, policy applications, investiga-\ntions, brokers, members, investigators, and payments.\n‚The connections that exist between key objects in the data model. For example, in\na banking scenario is it possible to connect the multiple accounts that a single customer\nmight own? Similarly, in an insurance scenario is it possible to connect the information\nfrom a policy application with the details (e.g., claims, payments, etc.) of the resulting\npolicy itself?\n‚The granularity of the data that the business has available. In a bricks-and-mortar\nretail scenario, data on sales might only be stored as a total number of sales per product\ntype per day, rather than as individual items sold to individual customers.\n‚The volume of data involved. The amount of data that is available to an analytics\nproject is important because (a) some modern datasets are so large that they can stretch\neven state-of-the-art machine learning tools; and (b) conversely, very small datasets can\nlimit our ability to evaluate the expected performance of a model after deployment.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":80,"page_label":"26","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"from a policy application with the details (e.g., claims, payments, etc.) of the resulting\npolicy itself?\n‚The granularity of the data that the business has available. In a bricks-and-mortar\nretail scenario, data on sales might only be stored as a total number of sales per product\ntype per day, rather than as individual items sold to individual customers.\n‚The volume of data involved. The amount of data that is available to an analytics\nproject is important because (a) some modern datasets are so large that they can stretch\neven state-of-the-art machine learning tools; and (b) conversely, very small datasets can\nlimit our ability to evaluate the expected performance of a model after deployment.\n‚The time horizon for which data is available. It is important that the data available\ncovers the period required for the analytics solution. For example, in an online gaming\nscenario, it might be possible to ﬁnd out every customer’s account balance today but\nutterly impossible to ﬁnd out what their balance was last month, or even yesterday.\nThe second issue affecting the feasibility of an analytics solution is the ability of the\nbusiness to utilize the insight that the solution provides. If a business is required to drasti-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":81,"page_label":"27","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.2 Assessing Feasibility 27\ncally revise all their processes to take advantage of the insights that can be garnered from a\npredictive model, the business may not be ready to do this no matter how good the model\nis. In many cases the best predictive analytics solutions are those that ﬁt easily into an\nexisting business process.\nBased on analysis of the associated data and capacity requirements, the analytics prac-\ntitioner can assess the feasibility of each predictive analytics solution proposed to address\na business problem. This analysis will eliminate some solutions altogether and for those\nsolutions that appear feasible will generate a list of the data and capacity required for suc-\ncessful implementation. Those solutions that are deemed feasible should then be presented\nto the business, and one or more should be selected for implementation.\nAs part of the process of agreeing on the solution to pursue, the analytics practitioner\nmust agree with the business, as far as possible, on the goals that will deﬁne a successful\nmodel implementation. These goals could be speciﬁed in terms of the required accuracy\nof the model and/or the impact of the model on the business.\n2.2.1 Case Study: Motor Insurance Fraud\nReturning to the motor insurance fraud detection case study, below we evaluate the feasibil-\nity of each proposed analytics solution in terms of data and business capacity requirements.\n‚[Claim prediction] Data Requirements: This solution would require that a large collec-\ntion of historical claims marked as fraudulent andnon-fraudulent exist. Similarly, the\ndetails of each claim, the related policy, and the related claimant would need to be avail-\nable. Capacity Requirements: Given that the insurance company already has a claims\ninvestigation team, the main requirements would be that a mechanism could be put in\nplace to inform claims investigators that some claims were prioritized above others. This\nwould also require that information about claims become available in a suitably timely\nmanner so that the claims investigation process would not be delayed by the model.\n‚[Member prediction] Data Requirements: This solution would not only require that a\nlarge collection of claims labeled as either fraudulent ornon-fraudulent exist with all\nrelevant details, but also that all claims and policies can be connected to an identiﬁable\nmember. It would also require that any changes to a policy are recorded and available","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":81,"page_label":"27","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"investigation team, the main requirements would be that a mechanism could be put in\nplace to inform claims investigators that some claims were prioritized above others. This\nwould also require that information about claims become available in a suitably timely\nmanner so that the claims investigation process would not be delayed by the model.\n‚[Member prediction] Data Requirements: This solution would not only require that a\nlarge collection of claims labeled as either fraudulent ornon-fraudulent exist with all\nrelevant details, but also that all claims and policies can be connected to an identiﬁable\nmember. It would also require that any changes to a policy are recorded and available\nhistorically. Capacity Requirements: This solution ﬁrst assumes that it is possible to\nrun a process every quarter that performs an analysis of the behavior of each customer.\nMore challenging, there is the assumption that the company has the capacity to contact\nmembers based on this analysis and can design a way to discuss this issue with customers\nhighlighted as likely to commit fraud without damaging the customer relationship so\nbadly as to lose the customer. Finally, there are possibly legal restrictions associated\nwith making this kind of contact.\n‚[Application prediction] Data Requirements: Again, a historical collection of claims\nmarked as fraudulent ornon-fraudulent along with all relevant details would be required.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":82,"page_label":"28","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"28 Chapter 2 Data to Insights to Decisions\nIt would also be necessary to be able to connect these claims back to the policies to which\nthey belong and to the application details provided when the member ﬁrst applied. It is\nlikely that the data required for this solution would stretch back over many years, as\nthe time between making a policy application and making a claim could cover decades.\nCapacity Requirements: The challenge in this case would be to integrate the automated\napplication assessment process into whatever application approval process currently ex-\nists within the company.\n‚[Payment prediction] Data Requirements: This solution would require the full details\nof policies and claims as well as data on the original amount speciﬁed in a claim and the\namount ultimately paid out. Capacity Requirements: Again, this solution assumes that\nthe company has the potential to run this model in a timely fashion whenever new claims\nrise and also has the capacity to make offers to claimants. This assumes the existence of\na customer contact center or something similar.\nFor the purposes of the case study, we assume that after the feasibility review, it was\ndecided to proceed with the claim prediction solution, in which a model will be built that\ncan predict the likelihood that an insurance claim is fraudulent.\n2.3 Designing the Analytics Base Table\nOnce we have decided which analytics solution we are going to develop in response to\na business problem, we need to begin to design the data structures that will be used to\nbuild, evaluate, and ultimately deploy the model. This work sits primarily in the Data\nUnderstanding phase of the CRISP-DM process (see Figure 1.4[16]) but also overlaps with\ntheBusiness Understanding andData Preparation phases (remember that the CRISP-\nDM process is not strictly linear).\nThe basic data requirements for predictive models are surprisingly simple. To build a\npredictive model, we need a large dataset of historical examples of the scenario for which\nwe will make predictions. Each of these historical examples must contain sufﬁcient data to\ndescribe the scenario and the outcome that we are interested in predicting. So, for example,\nif we are trying to predict whether or not insurance claims are fraudulent, we require a large\ndataset of historical insurance claims, and for each one we must know whether or not that\nclaim was found to be fraudulent.\nThe basic structure in which we capture these historical datasets is the analytics base","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":82,"page_label":"28","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"DM process is not strictly linear).\nThe basic data requirements for predictive models are surprisingly simple. To build a\npredictive model, we need a large dataset of historical examples of the scenario for which\nwe will make predictions. Each of these historical examples must contain sufﬁcient data to\ndescribe the scenario and the outcome that we are interested in predicting. So, for example,\nif we are trying to predict whether or not insurance claims are fraudulent, we require a large\ndataset of historical insurance claims, and for each one we must know whether or not that\nclaim was found to be fraudulent.\nThe basic structure in which we capture these historical datasets is the analytics base\ntable (ABT ), a schematic of which is shown in Table 2.1[29]. An analytics base table is a\nsimple, ﬂat, tabular data structure made up of rows and columns. The columns are divided\ninto a set of descriptive features and a single target feature . Each row contains a value\nfor each descriptive feature and the target feature and represents an instance about which\na prediction can be made.\nAlthough the ABT is the key structure that we use in developing machine learning mod-\nels, data in organizations is rarely kept in neat tables ready to be used to build predictive","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":83,"page_label":"29","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.3 Designing the Analytics Base Table 29\nTable 2.1\nThe basic structure of an analytics base table—descriptive features and a target feature.\nTarget\nDescriptive Features Feature\n——- ——- ——- ——- ——- ——- ——- ——-\n——- ——- ——- ——- ——- ——- ——- ——-\n——- ——- ——- ——- ——- ——- ——- ——-\n——- ——- ——- ——- ——- ——- ——- ——-\nmodels. Instead, we need to construct the ABT from the raw data sources that are available\nin an organization. These may be very diverse in nature. Figure 2.1[29]illustrates some of\nthe different data sources that are typically combined to create an ABT.\nBefore we can start to aggregate the data from these different sources, however, a signiﬁ-\ncant amount of work is required to determine the appropriate design for the ABT. In design-\ning an ABT, the ﬁrst decision an analytics practitioner needs to make is on the prediction\nsubject for the model they are trying to build. The prediction subject deﬁnes the basic\nlevel at which predictions are made, and each row in the ABT will represent one instance\nof the prediction subject—the phrase one-row-per-subject is often used to describe this\nstructure. For example, for the analytics solutions proposed for the motor insurance fraud\nscenario, the prediction subject of the claim prediction and payment prediction models\nFigure 2.1\nThe different data sources typically combined to create an analytics base table.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":84,"page_label":"30","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"30 Chapter 2 Data to Insights to Decisions\nwould be an insurance claim; for the member prediction model, the prediction subject\nwould be a member; and for the application prediction model, it would be an application.\nEach row in an ABT is composed of a set of descriptive features and a target feature. The\nactual features themselves can be based on any of the data sources within an organization,\nand deﬁning them can appear to be a mammoth task at ﬁrst. This task can be made easier\nby making a hierarchical distinction between the actual features contained in an ABT and\na set of domain concepts upon which features are based—see Figure 2.2[30].\nAnalytics\nSolution\nDomain\nConceptDomain\nConceptTarget\nConcept\nDomain\nSubconceptDomain\nSubconceptDomain\nSubconceptDomain\nSubconcept\nFeature Feature Feature Feature Feature Feature Feature FeatureTarget\nFeature\nFigure 2.2\nThe hierarchical relationship between an analytics solution, domain concepts, and descriptive fea-\ntures.\nA domain concept is a high-level abstraction that describes some characteristic of the\nprediction subject from which we derive a set of concrete features that will be included\nin an ABT. If we keep in mind that the ultimate goal of an analytics solution is to build\na predictive model that predicts a target feature from a set of descriptive features, domain\nconcepts are the characteristics of the prediction subject that domain experts and analytics\nexperts believe are likely to be useful in making this prediction. Often, in a collaboration\nbetween analytics experts and domain experts, we develop a hierarchy of domain con-\ncepts that starts from the analytics solution, proceeds through a small number of levels\nof abstraction to result in concrete descriptive features. Examples of domain concepts in-\nclude customer value ,behavioral change ,product usage mix , and customer lifecycle stage .\nThese are abstract concepts that are understood to be likely important factors in making\npredictions. At this stage we do not worry too much about exactly how a domain concept\nwill be converted into a concrete feature, but rather try to enumerate the different areas\nfrom which features will arise.\nObviously, the set of domain concepts that are important change from one analytics\nsolution to another. However, there are a number of general domain concepts that are often\nuseful:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":85,"page_label":"31","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.3 Designing the Analytics Base Table 31\n‚Prediction Subject Details: Descriptive details of any aspect of the prediction subject.\n‚Demographics: Demographic features of users or customers such as age, gender, occu-\npation, and address.\n‚Usage: Thefrequency andrecency with which customers or users have interacted with\nan organization. The monetary value of a customer’s interactions with a service. The\nmixof products or services offered by the organization that a customer or user has used.\n‚Changes in Usage: Any changes in the frequency, recency, or monetary value of a\ncustomer’s or user’s interactions with an organization (for example, has a cable TV sub-\nscriber changed packages in recent months?).\n‚Special Usage: How often a user or customer used services that an organization consid-\ners special in some way in the recent past (for example, has a customer called a customer\ncomplaints department in the last month?).\n‚Lifecycle Phase: The position of a customer or user in their lifecycle (for example, is a\ncustomer a new customer, a loyal customer, or a lapsing customer?).\n‚Network Links: Links between an item and other related items (for example, links\nbetween different customers or different products, or social network links between cus-\ntomers).\nThe actual process for determining domain concepts is essentially one of knowledge\nelicitation —attempting to extract from domain experts the knowledge about the scenario\nwe are trying to model. Often, this process will take place across multiple meetings, in-\nvolving the analytics and domain experts, where the set of relevant domain concepts for\nthe analytics solution are developed and reﬁned.\n2.3.1 Case Study: Motor Insurance Fraud\nAt this point in the motor insurance fraud detection project, we have decided to proceed\nwith the proposed claim prediction solution, in which a model will be built that can predict\nthe likelihood that an insurance claim is fraudulent. This system will examine new claims\nas they arise and ﬂag for further investigation those that look like they might be fraud\nrisks. In this instance the prediction subject is an insurance claim, and so the ABT for this\nproblem will contain details of historical claims described by a set of descriptive features\nthat capture likely indicators of fraud, and a target feature indicating whether a claim was\nultimately considered fraudulent. The domain concepts in this instance will be concepts","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":85,"page_label":"31","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"At this point in the motor insurance fraud detection project, we have decided to proceed\nwith the proposed claim prediction solution, in which a model will be built that can predict\nthe likelihood that an insurance claim is fraudulent. This system will examine new claims\nas they arise and ﬂag for further investigation those that look like they might be fraud\nrisks. In this instance the prediction subject is an insurance claim, and so the ABT for this\nproblem will contain details of historical claims described by a set of descriptive features\nthat capture likely indicators of fraud, and a target feature indicating whether a claim was\nultimately considered fraudulent. The domain concepts in this instance will be concepts\nfrom within the insurance domain that are likely to be important in determining whether a\nclaim is fraudulent. Figure 2.3[32]shows some domain concepts that are likely to be useful in\nthis case. This set of domain concepts would have been determined through consultations\nbetween the analytics practitioner and domain experts within the business.\nThe domain concepts shown here are Policy Details , which covers information relating\nto the policy held by the claimant (such as the age of the policy and the type of the policy);\nClaim Details , which covers the details of the claim itself (such as the incident type and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":86,"page_label":"32","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"32 Chapter 2 Data to Insights to Decisions\nMotor Insurance\nClaim Fraud\nPrediction\nPolicy\nDetailsClaim\nDetailsClaimant\nHistoryClaimant\nLinksClaimant\nDemographicsFraud\nOutcome\nClaim\nTypesClaim\nFrequencyLinks with\nOther ClaimsLinks with\nCurrent Claim\nFigure 2.3\nExample domain concepts for a motor insurance fraud prediction analytics solution.\nclaim amount); Claimant History , which includes information on previous claims made\nby the claimant (such as the different types of claims they have made in the past and the\nfrequency of past claims); Claimant Links , which captures links between the claimant and\nany other people involved in the claim (for example, the same people being involved in\nmultiple insurance claims together is often an indicator of fraud); and Claimant Demo-\ngraphics , which covers the demographic details of the claimant (such as age, gender, and\noccupation). Finally, a domain concept, Fraud Outcome , is included to cover the target\nfeature. It is important that this is included at this stage because target features often need\nto be derived from multiple raw data sources, and the effort that will be involved in this\nshould not be forgotten.\nIn Figure 2.3[32]the domain concepts Claimant History andClaimant Links have both\nbeen broken down into a number of domain subconcepts . In the case of Claimant His-\ntory, the domain subconcept of Claim Types explicitly recognizes the importance of de-\nsigning descriptive features to capture the different types of claims the claimant has been\ninvolved in in the past, and the Claim Frequency domain subconcept identiﬁes the need\nto have descriptive features relating to the frequency with which the claimant has been in-\nvolved in claims. Similarly, under Claimant Links , the Links with Other Claims andLinks\nwith Current Claim domain subconcepts highlight the fact that the links to or from this\nclaimant can be broken down into links related to the current claim and links relating to\nother claims. The expectation is that each domain concept, or domain subconcept, will\nlead to one or more actual descriptive features derived directly from organizational data\nsources. Together these descriptive features will make up the ABT.\n2.4 Designing and Implementing Features\nOnce domain concepts have been agreed on, the next task is to design and implement con-\ncrete features based on these concepts. A feature is any measure derived from a domain","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":86,"page_label":"32","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"volved in claims. Similarly, under Claimant Links , the Links with Other Claims andLinks\nwith Current Claim domain subconcepts highlight the fact that the links to or from this\nclaimant can be broken down into links related to the current claim and links relating to\nother claims. The expectation is that each domain concept, or domain subconcept, will\nlead to one or more actual descriptive features derived directly from organizational data\nsources. Together these descriptive features will make up the ABT.\n2.4 Designing and Implementing Features\nOnce domain concepts have been agreed on, the next task is to design and implement con-\ncrete features based on these concepts. A feature is any measure derived from a domain\nconcept that can be directly included in an ABT for use by a machine learning algorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":87,"page_label":"33","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 33\nImplementing features is often a process of approximation through which we attempt to\nexpress as much of each domain concept as possible from the data sources that are avail-\nable to us. Often it will take multiple features to express a domain concept. Also, we\nmay have to use some proxy features to capture something that is closely related to a\ndomain concept when direct measurement is not possible. In some extreme cases we may\nhave to abandon a domain concept completely if the data required to express it isn’t avail-\nable. Consequently, understanding and exploring the data sources related to each domain\nconcept that are available within an organization is a fundamental component of feature\ndesign. Although all the factors relating to data that were considered during the feasibility\nassessment of the analytics solution2are still relevant, three key data considerations are\nparticularly important when we are designing features.\nThe ﬁrst consideration is data availability , because we must have data available to im-\nplement any feature we would like to use. For example, in an online payments service\nscenario, we might deﬁne a feature that calculates the average of a customer’s account bal-\nance over the past six months. Unless the company maintains a historical record of account\nbalances covering the full six-month period, however, it will not be possible to implement\nthis feature.\nThe second consideration is the timing with which data becomes available for inclusion\nin a feature. With the exception of the deﬁnition of the target feature, data that will be used\nto deﬁne a feature must be available before the event around which we are trying to make\npredictions occurs. For example, if we were building a model to predict the outcomes of\nsoccer matches, we might consider including the attendance at the match as a descriptive\nfeature. The ﬁnal attendance at a match is not available until midway through the game, so\nif we were trying to make predictions before kickoff, this feature would not be feasible.\nThe third consideration is the longevity of any feature we design. There is the potential\nfor features to go stale if something about the environment from which they are generated\nchanges. For example, to make predictions on the outcome of loans granted by a bank,\nwe might use the borrower’s salary as a descriptive feature. Salaries, however, change all","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":87,"page_label":"33","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"predictions occurs. For example, if we were building a model to predict the outcomes of\nsoccer matches, we might consider including the attendance at the match as a descriptive\nfeature. The ﬁnal attendance at a match is not available until midway through the game, so\nif we were trying to make predictions before kickoff, this feature would not be feasible.\nThe third consideration is the longevity of any feature we design. There is the potential\nfor features to go stale if something about the environment from which they are generated\nchanges. For example, to make predictions on the outcome of loans granted by a bank,\nwe might use the borrower’s salary as a descriptive feature. Salaries, however, change all\nthe time based on inﬂation and other socioeconomic factors. If we were to use a model\nthat includes salary values over an extended period (for example, 10 years) the salary\nvalues used to initially train the model may have no relationship to the values that would\nbe presented to the model later on. One way to extend the longevity of a feature is to\nuse a derived ratio instead of a raw feature. For example, in the loan scenario a ratio\nbetween salary and requested loan amount might have a much longer useful life span than\nthe salary and loan amount values alone. As a result of these considerations, feature design\n2. See the discussion in Section 2.1[23]relating to data availability, data connections, data granularity, data vol-\nume, and data time horizons.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":88,"page_label":"34","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"34 Chapter 2 Data to Insights to Decisions\nand implementation is an iterative process in which data exploration informs the design\nand implementation of features, which in turn inform further data exploration, and so on.\n2.4.1 Different Types of Data\nThe data that the features in an ABT contain can be of a number of different types:\n‚Numeric: True numeric values that allow arithmetic operations (e.g., price, age)\n‚Interval: Values that allow ordering and subtraction, but do not allow other arithmetic\noperations (e.g., date, time)\n‚Ordinal: Values that allow ordering but do not permit arithmetic (e.g., size measured as\nsmall, medium, or large)\n‚Categorical: A ﬁnite set of values that cannot be ordered and allow no arithmetic (e.g.,\ncountry, product type)\n‚Binary: A set of just two values (e.g., gender)\n‚Textual: Free-form, usually short, text data (e.g., name, address)\nFigure 2.4[35]shows examples of these different data types. We often reduce this catego-\nrization to just two data types: continuous (encompassing the numeric and interval types),\nandcategorical (encompassing the categorical, ordinal, binary, and textual types). When\nwe talk about categorical features, we refer to the set of possible values that a categorical\nfeature can take as the levels of the feature or the domain of the feature. For example, in\nFigure 2.4[35]the levels of the C REDIT RATING feature are {aa,a,b,c}and the levels of the\nGENDER feature are {male ,female }. As we will see when we look at the machine learning\nalgorithms covered in Chapters 4[117]to 7[311], the presence of different types of descriptive\nand target features can have a big impact on how an algorithm works.\n2.4.2 Different Types of Features\nThe features in an ABT can be of two types: raw features orderived features . Raw\nfeatures are features that come directly from raw data sources. For example, customer age,\ncustomer gender, loan amount, or insurance claim type are all descriptive features that we\nwould most likely be able to transfer directly from a raw data source to an ABT.\nDerived descriptive features do not exist in any raw data source, so they must be con-\nstructed from data in one or more raw data sources. For example, average customer pur-\nchases per month, loan-to-value ratios, or changes in usage frequencies for different peri-\nods are all descriptive features that could be useful in an ABT but that most likely need to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":88,"page_label":"34","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4.2 Different Types of Features\nThe features in an ABT can be of two types: raw features orderived features . Raw\nfeatures are features that come directly from raw data sources. For example, customer age,\ncustomer gender, loan amount, or insurance claim type are all descriptive features that we\nwould most likely be able to transfer directly from a raw data source to an ABT.\nDerived descriptive features do not exist in any raw data source, so they must be con-\nstructed from data in one or more raw data sources. For example, average customer pur-\nchases per month, loan-to-value ratios, or changes in usage frequencies for different peri-\nods are all descriptive features that could be useful in an ABT but that most likely need to\nbe derived from multiple raw data sources. The variety of derived features that we might\nwish to use is limitless. For example, consider the number of features we can derive from\nthe monthly payment a customer makes on an electricity bill. From this single raw data\npoint, we can easily derive features that store the average payment over six months; the\nmaximum payment over six months; the minimum payment over six months; the average\npayment over three months; the maximum payment over three months; the minimum pay-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":89,"page_label":"35","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 35\nID  NAME DATE OF \nBIRTH GENDER  CREDIT  \nRATING COUNTR Y SALAR Y \n0034 Brian 22/05/78 male aa Ireland 67,000 \n0175 Mary 04/06/45 female c France 65,000 \n0456 Sinead 29/02/82 female b Ireland 112,000 \n0687 Paul 11/11/67 male a USA 34,000 \n0982 Donald 01/12/75 male b Australia 88,000 \n1103 Agnes 17/09/76 female aa Sweden 154,000 Ordinal\nTextual\nIntervalOrdinal\nCategorical\nNumericBinary\nFigure 2.4\nSample descriptive feature data illustrating numeric, binary, ordinal, interval, categorical, and textual\ntypes.\nment over three months; a ﬂag to indicate that a missed payment has occurred over the last\nsix months; a mapping of the last payment made to a low,medium , orhigh level; the ratio\nbetween the current and previous bill payments, and many more.\nDespite this limitless variety, however, there are a number of common derived feature\ntypes:\n‚Aggregates: These are aggregate measures deﬁned over a group or period and are usu-\nally deﬁned as the count, sum, average, minimum, or maximum of the values within a\ngroup. For example, the total number of insurance claims that a member of an insurance\ncompany has made over his or her lifetime might be a useful derived feature. Similarly,\nthe average amount of money spent by a customer at an online retailer over periods of\none, three, and six months might make an interesting set of derived features.\n‚Flags: Flags are binary features that indicate the presence or absence of some character-\nistic within a dataset. For example, a ﬂag indicating whether or not a bank account has\never been overdrawn might be a useful descriptive feature.\n‚Ratios: Ratios are continuous features that capture the relationship between two or more\nraw data values. Including a ratio between two values can often be much more powerful\nin a predictive model than including the two values themselves. For example, in a bank-\ning scenario, we might include a ratio between a loan applicant’s salary and the amount","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":90,"page_label":"36","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"36 Chapter 2 Data to Insights to Decisions\nfor which they are requesting a loan rather than including these two values themselves.\nIn a mobile phone scenario, we might include three ratio features to indicate the mix\nbetween voice, data, and SMS services that a customer uses.\n‚Mappings: Mappings are used to convert continuous features into categorical features\nand are often used to reduce the number of unique values that a model will have to deal\nwith. For example, rather than using a continuous feature measuring salary, we might\ninstead map the salary values to low,medium , and high levels to create a categorical\nfeature.\n‚Other: There are no restrictions to the ways in which we can combine data to make\nderived features. One especially creative example of feature design was when a large\nretailer wanted to use the level of activity at a competitor’s stores as a descriptive feature\nin one of their analytics solutions. Obviously, the competitor would not give the retailer\nthis information, and so the analytics team at the retailer sought to ﬁnd some proxy\nfeature that would give them much the same information. Being a large retailer, they had\nconsiderable resources at their disposal, one of which was the ability to regularly take\nhigh-resolution satellite photos. Using satellite photos of their competitor’s premises, the\nretailer was able to count the number of cars in the competitor’s parking lots and use this\nas a proxy measure of activity within the competitor’s stores!\nAlthough in some applications the target feature is a raw value copied directly from an\nexisting data source, in many others it must be derived. Implementing the target feature\nfor an ABT can demand signiﬁcant effort. For example, consider a problem in which we\nare trying to predict whether a customer will default on a loan obligation. Should we count\none missed payment as a default or, to avoid predicting that good customers will default,\nshould we consider a customer to have defaulted only after they miss three consecutive\npayments? Or three payments in a six-month period? Or two payments in a ﬁve-month\nperiod? Just like descriptive features, target features are based on a domain concept, and\nwe must determine what actual implementation is useful, feasible, and correct according to\nthe speciﬁcs of the domain in question. In deﬁning target features, it is especially important\nto seek input from domain experts.\n2.4.3 Handling Time","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":90,"page_label":"36","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"are trying to predict whether a customer will default on a loan obligation. Should we count\none missed payment as a default or, to avoid predicting that good customers will default,\nshould we consider a customer to have defaulted only after they miss three consecutive\npayments? Or three payments in a six-month period? Or two payments in a ﬁve-month\nperiod? Just like descriptive features, target features are based on a domain concept, and\nwe must determine what actual implementation is useful, feasible, and correct according to\nthe speciﬁcs of the domain in question. In deﬁning target features, it is especially important\nto seek input from domain experts.\n2.4.3 Handling Time\nMany of the predictive models that we build are propensity models , which predict the\nlikelihood (or propensity) of a future outcome based on a set of descriptive features de-\nscribing the past. For example, the goal in the insurance claim fraud scenario we have\nbeen considering is to make predictions about whether an insurance claim will turn out\nto be fraudulent after investigation based on the details of the claim itself and the details\nof the claimant’s behavior in the time preceding the claim . Propensity models inherently\nhave a temporal element, and when this is the case, we must take time into account when\ndesigning the ABT. For propensity modeling , there are two key periods: the observa-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":91,"page_label":"37","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 37\ntion period , over which descriptive features are calculated, and the outcome period , over\nwhich the target feature is calculated.3\nIn some cases the observation period and outcome period are measured over the same\ntime for all prediction subjects. Consider the task of predicting the likelihood that a cus-\ntomer will buy a new product based on past shopping behavior: features describing the past\nshopping behavior are calculated over the observation period, while the outcome period is\nthe time during which we observe whether the customer bought the product. In this situa-\ntion, the observation period for all the prediction subjects, in this case, customers, might be\ndeﬁned as the six months prior to the launch of the new product, and the outcome period\nmight cover the three months after the launch. Figure 2.5(a)[38]shows these two different\nperiods, assuming that the customer’s shopping behavior was measured from August 2012\nthrough January 2013, and that whether they bought the product of interest was observed\nfrom February 2013 through April 2013. Figure 2.5(b)[38]illustrates how the observation\nand outcome period for multiple customers are measured over the same period.\nOften, however, the observation period and outcome period will be measured over dif-\nferent dates for each prediction subject. Figure 2.6(a)[39]shows an example in which, rather\nthan being deﬁned by a ﬁxed date, the observation period and outcome period are deﬁned\nrelative to an event that occurs at different dates for each prediction subject. The insurance\nclaims fraud scenario we have been discussing throughout this section is a good example\nof this. In this example the observation period and outcome period are both deﬁned relative\nto the date of the claim event, which will happen on different dates for different claims.\nThe observation period is the time before the claim event, across which the descriptive fea-\ntures capturing the claimant’s behavior are calculated, while the outcome period is the time\nimmediately after the claim event, during which it will emerge whether the claim is fraud-\nulent or genuine. Figure 2.6(a)[39]shows an illustration of this kind of data, while Figure\n2.6(b)[39]shows how this is aligned so that descriptive and target features can be extracted\nto build an ABT. Note that in Figure 2.6(b)[39]the month names have been abstracted and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":91,"page_label":"37","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of this. In this example the observation period and outcome period are both deﬁned relative\nto the date of the claim event, which will happen on different dates for different claims.\nThe observation period is the time before the claim event, across which the descriptive fea-\ntures capturing the claimant’s behavior are calculated, while the outcome period is the time\nimmediately after the claim event, during which it will emerge whether the claim is fraud-\nulent or genuine. Figure 2.6(a)[39]shows an illustration of this kind of data, while Figure\n2.6(b)[39]shows how this is aligned so that descriptive and target features can be extracted\nto build an ABT. Note that in Figure 2.6(b)[39]the month names have been abstracted and\nare now deﬁned relative to the transition between the observation and outcome periods.\nWhen time is a factor in a scenario, the descriptive features and the target feature will\nnot necessarily both be time dependent. In some cases only the descriptive features have a\ntime component to them, and the target feature is time independent. Conversely, the target\nfeature may have a time component and the descriptive features may not.\nNext-best-offer models provide an example scenario where the descriptive features are\ntime dependent but the target feature is not. A next-best-offer model is used to determine\nthe least expensive incentive that needs to be offered to a customer who is considering\ncanceling a service, for example, a mobile phone contract, in order to make them reconsider\n3. It is important to remember for this discussion that all the data from which we construct an ABT for training\nand evaluating a model will be historical data.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":92,"page_label":"38","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"38 Chapter 2 Data to Insights to Decisions\n2012$ 2013$\nJun$ Jul$ Aug$ Sep$ Oct$ Nov$ Dec$ Jan$ Feb$ Mar$ Apr$ May$\nObserva(on*Period* Outcome*Period*\n(a) Observation period and outcome period\n2012% 2013%\nJun% Jul% Aug% Sep% Oct% Nov% Dec% Jan% Feb% Mar% Apr% May%\n(b) Observation and outcome periods for multiple customers (each line rep-\nresents a customer)\nFigure 2.5\nModeling points in time using an observation period and an outcome period.\nand stay. In this case the customer contacting the company to cancel their service is the\nkey event in time. The observation period that the descriptive features will be based on\nis the customer’s entire behavior up to the point at which they make this contact. There\nis no outcome period, as the target feature is determined by whether the company is able\nto entice the customer to reconsider and, if so, the incentive that was required to do this.\nFigure 2.7[39]illustrates this scenario.\nLoan default prediction is an example where the deﬁnition of the target feature has a\ntime element but the descriptive features are time independent. In loan default prediction,\nthe likelihood that an applicant will default on a loan is predicted based on the information\nthe applicant provides on the application form. There really isn’t an observation period in\nthis case, as all descriptive features will be based on information provided by the applicant\non the application form, rather than on observing the applicant’s behavior over time.4The\noutcome period in this case is considered the period of the lifetime of the loan during\n4. Some might argue that the information on the application form summarizes an applicant’s entire life, so this\nconstitutes the observation period in this case!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":93,"page_label":"39","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 39\n2012% 2013%\nJun% Jul% Aug% Sep% Oct% Nov% Dec% Jan% Feb% Mar% Apr% May%\n(a) Actual\nObservaCon%Period% Outcome%Period%\n6% 5% 4% 3% 2% 1% 1% 2% 3% (b) Aligned\nFigure 2.6\nObservation and outcome periods deﬁned by an event rather than by a ﬁxed point in time (each line\nrepresents a prediction subject, and stars signify events). (a) shows the actual data, and (b) shows the\nevent-aligned data.\n2013%\nJan% Feb% Mar% Apr% May% Jun% Jul% Aug% Sep% Oct% Nov% Dec%\n(a) Actual\nObserva=on%Period%\n12% 11% 10% 9% 8% 7% 6% 5% 4% 3% 2% 1% (b) Aligned\nFigure 2.7\nModeling points in time for a scenario with no real outcome period (each line represents a customer,\nand stars signify events). (a) shows the actual data, and (b) shows the event-aligned data.\nwhich the applicant will have either fully repaid or defaulted on the loan. In order to\nbuild an ABT for such a problem, a historical dataset of application details and subsequent\nrepayment behavior is required (this might stretch back over multiple years depending on\nthe terms of the loans in question). This scenario is illustrated in Figure 2.8[40].\n2.4.4 Legal Issues\nData analytics practitioners can often be frustrated by legislation that stops them from\nincluding features that appear to be particularly well suited to an analytics solution in\nan ABT. Organizations must operate within the relevant legislation that is in place in the\njurisdictions in which they operate, and it is important that models are not in breach of this.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":94,"page_label":"40","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"40 Chapter 2 Data to Insights to Decisions\nYear%\n2002% 2003% 2004% 2005% 2006% 2007% 2008% 2009% 2010% 2011% 2012% 2013%\n(a) Actual\nOutcome%Period%\n1% 2% 3% 4% (b) Aligned\nFigure 2.8\nModeling points in time for a scenario with no real observation period (each line represents a cus-\ntomer, and stars signify events). (a) shows the actual data, and (b) shows the event-aligned data.\nThere are signiﬁcant differences in legislation in different jurisdictions, but a couple of key\nrelevant principles almost always apply.\nThe ﬁrst is related to anti-discrimination legislation . Anti-discrimination legislation\nin most jurisdictions prohibits discrimination on the basis of some set of the following\ngrounds: sex, age, race, ethnicity, nationality, sexual orientation, religion, disability, and\npolitical opinions. For example, the United States Civil Rights Act of 19645made it il-\nlegal to discriminate against a person on the basis of race, color, religion, national origin,\nor sex. Subsequent legislation has added to this list (for example, disability was later\nadded as a further basis for non-discrimination). In the European Union the 1999 Treaty\nof Amsterdam6prohibits discrimination on the basis of sex, racial or ethnic origin, reli-\ngion or belief, disability, age, or sexual orientation. The exact implementation details of\nanti-discrimination law change, however, across the countries in the European Union.\nThe impact this has on designing features for inclusion in an ABT is that the use of some\nfeatures in analytics solutions that leads to some people being given preferential treatment\nis in breach of anti-discrimination law. For example, credit scoring models such as the\none discussed in Section 1.2[5]cannot use race as a descriptive feature because this would\ndiscriminate against people on this basis.\nThe second important principle relates to data protection legislation , and in particular\nthe rules surrounding the use of personal data . Personal data is deﬁned as data that relates\nto an identiﬁed or identiﬁable individual, who is known as a data subject . Although,\n5. The full text of the Civil Rights Act of 1964 is available at www.gpo.gov/fdsys/granule/STATUTE-78/\nSTATUTE-78-Pg241/content-detail.html.\n6. The full text of the EU Treaty of Amsterdam is available at www.europa.eu/eu-law/decision-making/treaties/\npdf/treaty ofamsterdam/treaty ofamsterdam en.pdf.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":95,"page_label":"41","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 41\ndata protection legislation changes signiﬁcantly across different jurisdictions, there are\nsome common tenets on which there is broad agreement. The Organisation for Economic\nCo-operation and Development (OECD, 2013) deﬁnes a set of eight general principles of\ndata protection legislation.7For the design of analytics base tables, three are especially\nrelevant: the collection limitation principle , the purpose speciﬁcation principle , and the\nuse limitation principle .\nThe collection limitation principle states that personal data should only be obtained by\nlawful means with the knowledge and consent of a data subject. This can limit the amount\nof data that an organization collects and, sometimes, restricts implementing features to\ncapture certain domain concepts because consent has not been granted to collect the re-\nquired data. For example, the developers of a smartphone app might decide that by turning\non location tracking, they could gather data that would be extremely useful in predicting\nfuture usage of the app. Doing this without the permission of the users of the app, however,\nwould be in breach of this principle.\nThe purpose speciﬁcation principle states that data subjects should be informed of the\npurpose for which data will be used at the time of its collection. The use limitation principle\nadds that collected data should not subsequently be used for purposes other than those\nstated at the time of collection. Sometimes this means that data collected by an organization\ncannot be included in an ABT because this would be incompatible with the original use for\nwhich the data was collected. For example, an insurance company might collect data on\ncustomers’ travel behaviors through their travel insurance policy and then use this data in\na model that predicts personalized prices for life insurance. Unless, however, this second\nuse was stated at the time of collection, this use would be in breach of this principle.\nThe legal considerations surrounding predictive analytics are of growing importance and\nneed to be seriously considered during the design of any analytics project. Although larger\norganizations have legal departments to whom proposed features can be handed over for\nassessment, in smaller organizations analysts are often required to make these assessments\nthemselves, and consequently they need to be aware of the legal implications relating to\ntheir decisions.\n2.4.5 Implementing Features","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":95,"page_label":"41","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"a model that predicts personalized prices for life insurance. Unless, however, this second\nuse was stated at the time of collection, this use would be in breach of this principle.\nThe legal considerations surrounding predictive analytics are of growing importance and\nneed to be seriously considered during the design of any analytics project. Although larger\norganizations have legal departments to whom proposed features can be handed over for\nassessment, in smaller organizations analysts are often required to make these assessments\nthemselves, and consequently they need to be aware of the legal implications relating to\ntheir decisions.\n2.4.5 Implementing Features\nOnce the initial design for the features in an ABT has been completed, we can begin to\nimplement the technical processes that are needed to extract, create, and aggregate the fea-\ntures into an ABT. It is at this point that the distinction between raw and derived features\nbecomes apparent. Implementing a raw feature is simply a matter of copying the rele-\nvant raw value into the ABT. Implementing a derived feature, however, requires data from\nmultiple sources to be combined into a set of single feature values.\n7. The full discussion of these principles is available at www.oecd.org/sti/ieconomy/privacy.htm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":96,"page_label":"42","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"42 Chapter 2 Data to Insights to Decisions\nA few key data manipulation operations are frequently used to calculate derived feature\nvalues: joining data sources, ﬁltering rows in a data source, ﬁltering ﬁelds in a data source,\nderiving new features by combining or transforming existing features, and aggregating data\nsources. Data manipulation operations are implemented in and performed by database\nmanagement systems ,data management tools , or data manipulation tools , and are\noften referred to as an extract-transform-load (ETL ) process.\n2.4.6 Case Study: Motor Insurance Fraud\nLet’s return to the motor insurance fraud detection solution to consider the design and im-\nplementation of the features that will populate the ABT. As we noted in our discussion\nregarding handling time, the motor insurance claim prediction scenario is a good example\nof a situation in which the observation period and outcome period are measured over dif-\nferent dates for each insurance claim (the prediction subject for this case study). For each\nclaim the observation and output periods are deﬁned relative to the speciﬁc date of that\nclaim. The observation period is the time prior to the claim event, over which the descrip-\ntive features capturing the claimant’s behavior are calculated, and the outcome period is\nthe time immediately after the claim event, during which it will emerge whether the claim\nis fraudulent or genuine.\nTheClaimant History domain concept that we developed for this scenario indicates the\nimportance of information regarding the previous claims made by the claimant to the task\nof identifying fraudulent claims. This domain concept is inherently related to the notion\nof an observation period, and as we will see, the descriptive features derived from the\ndomain subconcepts under Claimant History are time dependent. For example, the Claim\nFrequency domain subconcept under the Claimant History concept should capture the fact\nthat the number of claims a claimant has made in the past has an impact on the likelihood\nof a new claim being fraudulent. This could be expressed in a single descriptive feature\ncounting the number of claims that the claimant has made in the past. This single value,\nhowever, may not capture all the relevant information. Adding extra descriptive features\nthat give a more complete picture of a domain concept can lead to better predictive models.\nIn this example we might also include the number of claims made by the claimant in the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":96,"page_label":"42","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"domain subconcepts under Claimant History are time dependent. For example, the Claim\nFrequency domain subconcept under the Claimant History concept should capture the fact\nthat the number of claims a claimant has made in the past has an impact on the likelihood\nof a new claim being fraudulent. This could be expressed in a single descriptive feature\ncounting the number of claims that the claimant has made in the past. This single value,\nhowever, may not capture all the relevant information. Adding extra descriptive features\nthat give a more complete picture of a domain concept can lead to better predictive models.\nIn this example we might also include the number of claims made by the claimant in the\nlast three months, the average number of claims made by the claimant per year, and the\nratio of the average number of claims made by the claimant per year to the claims made by\nthe claimant in the last twelve months. Figure 2.9[43]shows these descriptive features in a\nportion of the domain concept diagram.\nTheClaim Types subconcept of the Claim History is also time dependent. This domain\nsubconcept captures the variety of claim types made by the claimant in the past, as these\nmight provide evidence of possible fraud. The features included under this subconcept, all\nof which are derived features, are shown in Figure 2.10[44]. The features place a particular\nemphasis on claims relating to soft tissue injuries (for example, whiplash) because it is\nunderstood within the insurance industry that these are frequently associated with fraud-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":97,"page_label":"43","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.4 Designing and Implementing Features 43\nMotor Insurance\nClaim Fraud Prediction\nClaimant History\n…\nClaim Frequency\n Number of Claims\nin\n Claimant Lifetime\nDerived\nAggregate\n Number of Claims\nby Claimant in \n Last 3 Months\nDerived\nAggregate\n Average Claims\n Per Year\nby Claimant\nDerived\nAggregate\n Ratio of Avg. Claims Per\nYear to Number of\nClaims in last 12 Months\nDerived\nRatio\nFigure 2.9\nA subset of the domain concepts and related features for a motor insurance fraud prediction analytics\nsolution.\nulent claims. The number of soft tissue injury claims the claimant has made in the past\nand the ratio between the number of soft tissue injury claims and other claims made by the\nclaimant are both included as descriptive features in the ABT. A ﬂag is also included to\nindicate that the claimant has had at least one claim refused in the past, because this might\nbe indicative of a pattern of making speculative claims. Finally, a feature is included that\nexpresses the variety of different claim types made by the claimant in the past. This uses\ntheentropy measure that is discussed in Section 4.2[120]as it does a good job of capturing\nin a single number the variety in a set of objects.\nHowever, not all the domain concepts in this scenario are time dependent. The Claim\nDetails domain concept, for example, highlights the importance of the details of the claim\nitself in distinguishing between fraudulent and genuine claims. The type of the claim and\namount of the claim are raw features calculated directly from a claims table contained in\none of the insurance company’s operational databases. A derived feature containing the\nratio between the claim amount and the total value of the premiums paid to date on the\npolicy is included. This is based on an expectation that fraudulent claims may be made\nearly in the lifetime of a policy before too much has been spent on premiums. Finally,\nthe insurance company divides their operations into a number of geographic areas deﬁned\ninternally based on the location of their branches, and a feature is included that maps raw\naddress data to these regions.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":98,"page_label":"44","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"44 Chapter 2 Data to Insights to Decisions\nMotor Insurance\nClaim Fraud Prediction\nClaimant History\nClaim  Types\n Number of Soft\nTissue Claims\nDerived\nAggregate\n Ratio of Soft Tissue\nClaims to Other Claims\nDerived\nRatio\nUnsuccessful\nClaim Made\nDerived\nFlag\nDiversity of Claim Types\n(measured using entropy)\nDerived\nOther\nFigure 2.10\nA subset of the domain concepts and related features for a motor insurance fraud prediction analytics\nsolution.\nTable 2.2[46]illustrates the structure of the ﬁnal ABT that was designed for the motor in-\nsurance claims fraud detection solution.8The table contains more descriptive features than\nthe ones we have discussed in this section.9The table also shows the ﬁrst four instances.\nIf we examine the table closely, we see a number of strange values (for example, ´99,999)\nand a number of missing values. In the next chapter, we describe the process we should\nfollow to evaluate the quality of the data in the ABT and the actions we can take if the\nquality isn’t good enough.\n2.5 Summary\nIt is important to remember that predictive data analytics models built using machine learn-\ning techniques are tools that we can use to help make better decisions within an organiza-\ntion and are not an end in themselves. It is paramount that, when tasked with creating a\npredictive model, we fully understand the business problem that this model is being con-\n8. The table is too wide to ﬁt on a page, so it has been split into three sections.\n9. The mapping between the features we have discussed here and the column names in Table 2.2[46]is as follows:\nNUMBER OF CLAIMANTS : NUM. CLMNTS .; N UMBER OF CLAIMS IN CLAIMANT LIFETIME : NUM. CLAIMS ;\nNUMBER OF CLAIMS BY CLAIMANT IN LAST 3 M ONTHS : NUM. CLAIMS 3 M ONTHS ; AVERAGE CLAIMS\nPERYEAR BY CLAIMANT : AVG. CLAIMS PERYEAR; RATIO OF AVERAGE CLAIMS PERYEAR TO NUMBER\nOFCLAIMS IN LAST 12 M ONTHS : AVG. CLAIMS RATIO ; NUMBER OF SOFT TISSUE CLAIMS : NUM. SOFT\nTISSUE ; RATIO OF SOFT TISSUE CLAIMS TO OTHER CLAIMS : % S OFT TISSUE ; UNSUCCESSFUL CLAIM\nMADE: U NSUCC . C LAIMS ; DIVERSITY OF CLAIM TYPES : CLAIM DIV.; C LAIM AMOUNT : CLAIM AMT.;\nCLAIM TO PREMIUM PAIDRATIO : CLAIM TO PREM .; A CCIDENT REGION : REGION .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":99,"page_label":"45","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.5 Summary 45\nMotor Insurance\nClaim Fraud Prediction\nClaim Details\nInjury Type\nRaw\nClaim Amount\nRaw\nClaim to Premium\nPaid Ratio\nDerived\nRatio\nAccident Region\nDerived\nMapping\nFigure 2.11\nA subset of the domain concepts and related features for a motor insurance fraud prediction analytics\nsolution.\nstructed to address and ensure that it does address it. This is the goal behind the process of\nconverting business problems into analytics solutions as part of the Business Understand-\ning phase of the CRISP-DM process. When undertaking this process, it is important to\ntake into account the availability of data and the capacity of a business to take advantage of\ninsights arising from analytics models, as otherwise it is possible to construct an apparently\naccurate prediction model that is in fact useless.\nPredictive data analytics models are reliant on the data that is used to build them—the an-\nalytics base table (ABT ) is the key data resource in this regard. An ABT, however, rarely\ncomes directly from a single source already existing within an organization. Instead, the\nABT has to be created by combining a range of operational data sources together. The\nmanner in which these data resources should be combined must be designed and imple-\nmented by the analytics practitioner in collaboration with domain experts. An effective\nway in which to do this is to start by deﬁning a set of domain concepts in collaboration\nwith the business, and then designing features that express these concepts in order to form\nthe actual ABT. Domain concepts cover the different aspects of a scenario that are likely\nto be important in the modeling task at hand.\nFeatures (both descriptive and target) are concrete numeric or symbolic representations\nof domain concepts. Features can be of many different types, but it is useful to think\nof a distinction between raw features that come directly from existing data sources and\nderived features that are constructed by manipulating values from existing data sources.\nCommon manipulations used in this process include aggregates, ﬂags, ratios, and map-\npings, although any manipulation is valid. Often multiple features are required to fully\nexpress a single domain concept.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":100,"page_label":"46","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"46 Chapter 2 Data to Insights to Decisions\nTable 2.2\nThe ABT for the motor insurance claims fraud detection solution.\nMARITAL NUM. I NJURY HOSPITAL CLAIM\nID T YPE INC. S TATUS CLMNTS . T YPE STAY AMT.\n1 ci 0 2 soft tissue no 1,625\n2 ci 0 2 back yes 15,028\n3 ci 54,613 married 1 broken limb no -99,999\n4 ci 0 3 serious yes 270,200\n......\nNUM. A VG. A VG. N UM. %\nTOTAL NUM. C LAIMS CLAIMS CLAIMS SOFT SOFT\nID C LAIMED CLAIMS 3 M ONTHS PERYEAR RATIO TISSUE TISSUE\n1 3,250 2 0 1 1 2 1\n2 60,112 1 0 1 1 0 0\n3 0 0 0 0 0 0 0\n4 0 0 0 0 0 0 0\n......\nCLAIM CLAIM\nUNSUCC . A MT. C LAIM TO FRAUD\nID C LAIMS REC. D IV. P REM . R EGION FLAG\n1 2 0 0 32.500 mn 1\n2 0 15,028 0 57.140 dl 0\n3 0 572 0 -89.270 wat 0\n4 0 270,200 0 30.186 dl 0\n......\nThe techniques described in this chapter cover the Business Understanding ,Data Un-\nderstanding , and (partially) Data Preparation phases of the CRISP-DM process. Fig-\nure 2.12[47]shows how the major tasks described in this chapter align with these phases.\nThe next chapter will describe the data understanding and data preparation techniques\nmentioned brieﬂy in this chapter in much more detail. It is important to remember that\nin reality, the Business Understanding ,Data Understanding , and Data Preparation\nphases of the CRISP-DM process are performed iteratively rather than linearly. The curved\narrows in Figure 2.12[47]show the most common iterations in the process.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":101,"page_label":"47","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.6 Further Reading 47\nBuild\t\r  \t\r  \nABT\t\r  Understand\t\r  \nBusiness\t\r  \nProblem\t\r  Propose\t\r  \nAnaly5cs\t\r  \nSolu5ons\t\r  Explore\t\r  \nData\t\r  \t\r  \n(1)\t\r  Assess\t\r  \nAnaly5cs\t\r  \nSolu5ons\t\r  Choose\t\r  \nAnaly5cs\t\r  \nSolu5on\t\r  \nDesign\t\r  \nDomain\t\r  \nConcepts\t\r  Brainstorm\t\r  \nDomain\t\r  \t\r  \nConcepts\t\r  Review\t\r  \nDomain\t\r  \nConcepts\t\r  Agree\t\r  on\t\r  \nAnaly5cs\t\r  \nGoals\t\r  \nExplore\t\r  \nData\t\r  \t\r  \n(2)\t\r  Design\t\r  \nFeatures\t\r  Review\t\r  \nFeatures\t\r  \nClean\t\r  &\t\r  \nPrepare\t\r  \nData\t\r  Business\t\r  \nUnderstanding\t\r  Data\t\r  \nUnderstanding\t\r  Data\t\r  \nPrepara5on\t\r  \nFigure 2.12\nA summary of the tasks in the Business Understanding, Data Understanding, and Data Preparation\nphases of the CRISP-DM process.\n2.6 Further Reading\nOn the topic of converting business problems into analytics solutions, Davenport (2006)\nand Davenport and Kim (2013) are good business-focused sources. Levitt and Dubner\n(2005), Ayres (2008), Silver (2012), and Siegel (2013) all provide nice dicusssions of\ndifferent applications of predictive data analytics.\nThe CRISP-DM process documentation (Chapman et al., 2000) is surprisingly readable,\nand adds a lot of extra detail to the tasks described in this chapter. For details on develop-\ning business concepts and designing features, Svolba (2007) is excellent (the approaches\ndescribed can be applied to any tool, not just SAS, which is the focus of Svolba’s book).\nFor further discussion of the legal issues surrounding data analytics, Tene and Polonetsky\n(2013) and Schwartz (2010) are useful. Chapter 2 of Siegel (2013) discusses the ethical\nissues surrounding predictive analytics.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":102,"page_label":"48","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"48 Chapter 2 Data to Insights to Decisions\n2.7 Exercises\n1.An online movie streaming company has a business problem of growing customer\nchurn —subscription customers canceling their subscriptions to join a competitor.\nCreate a list of ways in which predictive data analytics could be used to help address\nthis business problem. For each proposed approach, describe the predictive model that\nwill be built, how the model will be used by the business, and how using the model\nwill help address the original business problem.\n2.A national revenue commission performs audits on public companies to ﬁnd and ﬁne\ntax defaulters. To perform an audit, a tax inspector visits a company and spends a\nnumber of days scrutinizing the company’s accounts. Because it takes so long and\nrelies on experienced, expert tax inspectors, performing an audit is an expensive exer-\ncise. The revenue commission currently selects companies for audit at random. When\nan audit reveals that a company is complying with all tax requirements, there is a sense\nthat the time spent performing the audit was wasted, and more important, that another\nbusiness who is not tax compliant has been spared an investigation. The revenue com-\nmissioner would like to solve this problem by targeting audits at companies who are\nlikely to be in breach of tax regulations, rather than selecting companies for audit at\nrandom. In this way the revenue commission hopes to maximize the yield from the\naudits that it performs.\nTo help with situational ﬂuency for this scenario, here is a brief outline of how com-\npanies interact with the revenue commission. When a company is formed, it registers\nwith the company registrations ofﬁce. Information provided at registration includes\nthe type of industry the company is involved in, details of the directors of the com-\npany, and where the company is located. Once a company has been registered, it must\nprovide a tax return at the end of every ﬁnancial year. This includes all ﬁnancial de-\ntails of the company’s operations during the year and is the basis of calculating the tax\nliability of a company. Public companies also must ﬁle public documents every year\nthat outline how they have been performing, details of any changes in directorship,\nand so on.\n(a)Propose two ways in which predictive data analytics could be used to help ad-\ndress this business problem.10For each proposed approach, describe the predic-\ntive model that will be built, how the model will be used by the business, and how","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":102,"page_label":"48","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"pany, and where the company is located. Once a company has been registered, it must\nprovide a tax return at the end of every ﬁnancial year. This includes all ﬁnancial de-\ntails of the company’s operations during the year and is the basis of calculating the tax\nliability of a company. Public companies also must ﬁle public documents every year\nthat outline how they have been performing, details of any changes in directorship,\nand so on.\n(a)Propose two ways in which predictive data analytics could be used to help ad-\ndress this business problem.10For each proposed approach, describe the predic-\ntive model that will be built, how the model will be used by the business, and how\nusing the model will help address the original business problem.\n(b)For each analytics solution you have proposed for the revenue commission, outline\nthe type of data that would be required.\n10. Revenue commissioners around the world use predictive data analytics techniques to keep their processes as\nefﬁcient as possible. Cleary and Tax (2011) is a good example.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":103,"page_label":"49","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.7 Exercises 49\n(c)For each analytics solution you have proposed, outline the capacity that the rev-\nenue commission would need in order to utilize the analytics-based insight that\nyour solution would provide.\n3.The table below shows a sample of a larger dataset containing details of policyholders\nat an insurance company. The descriptive features included in the table describe each\npolicy holders’ ID, occupation, gender, age, the value of their car, the type of insurance\npolicy they hold, and their preferred contact channel.\nMOTOR POLICY PREF\nID O CCUPATION GENDER AGE VALUE TYPE CHANNEL\n1 lab tech female 43 42,632 planC sms\n2 farmhand female 57 22,096 planA phone\n3 biophysicist male 21 27,221 planA phone\n4 sheriff female 47 21,460 planB phone\n5 painter male 55 13,976 planC phone\n6 manager male 19 4,866 planA email\n7 geologist male 51 12,759 planC phone\n8 messenger male 49 15,672 planB phone\n9 nurse female 18 16,399 planC sms\n10 ﬁre inspector male 47 14,767 planC email\n(a)State whether each descriptive feature contains numeric, interval, ordinal, categor-\nical, binary, or textual data.\n(b)How many levels does each categorical, binary, or ordinal feature have?\n4.Select one of the predictive analytics models that you proposed in your answer to\nQuestion 2 about the revenue commission for exploration of the design of its analytics\nbase table (ABT ).\n(a)What is the prediction subject for the model that will be trained using this ABT?\n(b)Describe the domain concepts for this ABT.\n(c)Draw a domain concept diagram for the ABT.\n(d)Are there likely to be any legal issues associated with the domain concepts you\nhave included?\n˚5.Although their sales are reasonable, an online fashion retailer is struggling to generate\nthe volume of sales that they had originally hoped for when launching their site. List\na number of ways in which predictive data analytics could be used to help address this\nbusiness problem. For each proposed approach, describe the predictive model that will\nbe built, how the model will be used by the business, and how using the model will\nhelp address the original business problem.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":104,"page_label":"50","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"50 Chapter 2 Data to Insights to Decisions\n˚6.An oil exploration company is struggling to cope with the number of exploratory sites\nthat they need to drill in order to ﬁnd locations for viable oil wells. There are many po-\ntential sites that geologists at the company have identiﬁed, but undertaking exploratory\ndrilling at these sites is very expensive. If the company could increase the percentage\nof sites at which they perform exploratory drilling that actually lead to ﬁnding loca-\ntions for viable wells, they could save a huge amount of money.\nCurrently geologists at the company identify potential drilling sites by manually ex-\namining information from a variety of different sources. These include ordinance sur-\nvey maps, aerial photographs, characteristics of rock and soil samples taken from po-\ntential sites, and measurements from sensitive gravitational and seismic instruments.\n(a)Propose two ways in which predictive data analytics could be used to help ad-\ndress the problem that the oil exploration company is facing. For each proposed\napproach, describe the predictive model that will be built, how the model will\nbe used by the company, and how using the model will help address the original\nproblem.\n(b)For each analytics solution you have proposed, outline the type of data that would\nbe required.\n(c)For each analytics solution you have proposed, outline the capacity that would\nbe needed in order to utilize the analytics-based insight that your solution would\nprovide.\n˚7.Select one of the predictive analytics models that you proposed in your answer to the\nprevious question about the oil exploration company for exploration of the design of\nitsanalytics base table .\n(a)What is the prediction subject for the model that will be trained using this ABT?\n(b)Describe the domain concepts for this ABT.\n(c)Draw a domain concept diagram for the ABT.\n(d)Are there likely to be any legal issues associated with the domain concepts you\nhave included?\n˚8.The following table shows a sample of a larger dataset that has been collected to build\na model to predict whether newly released movies will be a hit or not.11The dataset\ncontains details of movies that have already been released, including their title, running\ntime, rating, genre, year of release, and whether or not the actor Kevin Costner had a\nstarring role. An indicator of whether they were a success—a hit—or not—a miss—\nbased on box ofﬁce returns compared to budget is also included.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":104,"page_label":"50","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"(b)Describe the domain concepts for this ABT.\n(c)Draw a domain concept diagram for the ABT.\n(d)Are there likely to be any legal issues associated with the domain concepts you\nhave included?\n˚8.The following table shows a sample of a larger dataset that has been collected to build\na model to predict whether newly released movies will be a hit or not.11The dataset\ncontains details of movies that have already been released, including their title, running\ntime, rating, genre, year of release, and whether or not the actor Kevin Costner had a\nstarring role. An indicator of whether they were a success—a hit—or not—a miss—\nbased on box ofﬁce returns compared to budget is also included.\n11. This dataset has been artiﬁcially created for this book, but machine learning has been used for this task, for\nexample, by Mishne and Glance (2006).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":105,"page_label":"51","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2.7 Exercises 51\nID T ITLE LENGTH RATING GENRE COSTNER YEAR HIT\n1 Jaws 124 PG action false 1975 hit\n2 Waterworld 135 PG-13 sci-ﬁ true 1995 miss\n3 Hudson Hawk 100 R adventure false 1991 miss\n4 Downfall 156 R drama false 2004 hit\n5 The Postman 177 R action true 1997 miss\n6 Toy Story 81 G children’s false 1995 hit\n7 Field of Dreams 107 G drama true 1989 hit\n8 Amelie 122 R comedy false 2001 hit\n(a)State whether each descriptive feature contains numeric, interval, ordinal, categor-\nical, binary, or textual data.\n(b)How many levels does each categorical, binary, or ordinal feature have?\n˚9.The management of a large hospital group are concerned about the readmission rate\nfor patients who are hospitalized with problems relating to diabetes . An analysis of\nhistorical data suggests that the rate of readmission within 30days of being discharged\nfor patients who were hospitalized for complications relating to diabetes is approxi-\nmately 20%, compared to an overall average for all patients of approximately 11%.\nSometimes patients are readmitted for a recurrence of the same problem for which\nthey were originally hospitalized, but at other times readmission is for different prob-\nlems. Hospital management are concerned that the cause of the high readmittance rate\nfor diabetes patients might be that they are discharged too early or that their care plans\nwhile in the hospital are not addressing all their needs.\nHospital management would like to explore the use of predictive analytics to address\nthis issue.12They would like to reduce the readmittance rate of diabetes patients, while\nat the same time not keeping patients in the hospital longer than they need to be.\n(a)Propose two ways in which predictive data analytics could be used to help address\nthis problem for the hospital group. For each proposed approach, describe the\npredictive model that will be built, how the model will be used by the business,\nand how using the model will help address the original problem.\n(b)For each analytics solution you have proposed for the hospital group, outline the\ntype of data that would be required.\n(c)For each analytics solution you have proposed, outline the capacity that the hospi-\ntal would need in order to use the analytics-based insight that your solution would\nprovide.\n12. There are many applications of predictive analytics in healthcare, and predicting readmittance rates for dia-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":105,"page_label":"51","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"(a)Propose two ways in which predictive data analytics could be used to help address\nthis problem for the hospital group. For each proposed approach, describe the\npredictive model that will be built, how the model will be used by the business,\nand how using the model will help address the original problem.\n(b)For each analytics solution you have proposed for the hospital group, outline the\ntype of data that would be required.\n(c)For each analytics solution you have proposed, outline the capacity that the hospi-\ntal would need in order to use the analytics-based insight that your solution would\nprovide.\n12. There are many applications of predictive analytics in healthcare, and predicting readmittance rates for dia-\nbetes patients, as well as patients suffering from other issues, is well studied, for example, by Rubin (2015) and\nKansagara et al. (2011).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":106,"page_label":"52","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"52 Chapter 2 Data to Insights to Decisions\n˚10.Select one of the predictive analytics models that you proposed in your answer to the\nprevious question about the readmission of diabetes patients for exploration of the\ndesign of its analytics base table .\n(a)What is the prediction subject for the model that will be trained using this ABT?\n(b)Describe the domain concepts for this ABT.\n(c)Draw a domain concept diagram for the ABT.\n(d)Are there likely to be any legal issues associated with the domain concepts you\nhave included?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":107,"page_label":"53","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3 Data Exploration\n“Fail to prepare, prepare to fail. ”\n—Roy Keane\nIn Chapter 2[23]we described the process of moving from a business problem to an analytics\nsolution and, from there, to the design and construction of an analytics base table (ABT ).\nAn ABT for a predictive analytics solution contains a set of instances that are represented\nby a set of descriptive features and a target feature. Before attempting to build predictive\nmodels based on an ABT, it is important that we undertake some exploratory analysis, or\ndata exploration , of the data contained in the ABT. Data exploration is a key part of both\ntheData Understanding andData Preparation phases of CRISP-DM.\nThere are two goals in data exploration. The ﬁrst goal is to fully understand the character-\nistics of the data in the ABT. It is important that for each feature in the ABT, we understand\ncharacteristics such as the types of values a feature can take, the ranges into which the val-\nues in a feature fall, and how the values in a dataset for a feature are distributed across the\nrange that they can take. We refer to this as getting to know the data. The second goal of\ndata exploration is to determine whether or not the data in an ABT suffer from any data\nquality issues that could adversely affect the models that we build. Examples of typical\ndata quality issues include an instance that is missing values for one or more descriptive\nfeatures, an instance that has an extremely high value for a feature, or an instance that has\nan inappropriate level for a feature. Some data quality issues arise due to invalid data and\nwill be corrected as soon as we discover them. Others, however, arise because of perfectly\nvalid data that may cause difﬁculty to some machine learning techniques. We note these\ntypes of data quality issues during exploration for potential handling when we reach the\nModeling phase of a project.\nThe most important tool used during data exploration is the data quality report . This\nchapter begins by describing the structure of a data quality report and explaining how it\nis used to get to know the data in an ABT and to identify data quality issues. We then\ndescribe a number of strategies for handling data quality issues and when it is appropriate\nto use them. Throughout the discussion of the data quality report and how we use it, we","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":108,"page_label":"54","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"54 Chapter 3 Data Exploration\nreturn to the motor insurance fraud case study from Chapter 2[23]. Toward the end of the\nchapter, we introduce some more advanced data exploration techniques that, although not\npart of the standard data quality report, can be useful at this stage of an analytics project\nand present some data preparation techniques that can be applied to the data in an ABT\nprior to modeling.\n3.1 The Data Quality Report\nThedata quality report is the most important tool of the data exploration process. A data\nquality report includes tabular reports (one for continuous features and one for categorical\nfeatures) that describe the characteristics of each feature in an ABT using standard statisti-\ncal measures of central tendency andvariation . The tabular reports are accompanied by\ndata visualizations that illustrate the distribution of the values in each feature in an ABT.\nReaders who are not already familiar with standard measures of central tendency ( mean ,\nmode , and median ), standard measures of variation ( standard deviation andpercentiles ),\nand standard data visualization plots ( bar plots ,histograms , and box plots ) should read\nAppendix A[745]for the necessary introduction.\nThe table in a data quality report that describes continuous features should include a row\ncontaining the minimum, 1stquartile, mean, median, 3rdquartile, maximum, and standard\ndeviation statistics for that feature as well as the total number of instances in the ABT,\nthe percentage of instances in the ABT that are missing a value for each feature and the\ncardinality of each feature, (cardinality measures the number of distinct values present in\nthe ABT for a feature). Table 3.1(a)[55]shows the structure of the table in a data quality\nreport that describes continuous features.\nThe table in the data quality report that describes categorical features should include a\nrow for each feature in the ABT that contains the two most frequent levels for the feature\n(the mode and 2ndmode) and the frequency with which these appear (both as raw frequen-\ncies and as a proportion of the total number of instances in the dataset). Each row should\nalso include the percentage of instances in the ABT that are missing a value for the feature\nand the cardinality of the feature. Table 3.1(b)[55]shows the structure of the table in a data\nquality report that describes categorical features.\nThe data quality report should also include a histogram for each continuous feature in an","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":108,"page_label":"54","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"report that describes continuous features.\nThe table in the data quality report that describes categorical features should include a\nrow for each feature in the ABT that contains the two most frequent levels for the feature\n(the mode and 2ndmode) and the frequency with which these appear (both as raw frequen-\ncies and as a proportion of the total number of instances in the dataset). Each row should\nalso include the percentage of instances in the ABT that are missing a value for the feature\nand the cardinality of the feature. Table 3.1(b)[55]shows the structure of the table in a data\nquality report that describes categorical features.\nThe data quality report should also include a histogram for each continuous feature in an\nABT. For continuous features with cardinality less than 10, we use bar plots instead of his-\ntograms as this usually produces more informative data visualization. For each categorical\nfeature in an ABT, a bar plot should be included in the data quality report.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":109,"page_label":"55","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.2 Getting to Know the Data 55\nTable 3.1\nThe structures of the tables included in a data quality report to describe (a) continuous features and\n(b) categorical features.\n(a) Continuous Features\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\n—— —— —— —— —— —— —— —— —— —— ——\n—— —— —— —— —— —— —— —— —— —— ——\n—— —— —— —— —— —— —— —— —— —— ——\n(b) Categorical Features\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\n—— —— —— —— —— —— —— —— —— ——\n—— —— —— —— —— —— —— —— —— ——\n—— —— —— —— —— —— —— —— —— ——\n3.1.1 Case Study: Motor Insurance Fraud\nTable 3.2[56]shows a portion of the ABT that has been developed for the motor insurance\nclaims fraud detection solution based on the design described in Section 2.4.6[42].1The data\nquality report for this ABT is shown across Table 3.3[57](tabular reports for continuous and\ncategorical features) and Figure 3.1[58](data visualizations for each feature in the dataset).\n3.2 Getting to Know the Data\nThe data quality report gives an in-depth picture of the data in an ABT, and we should\nstudy it in detail in order to get to know the data that we will work with. For each feature,\nwe should examine the central tendency and variation to understand the types of values\nthat each feature can take. For categorical features, we should ﬁrst examine the mode, 2nd\nmode, mode %, and 2ndmode % in the categorical features table in the data quality report.\nThese tell us the most common levels within these features and will identify if any levels\ndominate the dataset (these levels will have a very high mode %). The bar plots shown in\nthe data quality report are also very useful here. They give us a quick overview of all the\nlevels in the domain of each categorical feature and the frequencies of these levels.\n1. In order to allow this dataset ﬁt on one page, only a subset of the features described in the domain concept\ndiagrams in Figures 2.9[43], 2.10[44], and 2.11[45]are included.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":110,"page_label":"56","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 3.2\nPortions of the ABT for the motor insurance claims fraud detection problem discussed in Section 2.4.6[42].\nNUM. % C LAIM\nMARITAL NUM. I NJURY HOSPITAL CLAIM TOTAL NUM SOFT SOFT AMT. F RAUD\nID T YPE INC. S TATUS CLMNTS . T YPE STAY AMT. C LAIMED CLAIMS TISS. T ISS. R CVD. F LAG\n1 ci 0 2 soft tissue no 1,625 3,250 2 2 1.0 0 1\n2 ci 0 2 back yes 15,028 60,112 1 0 15,028 0\n3 ci 54,613 married 1 broken limb no -99,999 0 0 0 0 572 0\n4 ci 0 4 broken limb yes 5,097 11,661 1 1 1.0 7,864 0\n5 ci 0 4 soft tissue no 8,869 0 0 0 0 0 1\n.........\n300 ci 0 2 broken limb no 2,244 0 0 0 0 2,244 0\n301 ci 0 1 broken limb no 1,627 92,283 3 0 0 1,627 0\n302 ci 0 3 serious yes 270,200 0 0 0 0 270,200 0\n303 ci 0 1 soft tissue no 7,668 92,806 3 0 0 7,668 0\n304 ci 46,365 married 1 back no 3,217 0 0 0 1,653 0\n.........\n458 ci 48,176 married 3 soft tissue yes 4,653 8,203 1 0 0 4,653 0\n459 ci 0 1 soft tissue yes 881 51,245 3 0 0 0 1\n460 ci 0 3 back no 8,688 729,792 56 5 0.08 8,688 0\n461 ci 47,371 divorced 1 broken limb yes 3,194 11,668 1 0 0 3,194 0\n462 ci 0 1 soft tissue no 6,821 0 0 0 0 0 1\n.........\n496 ci 0 1 soft tissue no 2,118 0 0 0 0 0 1\n497 ci 29,280 married 4 broken limb yes 3,199 0 0 0 0 0 1\n498 ci 0 1 broken limb yes 32,469 0 0 0 0 16,763 0\n499 ci 46,683 married 1 broken limb no 179,448 0 0 0 179,448 0\n500 ci 0 1 broken limb no 8,259 0 0 0 0 0 1","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":111,"page_label":"57","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 3.3\nA data quality report for the motor insurance claims fraud detection ABT displayed in Table 3.2[56].\n(a) Continuous Features\n% 1st3rdStd.\nFeature Count Miss. Card. Min Qrt. Mean Median Qrt. Max Dev.\nINCOME 500 0.0 171 0.0 0.0 13,740.0 0.0 33,918.5 71,284.0 20,081.5\nNUM. CLAIMANTS 500 0.0 4 1.0 1.0 1.9 2 3.0 4.0 1.0\nCLAIM AMOUNT 500 0.0 493 -99,999 3,322.3 16,373.2 5,663.0 12,245.5 270,200.0 29,426.3\nTOTAL CLAIMED 500 0.0 235 0.0 0.0 9,597.2 0.0 11,282.8 729,792.0 35,655.7\nNUM. CLAIMS 500 0.0 7 0.0 0.0 0.8 0.0 1.0 56.0 2.7\nNUM. SOFT TISSUE 500 2.0 6 0.0 0.0 0.2 0.0 0.0 5.0 0.6\n% S OFT TISSUE 500 0.0 9 0.0 0.0 0.2 0.0 0.0 2.0 0.4\nAMOUNT RECEIVED 500 0.0 329 0.0 0.0 13,051.9 3,253.5 8,191.8 270,200.0 30,547.2\nFRAUD FLAG 500 0.0 2 0.0 0.0 0.3 0.0 1.0 1.0 0.5\n(b) Categorical Features\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nINSURANCE TYPE 500 0.0 1 ci 500 100.0 – – –\nMARITAL STATUS 500 61.2 4 married 99 51.0 single 48 24.7\nINJURY TYPE 500 0.0 4 broken limb 177 35.4 soft tissue 172 34.4\nHOSPITAL STAY 500 0.0 2 no 354 70.8 yes 146 29.2","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":112,"page_label":"58","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Density\n010000 30000 50000 700000.00000 0.00010 0.00020(a)INCOME\n1 2 3 4Density\n0.0 0.1 0.2 0.3 0.4 (b)NUM. CLAIMANTS\nDensity\n−100000 0 100000 2000000e+00 2e−05 4e−05 6e−05 (c)CLAIM AMOUNT\nDensity\n0 200000 400000 6000000e+00 1e−05 2e−05 3e−05\n(d)TOTAL CLAIMED\n0 1 2 3 4 5 56Density\n0.0 0.1 0.2 0.3 0.4 0.5 (e)NUM. CLAIMS\n0 1 2 3 5Density\n0.0 0.2 0.4 0.6 0.8 (f)NUM. SOFT TISSUE\nDensity\n0.0 0.5 1.0 1.5 2.00 1 2 3\n(g)% S OFT TISSUE\nDensity\n0 50000 150000 2500000e+00 2e−05 4e−05 6e−05 8e−05 (h)AMOUNT RECEIVED\n0 1Density\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 (i)FRAUD FLAG\nCIDensity\n0.0 0.2 0.4 0.6 0.8 1.0\n(j)INSURANCE TYPE\nMarr ied Single DivorcedDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nMissing (k)MARITAL STATUS\nBroken Limb Back SeriousDensity\n0.00 0.10 0.20 0.30\nSoft Tissue (l)INJURY TYPE\nNo YesDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (m) HOSPITAL STAY\nFigure 3.1\nVisualizations of the continuous and categorical features in the motor insurance claims fraud detec-\ntion ABT in Table 3.2[56].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":113,"page_label":"59","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.2 Getting to Know the Data 59\nFor continuous features we should ﬁrst examine the mean and standard deviation of each\nfeature to get a sense of the central tendency and variation of the values within the dataset\nfor the feature. We should also examine the minimum and maximum values to understand\nthe range that is possible for each feature. The histograms for each continuous feature in-\ncluded in a data quality report are a very easy way for us to understand how the values for\na feature are distributed across the range they can take.2When we generate histograms of\nfeatures, there are a number of common, well-understood shapes that we should look out\nfor. These shapes relate to well-known standard probability distributions ,3and recogniz-\ning that the distribution of the values in an ABT for a feature closely matches one of these\nstandard distributions can help us when building machine learning models. During data\nexploration we don’t need to go any further than simply recognizing that features seem\nto follow particular distributions, and this can be done from examining the histogram for\neach feature. Figure 3.2[60]shows a selection of histogram shapes that exhibit characteristics\ncommonly seen when analyzing features and that are indicative of standard, well-known\nprobability distributions.\nFigure 3.2(a)[60]shows a histogram exhibiting a uniform distribution . A uniform distri-\nbution indicates that a feature is equally likely to take a value in any of the ranges present.\nSometimes a uniform distribution is indicative of a descriptive feature that contains an ID\nrather than a measure of something more interesting.\nFigure 3.2(b)[60]shows a shape indicative of a normal distribution . Features following a\nnormal distribution are characterized by a strong tendency toward a central value and sym-\nmetrical variation to either side of this central tendency. Naturally occurring phenomena—\nfor example, the heights or weights of a randomly selected group of men or women—tend\nto follow a normal distribution. Histograms that follow a normal distribution can also be\ndescribed as unimodal because they have a single peak around the central tendency. Find-\ning features that exhibit a normal distribution is a good thing, as many of the modeling\ntechniques we discuss in later chapters work particularly well with normally distributed\ndata.\nFigures 3.2(c)[60]and 3.2(d)[60]show unimodal histograms that exhibit skew . Skew is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":113,"page_label":"59","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"normal distribution are characterized by a strong tendency toward a central value and sym-\nmetrical variation to either side of this central tendency. Naturally occurring phenomena—\nfor example, the heights or weights of a randomly selected group of men or women—tend\nto follow a normal distribution. Histograms that follow a normal distribution can also be\ndescribed as unimodal because they have a single peak around the central tendency. Find-\ning features that exhibit a normal distribution is a good thing, as many of the modeling\ntechniques we discuss in later chapters work particularly well with normally distributed\ndata.\nFigures 3.2(c)[60]and 3.2(d)[60]show unimodal histograms that exhibit skew . Skew is\nsimply a tendency toward very high ( right skew as seen in Figure 3.2(c)[60]) or very low\n(left skew as seen in Figure 3.2(d)[60]) values. Features recording salaries often follow\na right skewed distribution, as most people are paid salaries near a well-deﬁned central\ntendency, but there are usually a small number of people who are paid very large salaries.\nSkewed distributions are often said to have long tails toward these very high or very low\nvalues.\n2. Note that in a density histogram, the height of each bar represents the likelihood that a value in the range\ndeﬁning that bar will occur in a data sample (see Section A.4.2[752]).\n3. We discuss probability distributions in more depth in Chapter 6[243].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":114,"page_label":"60","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"60 Chapter 3 Data Exploration\n(a) Uniform\n (b) Normal (unimodal)\n (c) Unimodal (skewed right)\n(d) Unimodal (skewed left)\n (e) Exponential\n (f) Multimodal\nFigure 3.2\nHistograms for six different sets of data, each of which exhibit well-known, common characteristics.\nIn a feature following an exponential distribution , as shown in Figure 3.2(e)[60], the\nlikelihood of low values occurring is very high but diminishes rapidly for higher values.\nFeatures such as the number of times a person has made an insurance claim or the number\nof times a person has been married tend to follow an exponential distribution. Recognizing\nthat a feature follows an exponential distribution is another clear warning sign that outliers\nare likely. As shown in Figure 3.2(e)[60], exponential distributions have a long tail, and so\nvery high values are not uncommon.\nFinally, a feature characterized by a multimodal distribution has two or more very\ncommonly occurring ranges of values that are clearly separated. Figure 3.2(f)[60]shows a\nbimodal distribution with two clear peaks—we can think of this as two normal distribu-\ntions pushed together. Multimodal distributions tend to occur when a feature contains a\nmeasurement made across a number of distinct groups. For example, if we were to mea-\nsure the heights of a randomly selected group of Irish men and women, we would expect a\nbimodal distribution with a peak at around 1.635m for women and 1.775m for men.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":115,"page_label":"61","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.2 Getting to Know the Data 61\nObserving a multimodal distribution is cause for both caution and optimism. The caution\ncomes from the fact that measures of central tendency and variation tend to break down\nfor multimodal data. For example, consider that the mean value of the distribution shown\nin Figure 3.2(f)[60]is likely to sit right in the valley between the two peaks, even though\nvery few instances actually have this value. The optimism associated with ﬁnding multi-\nmodally distributed data stems from the fact that, if we are lucky, the separate peaks in the\ndistribution will be associated with the different target levels we are trying to predict. For\nexample, if we were trying to predict gender from a set of physiological measurements,\nheight would most likely be a very predictive value, as it would separate people into male\nand female groups.\nThis stage of data exploration is mostly an information-gathering exercise, the output\nof which is just a better understanding of the contents of an ABT. It does, however, also\npresent a good opportunity to discuss anything unusual that we notice about the central\ntendency and variation of features within the ABT. For example, a salary feature with a\nmean of 40would seem unlikely ( 40,000would seem more reasonable) and should be\ninvestigated.\n3.2.1 The Normal Distribution\nThenormal distribution (also known as a Gaussian distribution ) is so important that it\nis worth spending a little extra time discussing its characteristics. Standard probability dis-\ntributions have associated probability density functions , which deﬁne the characteristics\nof the distribution. The probability density function for the normal distribution is\nNpx,µ,σq“1\nσ?\n2πe´px´µq2\n2σ2(3.1)\nwhere xis any value, and µandσare parameters that deﬁne the shape of the distribution.\nGiven a probability density function, we can plot the density curve associated with a dis-\ntribution, which gives us a different way to visualize standard distributions like the normal.\nFigure 3.3[62]shows the density curves for a number of different normal distributions. The\nhigher the curve for a particular value on the horizontal axis, the more likely that value is.\nThe curve deﬁned by a normal probability distribution is symmetric around a single\npeak value. The location of the peak value is deﬁned by the parameter µ(pronounced\nmu), which denotes the population mean (in other words, the mean value of the feature","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":115,"page_label":"61","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Npx,µ,σq“1\nσ?\n2πe´px´µq2\n2σ2(3.1)\nwhere xis any value, and µandσare parameters that deﬁne the shape of the distribution.\nGiven a probability density function, we can plot the density curve associated with a dis-\ntribution, which gives us a different way to visualize standard distributions like the normal.\nFigure 3.3[62]shows the density curves for a number of different normal distributions. The\nhigher the curve for a particular value on the horizontal axis, the more likely that value is.\nThe curve deﬁned by a normal probability distribution is symmetric around a single\npeak value. The location of the peak value is deﬁned by the parameter µ(pronounced\nmu), which denotes the population mean (in other words, the mean value of the feature\nif we had access to every value that could possibly occur). The height and slope of the\ncurve is dependent on the parameter σ(pronounced sigma ), which denotes the population\nstandard deviation . The larger the value of σ, the lower the maximum height of the curve\nand the shallower the slope. Figure 3.3(a)[62]illustrates how the location of the peak moves\nas the value for µchanges, and Figure 3.3(b)[62]illustrates how the shape of the curve\nchanges as we vary the value for σ. Notice that in both ﬁgures, the normal distribution","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":116,"page_label":"62","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"62 Chapter 3 Data Exploration\n−6 −4 −2 0 2 4 60.0 0.1 0.2 0.3 0.4\nValueDensityµ=0, σ=1\nµ=−2, σ=1\nµ=+2, σ=1\n(a) Different means\n−6 −4 −2 0 2 4 60.0 0.1 0.2 0.3 0.4\nValueDensityµ=0, σ=1\nµ=0, σ=2\nµ=0, σ=6 (b) Different standard deviations\nFigure 3.3\n(a) Three normal distributions with different means but identical standard deviations; and (b) three\nnormal distributions with identical means but different standard deviations.\nplotted with the continuous black line has mean µ“0and standard deviation σ“1. This\nnormal distribution is known as the standard normal distribution . The statement X is\nNpµ,σqis often used as a shorthand for X is a normally distributed feature with mean µ\nand standard deviation σ.4One important characteristic of the normal distribution is often\ndescribed as the 68´95´99.7rule. The rule states that approximately 68% of the values\nin a sample that follows a normal distribution will be within one σofµ,95% of the values\nwill be within two σofµ, and 99.7%of values will be within three σofµ. Figure 3.4[63]\nillustrates this rule. This rule highlights that in data that follows a normal distribution, there\nis a very low probability of observations occurring that differ from the mean by more than\ntwo standard deviations.\n3.2.2 Case Study: Motor Insurance Fraud\nThe data quality report in Table 3.3[57]and in Figure 3.1[58]allows us to very quickly become\nfamiliar with the central tendency and variation of each feature in the ABT. These were all\nas the business expected. In the bar plots in Figure 3.1[58], the different levels in the domain\nof each categorical feature, and how these levels are distributed, are obvious. For example,\nINJURY TYPE has four levels. Three of these, broken limb ,soft tissue , and back, are quite\nfrequent in the ABT, while serious is quite rare. The distribution of I NSURANCE TYPE is\na little strange, as it displays only one level.\nFrom the histograms in Figure 3.1[58], we see that all the continuous features except for\nINCOME and F RAUD FLAG seem to follow an exponential distribution pretty closely. I N-\n4. Sometimes, the variance of a feature, σ2, rather than its standard deviation, σ, is listed as the parameter for\nthe normal distribution. In this text we always use the standard deviation σ.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":117,"page_label":"63","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.3 Identifying Data Quality Issues 63\nµ−3 σ µ−2 σ µ−σ µ µ+σ µ+2 σ µ+3 σ\nFigure 3.4\nAn illustration of the 68´95´99.7rule. The gray region deﬁnes the area where 95% of values in a\nsample are expected.\nCOME is interesting as it seems to follow what looks like a normal distribution except that\nthere is one large bar at about 0. The distribution of the F RAUD FLAG feature that can be\nseen in its histogram is not typical of a continuous feature.\nBy analyzing the data quality report, we are able to understand the characteristics of\nthe data in the ABT. We will return to the features that seemed to have slightly peculiar\ndistributions.\n3.3 Identifying Data Quality Issues\nAfter getting to know the data, the second goal of data exploration is to identify any data\nquality issues in an ABT. A data quality issue is loosely deﬁned as anything unusual about\nthe data in an ABT. The most common data quality issues, however, are missing values ,\nirregular cardinality problems, and outliers . In this section we describe each of these\ndata quality issues and outline how the data quality report can be used to identify them.\nThe data quality issues we identify from a data quality report will be of two types: data\nquality issues due to invalid data and data quality issues due to valid data . Data quality\nissues due to invalid data typically arise because of errors in the process used to generate\nan ABT, usually in relation to calculating derived features. When we identify data quality\nissues due to invalid data, we should take immediate action to correct them, regenerate\nthe ABT, and re-create the data quality report. Data quality issues due to valid data can\narise for a range of domain-speciﬁc reasons (we discuss some of these later in this section),\nand we do not necessarily need to take any corrective action to address these issues. We\ndo not correct data quality issues due to valid data unless the predictive models we will\nuse the data in the ABT to train require that particular data quality issues be corrected.\nFor example, we cannot train error-based models with data that contains missing values,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":118,"page_label":"64","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"64 Chapter 3 Data Exploration\nTable 3.4\nThe structure of a data quality plan.\nFeature Data Quality Issue Potential Handling Strategies\n—— ———————— ————————————————\n—— ———————— ————————————————\n—— ———————— ————————————————\n—— ———————— ————————————————\nand data that contains outliers signiﬁcantly damages the performance of similarity-based\nmodels. At this stage we simply record any data quality issues due to valid data in a data\nquality plan so that we remain aware of them and can handle them later if required. Table\n3.4[64]shows the structure of a data quality plan. For each of the data quality issues found,\nwe include the feature it was found in and the details of the data quality issue. Later we\nadd information on potential handling strategies for each data quality issue.\n3.3.1 Missing Values\nOften when an ABT is generated, some instances will be missing values for one or more\nfeatures. The % Miss. columns in the data quality report highlight the percentage of\nmissing values for each feature (both continuous and categorical) in an ABT, and so it is\nvery easy to identify which features suffer from this issue. If features have missing values,\nwe must ﬁrst determine why the values are missing. Often missing values arise from errors\nin data integration or in the process of generating values for derived ﬁelds. If this is the\ncase, these missing values are due to invalid data, so the data integration errors can be\ncorrected, and the ABT can be regenerated to populate the missing values. Missing values\ncan also arise for legitimate reasons, however. Sometimes in an organization, certain values\nwill only have been collected after a certain date, and the data used to generate an ABT\nmight cover time both before and after this date. In other cases, particularly where data\narises from manual entry, certain personally sensitive values (for example, salary, age, or\nweight) may be entered only for a small number of instances. These missing values are\ndue to valid data, so they do not need to be handled but should instead be recorded in the\ndata quality plan.\nThere is one case in which we might deal directly with missing values that arise from\nvalid data during data exploration. If the proportion of missing values for a feature is very\nhigh, a good rule of thumb is anything in excess of 60%, then the amount of information\nstored in the feature is so low that it is probably a good idea to simply remove that feature\nfrom the ABT.\n3.3.2 Irregular Cardinality","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":118,"page_label":"64","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"arises from manual entry, certain personally sensitive values (for example, salary, age, or\nweight) may be entered only for a small number of instances. These missing values are\ndue to valid data, so they do not need to be handled but should instead be recorded in the\ndata quality plan.\nThere is one case in which we might deal directly with missing values that arise from\nvalid data during data exploration. If the proportion of missing values for a feature is very\nhigh, a good rule of thumb is anything in excess of 60%, then the amount of information\nstored in the feature is so low that it is probably a good idea to simply remove that feature\nfrom the ABT.\n3.3.2 Irregular Cardinality\nTheCard. column in the data quality report shows the number of distinct values present for\na feature within an ABT. A data quality issue arises when the cardinality for a feature does","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":119,"page_label":"65","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.3 Identifying Data Quality Issues 65\nnot match what we expect, a mismatch called an irregular cardinality . The ﬁrst things\nto check the cardinality column for are features with a cardinality of 1. This indicates a\nfeature that has the same value for every instance and contains no information useful for\nbuilding predictive models. Features with a cardinality of 1should ﬁrst be investigated to\nensure that the issue is not due to an ABT generation error. If this is the case, then the error\nshould be corrected, and the ABT should be regenerated. If the generation process proves\nto be error-free, then features with a cardinality of 1, although valid, should be removed\nfrom an ABT because they will not be of any value in building predictive models.\nThe second things to check for in the cardinality column are categorical features incor-\nrectly labeled as continuous. Continuous features will usually have a cardinality value\nclose to the number of instances in the dataset. If the cardinality of a continuous feature is\nsigniﬁcantly less than the number of instances in the dataset, then it should be investigated.\nSometimes a feature is actually continuous but in practice can assume only a small range\nof values—for example, the number of children a person has. In this case there is nothing\nwrong, and the feature should be left alone. In other cases, however, a categorical feature\nwill have been developed to use numbers to indicate categories and might be mistakenly\nidentiﬁed as a continuous feature in a data quality report. Checking for features with a low\ncardinality will highlight these features. For example, a feature might record gender using\n1for female and 0for male. If treated as a continuous feature in a data quality report,\nthis would have a cardinality of 2. Once identiﬁed, these features should be recoded as\ncategorical features.\nThe third way in which a data quality issue can arise due to an irregular cardinality is if a\ncategorical feature has a much higher cardinality than we would expect given the deﬁnition\nof the feature. For example, a categorical feature storing gender with a cardinality of 6is\nworthy of further investigation. This issue often arises because multiple levels are used to\nrepresent the same thing—for example, in a feature storing gender, we might ﬁnd levels of\nmale ,female ,m,f,M, and F, which all represent male and female in slightly different ways.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":119,"page_label":"65","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1for female and 0for male. If treated as a continuous feature in a data quality report,\nthis would have a cardinality of 2. Once identiﬁed, these features should be recoded as\ncategorical features.\nThe third way in which a data quality issue can arise due to an irregular cardinality is if a\ncategorical feature has a much higher cardinality than we would expect given the deﬁnition\nof the feature. For example, a categorical feature storing gender with a cardinality of 6is\nworthy of further investigation. This issue often arises because multiple levels are used to\nrepresent the same thing—for example, in a feature storing gender, we might ﬁnd levels of\nmale ,female ,m,f,M, and F, which all represent male and female in slightly different ways.\nThis is another example of a data quality issue due to invalid data. It should be corrected\nthrough a mapping to a standard set of levels, and the ABT should be regenerated.\nThe ﬁnal example of a data quality issue due to an irregular cardinality is when a categor-\nical feature simply has a very high number of levels—anything over 50is worth investiga-\ntion. There are many genuine examples of features that will have such high cardinality, but\nsome of the machine learning algorithms that we will look at will struggle to effectively\nuse features with such high cardinality. This is an example of a data issue due to valid data,\nso if this occurs for features in an ABT, it should be noted in the data quality plan.\n3.3.3 Outliers\nOutliers are values that lie far away from the central tendency of a feature. There are two\nkinds of outliers that might occur in an ABT: invalid outliers andvalid outliers . Invalid\noutliers are values that have been included in a sample through error and are often referred","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":120,"page_label":"66","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"66 Chapter 3 Data Exploration\nto as noise in the data. Invalid outliers can arise for all sorts of different reasons. For\nexample, during a manual data entry process, a fat ﬁngered5analyst may have entered\n100,000instead of 1,000. Valid outliers are correct values that are simply very different\nfrom the rest of the values for a feature, for example, a billionaire who has a massive salary\ncompared to everyone else in a sample.\nThere are two main ways that the data quality report can be used to identify outliers\nwithin a dataset. The ﬁrst is to examine the minimum and maximum values for each feature\nand use domain knowledge to determine whether these are plausible values. For example,\na minimum age value of ´12would jump out as an error. Outliers identiﬁed in this way\nare likely to be invalid outliers and should immediately be either corrected, if data sources\nallow this, or removed and marked as missing values if correction is not possible. In some\ncases we might even remove a complete instance from a dataset based on the presence of\nan outlier.\nThe second approach to identifying outliers is to compare the gaps between the median,\nminimum, maximum, 1stquartile, and 3rdquartile values. If the gap between the 3rd\nquartile and the maximum value is noticeably larger than the gap between the median and\nthe3rdquartile, this suggests that the maximum value is unusual and is likely to be an\noutlier. Similarly, if the gap between the 1stquartile and the minimum value is noticeably\nlarger than the gap between the median and the 1stquartile, this suggests that the minimum\nvalue is unusual and is likely to be an outlier. The outliers shown in box plots also help\nto make this comparison. Exponential or skewed distributions in histograms are also good\nindicators of the presence of outliers.\nIt is likely that outliers found using the second approach are valid outliers, so they are\na data quality issue due to valid data. Some machine learning techniques do not perform\nwell in the presence of outliers, so we should note these in the data quality plan for possible\nhandling later in the project.\n3.3.4 Case Study: Motor Insurance Fraud\nUsing the data quality report in Table 3.3[57]and Figure 3.1[58]together with the ABT extract\nin Table 3.2[56], we can perform an analysis of this ABT for data quality issues. We do this\nby describing separately missing values, irregular cardinality, and outliers.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":120,"page_label":"66","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"to make this comparison. Exponential or skewed distributions in histograms are also good\nindicators of the presence of outliers.\nIt is likely that outliers found using the second approach are valid outliers, so they are\na data quality issue due to valid data. Some machine learning techniques do not perform\nwell in the presence of outliers, so we should note these in the data quality plan for possible\nhandling later in the project.\n3.3.4 Case Study: Motor Insurance Fraud\nUsing the data quality report in Table 3.3[57]and Figure 3.1[58]together with the ABT extract\nin Table 3.2[56], we can perform an analysis of this ABT for data quality issues. We do this\nby describing separately missing values, irregular cardinality, and outliers.\n3.3.4.1 Missing values The% Miss. column of the data quality report in Table 3.3[57]\nshows that M ARITAL STATUS and N UM. SOFT TISSUE are the only features with an ob-\nvious problem with missing values. Indeed, over 60% of the values for M ARITAL STATUS\nare missing, so this feature should almost certainly be removed from the ABT (we return\nto this feature shortly). Only 2%of the values for the N UM. SOFT TISSUE feature are\n5.Fat ﬁnger is a phrase often used in ﬁnancial trading to refer to mistakes that arise when a trader enters extra\nzeros by mistake and buys or sells much more of a stock than intended.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":121,"page_label":"67","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.3 Identifying Data Quality Issues 67\nmissing, so removal would be extreme in this case. This issue should be noted in the data\nquality plan.\nAn examination of the histogram for the I NCOME feature (shown in Figure 3.1(a)[58]) and\nthe actual data for this feature in Table 3.2[56]reveals an interesting pattern. In the histogram\nwe can see an unusual number of zero values for I NCOME that seems set apart from the\ncentral tendency of the data, which appears to be at about 40,000. Examining the I NCOME\nrow in the data quality report also shows a large difference between the mean and median\nvalues, which is unusual. Examining the actual raw data in Table 3.2[56]shows that these\nzeros always co-occur with missing values in the M ARITAL STATUS feature. This pattern\nwas investigated with the business to understand whether this was an issue due to valid or\ninvalid data. It was conﬁrmed by the business that the zeros in the I NCOME feature actually\nrepresent missing values and that M ARITAL STATUS and I NCOME were collected together,\nleading to their both being missing for the same instances in the ABT. No other data source\nexisted from which these features could be populated, so it was decided to remove both of\nthem from the ABT.\n3.3.4.2 Irregular cardinality Reading down the Card. column of the data quality\nreport, we can see that the cardinality of the I NSURANCE TYPE feature is 1, an obvious\ndata problem that needs investigation. The cardinality value indicates that every instance\nhas the same value for this feature, ci. Investigation of this issue with the business revealed\nthat nothing had gone wrong during the ABT generation process, and that cirefers to car\ninsurance . Every instance in this ABT should have that value, and this feature was removed\nfrom the ABT.\nMany of the continuous features in the dataset also have very low cardinality values.\nNUM. CLAIMANTS , NUM. CLAIMS , NUM. SOFT TISSUE , % S OFT TISSUE , and F RAUD\nFLAG all have cardinality less than 10, which is unusual in a dataset of 500instances.\nThese low cardinalities were investigated with the business. The low cardinality for the\nNUM. CLAIMANTS , NUM. CLAIMS , and N UM. SOFT TISSUE features was found to be\nvalid, because these are categorical features and can only take values in a small range, as\npeople tend not to make very many claims. The % S OFT TISSUE feature is a ratio of the\nNUM. CLAIMS and N UM. SOFT TISSUE features, and its low cardinality arises from their\nlow cardinality.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":121,"page_label":"67","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"from the ABT.\nMany of the continuous features in the dataset also have very low cardinality values.\nNUM. CLAIMANTS , NUM. CLAIMS , NUM. SOFT TISSUE , % S OFT TISSUE , and F RAUD\nFLAG all have cardinality less than 10, which is unusual in a dataset of 500instances.\nThese low cardinalities were investigated with the business. The low cardinality for the\nNUM. CLAIMANTS , NUM. CLAIMS , and N UM. SOFT TISSUE features was found to be\nvalid, because these are categorical features and can only take values in a small range, as\npeople tend not to make very many claims. The % S OFT TISSUE feature is a ratio of the\nNUM. CLAIMS and N UM. SOFT TISSUE features, and its low cardinality arises from their\nlow cardinality.\nThe cardinality of 2for the F RAUD FLAG feature highlights the fact that this is not really\na continuous feature. Rather, F RAUD FLAG is a categorical feature that just happens to\nuse0and1as its category labels, which has led to its being treated as continuous in the\nABT. F RAUD FLAG was changed to be a categorical feature. This is particularly important\nin this case because F RAUD FLAG is the target feature, and as we will see in upcoming\nchapters, the type of the target feature has a big impact on how we apply machine learning\ntechniques.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":122,"page_label":"68","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"68 Chapter 3 Data Exploration\nTable 3.5\nThe data quality plan for the motor insurance fraud prediction ABT.\nFeature Data Quality Issue Potential Handling Strategies\nNUM. SOFT TISSUE Missing values ( 2%)\nCLAIM AMOUNT Outliers (high)\nAMOUNT RECEIVED Outliers (high)\n3.3.4.3 Outliers From an examination of the minimum and maximum values for each\ncontinuous feature in Table 3.3(a)[57], CLAIM AMOUNT jumps out as having an unusual\nminimum value of ´99,999. A little investigation revealed that this minimum value arises\nfrom d3in Table 3.2[56]. The absence of a large bar at ´99,999in Figure 3.1(c)[58]conﬁrms\nthat there are not multiple occurrences of this value. The pattern 99,999also suggests that\nthis is most likely a data entry error or a system default remaining in the ABT. This was\nconﬁrmed with the business in this case, and this value was treated as an invalid outlier and\nreplaced with a missing value.\nCLAIM AMOUNT , TOTAL CLAIMED , NUM. C LAIMS and A MOUNT RECEIVED all\nseem to have unusually high maximum values, especially when compared to their median\nand3rdquartile values. To investigate outliers, we should always start by locating the\ninstance in the dataset that contains the strange maximum or minimum values. In this case\nthe maximum values for T OTAL CLAIMED and N UM. C LAIMS both come from d460in\nTable 3.2[56]. This policyholder seems to have made many more claims than anyone else,\nand the total amount claimed reﬂects this. This deviation from the norm was investigated\nwith the business, and it turned out that although these ﬁgures were correct, this policy was\nactually a company policy rather than an individual policy, which was included in the ABT\nby mistake. For this reason, instance d460was removed from the ABT.\nThe offending large maximums for C LAIM AMOUNT and A MOUNT RECEIVED both\ncome from d302in Table 3.2[56]. Investigation of this claim with the business revealed that\nthis is in fact a valid outlier and represents an unusually large claim for a very serious in-\njury. Examination of the histograms in Figures 3.1(c)[58]and 3.1(h)[58]show that the C LAIM\nAMOUNT and A MOUNT RECEIVED features have a number of large values (evidenced by\nthe small bars to the right-hand side of these histograms) and that d302is not unique. These\noutliers should be noted in the data quality plan for possible handling later in the project.\n3.3.4.4 The data quality plan Based on the analysis described in the preceding sec-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":122,"page_label":"68","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"The offending large maximums for C LAIM AMOUNT and A MOUNT RECEIVED both\ncome from d302in Table 3.2[56]. Investigation of this claim with the business revealed that\nthis is in fact a valid outlier and represents an unusually large claim for a very serious in-\njury. Examination of the histograms in Figures 3.1(c)[58]and 3.1(h)[58]show that the C LAIM\nAMOUNT and A MOUNT RECEIVED features have a number of large values (evidenced by\nthe small bars to the right-hand side of these histograms) and that d302is not unique. These\noutliers should be noted in the data quality plan for possible handling later in the project.\n3.3.4.4 The data quality plan Based on the analysis described in the preceding sec-\ntions, the data quality plan shown in Table 3.5[68]was created. This records each of the\ndata quality issues due to valid data that have been identiﬁed in the motor insurance fraud\nABT. During the Modeling phase of the project, we will use this table as a reminder of data\nquality issues that could affect model training. At the end of the next section, we complete\nthis table by adding potential handling strategies.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":123,"page_label":"69","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.4 Handling Data Quality Issues 69\n3.4 Handling Data Quality Issues\nWhen we ﬁnd data quality issues due to valid data during data exploration, we should note\nthese issues in a data quality plan for potential handling later in the project. The most\ncommon issues in this regard are missing values and outliers, which are both examples of\nnoise in the data. Although we usually delay handling noise issues until the modeling phase\nof a project (different predictive model types require different levels of noise handling, and\nwe should in general do as little noise handling as we can), in this section we describe the\nmost common techniques used to handle missing values and outliers. It is a good idea to\nadd suggestions for the best technique to handle each data quality issue in the data quality\nplan during data exploration as it will save time during modeling.\n3.4.1 Handling Missing Values\nThe simplest approach to handling missing values is to simply drop from an ABT any\nfeatures that have them. This, however, can result in massive, and frequently needless, loss\nof data. For example, if in an ABT containing 1,000instances, one value is missing for a\nparticular feature, it would be pretty extreme to remove that whole feature. As a general\nrule of thumb, only features that are missing in excess of 60% of their values should be\nconsidered for complete removal, and more subtle handling techniques should be used for\nfeatures missing less data.\nAn alternative to entirely deleting features that suffer from large numbers of missing\nvalues is to derive a missing indicator feature from them. This is a binary feature that\nﬂags whether the value was present or missing in the original feature. This can be useful\nif the reason that speciﬁc values for a feature are missing might have some relationship to\nthe target feature—for example, if a feature that has missing values represented sensitive\npersonal data, people’s readiness to provide this data (or not) might tell us something about\nthem. When missing indicator features are used, the original feature is usually discarded.\nAnother simple approach to handling missing values is complete case analysis , which\ndeletes from an ABT any instances that are missing one or more feature values. This\napproach, however, can result in signiﬁcant amounts of data loss and can introduce a bias\ninto the dataset if the distribution of missing values in the dataset is not completely random.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":123,"page_label":"69","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"if the reason that speciﬁc values for a feature are missing might have some relationship to\nthe target feature—for example, if a feature that has missing values represented sensitive\npersonal data, people’s readiness to provide this data (or not) might tell us something about\nthem. When missing indicator features are used, the original feature is usually discarded.\nAnother simple approach to handling missing values is complete case analysis , which\ndeletes from an ABT any instances that are missing one or more feature values. This\napproach, however, can result in signiﬁcant amounts of data loss and can introduce a bias\ninto the dataset if the distribution of missing values in the dataset is not completely random.\nIn general, we recommend the use of complete case analysis only to remove instances that\nare missing the value of the target feature. Indeed, any instances with a missing value for\nthe target feature should always be removed from an ABT.\nImputation replaces missing feature values with a plausible estimated value based on\nthe feature values that are present. The most common approach to imputation is to replace\nmissing values for a feature with a measure of the central tendency of that feature. For con-\ntinuous features, the mean or median is most commonly used, and for categorical features,\nthe mode is most commonly used.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":124,"page_label":"70","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"70 Chapter 3 Data Exploration\nImputation, however, should not be used for features that have very large numbers of\nmissing values because imputing a very large number of missing values will change the\ncentral tendency of a feature too much. We would be reluctant to use imputation on features\nmissing in excess of 30% of their values and would strongly recommend against the use of\nimputation on features missing in excess of 50% of their values.\nThere are other, more complex approaches to imputation. For example, we can actually\nbuild a predictive model that estimates a replacement for a missing value based on the\nfeature values that are present in a dataset for a given instance. We recommend, however,\nusing simple approaches ﬁrst and turning to more complex ones only if required.\nImputation techniques tend to give good results and avoid the data loss associated with\ndeleting features or complete case analysis. It is important to note, however, that all impu-\ntation techniques suffer from the fact that they change the underlying data in an ABT and\ncan cause the variation within a feature to be underestimated, which can negatively bias\nthe relationships between a descriptive feature and a target feature.\n3.4.2 Handling Outliers\nThe easiest way to handle outliers is to use a clamp transformation . This clamps all\nvalues above an upper threshold and below a lower threshold to these threshold values,\nthus removing the offending outliers:\nai“$\n’’&\n’’%lower ifaiălower\nupper ifaiąupper\nai otherwise(3.2)\nwhere aiis a speciﬁc value of feature a, and lower andupper are the lower and upper\nthresholds.\nThe upper and lower thresholds can be set manually based on domain knowledge or can\nbe calculated from data. One common way to calculate clamp thresholds is to set the lower\nthreshold to the 1stquartile value minus 1.5times the inter-quartile range and the upper\nthreshold to the 3rdquartile plus 1.5times the inter-quartile range. This works effectively\nand takes into account the fact that the variation in a dataset can be different on either side\nof a central tendency.\nIf this approach were to be used for the C LAIM AMOUNT feature from the motor claims\ninsurance fraud detection scenario, then the upper and lower thresholds would be deﬁned\nas follows:\nlower“3,322.3´1.5ˆ8,923.2“´10,062.5\nupper“12,245.5`1.5ˆ8,923.2“25,630.3\nwhere the values used are extracted from Table 3.3[57]. Any values outside these thresholds","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":124,"page_label":"70","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"be calculated from data. One common way to calculate clamp thresholds is to set the lower\nthreshold to the 1stquartile value minus 1.5times the inter-quartile range and the upper\nthreshold to the 3rdquartile plus 1.5times the inter-quartile range. This works effectively\nand takes into account the fact that the variation in a dataset can be different on either side\nof a central tendency.\nIf this approach were to be used for the C LAIM AMOUNT feature from the motor claims\ninsurance fraud detection scenario, then the upper and lower thresholds would be deﬁned\nas follows:\nlower“3,322.3´1.5ˆ8,923.2“´10,062.5\nupper“12,245.5`1.5ˆ8,923.2“25,630.3\nwhere the values used are extracted from Table 3.3[57]. Any values outside these thresholds\nwould be converted to the threshold values. Examining the histogram in Figure 3.1(c)[58]is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":125,"page_label":"71","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.4 Handling Data Quality Issues 71\nuseful in considering the impact of applying the clamp transformation using these thresh-\nolds. Locating 25,630.3on the horizontal axis shows that this upper threshold would cause\na relatively large number of values to be changed. The impact of the clamp transformation\ncan be reduced by changing the multiplier used to calculate the thresholds from 1.5to a\nlarger value.\nAnother commonly used approach to setting the upper and lower thresholds is to use the\nmean value of a feature plus or minus 2times the standard deviation.6Again this works\nwell, but it does assume that the underlying data follows a normal distribution.\nIf this approach were to be used for the A MOUNT RECEIVED feature from the motor\nclaims insurance fraud detection scenario, then the upper and lower thresholds would be\ndeﬁned as follows:\nlower“13,051.9´2ˆ30,547.2“´48,042.5\nupper“13,051.9`2ˆ30,547.2“74,146.3\nwhere the values used are again extracted from Table 3.3[57]. Examining the histogram in\nFigure 3.1(h)[58]is again a good indication of the impact of using this transformation. This\nimpact can be reduced by changing the multiplier used to calculate the thresholds from 2\nto a larger value.\nOpinions vary widely on when transformations like the clamp transformation should be\nused to handle outliers in data. Many argue that performing this type of transformation may\nremove the most interesting and, from a predictive modeling point of view, informative in-\nstances from a dataset. On the other hand, some of the machine learning techniques that\nwe discuss in upcoming chapters perform poorly in the presence of outliers. We recom-\nmend only applying the clamp transformation in cases where it is suspected that a model\nis performing poorly due to the presence of outliers. The impact of the clamp transforma-\ntion should then be evaluated by comparing the performance of different models trained\non datasets where the transformation has been applied and where it has not.\n3.4.3 Case Study: Motor Insurance Fraud\nIf we needed to do it, the most sensible approach to handling the missing values in the\nNUM. SOFT TISSUE feature would be to use imputation. There are very few missing val-\nues for this feature ( 2%), so replacing them with an imputed value should not excessively\naffect the variance of the feature. In this case the median value of 0.0(shown in Table\n3.3(a)[57]) is the most appropriate value to use to replace the missing values; because this","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":125,"page_label":"71","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is performing poorly due to the presence of outliers. The impact of the clamp transforma-\ntion should then be evaluated by comparing the performance of different models trained\non datasets where the transformation has been applied and where it has not.\n3.4.3 Case Study: Motor Insurance Fraud\nIf we needed to do it, the most sensible approach to handling the missing values in the\nNUM. SOFT TISSUE feature would be to use imputation. There are very few missing val-\nues for this feature ( 2%), so replacing them with an imputed value should not excessively\naffect the variance of the feature. In this case the median value of 0.0(shown in Table\n3.3(a)[57]) is the most appropriate value to use to replace the missing values; because this\nfeature only actually takes discrete values, the mean value of 0.2never naturally occurs in\nthe dataset.\n6. Recall that in Section 3.2[55]we discussed the 68´95´99.7rule associated with the normal distribution. This\napproach to handling outliers is based directly on this rule.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":126,"page_label":"72","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"72 Chapter 3 Data Exploration\nTable 3.6\nThe data quality plan with potential handling strategies for the motor insurance fraud prediction ABT.\nFeature Data Quality Issue Potential Handling Strategies\nNUM. SOFT TISSUE Missing values ( 2%) Imputation (median: 0.0)\nCLAIM AMOUNT Outliers (high) Clamp transformation (manual: 0, 80,000)\nAMOUNT RECEIVED Outliers (high) Clamp transformation (manual: 0, 80,000)\nThe outliers present in the C LAIM AMOUNT and A MOUNT RECEIVED features could be\neasily handled using a clamp transformation. Both features follow a broadly exponential\ndistribution, however, which means that the methods described for setting the thresholds of\nthe clamp will not work especially well (both methods work best for normally distributed\ndata). Therefore, manually setting upper and lower thresholds based on domain knowl-\nedge is most appropriate in this case. The business advised that for both features, a lower\nthreshold of 0and an upper threshold of 80,000would make sense.\nWe completed the data quality plan by including these potential handling strategies. The\nﬁnal data quality plan is shown in Table 3.6[72]. Together with the data quality report, these\nare the outputs of the data exploration work for the motor insurance fraud detection project.\n3.5 Advanced Data Exploration\nAll the descriptive statistics and data visualization techniques that we have used in the\nprevious sections of this chapter have focused on the characteristics of individual features.\nThis section will introduce techniques that enable us to examine relationships between\npairs of features.\n3.5.1 Visualizing Relationships between Features\nIn preparing to create predictive models, it is always a good idea to investigate the relation-\nships between pairs of features. This can help indicate which descriptive features might\nbe useful for predicting a target feature and help ﬁnd pairs of descriptive features that are\nclosely related. Identifying pairs of closely related descriptive features is one way to re-\nduce the size of an ABT because if the relationship between two descriptive features is\nstrong enough, we may not need to include both. In this section we describe approaches\nto visualizing the relationships between pairs of continuous features, pairs of categorical\nfeatures, and pairs including one categorical and one continuous feature.\nFor the examples in this section, we introduce a new dataset. Table 3.7[73]shows the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":126,"page_label":"72","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ships between pairs of features. This can help indicate which descriptive features might\nbe useful for predicting a target feature and help ﬁnd pairs of descriptive features that are\nclosely related. Identifying pairs of closely related descriptive features is one way to re-\nduce the size of an ABT because if the relationship between two descriptive features is\nstrong enough, we may not need to include both. In this section we describe approaches\nto visualizing the relationships between pairs of continuous features, pairs of categorical\nfeatures, and pairs including one categorical and one continuous feature.\nFor the examples in this section, we introduce a new dataset. Table 3.7[73]shows the\ndetails of 30 players on a professional basketball team. The dataset includes the H EIGHT ,\nWEIGHT , and A GEof each player; the P OSITION that the player normally plays ( guard ,\ncenter , orforward ); the C AREER STAGE of the player ( rookie ,mid-career , orveteran ); the\naverage weekly S PONSORSHIP EARNINGS of each player; and whether the player has a\nSHOE SPONSOR (yesorno).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":127,"page_label":"73","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 73\nTable 3.7\nThe details of a professional basketball team.\nCAREER SPONSORSHIP SHOE\nID P OSITION HEIGHT WEIGHT STAGE AGE EARNINGS SPONSOR\n1 forward 192 218 veteran 29 561 yes\n2 center 218 251 mid-career 35 60 no\n3 forward 197 221 rookie 22 1,312 no\n4 forward 192 219 rookie 22 1,359 no\n5 forward 198 223 veteran 29 362 yes\n6 guard 166 188 rookie 21 1,536 yes\n7 forward 195 221 veteran 25 694 no\n8 guard 182 199 rookie 21 1,678 yes\n9 guard 189 199 mid-career 27 385 yes\n10 forward 205 232 rookie 24 1,416 no\n11 center 206 246 mid-career 29 314 no\n12 guard 185 207 rookie 23 1,497 yes\n13 guard 172 183 rookie 24 1,383 yes\n14 guard 169 183 rookie 24 1,034 yes\n15 guard 185 197 mid-career 29 178 yes\n16 forward 215 232 mid-career 30 434 no\n17 guard 158 184 veteran 29 162 yes\n18 guard 190 207 mid-career 27 648 yes\n19 center 195 235 mid-career 28 481 no\n20 guard 192 200 mid-career 32 427 yes\n21 forward 202 220 mid-career 31 542 no\n22 forward 184 213 mid-career 32 12 no\n23 forward 190 215 rookie 22 1,179 no\n24 guard 178 193 rookie 21 1,078 no\n25 guard 185 200 mid-career 31 213 yes\n26 forward 191 218 rookie 19 1,855 no\n27 center 196 235 veteran 32 47 no\n28 forward 198 221 rookie 22 1,409 no\n29 center 207 247 veteran 27 1,065 no\n30 center 201 244 mid-career 25 1,111 yes\n3.5.1.1 Visualizing pairs of continuous features The scatter plot is one of the most\nimportant tools in data visualization. A scatter plot is based on two axes: the horizontal\naxis represents one feature, and the vertical axis represents a second. Each instance in a\ndataset is represented by a point on the plot determined by the values for that instance of the\ntwo features being plotted. Figure 3.5(a)[74]shows an example scatter plot for the H EIGHT\nand W EIGHT features from the dataset in Table 3.7[73]. The points in this scatter plot are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":128,"page_label":"74","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"74 Chapter 3 Data Exploration\narranged in a broadly linear pattern diagonally across the scatter plot. This suggests that\nthere is a strong, positive, linear relationship between the H EIGHT and W EIGHT features—\nas height increases, so does weight. We say that features with this kind of relationship are\npositively covariant. Figure 3.5(b)[74]shows a scatter plot for the S PONSORSHIP EARN-\nINGS and A GEfeatures from Table 3.7[73]. These features are strongly negatively covariant.\nFigure 3.5(c)[74]shows a scatter plot of the H EIGHT and A GEfeatures. These features are\nnot strongly covariant either positively or negatively.\n160 170 180 190 200 210 220190 200 210 220 230 240 250\nHeightWeight\n(a)\n0 500 1000 150020 25 30 35\nSponsorship Ear ningsAge (b)\n160 170 180 190 200 210 22020 25 30 35\nHeightAge (c)\nFigure 3.5\nExample scatter plots for pairs of features from the dataset in Table 3.7[73], showing (a) the strong pos-\nitive covariance between H EIGHT and W EIGHT ; (b) the strong negative covariance between S PON-\nSORSHIP EARNINGS and A GE; and (c) the lack of strong covariance between H EIGHT and A GE.\nAscatter plot matrix (SPLOM ) shows scatter plots for a whole collection of features\narranged into a matrix. This is useful for exploring the relationships between groups of\nfeatures—for example, all the continuous features in an ABT. Figure 3.6[75]shows an ex-\nample scatter plot matrix for the continuous features from the professional basketball team\ndataset in Table 3.7[73]: H EIGHT , W EIGHT , AGE, and S PONSORSHIP EARNINGS . Each\nrow and column represent the feature named in the cells along the diagonal. The cells\nabove and below the diagonal show scatter plots of the features in the row and column that\nmeet at that cell.\nA scatter plot matrix is a very quick way to explore the relationships within a whole\nset of continuous features. The effectiveness of scatter plot matrices, however, diminishes\nonce the number of features in the set goes beyond 8because the graphs become too small.\nUsing interactive tools that aid data exploration can help overcome this limitation.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":129,"page_label":"75","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 75\nFigure 3.6\nA scatter plot matrix showing scatter plots of the continuous features from the professional basketball\nteam dataset in Table 3.7[73].\n3.5.1.2 Visualizing pairs of categorical features The simplest way to visualize the\nrelationship between two categorical features is to use a collection of bar plots. This is\noften referred to as a small multiples visualization. First, we draw a simple bar plot\nshowing the densities of the different levels of the ﬁrst feature. Then, for each level of\nthe second feature, we draw a bar plot of the ﬁrst feature using only the instances in the\ndataset for which the second feature has that level. If the two features being visualized\nhave a strong relationship, then the bar plots for each level of the second feature will look\nnoticeably different from one another and from the overall bar plot for the ﬁrst feature. If\nthere is no relationship, then we should expect that the levels of the ﬁrst feature will be\nevenly distributed among the instances having the different levels of the second feature, so\nall bar plots will look much the same.\nFigure 3.7(a)[76]shows an example for the C AREER STAGE and S HOE SPONSOR features\nfrom the professional basketball team dataset in Table 3.7[73]. The bar plot on the left\nshows the distribution of the different levels of the C AREER STAGE feature across the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":130,"page_label":"76","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"76 Chapter 3 Data Exploration\nmid−career rookie veteran\nCareer StageDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nmid−career rookie veteranShoe Sponsor = no\nCareer StageDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nmid−career rookie veteranShoe Sponsor = yes\nCareer StageDensity\n0.0 0.1 0.2 0.3 0.4 0.5\n(a) Career Stage and Shoe Sponsor\ncenter forward guard\nPositionDensity\n0.0 0.2 0.4 0.6 0.8\ncenter forward guardShoe Sponsor = no\nPositionDensity\n0.0 0.2 0.4 0.6 0.8\ncenter forward guardShoe Sponsor = yes\nPositionDensity\n0.0 0.2 0.4 0.6 0.8\n(b) Position and Shoe Sponsor\nFigure 3.7\nExamples of using small multiple bar plot visualizations to illustrate the relationship between two\ncategorical features: (a) the C AREER STAGE and S HOE SPONSOR features; and (b) the P OSITION\nand S HOE SPONSOR features. All data comes from Table 3.7[73].\nentire dataset. The two plots on the right show the distributions for those players with\nand without a shoe sponsor. Since all three plots show very similar distributions, we can\nconclude that no real relationship exists between these two features and that players of any\ncareer stage are equally likely to have a shoe sponsor or not.\nFigure 3.7(b)[76]shows another example, for the P OSITION and S HOE SPONSOR features\nfrom the same dataset. In this case, the three plots are very different, so we can conclude\nthat there is a relationship between these two features. It seems that players who play in\nthe guard position are much more likely to have a shoe sponsor than forwards or centers.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":131,"page_label":"77","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 77\nWhen using small multiples, it is important that all the small charts are kept consistent\nbecause this ensures that only genuine differences within the data are highlighted, rather\nthan differences that arise from formatting. For example, the scales of the axes must always\nbe kept consistent, as should the order of the bars in the individual bar plots. It is also\nimportant that densities are shown rather than frequencies, as the overall bar plots on the\nleft of each visualization cover much more of the dataset than the other two plots, so\nfrequency-based plots would look very uneven.\nIf the number of levels of one of the features being compared is small (we recommend no\nmore than three), we can use stacked bar plots as an alternative to the small multiples bar\nplots approach. When this approach is used, we show a bar plot of the ﬁrst feature above a\nbar plot that shows the relative distribution of the levels of the second feature within each\nlevel of the ﬁrst. Because relative distributions are used, the bars in the second bar plot\ncover the full range of the space available—these are often referred to as 100% stacked bar\nplots. If two features are unrelated, then we would expect to see the same proportion of\neach level of the second feature within the bars for each level of the ﬁrst.\nFigure 3.8[78]shows two examples of using stacked bar plots. In the ﬁrst example, Figure\n3.8(a)[78], a bar plot of the C AREER STAGE feature is shown above a 100% stacked bar plot\nshowing how the levels of the S HOE SPONSOR feature are distributed in instances having\neach level of C AREER STAGE . The distributions of the levels of S HOE SPONSOR are\nalmost the same for each level of C AREER STAGE , and therefore we can conclude that there\nis no relationship between these two features. The second example, Figure 3.8(b)[78], shows\nthe P OSITION and S HOE SPONSOR features. In this case we can see that distributions of\nthe levels of the S HOE SPONSOR feature are not the same for each position. From this we\ncan again conclude that guards are more likely to have a shoe sponsor than players in the\nother positions.\n3.5.1.3 Visualizing a categorical feature and a continuous feature The best way to\nvisualize the relationship between a continuous feature and a categorical feature is to use a\nsmall multiples approach, drawing a density histogram of the values of the continuous fea-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":131,"page_label":"77","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"almost the same for each level of C AREER STAGE , and therefore we can conclude that there\nis no relationship between these two features. The second example, Figure 3.8(b)[78], shows\nthe P OSITION and S HOE SPONSOR features. In this case we can see that distributions of\nthe levels of the S HOE SPONSOR feature are not the same for each position. From this we\ncan again conclude that guards are more likely to have a shoe sponsor than players in the\nother positions.\n3.5.1.3 Visualizing a categorical feature and a continuous feature The best way to\nvisualize the relationship between a continuous feature and a categorical feature is to use a\nsmall multiples approach, drawing a density histogram of the values of the continuous fea-\nture for each level of the categorical feature. Each histogram includes only those instances\nin the dataset that have the associated level of the categorical feature. Similar to using\nsmall multiples for categorical features, if the features are unrelated (or independent), then\nthe histograms for each level should be very similar. If the features are related, however,\nthen the shapes and/or the central tendencies of the histograms will be different.\nFigure 3.9(a)[79]shows a histogram of the A GEfeature from the dataset in Table 3.7[73]. We\ncan see from this histogram that A GEfollows a uniform distribution across a range from\nabout 19to about 35. Figure 3.9(c)[79]shows small multiple histograms for values of A GE\nbroken down by the different levels of the P OSITION feature. These histograms show a\nslight tendency for centers to be a little older than guards and forwards, but the relationship\ndoes not appear very strong, as each of the smaller histograms are similar to the overall","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":132,"page_label":"78","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"78 Chapter 3 Data Exploration\nFrequency\n0 2 4 6 810 12\nmid−career veteran\nCareer StagePercentageShoe Sponsor\nno\nyes0 0.5 1\nrookie\n(a) Career Stage and Shoe Sponsor\nFrequency\n024681012\ncenter forward guard\nPositionPercentageShoe Sponsor\nno\nyes0 0.5 1 (b) Position and Shoe Sponsor\nFigure 3.8\nExamples of using stacked bar plot visualizations to illustrate the relationship between two cate-\ngorical features: (a) C AREER STAGE and S HOE SPONSOR features; and (b) P OSITION and S HOE\nSPONSOR features, all from Table 3.7[73].\nuniform distribution of the A GEfeature. Figures 3.9(b)[79]and 3.9(d)[79]show a second\nexample, this time for the H EIGHT and P OSITION features. From Figure 3.9(b)[79]we can\nsee that H EIGHT follows a normal distribution centered around a mean of approximately\n194. The three smaller histograms depart from this distribution and suggest that centers\ntend to be taller than forwards, who in turn tend to be taller than guards.\nAn alternative approach to using small multiples to visualize the relationship between a\ncategorical feature and a continuous feature is to use a collection of box plots. For each\nlevel of the categorical feature, a box plot of the corresponding values of the continuous\nfeature is drawn. This gives multiple box plots that offer an easy comparison of how the\ncentral tendency and variation of the continuous feature change for the different levels of\nthe categorical feature. When a relationship exists between the two features, the box plots\nshould show differing central tendencies and variations. When no relationship exists, the\nbox plots should all appear similar.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":133,"page_label":"79","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 79\nAgeDensity\n20 25 30 350.00 0.10 0.20\n(a) Age\nHeightDensity\n160 170 180 190 200 210 2200.00 0.02 0.04 0.06 (b) Height\nPosition = gua rd\nAgeDensity\n20 25 30 350.00 0.05 0.10 0.15Position = center\nAgeDensity\n20 25 30 350.00 0.05 0.10 0.15Position = forwar d\nAgeDensity\n20 25 30 350.00 0.05 0.10 0.15\n(c) Age and Position\nPosition = gua rd\nHeightDensity\n160 170 180 190 200 210 2200.00 0.02 0.04 0.06Position = center\nHeightDensity\n160 170 180 190 200 210 2200.00 0.02 0.04 0.06Position = forwar d\nHeightDensity\n160 170 180 190 200 210 2200.00 0.02 0.04 0.06\n(d) Height and Position\nFigure 3.9\nExample of using small multiple histograms to visualize the relationship between a categorical fea-\nture and a continuous feature. All examples use data from the professional basketball team dataset\nin Table 3.7[73]: (a) a histogram of the A GEfeature; (b) a histogram of the H EIGHT feature; (c)\nhistograms of the A GEfeature for instances displaying each level of the P OSITION feature; and (d)\nhistograms of the H EIGHT feature for instances displaying each level of the P OSITION feature.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":134,"page_label":"80","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"80 Chapter 3 Data Exploration\n20 25 30 35Age\n(a) A GE\ncenter forward guard20 25 30 35\nPositionAge (b) A GEand P OSITION\n160 180 200 220Height\n(c) H EIGHT\ncenter forward guard160 180 200 220\nPositionHeight (d) H EIGHT and P OSITION\nFigure 3.10\nUsing box plots to visualize the relationships between categorical and continuous features from Table\n3.7[73]: (a) and (b) show the relationship between the P OSITION feature and the A GEfeature; and (c)\nand (d) show the relationship between the P OSITION feature and the H EIGHT feature.\nIn Figures 3.10(a)[80]and 3.10(b)[80]we illustrate the multiple box plot approach using the\nAGEand P OSITION features from the dataset in Table 3.7[73]. Figure 3.10(a)[80]shows a box\nplot for A GEacross the full dataset, while Figure 3.10(b)[80]shows individual box plots for\nAGEfor each level of the P OSITION feature. Similar to the histograms in Figure 3.9[79],\nthis visualization shows a slight indication that centers tend to be older than forwards and\nguards, but the three box plots overlap signiﬁcantly, suggesting that this relationship is not\nvery strong.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":135,"page_label":"81","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 81\nFigures 3.10(c)[80]and 3.10(d)[80]show a similar pair of visualizations for the H EIGHT and\nPOSITION features. Figure 3.10(d)[80]is typical of a series of box plots showing a strong\nrelationship between a continuous and a categorical feature. We can see that the average\nheight of centers is above that of forwards, which in turn is above that of guards. Although\nthe whiskers show that there is some overlap between the three groups, they do appear to\nbe well separated.\nHistograms show more detail than box plots, so small multiple histograms offer a more\ndetailed view of the relationship between two features. The differences in central tendency\nand variation between levels can, however, be easier to see in box plots. Box plots are\nalso better suited when the categorical feature has many levels—beyond four levels, small\nmultiple histograms tend to be difﬁcult to interpret. A good approach is to use box plots to\ninitially determine which pairs of features might have a strong relationship and then further\ninvestigate these pairs using small multiple histograms.\n3.5.2 Measuring Covariance and Correlation\nAs well as visually inspecting scatter plots, we can calculate formal measures of the re-\nlationship between two continuous features using covariance andcorrelation . For two\nfeatures, aandb, in a dataset of ninstances, the sample covariance between aandbis\ncovpa,bq“1\nn´1nÿ\ni“1´\npai´aqˆ´\nbi´b¯¯\n(3.3)\nwhere aiandbiare values of features aandbfor the ithinstance in a dataset, and aandb\nare the sample means of features aandb. Covariance values fall into the range r´8,8s\nwhere negative values indicate a negative relationship, positive values indicate a positive\nrelationship, and values near zero indicate that there is little or no relationship between the\nfeatures.\nTable 3.8[82]shows the workings for the calculation of the covariance between the H EIGHT\nfeature and the W EIGHT and A GEfeatures from the dataset in Table 3.7[73]. The table shows\nhow the´\npai´aqˆ´\nbi´b¯¯\nportion of Equation (3.3)[81]is calculated for each instance\nin the dataset for the two covariance calculations. Given this table we can calculate the\ncovariances as follows:\ncovpHEIGHT,WEIGHTq“7,009.9\n29“241.72\ncovpHEIGHT,AGEq“570.8\n29“ 19.7\nThese ﬁgures indicate that there is a strong positive relationship between the height and\nweight of a player, and a much smaller positive relationship between height and age. This","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":135,"page_label":"81","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"relationship, and values near zero indicate that there is little or no relationship between the\nfeatures.\nTable 3.8[82]shows the workings for the calculation of the covariance between the H EIGHT\nfeature and the W EIGHT and A GEfeatures from the dataset in Table 3.7[73]. The table shows\nhow the´\npai´aqˆ´\nbi´b¯¯\nportion of Equation (3.3)[81]is calculated for each instance\nin the dataset for the two covariance calculations. Given this table we can calculate the\ncovariances as follows:\ncovpHEIGHT,WEIGHTq“7,009.9\n29“241.72\ncovpHEIGHT,AGEq“570.8\n29“ 19.7\nThese ﬁgures indicate that there is a strong positive relationship between the height and\nweight of a player, and a much smaller positive relationship between height and age. This\nsupports the relationships suggested by the scatter plots of these pairs of features shown in\nFigures 3.5(a)[74]and 3.5(c)[74].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":136,"page_label":"82","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"82 Chapter 3 Data Exploration\nTable 3.8\nCalculating covariance.\nHEIGHT WEIGHT ph´hqˆ AGE ph´hqˆ\nID ( h)h´h (w)w´wpw´wq (a)a´apa´aq\n1 192 0.9 218 3 .0 2 .7 29 2 .6 2 .3\n2 218 26.9 251 36 .0 967 .5 35 8 .6 231 .3\n3 197 5.9 221 6 .0 35 .2 22´4.4´26.0\n4 192 0.9 219 4 .0 3 .6 22´4.4´4.0\n5 198 6.9 223 8 .0 55 .0 29 2 .6 17 .9\n...\n26 191´0.1 218 3 .0´0.3 19´7.4 0 .7\n27 196 4.9 235 20 .0 97 .8 32 5 .6 27 .4\n28 198 6.9 221 6 .0 41 .2 22´4.4´30.4\n29 207 15.9 247 32 .0 508 .3 27 0 .6 9 .5\n30 201 9.9 244 29 .0 286 .8 25´1.4´13.9\nMean 191.1 215 .0 26 .4\nStd. Dev. 13.6 19 .8 4 .2\nSum 7,009.9 570 .8\nThe table shows how the´\npai´aqˆ´\nbi´b¯¯\nportion of Equation (3.3)[81]is calculated for each instance in a\ndataset to arrive at the sum required in the calculation. The relevant means and standard deviations are also shown\n(standard deviation is not required to calculate covariance but is included as it will be useful later for calculating\ncorrelation).\nThis example also illustrates a problem with using covariance. Covariance is measured\nin the same units as the features that it measures. As a result, comparing the covariance\nbetween pairs of features only makes sense if each pair of features is composed of the same\nmixture of units. Correlation7is a normalized form of covariance that ranges between ´1\nand`1. We calculate the correlation between two features by dividing the covariance be-\ntween the two features by the product of their standard deviations. The correlation between\ntwo features, aandb, can be calculated as\ncorrpa,bq“covpa,bq\nsdpaqˆsdpbq(3.4)\nwhere covpa,bqis the covariance between features aandbandsdpaqandsdpbqare the\nstandard deviations of aandbrespectively. Because correlation is normalized, it is dimen-\n7. The correlation coefﬁcient presented here is more fully known as the Pearson product-moment correlation\ncoefﬁcient or Pearson’s rand is named after Karl Pearson, one of the giants of statistics.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":137,"page_label":"83","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 83\nsionless and, consequently, does not suffer from the interpretability difﬁculties associated\nwith covariance. Correlation values fall into the range r´1,1s, where values close to ´1\nindicate a very strong negative correlation (or covariance), values close to 1indicate a very\nstrong positive correlation, and values around 0indicate no correlation. Features that have\nno correlation are said to be independent .\nThe correlations between the H EIGHT and W EIGHT and A GEfeatures can be calculated,\nusing the covariances and standard deviations from Table 3.8[82], as follows:\ncorrpHeight,Weightq“241.72\n13.6ˆ19.8“0.898\ncorrpHeight,Ageq“19.7\n13.6ˆ4.2“0.345\nThese correlation values are much more useful than the covariances calculated previously\nbecause they are on a normalized scale, which allows us to compare the strength of the\nrelationships to each other. There is a strong positive correlation between H EIGHT and\nWEIGHT features, but very little correlation between H EIGHT and A GE.\nIn the majority of ABTs there are multiple continuous features between which we would\nlike to explore relationships. Two tools that can be useful for this are the covariance matrix\nand the correlation matrix. A covariance matrix contains a row and column for each fea-\nture, and each element of the matrix lists the covariance between the corresponding pairs\nof features. As a result, the elements along the main diagonal list the covariance between\na feature and itself, in other words, the variance of the feature. The covariance matrix,\nusually denoted asř, between a set of continuous features, ta,b,..., zu, is given as\nÿ\nta,b,...,zu“»\n————–varpaqcovpa,bq ¨¨¨ covpa,zq\ncovpb,aqvarpbq ¨¨¨ covpb,zq\n............\ncovpz,aqcovpz,bq ¨¨¨ varpzqﬁ\nﬃﬃﬃﬃﬂ(3.5)\nSimilarly, the correlation matrix is just a normalized version of the covariance matrix\nand shows the correlation between each pair of features:\ncorrelation matrix\nta,b,...,zu“»\n————–corrpa,aqcorrpa,bq ¨¨¨ corrpa,zq\ncorrpb,aqcorrpb,bq ¨¨¨ corrpb,zq\n............\ncorrpz,aqcorrpz,bq ¨¨¨ corrpz,zqﬁ\nﬃﬃﬃﬃﬂ(3.6)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":138,"page_label":"84","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"84 Chapter 3 Data Exploration\nThe covariance and correlation matrices for the H EIGHT , W EIGHT and A GEfeatures are\nÿ\ntHeight,Weight,Ageu“»\n—–185.128 241.72 19.7\n241.72 392.102 24.469\n19.7 24.469 17.697ﬁ\nﬃﬂ\nand\ncorrelation matrix\ntHeight,Weight,Ageu“»\n—–1.0 0.898 0.345\n0.898 1.0 0.294\n0.345 0.294 1.0ﬁ\nﬃﬂ\nThe scatter plot matrices (SPLOMs) described in Section 3.5.1[72]are really a visualiza-\ntion of the correlation matrix. This can be made more obvious by including the correlation\ncoefﬁcients in SPLOMs in the cells above the diagonal. In Figure 3.11[85]the cells above\nthe diagonal show the correlation coefﬁcients for each pair of features. The font sizes of\nthe correlation coefﬁcients are scaled according to the absolute value of the strength of the\ncorrelation to draw attention to those pairs of features with the strongest relationships.\nCorrelation is a good measure of the relationship between two continuous features, but\nit is not by any means perfect. First, the correlation measure given in Equation (3.4)[82]\nresponds only to linear relationships between features. In a linear relationship between\ntwo features, as one feature increases or decreases, the other feature increases or decreases\nby a corresponding amount. Frequently, features will have very strong non-linear relation-\nships that correlation does not respond to. Also, peculiarities in a dataset can affect the\ncalculation of the correlation between two features.\nThis problem is illustrated very clearly in the famous example of Anscombe’s quartet,8\nshown in Figure 3.12[86]. This is a series of four pairs of features that all have the same\ncorrelation value of 0.816, even though they exhibit very different relationships.\nPerhaps the most important thing to remember in relation to correlation is that correlation\ndoes not necessarily imply causation . Just because the values of two features are correlated\ndoes not mean that an actual causal relationship exists between the two. There are two\nmain ways in which causation can be mistakenly assumed. The ﬁrst is by mistaking the\norder of a causal relationship. For example, based on correlations tests alone, we might\nconclude that the presence of swallows cause hot weather, that spinning windmills cause\nwind, and that playing basketball causes people to be tall. In fact, swallows migrate to\nwarmer countries, windmills are made to spin by wind, and tall people often choose to\nplay basketball because of the advantage their height gives them in that game.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":138,"page_label":"84","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"does not necessarily imply causation . Just because the values of two features are correlated\ndoes not mean that an actual causal relationship exists between the two. There are two\nmain ways in which causation can be mistakenly assumed. The ﬁrst is by mistaking the\norder of a causal relationship. For example, based on correlations tests alone, we might\nconclude that the presence of swallows cause hot weather, that spinning windmills cause\nwind, and that playing basketball causes people to be tall. In fact, swallows migrate to\nwarmer countries, windmills are made to spin by wind, and tall people often choose to\nplay basketball because of the advantage their height gives them in that game.\nThe second kind of mistake that makes people incorrectly infer causation between two\nfeatures is ignoring a third important, but hidden, feature. In a famous example of this,\nan article was published in the prestigious journal Nature outlining a causal relationship\n8. Francis Anscombe was a famous statistician who published his quartet in 1973 (Anscombe, 1973).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":139,"page_label":"85","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.5 Advanced Data Exploration 85\n\u0018\nFigure 3.11\nA scatter plot matrix showing scatter plots of the continuous features from the professional basketball\nteam dataset in Table 3.7[73]with correlation coefﬁcients included.\nbetween young children sleeping with a night-light turned on and these children develop-\ning near-sightedness in later life (Quinn et al., 1999). Later studies (Zadnik et al., 2000;\nGwiazda et al., 2000), however, could not replicate this link, and eventually a more plau-\nsible explanation for the correlation between night-light use and near-sightedness was un-\ncovered. Near-sighted parents, because of their poor night vision, tend to favor the use of\nnight-lights to help them ﬁnd their way around their children’s bedrooms at night. Near-\nsighted parents are more likely to have near-sighted children, and it is this that accounts\nfor the correlation between night-light use and near-sightedness in children, rather than\nany causal link. This is an example of a confounding feature, a feature that inﬂuences two\nothers and so leads to the appearance of a causal relationship. Confounding features are\na common explanation of mistaken conclusions about causal relationships. The lesson to\nbe learned here is that before causation is concluded based on a strong correlation between","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":140,"page_label":"86","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"86 Chapter 3 Data Exploration\nAB\n4 7 10 13 16 194 7 10 13\nAB\n4 7 10 13 16 194 7 10 13\nAB\n4 7 10 13 16 194 7 10 13\nAB\n4 7 10 13 16 194 7 10 13\nFigure 3.12\nAnscombe’s quartet. For all four samples, the correlation measure returns the same value ( 0.816)\neven though the relationship between the features is very different in each case.\ntwo features, in-depth studies involving domain experts are required—correlation alone is\njust not enough. In spite of these difﬁculties, for machine learning purposes, correlation is\na very good measure of the relationship between two continuous features.9\n9. There are approaches to formally measuring the relationship between a pair of categorical features (for ex-\nample, theχ2test) and for measuring the relationship between a categorical feature and a continuous feature (for\nexample, the ANOV A test ). We do not cover these in this book, however, and readers are directed to the further\nreading section at the end of this chapter for information on these approaches.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":141,"page_label":"87","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.6 Data Preparation 87\n3.6 Data Preparation\nInstead of explicitly handling problems like noise within the data in an ABT, some data\npreparation techniques change the way data is represented just to make it more compatible\nwith certain machine learning algorithms. This section describes two of the most common\nsuch techniques: binning and normalization. Both techniques focus on transforming an\nindividual feature in some way. There are also situations, however, where we wish to\nchange the size and/or the distributions of target values within the ABT. We describe a\nrange of different sampling techniques that can be used to do this. As with the techniques\ndescribed in the previous section, sometimes these techniques are performed as part of the\nData Preparation phase of CRISP-DM, but sometimes they are performed as part of the\nModeling phase.\n3.6.1 Normalization\nHaving continuous features in an ABT that cover very different ranges can cause difﬁculty\nfor some machine learning algorithms. For example, a feature representing customer ages\nmight cover the range r16,96s, whereas a feature representing customer salaries might\ncover the range r10,000,100,000s.Normalization techniques can be used to change a\ncontinuous feature to fall within a speciﬁed range while maintaining the relative differ-\nences between the values for the feature. The simplest approach to normalization is range\nnormalization , which performs a linear scaling of the original values of the continuous\nfeature into a given range. We use range normalization to convert a feature value into the\nrangerlow,highsas follows:\na1\ni“ai´minpaq\nmaxpaq´minpaqˆphigh´lowq`low (3.7)\nwhere a1\niis the normalized feature value, aiis the original value, minpaqis the minimum\nvalue of feature a,maxpaqis the maximum value of feature a, and lowandhigh are the\nminimum and maximum values of the desired range. Typical ranges used for normalizing\nfeature values are r0,1sandr´1,1s. Table 3.9[88]shows the effect of applying range nor-\nmalization to a small sample of the H EIGHT and S PONSORSHIP EARNINGS features from\nthe dataset in Table 3.7[73].\nRange normalization has the drawback that it is quite sensitive to the presence of outliers\nin a dataset. Another way to normalize data is to standardize it into standard scores .10\nA standard score measures how many standard deviations a feature value is from the mean\nfor that feature. To calculate a standard score, we compute the mean and standard deviation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":141,"page_label":"87","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"value of feature a,maxpaqis the maximum value of feature a, and lowandhigh are the\nminimum and maximum values of the desired range. Typical ranges used for normalizing\nfeature values are r0,1sandr´1,1s. Table 3.9[88]shows the effect of applying range nor-\nmalization to a small sample of the H EIGHT and S PONSORSHIP EARNINGS features from\nthe dataset in Table 3.7[73].\nRange normalization has the drawback that it is quite sensitive to the presence of outliers\nin a dataset. Another way to normalize data is to standardize it into standard scores .10\nA standard score measures how many standard deviations a feature value is from the mean\nfor that feature. To calculate a standard score, we compute the mean and standard deviation\n10. A standard score is equivalent to a z-score , and standardizing in the way described here is also known as\napplying a z-transform to the data.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":142,"page_label":"88","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"88 Chapter 3 Data Exploration\nfor the feature and normalize the feature values using the following equation:\na1\ni“ai´a\nsdpaq(3.8)\nwhere a1\niis the normalized feature value, aiis the original value, ais the mean for fea-\nturea, and sdpaqis the standard deviation for a. Standardizing feature values in this ways\nsquashes the values of the feature so that the feature values have a mean of 0and a standard\ndeviation of 1. This results in the majority of feature values being in a range of r´1,1s. We\nshould take care when using standardization as it assumes that data is normally distributed.\nIf this assumption does not hold, then standardization may introduce some distortions.\nTable 3.9[88]also shows the effect of applying standardization to the H EIGHT and S PON-\nSORSHIP EARNINGS features.\nTable 3.9\nA small sample of the H EIGHT and S PONSORSHIP EARNINGS features from the professional bas-\nketball team dataset in Table 3.7[73], showing the result of range normalization and standardization.\nHEIGHT SPONSORSHIP EARNINGS\nValues Range Standard Values Range Standard\n192 0.500 -0.073 561 0.315 -0.649\n197 0.679 0.533 1,312 0.776 0.762\n192 0.500 -0.073 1,359 0.804 0.850\n182 0.143 -1.283 1,678 1.000 1.449\n206 1.000 1.622 314 0.164 -1.114\n192 0.500 -0.073 427 0.233 -0.901\n190 0.429 -0.315 1,179 0.694 0.512\n178 0.000 -1.767 1,078 0.632 0.322\n196 0.643 0.412 47 0.000 -1.615\n201 0.821 1.017 1,111 0.652 0.384\nMax 206 1,678\nMin 178 47\nMean 193 907\nStd. Dev. 8.26 532.18\nIn upcoming chapters we use normalization to prepare data for use with machine learning\nalgorithms that require descriptive features to be in particular ranges. As is so often the\ncase in data analytics, there is no hard and fast rule that says which is the best normalization\ntechnique, and this decision is generally made based on experimentation.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":143,"page_label":"89","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.6 Data Preparation 89\n3.6.2 Binning\nBinning involves converting a continuous feature into a categorical feature. To perform\nbinning, we deﬁne a series of ranges (called bins) for the continuous feature that corre-\nspond to the levels of the new categorical feature we are creating. The values for the new\ncategorical feature are then created by assigning to instances in the dataset the level of the\nnew feature that corresponds to the range that their value of the continuous feature falls\ninto. There are many different approaches to binning. We will introduce two of the more\npopular: equal-width binning andequal-frequency binning .\nBoth equal-width and equal-frequency binning require that we manually specify how\nmany bins we would like to use. Deciding on the number of bins can be difﬁcult. The\ngeneral trade-off is this:\n‚If we set the number of bins to a very low number—for example, two or three bins—\n(in other words, we abstract to a very low level of resolution), we may lose a lot of\ninformation with respect to the distribution of values in the original continuous feature.\nUsing a small number of bins, however, has the advantage of having a large number of\ninstances in each bin.\n‚If we set the number of bins to a high number—for example, 10 or more—then, just\nbecause there are more bin boundaries, it is more likely that at least some of our bins will\nalign with interesting features of the distribution of the original continuous feature. This\nmeans that our binning categories will provide a better representation of this distribution.\nHowever, the more bins we have, the fewer instances we will have in each bin. Indeed,\nas the number of bins grows, we can end up with empty bins.\nFigure 3.13[90]illustrates the effect of using different numbers of bins.11In this example,\nthe dashed line represents a multimodal distribution from which a set of continuous fea-\nture values has been generated. The histogram represents the bins. Ideally the histogram\nheights should follow the dashed line. In Figure 3.13(a)[90]there are three bins that are each\nquite wide, and the histogram heights don’t really follow the dashed line. This indicates\nthat this binning does not accurately represent the real distribution of values in the under-\nlying continuous feature. In Figure 3.13(b)[90]there are 14 bins. In general, the histogram\nheights follow the dashed line, so the resulting bins can be considered a reasonable repre-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":143,"page_label":"89","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Figure 3.13[90]illustrates the effect of using different numbers of bins.11In this example,\nthe dashed line represents a multimodal distribution from which a set of continuous fea-\nture values has been generated. The histogram represents the bins. Ideally the histogram\nheights should follow the dashed line. In Figure 3.13(a)[90]there are three bins that are each\nquite wide, and the histogram heights don’t really follow the dashed line. This indicates\nthat this binning does not accurately represent the real distribution of values in the under-\nlying continuous feature. In Figure 3.13(b)[90]there are 14 bins. In general, the histogram\nheights follow the dashed line, so the resulting bins can be considered a reasonable repre-\nsentation of the continuous feature. Also, there are no gaps between the histogram bars,\nwhich indicates that there are no empty bins. Finally, Figure 3.13(c)[90]illustrates what\nhappens when we used 60 bins. The histogram heights ﬁt the contour line to an extent, but\nthere is a greater variance in the heights across the bins in this image. Some of the bins\nare very tall and other bins are empty, as indicated by the gaps between the bars. When we\n11. These images were generated using equal-width binning. However, the points discussed in the text are also\nrelevant to equal-frequency binning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":144,"page_label":"90","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"90 Chapter 3 Data Exploration\nValueDensity\n50 60 70 80 900.00 0.01 0.02 0.03 0.04 0.05 0.06\n(a) 3 bins\nValueDensity\n50 60 70 80 900.00 0.01 0.02 0.03 0.04 0.05 0.06 (b) 14 bins\nValueDensity\n50 60 70 80 900.00 0.01 0.02 0.03 0.04 0.05 0.06 (c) 60 bins\nFigure 3.13\nThe effect of using different numbers of bins when using binning to convert a continuous feature into\na categorical feature.\ncompare, the three images, 14 bins seems to best model the data. Unfortunately, there is no\nguaranteed way of ﬁnding the optimal number of bins for a set of values for a continuous\nfeature. Often, choosing the number of bins comes down to intuition and a process of trial\nand error experimentation.\nOnce the number of bins, b, has been chosen, the equal-width binning algorithm splits\nthe range of the feature values into bbins each of sizerange\nb. For example, if the values for\na feature fell between zero and 100 and we wished to have 10 bins, then bin 1 would cover\nthe interval12r0,10q, bin 2 would cover the interval r10,20q, and so on, up to bin 10, which\nwould cover the interval r90,100s. Consequently, an instance with a feature value of 18\nwould be placed into bin 2.\nEqual-width binning is simple and intuitive, and can work well in practice. However, as\nthe distribution of values in the continuous feature moves away from a uniform distribution,\nthen some bins will end up with very few instances in them, and other bins will have a lot\nof instances in them. For example, imagine our data followed a normal distribution: then\nthe bins covering the intervals of the feature range at the tails of the normal distribution\nwill have very few instances, and the bins covering the intervals of the feature range near\nthe mean will contain a lot of instances. This scenario is illustrated in Figures 3.14(a)[92]to\n3.14(c)[92], which shows a continuous feature following a normal distribution converted into\ndifferent numbers of bins using equal-width binning. The problem with this is that we are\nessentially wasting bins because some of the bins end up representing a very small number\n12. In interval notation, a square bracket, rors, indicates that the boundary value is included in the interval, and\na parenthesis,porq, indicates that it is excluded from the interval.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":145,"page_label":"91","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.6 Data Preparation 91\nof instances (the height of the bars in the diagram shows the number of instances in each\nbin). If we were able to merge the bins in the regions where there are very few instances,\nthen the resulting spare bins could be used to represent the differences between instances\nin the regions where lots of instances are clustered together. Equal-frequency binning does\nthis.\nEqual-frequency binning ﬁrst sorts the continuous feature values into ascending order\nand then places an equal number of instances into each bin, starting with bin 1. The\nnumber of instances placed in each bin is simply the total number of instances divided\nby the number of bins, b. For example, if we had 10,000 instances in our dataset and\nwe wish to have 10 bins, then bin 1 would contain the 1,000 instances with the lowest\nvalues for the feature, and so on, up to bin 10, which would contain the 1,000 instances\nwith the highest feature values. Figures 3.14(d)[92]to 3.14(f)[92]show the same normally\ndistributed continuous feature mentioned previously binned into different numbers of bins\nusing equal-frequency binning.13\nUsing Figure 3.14[92]to compare these two approaches to binning, we can see that by\nvarying the width of the bins, equal-frequency binning uses bins to more accurately model\nthe heavily populated areas of the range of values the continuous feature can take. The\ndownside to this is that the resulting bins can appear slightly less intuitive because they are\nof varying sizes.\nRegardless of the binning approach used, once the values for a continuous feature have\nbeen binned, the continuous feature is discarded and replaced by a categorical feature,\nwhich has a level for each bin—the bin numbers can be used, or a more meaningful label\ncan be manually generated. We will see in forthcoming chapters that using binning to\ntransform a continuous feature into a categorical feature is often the easiest way for some\nof the machine learning approaches to handle a continuous feature. Another advantage\nof binning, especially equal-frequency binning, is that it goes some way toward handling\noutliers. Very large or very small values simply end up in the highest or lowest bin. It is\nimportant to remember, though, that no matter how well it is done, binning always discards\ninformation from the dataset because it abstracts from a continuous representation to a\ncoarser categorical resolution.\n3.6.3 Sampling","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":145,"page_label":"91","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"can be manually generated. We will see in forthcoming chapters that using binning to\ntransform a continuous feature into a categorical feature is often the easiest way for some\nof the machine learning approaches to handle a continuous feature. Another advantage\nof binning, especially equal-frequency binning, is that it goes some way toward handling\noutliers. Very large or very small values simply end up in the highest or lowest bin. It is\nimportant to remember, though, that no matter how well it is done, binning always discards\ninformation from the dataset because it abstracts from a continuous representation to a\ncoarser categorical resolution.\n3.6.3 Sampling\nIn some predictive analytics scenarios, the dataset we have is so large that we do not use all\nthe data available to us in an ABT and instead sample a smaller percentage from the larger\ndataset. We need to be careful when sampling, however, to ensure that the resulting datasets\nare still representative of the original data and that no unintended bias is introduced during\n13. The bins created when equal-frequency binning is used are equivalent to percentiles (discussed in Section\nA.1[745]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":146,"page_label":"92","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"92 Chapter 3 Data Exploration\nValueDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008\n(a) 5 equal-width bins\nDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008\nValue (b) 10 equal-width bins\nValueDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008 (c) 15 equal-width bins\nValueDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008\n(d) 5 equal-frequency bins\nValueDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008 (e) 10 equal-frequency bins\nValueDensity\n0 100 200 300 4000.000 0.002 0.004 0.006 0.008 (f) 15 equal-frequency bins\nFigure 3.14\n(a)–(c) Equal-frequency binning of normally distributed data with different numbers of bins; and\n(d)–(f) the same data binned into the same number of bins using equal-width binning. The dashed\nlines illustrate the distribution of the original continuous feature values, and the gray boxes represent\nthe bins.\nthis process. Biases are introduced when, due to the sampling process, the distributions of\nfeatures in the sampled dataset are very different from the distributions of features in the\noriginal dataset. The danger of this is that any analysis or modeling we perform on this\nsample will not be relevant to the overall dataset.\nThe simplest form of sampling is top sampling , which simply selects the top s%of\ninstances from a dataset to create a sample. Top sampling runs a serious risk of introducing\nbias, however, as the sample will be affected by any ordering of the original dataset. For\nthis reason, we recommend that top sampling be avoided.\nA better choice, and our recommended default, is random sampling , which randomly\nselects a proportion of s%of the instances from a large dataset to create a smaller set.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":147,"page_label":"93","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.6 Data Preparation 93\nRandom sampling is a good choice in most cases, as the random nature of the selection of\ninstances should avoid introducing bias.\nSometimes there are very speciﬁc relationships in a dataset that we want to maintain in\na sample. For example, if we have a categorical target feature, we may want to ensure that\nthe sample has exactly the same distribution of the different levels of the target feature as\nthe original dataset. In most cases random sampling will maintain distributions; however,\nif there are one or more levels of a categorical feature that only a very small proportion of\ninstances in a dataset have, there is a chance that these will be omitted or underrepresented\nby random sampling. Stratiﬁed sampling is a sampling method that ensures that the\nrelative frequencies of the levels of a speciﬁc stratiﬁcation feature are maintained in the\nsampled dataset.\nTo perform stratiﬁed sampling, the instances in a dataset are ﬁrst divided into groups\n(or strata), where each group contains only instances that have a particular level for the\nstratiﬁcation feature. The s%of the instances in each stratum are then randomly selected,\nand these selections are combined to give an overall sample of s%of the original dataset.\nRemember that each stratum will contain a different number of instances, so by sampling\non a percentage basis from each stratum, the number of instances taken from each stratum\nwill be proportional to the number of instances in each stratum. As a result, this sampling\nstrategy is guaranteed to maintain the relative frequencies of the different levels of the\nstratiﬁcation feature.\nIn contrast to stratiﬁed sampling, sometimes we would like a sample to contain different\nrelative frequencies of the levels of a particular feature than the distribution in the original\ndataset. For example, we may wish to create a sample in which the levels of a particular\ncategorical feature are represented equally, rather than with whatever distribution they had\nin the original dataset. To do this, we can use under-sampling orover-sampling .\nLike stratiﬁed sampling, under-sampling begins by dividing a dataset into groups, where\neach group contains only instances that have a particular level for the feature to be under-\nsampled. The number of instances in the smallest group is the under-sampling target size.\nEach group containing more instances than the smallest one is then randomly sampled by","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":147,"page_label":"93","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"relative frequencies of the levels of a particular feature than the distribution in the original\ndataset. For example, we may wish to create a sample in which the levels of a particular\ncategorical feature are represented equally, rather than with whatever distribution they had\nin the original dataset. To do this, we can use under-sampling orover-sampling .\nLike stratiﬁed sampling, under-sampling begins by dividing a dataset into groups, where\neach group contains only instances that have a particular level for the feature to be under-\nsampled. The number of instances in the smallest group is the under-sampling target size.\nEach group containing more instances than the smallest one is then randomly sampled by\nthe appropriate percentage to create a subset that is the under-sampling target size. These\nunder-sampled groups are then combined to create the overall under-sampled dataset.\nOver-sampling addresses the same issue as under-sampling but in the opposite way\naround. After dividing the dataset into groups, the number of instances in the largest\ngroup becomes the over-sampling target size. From each smaller group, we then create a\nsample containing that number of instances. To create a sample that is larger than the size\nof the group that we are sampling from, we use random sampling with replacement. This\nmeans that when an instance is randomly selected from the original dataset, it is replaced\ninto the dataset so that it might be selected again. The consequence of this is that each in-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":148,"page_label":"94","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"94 Chapter 3 Data Exploration\nstance from the original dataset can appear more than once in the sampled dataset.14After\nhaving created the larger samples from each group, we combine these to form the overall\nover-sampled dataset.\nSampling techniques can be used to reduce the size of a large ABT to make exploratory\nanalysis easier, to change the distributions of target features in an ABT, and to generate\ndifferent portions of an ABT to use for training and evaluating a model.\n3.7 Summary\nFor a data analytics practitioner, the key outcomes of the data exploration process (which\nstraddles the Data Understanding andData Preparation phases of CRISP-DM ) are that\nthe practitioner should\n1.Have gotten to know the features within the ABT, especially their central tendencies,\nvariations, and distributions .\n2.Have identiﬁed any data quality issues within the ABT, in particular, missing values ,\nirregular cardinality , and outliers .\n3.Have corrected any data quality issues due to invalid data .\n4.Have recorded any data quality issues due to valid data in adata quality plan along\nwith potential handling strategies.\n5.Be conﬁdent that enough good-quality data exists to continue with a project.\nAlthough the data quality report is just a collection of simple descriptive statistics and\nvisualizations of the features in an analytics base table , it is a very powerful tool and the\nkey to achieving the outcomes listed above. By examining the data quality report, analytics\npractitioners can get a complete picture of the data that they will work with for the rest of\nan analytics project. In this chapter we have focused on using the data quality report to\nexplore the data in an ABT. A data quality report, however, can also be used to explore any\ndataset and is commonly used to understand the data in the raw data sources that are used\nto populate an ABT.\nWe also took our ﬁrst steps toward building predictive models in this chapter when we\nlooked at correlation . A descriptive feature that correlates strongly with a target feature\nwould be a good place to start building a predictive model, and we return to correlations in\nlater chapters. Examining correlation between features as part of data exploration allows\nus to add extra outcomes to the list at the beginning of this section:\n1.Be aware of the relationships between features in an ABT.\n2.Have begun the feature selection exercise by removing some features from the ABT.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":148,"page_label":"94","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"dataset and is commonly used to understand the data in the raw data sources that are used\nto populate an ABT.\nWe also took our ﬁrst steps toward building predictive models in this chapter when we\nlooked at correlation . A descriptive feature that correlates strongly with a target feature\nwould be a good place to start building a predictive model, and we return to correlations in\nlater chapters. Examining correlation between features as part of data exploration allows\nus to add extra outcomes to the list at the beginning of this section:\n1.Be aware of the relationships between features in an ABT.\n2.Have begun the feature selection exercise by removing some features from the ABT.\n14. Although we didn’t mention it explicitly in other cases where we mentioned random sampling, we meant\nrandom sampling without replacement.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":149,"page_label":"95","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.8 Further Reading 95\nThe previous section of the chapter (Section 3.6[87]) focused on data preparation tech-\nniques that we can use on the data in an ABT. It is important to remember that when we\nperform data preparations (such as those in Section 3.6[87]or those described in Section\n3.4[69]), we are changing the data that we will use to subsequently train predictive models.\nIf we change the data too much, then the models that we build will not relate well to the\noriginal data sources when we deploy them. There is, therefore, a delicate balance that\nwe need to strike between preparing the data so that it is appropriate for use with machine\nlearning algorithms and keeping the data true to the underlying processes that generate it.\nWell-designed evaluation experiments are the best way to ﬁnd this balance (we discuss\nevaluation in detail in Chapter 9[533]).\nThe last point worth mentioning is that this chapter relates to deployment. The data in\nan ABT is historical data from the disparate data sources within an organization. We use\nthis data to train and evaluate a machine learning model that will then be deployed for use\non newly arising data. For example, in the motor insurance fraud detection example that\nwe used in this chapter, the claims in the ABT were all historical. The prediction model\nthat we would build using this data would be deployed to predict whether newly arising\nclaims are likely to be fraudulent. It is important that the details of any data preparation\ntechniques we perform on the data in the ABT be saved (usually in the data quality plan)\nso that we can also apply the same techniques to newly arising data. This is an important\ndetail of model deployment that is sometimes overlooked, which can lead to strange model\nperformance.\n3.8 Further Reading\nThe basis of data exploration is statistics. Montgomery and Runger (2010) is an excellent\napplied introductory text in statistics and covers, in more detail, all the basic measures\nused in this chapter. It also covers advanced topics, such as the χ2test and ANOV A\ntestmentioned in the notes for Section 3.5.2[81]. Rice (2006) provides a good—if more\ntheoretical—treatment of statistics.\nFor the practical details of building a data quality report, Svolba (2012, 2007) are very\ngood, even if the SAS language is not being used. Similarly, Dalgaard (2008) is very good\neven if the R language is not being used. As an example of a detailed investigation into the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":149,"page_label":"95","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"performance.\n3.8 Further Reading\nThe basis of data exploration is statistics. Montgomery and Runger (2010) is an excellent\napplied introductory text in statistics and covers, in more detail, all the basic measures\nused in this chapter. It also covers advanced topics, such as the χ2test and ANOV A\ntestmentioned in the notes for Section 3.5.2[81]. Rice (2006) provides a good—if more\ntheoretical—treatment of statistics.\nFor the practical details of building a data quality report, Svolba (2012, 2007) are very\ngood, even if the SAS language is not being used. Similarly, Dalgaard (2008) is very good\neven if the R language is not being used. As an example of a detailed investigation into the\nimpact of applying data preparation techniques, Batista and Monard (2003) is interesting.\nData visualization is a mix of statistics, graphic design, art, and psychology. Chang\n(2012) and Fry (2007) both provide great detail on visualization in general and the R lan-\nguage in particular (the visualizations in this book are almost all generated in R). For more\nconceptual discussions of data visualization, Tufte (2001) and Bertin (2010) are important\nworks in the ﬁeld.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":150,"page_label":"96","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"96 Chapter 3 Data Exploration\n3.9 Exercises\n1.The table below shows the age of each employee at a cardboard box factory.\nID 1 2 3 4 5 6 7 8 9 10\nAGE 51 39 34 27 23 43 41 55 24 25\nID 11 12 13 14 15 16 17 18 19 20\nAGE 38 17 21 37 35 38 31 24 35 33\nBased on this data, calculate the following summary statistics for the A GEfeature:\n(a)Minimum, maximum, and range\n(b)Mean and median\n(c)Variance and standard deviation\n(d)1stquartile ( 25thpercentile) and 3rdquartile ( 75thpercentile)\n(e)Inter-quartile range\n(f)12thpercentile\n2.The table below shows the policy type held by customers at a life insurance company.\nID P OLICY\n1 Silver\n2 Platinum\n3 Gold\n4 Gold\n5 Silver\n6 Silver\n7 BronzeID P OLICY\n8 Silver\n9 Platinum\n10 Platinum\n11 Silver\n12 Gold\n13 Platinum\n14 SilverID P OLICY\n15 Platinum\n16 Silver\n17 Platinum\n18 Platinum\n19 Gold\n20 Silver\n(a)Based on this data, calculate the following summary statistics for the P OLICY\nfeature:\ni.Mode and 2ndmode\nii.Mode % and 2ndmode %\n(b)Draw a bar plot for the P OLICY feature.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":151,"page_label":"97","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 97\n3.An analytics consultant at an insurance company has built an ABT that will be used to\ntrain a model to predict the best communications channel to use to contact a potential\ncustomer with an offer of a new insurance product.15The following table contains an\nextract from this ABT—the full ABT contains 5,200instances.\nHEALTH HEALTH\nMOTOR MOTOR HEALTH HEALTH DEPS DEPS PREF\nID O CC GENDER AGE LOC INS VALUE INS TYPE ADULTS KIDS CHANNEL\n1 Student female 43 urban yes 42,632 yes PlanC 1 2 sms\n2 female 57 rural yes 22,096 yes PlanA 1 2 phone\n3 Doctor male 21 rural yes 27,221 no phone\n4 Sheriff female 47 rural yes 21,460 yes PlanB 1 3 phone\n5 Painter male 55 rural yes 13,976 no phone\n.........\n14 male 19 rural yes 48,66 no email\n15 Manager male 51 rural yes 12,759 no phone\n16 Farmer male 49 rural no no phone\n17 female 18 urban yes 16,399 no sms\n18 Analyst male 47 rural yes 14,767 no email\n.........\n2747 female 48 rural yes 35,974 yes PlanB 1 2 phone\n2748 Editor male 50 urban yes 40,087 no phone\n2749 female 64 rural yes 156,126 yes PlanC 0 0 phone\n2750 Reporter female 48 urban yes 27,912 yes PlanB 1 2 email\n.........\n4780 Nurse male 49 rural no yes PlanB 2 2 email\n4781 female 46 rural yes 18,562 no phone\n4782 Courier male 63 urban no yes PlanA 2 0 email\n4783 Sales male 21 urban no no sms\n4784 Surveyor female 45 rural yes 17,840 no sms\n.........\n5199 Clerk male 48 rural yes 19,448 yes PlanB 1 3 email\n5200 Cook 47 female rural yes 16,393 yes PlanB 1 2 sms\nThe descriptive features in this dataset are deﬁned as follows:\n‚AGE: The customer’s age\n‚GENDER : The customer’s gender ( male orfemale )\n‚LOC: The customer’s location ( rural orurban )\n‚OCC: The customer’s occupation\n15. The data used in this question have been artiﬁcially generated for this book. Channel propensity modeling is\nused widely in industry; for example, see Hirschowitz (2001).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":152,"page_label":"98","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"98 Chapter 3 Data Exploration\n‚MOTOR INS: Whether the customer holds a motor insurance policy with the com-\npany ( yesorno)\n‚MOTOR VALUE : The value of the car on the motor policy\n‚HEALTH INS: Whether the customer holds a health insurance policy with the com-\npany ( yesorno)\n‚HEALTH TYPE: The type of the health insurance policy ( PlanA ,PlanB , orPlanC )\n‚HEALTH DEPSADULTS : How many dependent adults are included on the health\ninsurance policy\n‚HEALTH DEPSKIDS: How many dependent children are included on the health\ninsurance policy\n‚PREFCHANNEL : The customer’s preferred contact channel ( email ,phone , orsms)\nThe consultant generated the following data quality report from the ABT (visualiza-\ntions of binary features have been omitted for space saving).\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nAGE 5,200 0 51 18 22 41.59 47 50 80 15.66\nMOTOR VALUE 5,200 17.25 3,934 4,352 15,089.5 23,479 24,853 32,078 166,993 11,121\nHEALTH DEPSADULTS 5,200 39.25 4 0 0 0.84 1 1 2 0.65\nHEALTH DEPSKIDS 5,200 39.25 5 0 0 1.77 2 3 3 1.11\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nGENDER 5,200 0 2 female 2,626 50.5 male 2,574 49.5\nLOC 5,200 0 2 urban 2,948 56.69 rural 2,252 43.30\nOCC 5,200 37.71 1,828 Nurse 11 0.34 Sales 9 0.28\nMOTOR INS 5,200 0 2 yes 4,303 82.75 no 897 17.25\nHEALTH INS 5,200 0 2 yes 3,159 60.75 no 2,041 39.25\nHEALTH TYPE 5,200 39.25 4 PlanB 1,596 50.52 PlanA 796 25.20\nPREFCHANNEL 5,200 0 3 email 2,296 44.15 phone 1,975 37.98","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":153,"page_label":"99","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 99\nDensity\n20 30 40 50 60 70 800.00 0.02 0.04 0.06 0.08 0.10\nDensity\n0 50000 100000 1500000.00000 0.00001 0.00002 0.00003\n0 1 2Density\n0.00 0.10 0.20 0.30\nAGE MOTOR VALUE HEALTH DEPSADULTS\n0 1 2 3Density\n0.00 0.05 0.10 0.15 0.20 0.25\nPlanA PlanB PlanCDensity\n0.0 0.1 0.2 0.3\nEmail Phone SMSDensity\n0.0 0.1 0.2 0.3 0.4\nHEALTH DEPSKIDS HEALTH TYPE PREFCHANNEL\nDiscuss this data quality report in terms of the following:\n(a)Missing values\n(b)Irregular cardinality\n(c)Outliers\n(d)Feature distributions\n4.The following data visualizations are based on the channel prediction dataset given\nin Question 3. Each visualization illustrates the relationship between a descriptive\nfeature and the target feature, P REFCHANNEL . Each visualization is composed of\nfour plots: one plot of the distribution of the descriptive feature values in the entire\ndataset, and three plots illustrating the distribution of the descriptive feature values\nfor each level of the target. Discuss the strength of the relationships shown in each\nvisualization.\n(a)The visualization below illustrates the relationship between the continuous feature\nAGEand the target feature, P REFCHANNEL .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":154,"page_label":"100","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"100 Chapter 3 Data Exploration\nAgeDensity\n20 30 40 50 60 70 800.00 0.02 0.04 0.06 0.08 0.10\nPrefChannel = SMS\nAgeDensity\n20 30 40 50 60 70 800.00 0.05 0.10 0.15 0.20PrefChannel = Phone\nAgeDensity\n20 30 40 50 60 70 800.00 0.05 0.10 0.15 0.20PrefChannel = Email\nAgeDensity\n20 30 40 50 60 70 800.00 0.05 0.10 0.15 0.20\n(b)The visualization below illustrates the relationship between the categorical feature\nGENDER and the target feature P REFCHANNEL .\nfemale male\nGenderDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nfemale malePrefChannel = SMS\nGenderDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nfemale malePrefChannel = Phone\nGenderDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nfemale malePrefChannel = Email\nGenderDensity\n0.0 0.1 0.2 0.3 0.4 0.5","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":155,"page_label":"101","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 101\n(c)The visualization below illustrates the relationship between the categorical feature\nLOCand the target feature, P REFCHANNEL .\nRural Urban\nLocDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nRural UrbanPrefChannel = SMS\nLocDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nRural UrbanPrefChannel = Phone\nLocDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nRural UrbanPrefChannel = Email\nLocDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\n5.The table below shows the scores achieved by a group of students on an exam.\nID 1 2 3 4 5 6 7 8 9 10\nSCORE 42 47 59 27 84 49 72 43 73 59\nID 11 12 13 14 15 16 17 18 19 20\nSCORE 58 82 50 79 89 75 70 59 67 35\nUsing this data, perform the following tasks on the S CORE feature:\n(a)Arange normalization that generates data in the range p0,1q\n(b)Arange normalization that generates data in the range p´1,1q\n(c)Astandardization of the data\n6.The following table shows the IQs for a group of people who applied to take part in a\ntelevision general-knowledge quiz.\nID 1 2 3 4 5 6 7 8 9 10\nIQ 92 107 83 101 107 92 99 119 93 106\nID 11 12 13 14 15 16 17 18 19 20\nIQ 105 88 106 90 97 118 120 72 100 104","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":156,"page_label":"102","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"102 Chapter 3 Data Exploration\nUsing this dataset, generate the following binned versions of the IQ feature:\n(a)An equal-width binning using 5 bins.\n(b)An equal-frequency binning using 5 bins\n˚7.Comment on the distributions of the features shown in each of the following his-\ntograms.\nHeightDensity\n50 100 150 200 2500.000 0.004 0.008 0.012\nConvictionsDensity\n0 10 20 30 400.00 0.05 0.10 0.15 0.20 0.25\nLDL cholesterolDensity\n80100 120 140 160 180 2000.000 0.010 0.020\n(a) (b) (c)\nIDDensity\n0 200 400 600 800 10000.0000 0.0004 0.0008\nSalar yDensity\n20000 60000 1000000.00000 0.00001 0.00002 0.00003\n(d) (e)\n(a)The height of employees in a trucking company.\n(b)The number of prior criminal convictions held by people given prison sentences\nin a city district over the course of a full year.\n(c)The LDL cholesterol values for a large group of patients, including smokers and\nnon-smokers.\n(d)The employee ID numbers of the academic staff at a university.\n(e)The salaries of car insurance policyholders.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":157,"page_label":"103","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 103\n˚8.The table below shows socioeconomic data for a selection of countries for the year\n2009,16using the following features:\n‚COUNTRY : The name of the country\n‚LIFEEXPECTANCY : The average life expectancy (in years)\n‚INFANT MORTALITY : The infant mortality rate (per 1,000live births)\n‚EDUCATION : Spending per primary student as a percentage of GDP\n‚HEALTH : Health spending as a percentage of GDP\n‚HEALTH USD: Health spending per person converted into US dollars\nLIFE INFANT HEALTH\nCOUNTRY EXPECTANCY MORTALITY EDUCATION HEALTH USD\nArgentina 75.592 13.500 16.841 9.525 734.093\nCameroon 53.288 67.700 7.137 4.915 60.412\nChile 78.936 7.800 17.356 8.400 801.915\nColombia 73.213 16.500 15.589 7.600 391.859\nCuba 78.552 4.800 44.173 12.100 672.204\nGhana 60.375 52.500 11.365 5.000 54.471\nGuyana 65.560 31.200 8.220 6.200 166.718\nLatvia 71.736 8.500 31.364 6.600 756.401\nMalaysia 74.306 7.100 14.621 4.600 316.478\nMali 53.358 85.500 14.979 5.500 33.089\nMongolia 66.564 26.400 15.121 5.700 96.537\nMorocco 70.012 29.900 16.930 5.200 151.513\nSenegal 62.653 48.700 17.703 5.700 59.658\nSerbia 73.532 6.900 61.638 10.500 576.494\nThailand 73.627 12.700 24.351 4.200 160.136\n(a)Calculate the correlation between the L IFEEXPECTANCY and I NFANT MORTAL -\nITYfeatures.\n(b)The image below shows a scatter plot matrix of the continuous features from this\ndataset (the correlation between L IFEEXPECTANCY and I NFANT MORTALITY has\nbeen omitted). Discuss the relationships between the features in the dataset that\nthis scatter plot highlights.\n16. The data listed in this table is real and was amalgamated from a number of reports that were retrieved\nfrom Gapminder (www.gapminder.org). The E DUCATION data is based on a report from the World Bank\n(data.worldbank.org/indicator/SE.XPD.PRIM.PC.ZS); the H EALTH and H EALTH USD data are based on reports\nfrom the World Health Organization (www.who.int); all the other features are based on reports created by Gap-\nminder.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":158,"page_label":"104","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"104 Chapter 3 Data Exploration\n˚9.Tachycardia is a condition that causes the heart to beat faster than normal at rest. The\noccurrence of tachycardia can have serious implications including increased risk of\nstroke or sudden cardiac arrest. An analytics consultant has been hired by a major\nhospital to build a predictive model that predicts the likelihood that a patient at a heart\ndisease clinic will suffer from tachycardia in the month following a visit to the clinic.\nThe hospital will use this model to make predictions for each patient when they visit\nthe clinic and offer increased monitoring for those deemed to be at risk. The analytics\nconsultant has generated an ABT to be used to train this model.17The descriptive\nfeatures in this dataset are deﬁned as follows:\n‚AGE: The patient’s age\n‚GENDER : The patient’s gender ( male orfemale )\n‚WEIGHT : The patient’s weight\n‚HEIGHT : The patient’s height\n17. The data used in this question have been artiﬁcially generated for this book. This type of application of\nmachine learning techniques, however, is common; for example, see Osowski et al. (2004).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":159,"page_label":"105","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 105\n‚BMI: The patient’s body mass index (BMI) which is calculated asweight\nheight2where\nweight is measured in kilograms and height in meters.\n‚SYS. B.P.: The patient’s systolic blood pressure\n‚DIA. B.P.: The patient’s diastolic blood pressure\n‚HEART RATE: The patient’s heart rate\n‚H.R. D IFF.: The difference between the patient’s heart rate at this visit and at their\nlast visit to the clinic\n‚PREV. TACHY .: Has the patient suffered from tachycardia before?\n‚TACHYCARDIA : Is the patient at high risk of suffering from tachycardia in the next\nmonth?\nThe following table contains an extract from this ABT—the full ABT contains 2,440\ninstances.\nSYS. D IA. H EART H.R. P REV.\nID A GE GENDER WEIGHT HEIGHT BMI B.P. B.P. R ATE DIFF. T ACHY . T ACHYCARDIA\n1 6 male 78 165 28.65 161 97 143 true\n2 5 m 117 171 40.01 216 143 162 17 true true\n.........\n143 5 male 108 1.88 305,568.13 139 99 84 21 false true\n144 4 male 107 183 31.95 1,144 90 94 -8 false true\n.........\n1,158 6 female 92 1.71 314,626.72 111 75 75 -5 false\n1,159 3 female 151 1.59 596,495.39 124 91 115 23 true true\n.........\n1,702 3 male 86 193 23.09 138 81 83 false false\n1,703 6 f 73 166 26.49 134 86 84 -4 false\n.........\nThe consultant generated the following data quality report from the ABT.\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nAGE 2,440 0.00 7 1.00 3.00 3.88 4.00 5.00 7.00 1.22\nWEIGHT 2,440 0.00 174 0.00 81.00 95.70 95.00 107.00 187.20 20.89\nHEIGHT 2,440 0.00 109 1.47 162.00 162.21 171.50 179.00 204.00 41.06\nBMI 2,440 0.00 1,385 0.00 27.64 18,523.40 32.02 38.57 596,495.39 77,068.75\nSYS.B.P. 2,440 0.00 149 62.00 115.00 127.84 124.00 135.00 1,144.00 29.11\nDIA. B.P. 2,440 0.00 109 46.00 77.00 86.34 84.00 92.00 173.60 14.25\nHEART RATE 2,440 0.00 119 57.00 91.75 103.28 100.00 110.00 190.40 18.21\nH.R. D IFF. 2,440 13.03 78 -50.00 -4.00 3.00 1.00 8.00 47.00 12.38","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":160,"page_label":"106","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"106 Chapter 3 Data Exploration\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nGENDER 2,440 0.00 4 male 1,591.00 65.20 female 647.00 26.52\nPREV. TACHY . 2,440 44.02 3 false 714.00 52.27 true 652.00 47.73\nTACHYCARDIA 2,440 2.01 3 false 1,205.00 50.40 true 1,186.00 49.60\n1 2 3 4 5 6 7Density\n0.00 0.10 0.20 0.30\nf female m maleDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nDensity\n0 50 100 1500.000 0.005 0.010 0.015 0.020\nAGE GENDER WEIGHT\nDensity\n0 50 100 150 2000.000 0.010 0.020 0.030\nDensity\n0 300000 6000000.00000 0.00002 0.00004\nDensity\n200 400 600 800 10000.000 0.005 0.010 0.015 0.020\nHEIGHT BMI S YS. B.P.\nDensity\n60 80 100 120 140 1600.00 0.01 0.02 0.03\nDensity\n60 80100 120 140 160 1800.000 0.010 0.020 0.030\nDensity\n−40−20020400.000.010.020.030.040.05\nDIA. B.P. H EART RATE H.R D IFF.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":161,"page_label":"107","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 107\nfalse trueDensity\n0.0 0.1 0.2 0.3 0.4\nfalse trueDensity\n0.0 0.1 0.2 0.3 0.4\nPREV. TACHY . T ACHYCARDIA\nDiscuss this data quality report in terms of the following:\n(a)Missing values\n(b)Irregular cardinality\n(c)Outliers\n(d)Feature distributions\n˚10.The following data visualizations are based on the tachycardia prediction dataset from\nQuestion 9 (after the instances with missing T ACHYCARDIA values have been re-\nmoved and all outliers have been handled). Each visualization illustrates the rela-\ntionship between a descriptive feature and the target feature, T ACHYCARDIA and is\ncomposed of three plots: a plot of the distribution of the descriptive feature values in\nthe full dataset, and plots showing the distribution of the descriptive feature values for\neach level of the target. Discuss the relationships shown in each visualizations.\n(a)The visualization below illustrates the relationship between the continuous feature\nDIA. B.P. and the target feature, T ACHYCARDIA .\nDia. B.P.Density\n60 80100 120 140 1600.00 0.01 0.02 0.03 0.04\nTachycardia = false\nDia. B.P.Density\n6080 120 1600.00 0.01 0.02 0.03 0.04Tachycardia = true\nDia. B.P.Density\n6080 120 1600.00 0.01 0.02 0.03 0.04","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":162,"page_label":"108","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"108 Chapter 3 Data Exploration\n(b)The visualization below illustrates the relationship between the continuous H EIGHT\nfeature and the target feature T ACHYCARDIA .\nHeightDensity\n150 160 170 180 190 2000.00 0.01 0.02 0.03 0.04\nTachycardia = false\nHeightDensity\n150 170 1900.00 0.01 0.02 0.03 0.04Tachycardia = true\nHeightDensity\n150 170 1900.00 0.01 0.02 0.03 0.04\n(c)The visualization below illustrates the relationship between the categorical feature\nPREV. TACHY . and the target feature, T ACHYCARDIA .\nFrequency\n0200 600 1000\nfalse true\nPrev. Tachy.PercentTachycardia\nfalse\ntrue0 0.5 1","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":163,"page_label":"109","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 109\n˚11.Worldwide breast cancer is the most common form of cancer for women, and the\nsecond most common form of cancer overall.18Reliable, population-wide screening\nis one tool that can be used to reduce the impact of breast cancer, and there is an\nopportunity for machine learning to be used for this. A large hospital group has col-\nlected a cancer screening dataset for possible use with machine learning that contains\nfeatures extracted from tissue samples extracted by biopsy from adults presenting for\nscreening. Features have been extracted from these biopsies by lab technicians who\nrate samples across a number of cagegories on a scale of 1to10. The samples have\nthen been manually categorized by clinicians as either benign ormalignant .19The\ndescriptive features in this dataset are deﬁned as follows:\n‚AGE: The age of the person screened.\n‚SEX: The sex of the person screened, either male orfemale .\n‚SIZEUNIFORMITY : A measure of the variation in size of cells in the tissue samples,\nhigher values indicate more uniform sizes ( 1to10).\n‚SHAPE UNIFORMITY : A measure of the variation in shape of cells in the tissue\nsamples, higher values indicate more uniform shapes ( 1to10).\n‚MARGINAL ADHESION : A measure of how much cells in the biopsy stick together\n(1to10).\n‚MITOSES : A measure of how fast cells are growing ( 1to10).\n‚CLUMP THICKNESS : A measurae of the amount of layering in cells ( 1to10).\n‚BLAND CHROMATIN : A measure of the texture of cell nuclei ( 1to10).\n‚CLASS : The clinician’s assessment of the biopsy sample as either benign ormalig-\nnant.\n18. Based on data from Bray et al. (2018).\n19. The data in this question have been artiﬁcially created but were inspired by the famous Wisconsin breast can-\ncer dataset ﬁrst described by Mangasarian and Wolberg (1990), and available from the UCI Machine Learning\nRepository (Bache and Lichman, 2013).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":164,"page_label":"110","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"110 Chapter 3 Data Exploration\nThe following table contains an extract from this ABT—the full ABT contains 680\ninstances.\nCLUMP SIZE SHAPE MARGINAL BLAND\nID A GE SEX THICKNESS UNIFORMITY UNIFORMITY ADHESION CHROMATIN MITOSES CLASS\n1 56 female 3 4 5 3 4 1 benign\n2 77 female 2 1 0 1 1 1 benign\n.........\n48 34 female 5 2 4 1 1 benign\n49 46 female 5 3 1 2 2 1 b\n50 106 female 2 1 1 1 1 1 benign\n.........\n303 95 female 1 1 1 1 benign\n304 28 male 5 1 1 1 1 benign\n305 0 female 3 1 3 1 benign\n.........\n679 48 female 10 8 7 4 7 1 m\n680 43 female 5 4 6 7 1 malignant\nThe following data quality report has been generated from the ABT.\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nSEX 680 0 2 female 630 92.65 male 50 7.35\nCLASS 680 0 4 benign 392 57.65 malignant 211 31.03\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nAGE 680 0.00 80 0 34 49.13 50 65 106 22.95\nCLUMP THICKNESS 680 0.00 10 1 2 4.45 4 6 10 2.82\nSIZEUNIFORMITY 680 9.26 10 1 1 3.14 1 5 10 3.08\nSHAPE UNIFORMITY 680 9.26 11 0 1 3.20 1 5 10 3.00\nMARGINAL ADHESION 680 0.00 10 1 1 2.84 1 4 10 2.87\nBLAND CHROMATIN 680 22.94 10 1 2 3.38 3 4 10 2.44\nMITOSES 680 0.00 9 1 1 1.61 1 1 10 1.74","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":165,"page_label":"111","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 111\nDensity\n0 20 40 60 80 1000.000 0.005 0.010 0.015 0.020\nfemale maleDensity\n0.0 0.2 0.4 0.6 0.8\nDensity\n2 4 6 8 100.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nAGE SEX SIZEUNIFORMITY\nDensity\n0 2 4 6 8 100.0 0.1 0.2 0.3 0.4 0.5 0.6\nDensity\n2 4 6 8 100.0 0.1 0.2 0.3 0.4 0.5 0.6\nDensity\n2 4 6 8 100.0 0.2 0.4 0.6 0.8\nSHAPE UNIFORMITY MARGINAL ADHESION MITOSES\nDensity\n2 4 6 8 100.00 0.05 0.10 0.15 0.20\nDensity\n2 4 6 8 100.00 0.10 0.20 0.30\nb benign m malignantDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nCLUMP THICKNESS BLAND CHROMATIN CLASS\nDiscuss this data quality report in terms of the following:\n(a)Missing values\n(b)Irregular cardinality\n(c)Outliers\n(d)Feature distributions","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":166,"page_label":"112","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"112 Chapter 3 Data Exploration\n˚12.The following data visualizations are based on the breast cancer prediction dataset\nfrom Question 11 (after some data quality issues present in the dataset have been\ncorrected). Each visualization illustrates the relationship between a descriptive feature\nand the target feature, C LASS and is composed of three plots: a plot of the distribution\nof the descriptive feature values in the full dataset, and plots showing the distribution\nof the descriptive feature values for each level of the target. Discuss the relationships\nshown in each visualizations.\n(a)The visualization below illustrates the relationship between the continuous feature\nAGEand the target feature, C LASS .\nAgeDensity\n20 40 60 80 1000.000 0.005 0.010 0.015 0.020\nClass = benign\nAgeDensity\n20 40 60 801000.000 0.010 0.020Class = malignant\nAgeDensity\n20 40 60 801000.000 0.010 0.020\n(b)The visualization below illustrates the relationship between the continuous B LAND -\nCHROMATIN feature and the target feature C LASS .\nBlandChromatinDensity\n2 4 6 8 100.00 0.10 0.20 0.30\nClass = benign\nBlandChromatinDensity\n2468100.0 0.1 0.2 0.3 0.4Class = malignant\nBlandChromatinDensity\n2468100.0 0.1 0.2 0.3 0.4","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":167,"page_label":"113","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"3.9 Exercises 113\n(c)The visualization below illustrates the relationship between the categorical feature\nSEXand the target feature, C LASS .\n0 200 400 600\nfemale male\nSexPercent\n0 0.5 1Frequency","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":169,"page_label":"115","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"II PREDICTIVE DATA ANALYTICS","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":171,"page_label":"117","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4 Information-Based Learning\n“Information is the resolution of uncertainty. ”\n—Claude Elwood Shannon\nIn this chapter we discuss the ways in which concepts from information theory can be\nused to build prediction models. We start by discussing decision trees , the fundamental\nstructure used in information-based machine learning, before presenting the fundamental\nmeasures of information content that are used: entropy andinformation gain . We then\npresent the ID3 algorithm, the standard algorithm used to induce a decision tree from a\ndataset. The extensions and variations to this standard approach that we present describe\nhow different data types can be handled, how overﬁtting can be avoided using decision tree\npruning , and how multiple prediction models can be combined in ensembles to improve\nprediction accuracy.\n4.1 Big Idea\nWe start off by playing a game. Guess Who is a two-player game in which one player\nchooses a card with a picture of a character on it from a deck, and the other player tries to\nguess which character is on the card by asking a series of questions to which the answer\ncan be only yes or no. The player asking the questions wins by guessing who is on the card\nwithin a small number of questions and loses otherwise. Figure 4.1[118]shows the set of\ncards that we will use for our game. We can represent these cards using the dataset given\nin Table 4.1[118].\nNow, imagine that we have picked one of these cards and you have to guess which one\nby asking questions. Which of the following questions would you ask ﬁrst?\n1.Is it a man?\n2.Does the person wear glasses?\nMost people would ask Question 1 ﬁrst. Why is this? At ﬁrst, this choice of question\nmight seem ineffective. For example, if you ask Question 2, and we answer yes, you can\nbe sure that we have picked Brian without asking any more questions. The problem with","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":172,"page_label":"118","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"118 Chapter 4 Information-Based Learning\nBrian\n John\n Aphra\n Aoife\nFigure 4.1\nCards showing character faces and names for the Guess Who game.\nTable 4.1\nA dataset that represents the characters in the Guess Who game.\nMan Long Hair Glasses Name\nYes No Yes Brian\nYes No No John\nNo Yes No Aphra\nNo No No Aoife\nthis reasoning, however, is that, on average, the answer to Question 2 will be yesonly one\nout of every four times you play. That means that three out of every four times you ask\nQuestion 2, the answer will be no, and you will still have to distinguish between the three\nremaining characters.\nFigure 4.2[119]illustrates the possible question sequences that can follow in a game begin-\nning with Question 2. In Figure 4.2(a)[119]we next ask, Is it a man? and then, if required,\nDo they have long hair? In Figure 4.2(b)[119]we reverse this order. In both of these dia-\ngrams, one path to an answer about the character on a card is 1 question long, one path is\n2 questions long, and two paths are 3 questions long. Consequently, if you ask Question 2\nﬁrst, the average number of questions you have to ask per game is\n1`2`3`3\n4“2.25\nOn the other hand, if you ask Question 1 ﬁrst, there is only one sequence of questions\nwith which to follow it. This sequence is shown in Figure 4.3[120]. Irrespective of the\nanswers to the questions, you always have to follow a path through this sequence that is\n2 questions long to reach an answer about the character on a card. This means that if you\nalways ask Question 1 ﬁrst, the average number of questions you have to ask per game is\n2`2`2`2\n4“2","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":173,"page_label":"119","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.1 Big Idea 119\nDoes the person wear glasses?\nBrianYes\nIs it a man?No\nJohnYes\nDo they have long hair?No\nAphraYes\nAoifeNo\n(a)\nDoes the person wear glasses?\nBrianYes\nDo they have long hair?No\nAphraYes\nIs it a man?No\nJohnYes\nAoifeNo (b)\nFigure 4.2\nThe different question sequences that can follow in a game of Guess Who beginning with the question\nDoes the person wear glasses?\nWhat is interesting here is that no matter what question you ask, the answer is always\neither yesorno, but, on average, an answer to Question 1 seems to carry more information\nthan an answer to Question 2. This is not because of the literal message in the answer\n(either yesorno). Rather, it is because of the way that the answer to each question splits\nthe character cards into different sets based on the value of the descriptive feature the ques-\ntion is asked about (M AN, LONG HAIRor G LASSES ) and the likelihood of each possible\nanswer to the question.\nAn answer to Question 1, Is it a man? , splits the game domain into two sets of equal\nsize: one containing Brian andJohn and one containing Aphra andAoife . One of these\nsets contains the solution, which leaves you with just one more question to ask to ﬁnish the\ngame. By contrast, an answer to Question 2 splits the game domain into one set containing\none element, Brian , and another set containing three elements: John ,Aphra , and Aoife .\nThis works out really well when the set containing the single element contains the solution.\nIn the more likely case that the set containing three elements contains the solution, however,\nyou may have to ask two more questions to uniquely identify the answer. So, when you\nconsider both the likelihood of an answer and how an answer splits up the domain of\nsolutions, it becomes clear that an answer to Question 2 leaves you with more work to do\nto solve the game than an answer to Question 1.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":174,"page_label":"120","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"120 Chapter 4 Information-Based Learning\nIs it a man?\nDoes the person wear glasses?Yes\nDo they have long hair?No\nBrianYes\nJohnNo\nAphraYes\nAoifeNo\nFigure 4.3\nThe different question sequences that can follow in a game of Guess Who beginning with the question\nIs it a man?\nSo, the big idea here is to ﬁgure out which features are the most informative ones to\nask questions about by considering the effects of the different answers to the questions,\nin terms of how the domain is split up after the answer is received and the likelihood of\neach of the answers. Somewhat surprisingly, people seem to be able to easily do this on\nthe basis of intuition. Information-based machine learning algorithms use the same idea.\nThese algorithms determine which descriptive features provide the most information about\na target feature and make predictions by sequentially testing the features in order of their\ninformativeness.\n4.2 Fundamentals\nIn this section we introduce Claude Shannon’s approach to measuring information,1in\nparticular his model of entropy and how it is used in the information gain measure to\n1. Claude Shannon is considered to be the father of information theory. Shannon worked for AT&T Bell Labs,\nwhere he worked on the efﬁcient encoding of messages for telephone communication. It was this focus on\nencoding that motivated his approach to measuring information. In information theory, the meaning of the word\ninformation deliberately excludes the psychological aspects of the communication and should be understood as\nmeasuring the optimal encoding length of a message given the set of possible messages that could be sent within\nthe communication.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":175,"page_label":"121","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 121\ncapture the informativeness of a descriptive feature. Before this we introduce decision\ntrees , the actual prediction models that we are trying to build.\n4.2.1 Decision Trees\nJust as we did when we played Guess Who , an effective way to generate a prediction is\nto carry out a series of tests on the values of the descriptive features describing a query\ninstance and use the answers to these tests to determine the prediction. Decision trees\ntake this approach. To illustrate how a decision tree works, we use the dataset listed in\nTable 4.2[121]. This dataset contains a set of training instances that can be used to build a\nmodel to predict whether emails are spam orham (genuine). The dataset has three binary\ndescriptive features: S USPICIOUS WORDS istrue if an email contains one or more words\nthat are typically found in spam email (e.g., casino ,viagra ,bank , oraccount ); U NKNOWN\nSENDER istrueif the email is from an address that is not listed in the contacts of the person\nwho received the email; and C ONTAINS IMAGES istrueif the email contains one or more\nimages.\nTable 4.2\nAn email spam prediction dataset.\nSUSPICIOUS UNKNOWN CONTAINS\nID W ORDS SENDER IMAGES CLASS\n376 true false true spam\n489 true true false spam\n541 true true false spam\n693 false true true ham\n782 false false false ham\n976 false false false ham\nFigure 4.4[122]shows two decision trees that are consistent with the spam dataset. Deci-\nsion trees look very like the game trees that we developed for the Guess Who game. As\nwith all tree representations, a decision tree consists of a root node (or starting node), in-\nterior nodes , and leaf nodes (or terminating nodes) that are connected by branches . Each\nnon-leaf node (root and interior) in the tree speciﬁes a test to be carried out on a descriptive\nfeature. The number of possible levels that a descriptive feature can take determines the\nnumber of downward branches from a non-leaf node. Each of the leaf nodes speciﬁes a\npredicted level of the target feature.\nIn the diagrams in Figure 4.4[122], ellipses represent root or interior nodes, and rectangles\nrepresent leaf nodes. The labels of the ellipses indicate which descriptive feature is tested\nat that node. The labels on each branch indicate one of the possible feature levels that the\ndescriptive feature at the node above can take. The labels on the rectangular leaf nodes","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":176,"page_label":"122","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"122 Chapter 4 Information-Based Learning\nindicate the target level that should be predicted when the tests on the interior nodes create\na path that terminates at that leaf node.\nContains\nImages\nSuspicious\nWordstrue\nUnknown\nSenderfalse\nspamtrue\nhamfalse\nspamtrue\nhamfalse\n(a)\nSuspicious\nWords\nspamtrue\nhamfalse (b)\nContains\nImages\nSuspicious\nWords true\nUnknown\nSenderfalse\nspam true\nhamfalse\nspamtrue\nhamfalse (c)\nFigure 4.4\nTwo decision trees, (a) and (b), that are consistent with the instances in the spam dataset; and (c) the\npath taken through the tree shown in (a) to make a prediction for the query instance S USPICIOUS\nWORDS =true, UNKNOWN SENDER =true, and C ONTAINS IMAGES =true.\nThe process of using a decision tree to make a prediction for a query instance starts with\ntesting the value of the descriptive feature at the root node of the tree. The result of this test\ndetermines which of the root node’s children the process should then descend to. These\ntwo steps of testing the value of a descriptive feature and descending a level in the tree are\nthen repeated until the process comes to a leaf node at which a prediction can be made.\nTo demonstrate how this process works, imagine that we were given the query email\nSUSPICIOUS WORDS =true, UNKNOWN SENDER =true, CONTAINS IMAGES =true,\nand asked to predict whether it is spam orham. Applying the decision tree from Figure\n4.4(a)[122]to this query, we see that the root node of this tree tests the C ONTAINS IMAGES\nfeature. The query instance value for C ONTAINS IMAGES istrue so the process descends\nthe left branch from the root node, labeled true, to an interior node that tests the S USPI -\nCIOUS WORDS feature. The query instance value for this feature is true, and so on the\nbasis of the result of the test at this node, the process descends the left branch, labeled true,\nto a leaf node labeled spam . As the process has arrived at a leaf node, it terminates, and\nthe target level indicated by the leaf node, spam , is predicted for the query instance. The\npath through the decision tree for this query instance is shown in Figure 4.4(c)[122].\nThe decision tree in Figure 4.4(b)[122]would have returned the same prediction for the\nquery instance. Indeed, both of the decision trees in Figures 4.4(a)[122]and 4.4(b)[122]are\nconsistent with the dataset in Table 4.2[121]and can generalize sufﬁciently to make predic-\ntions for query instances like the one considered in our example. The fact that there are, at","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":177,"page_label":"123","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 123\nleast, two decision trees that can do this raises the question: How do we decide which is\nthe best decision tree to use?\nWe can apply almost the same approach that we used in the Guess Who game to make\nthis decision. Looking at the decision trees in Figures 4.4(a)[122]and 4.4(b)[122], we notice\nthat the tree in Figure 4.4(a)[122]performs tests on two features in order to make a prediction,\nwhereas the decision tree in Figure 4.4(b)[122]needs to test only the value of one feature.\nThe reason for this is that S USPICIOUS WORDS , the descriptive feature tested at the root\nnode of the tree in Figure 4.4(b)[122], perfectly splits the data into a pure group of spam\nemails and a pure group of ham emails. We can say that because of the purity of the splits\nthat it makes, the S USPICIOUS WORDS feature provides more information about the value\nof the target feature for an instance than the C ONTAINS IMAGES feature, and so a tree that\ntests this descriptive feature at the root node is preferable.\nThis gives us a way to choose between a set of different decision trees that are all consis-\ntent with a set of training instances. We can introduce a preference for decision trees that\nuse fewer tests—in other words, trees that are on average shallower.2This is the primary\ninductive bias that a machine learning algorithm taking an information-based approach en-\ncodes. To build shallow trees, we need to put the descriptive features that best discriminate\nbetween instances that have different target feature values toward the top of the tree. To\ndo this we need a formal measure of how well a descriptive feature discriminates between\nthe levels of the target feature. Similar to the way that we analyzed the questions in the\nGuess Who game, we measure the discriminatory power of a descriptive feature by analyz-\ning the size and probability of each set of instances created when we test the value of the\nfeature and how pure each set of instances is with respect to the target feature values of the\ninstances it contains. The formal measure we use to do this is Shannon’s entropy model.\n4.2.2 Shannon’s Entropy Model\nClaude Shannon’s entropy model deﬁnes a computational measure of the impurity of the\nelements in a set. Before we examine the mathematical deﬁnition of entropy, we ﬁrst\nprovide an intuitive explanation of what it means. Figure 4.5[124]shows a collection of sets\nof playing cards of contrasting entropy. An easy way to understand the entropy of a set is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":177,"page_label":"123","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Guess Who game, we measure the discriminatory power of a descriptive feature by analyz-\ning the size and probability of each set of instances created when we test the value of the\nfeature and how pure each set of instances is with respect to the target feature values of the\ninstances it contains. The formal measure we use to do this is Shannon’s entropy model.\n4.2.2 Shannon’s Entropy Model\nClaude Shannon’s entropy model deﬁnes a computational measure of the impurity of the\nelements in a set. Before we examine the mathematical deﬁnition of entropy, we ﬁrst\nprovide an intuitive explanation of what it means. Figure 4.5[124]shows a collection of sets\nof playing cards of contrasting entropy. An easy way to understand the entropy of a set is\nto think in terms of the uncertainty associated with guessing the result if you were to make\na random selection from the set. For example, if you were to randomly select a card from\nthe set shown in Figure 4.5(a)[124], you would have zero uncertainty, as you would know for\nsure that you would select an ace of spades. So, this set has zero entropy. If, however, you\n2. In fact, it can be argued that a preference toward shallower decision trees is a good idea in general and can\nbe viewed as following Occam’s razor . Occam’s razor is the principle of keeping theories as simple as possible.\nIt is named after a 14th-century Franciscan monk, William of Occam (sometimes spelled Ockham ), who was one\nof the ﬁrst to formulate this principle. The razor in the title comes from the idea of shaving off any unnecessary\nassumptions from a theory.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":178,"page_label":"124","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"124 Chapter 4 Information-Based Learning\n(a)Hpcardq“0.00\n (b)Hpcardq“0.81\n (c)Hpcardq“1.00\n(d)Hpcardq“1.50\n (e)Hpcardq“1.58\n (f)Hpcardq“3.58\nFigure 4.5\nThe entropy of different sets of playing cards measured in bits.\nwere to randomly select an element from the set in Figure 4.5(f)[124], you would be very\nuncertain about any prediction as there are 12 possible outcomes, each of which is equally\nlikely. This is why this set has very high entropy. The other sets in Figure 4.5[124]have\nentropy values between these two extremes.\nThis gives us a clue as to how we should deﬁne a computational model of entropy. We\ncan transform the probabilities3of the different possible outcomes when we randomly\nselect an element from a set to entropy values. An outcome with a large probability should\nmap to a low entropy value, and an outcome with a small probability should map to a\nlarge entropy value. The mathematical logarithm , orlog, function4does almost exactly\nthe transformation that we need.\nIf we examine the graph of the binary logarithm (a logarithm to the base 2) of proba-\nbilities ranging from 0to1, shown in Figure 4.6(a)[125], we see that the logarithm function\nreturns large negative numbers for low probabilities and small negative numbers for high\n3. We use some simple elements of probability theory in this chapter. Readers unfamiliar with the way proba-\nbilities are calculated based on the relative frequencies of events should read the ﬁrst section of Appendix B[757]\nbefore continuing with this chapter.\n4. The logofato the base b, written as logbpaq, is the number to which we must raise bto get a. For example,\nlog2p8q“3because 23“8andlog5p625q“4because 54“625.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":179,"page_label":"125","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 125\n0.0 0.2 0.4 0.6 0.8 1.0−10 −8−6−4−2 0\nP(x)log2P(x)\n(a)Ppxqandlog2Ppxq\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 10\nP(x)−log 2P(x) (b)Ppxqand´log2Ppxq\nFigure 4.6\n(a) A graph illustrating how the value of a binary log (the log to the base 2) of a probability changes\nacross the range of probability values; and (b) the impact of multiplying these values by ´1.\nprobabilities. Aside from the fact that the logarithm function returns negative numbers, the\nmagnitude of the numbers it returns is ideal as a measure of entropy: large numbers for\nlow probabilities and small numbers (near zero) for high probabilities. It should also be\nnoted that the range of values for the binary logarithm of a probability, r´8,0s, is much\nlarger than those taken by the probability itself r0,1s. This is also an attractive characteris-\ntic of this function. It is more convenient for us to convert the output of the log function to\npositive numbers by multiplying them by ´1. Figure 4.6(b)[125]shows the impact of this.\nShannon’s model of entropy is a weighted sum of the logs of the probabilities of each\npossible outcome when we make a random selection from a set. The weights used in the\nsum are the probabilities of the outcomes themselves, so that outcomes with high probabil-\nities contribute more to the overall entropy of a set than outcomes with low probabilities.\nShannon’s model of entropy is deﬁned as\nHptq“´lÿ\ni“1pPpt“iqˆlogspPpt“iqqq (4.1)\nwhere Ppt“iqis the probability that the outcome of randomly selecting an element t\nis the type i;lis the number of different types of things in the set; and sis an arbitrary\nlogarithmic base. The minus sign at the beginning of the equation is simply added to\nconvert the negative numbers returned by the log function to positive ones (as described\nabove). We will always use 2 as the base, s, when we calculate entropy, which means that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":180,"page_label":"126","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"126 Chapter 4 Information-Based Learning\nwe measure entropy in bits.5Equation (4.1)[125]is the cornerstone of modern information\ntheory and is an excellent measure of the impurity— heterogeneity —of a set.\nTo understand how Shannon’s entropy model works, consider the example of a set of\n52 different playing cards. The probability of randomly selecting any speciﬁc card ifrom\nthis set, Ppcard“iq, is quite low, just1\n52. The entropy of the set of 52 playing cards is\ncalculated\nHpcardq “ ´52ÿ\ni“1Ppcard“iqˆlog2pPpcard“iqq\n“ ´52ÿ\ni“10.019ˆlog2p0.019q\n“ ´52ÿ\ni“1´0.1096\n“ 5.700bits\nIn this calculation, for each possible card Shannon’s model multiplies a small probability,\nPpcardq“i, by a large negative number, log2pPpcardq“iq, resulting in a relatively large\nnegative number. The individual relatively large negative numbers calculated for all the\ncards are then summed to return one large negative number. The sign of this is inverted to\ngive a large positive value for the entropy of this very impure set.\nBy contrast, consider the example of calculating the entropy of a set of 52 playing cards\nif we distinguish between cards on the sole basis of their suit (hearts ♥, clubs♣, diamonds\n♦or, spades ♠). This time, there are only four possible outcomes when a random card\nis selected from this set, each with a reasonably large probability of13\n52. The entropy\n5. Using binary logs, the maximum entropy for a set with two types of elements is 1.00 bit, but the entropy for\na set with more than two types of elements may be greater than 1.00 bit. The choice of base used in Shannon’s\nmodel, in the context in which it is used in this chapter, is arbitrary. The choice of base 2 is due partly to a\nconventional computer science background and partly to its allowing us to use the bitsunit of information.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":181,"page_label":"127","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 127\nassociated with this set can be calculated\nHpsuitq“ ´ÿ\nlPt♥,♣,♦,♠uPpsuit“lqˆlog2pPpsuit“lqq\n“ ´`\npPp♥qˆlog2pPp♥qqq`pPp♣qˆlog2pPp♣qqq\n`pPp♦qˆlog2pPp♦qqq`pPp♠qˆlog2pPp♠qqq˘\n“ ´´`13{52ˆlog2p13{52q˘\n``13{52ˆlog2p13{52q˘\n``13{52ˆlog2p13{52q˘\n``13{52ˆlog2p13{52q˘¯\n“ ´pp0.25ˆ´2q`p0.25ˆ´2q`p0.25ˆ´2q`p0.25ˆ´2qq\n“2bits\nIn this calculation Shannon’s model multiples the large probability of selecting a speciﬁc\nsuit, Ppsuit“lq, by a small negative number, log2pPpsuit“lqq, to return a relatively\nsmall negative number. The relatively small negative numbers associated with each suit\nare summed to result in a small negative number overall. Again, the sign of this number is\ninverted to result in a small positive value for the entropy of this much purer set.\nTo further explore the entropy, we can return to look at the entropy values of each set\nof cards shown in Figure 4.5[124]. In the set shown in Figure 4.5(a)[124], all the cards are\nidentical. This means that there is no uncertainty about the result when a selection is made\nfrom this set. Shannon’s model of information is designed to reﬂect this intuition, and the\nentropy value for this set is 0.00 bits. In the sets shown in Figures 4.5(b)[124]and 4.5(c)[124],\nthere is a mixture of two different types of cards, so that these have higher entropy values—\nin these instances, 0.81 bits and 1.00 bit. The maximum entropy for a set with two types\nof elements is 1.00 bit, which occurs when there are equal numbers of each type in the set.\nThe sets shown in Figures 4.5(d)[124]and 4.5(e)[124]both have three types of cards. The\nmaximum entropy for sets with three elements is 1.58and occurs when there are equal\nnumbers of each type in the set, as is the case shown in Figure 4.5(e)[124]. Figure 4.5(d)[124]\nshows one card type to be more present than the others, so the overall entropy is slightly\nlower, 1.50 bits. Finally, the set shown in Figure 4.5(f)[124]has a large number of card types,\neach represented only once, which leads to the high entropy value of 3.58 bits.\nThis discussion highlights that entropy is essentially a measure of the heterogeneity of\na set. As the composition of the sets changed from the set with only one type of ele-\nment (Figure 4.5(a)[124]) to a set with many different types of elements, each with an equal\nlikelihood of being selected (Figure 4.5(f)[124]), the entropy score for the sets increased.\n4.2.3 Information Gain","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":181,"page_label":"127","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"numbers of each type in the set, as is the case shown in Figure 4.5(e)[124]. Figure 4.5(d)[124]\nshows one card type to be more present than the others, so the overall entropy is slightly\nlower, 1.50 bits. Finally, the set shown in Figure 4.5(f)[124]has a large number of card types,\neach represented only once, which leads to the high entropy value of 3.58 bits.\nThis discussion highlights that entropy is essentially a measure of the heterogeneity of\na set. As the composition of the sets changed from the set with only one type of ele-\nment (Figure 4.5(a)[124]) to a set with many different types of elements, each with an equal\nlikelihood of being selected (Figure 4.5(f)[124]), the entropy score for the sets increased.\n4.2.3 Information Gain\nWhat is the relationship between a measure of heterogeneity of a set and predictive ana-\nlytics? If we can construct a sequence of tests that splits the training data into pure sets\nwith respect to the target feature values, then we can label queries by applying the same","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":182,"page_label":"128","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"128 Chapter 4 Information-Based Learning\nSuspicious\nWords\nID Class\n376  spam  \n489  spam  \n541  spam   true\nIDClass\n693 ham\n782 ham\n976 hamfalse\n(a)\nUnknown\nSender\nIDClass\n489 spam  \n541 spam  \n693 hamtrue\nIDClass\n376 spam  \n782 ham\n976 hamfalse (b)\nContains\nImages\nID Class\n376 spam  \n693 hamtrue\nID Class\n489 spam  \n541 spam  \n782 ham\n976 hamfalse (c)\nFigure 4.7\nHow the instances in the spam dataset split when we partition using each of the different descriptive\nfeatures from the spam dataset in Table 4.2[121].\nsequence of tests to a query and labeling it with the target feature value of instances in the\nset in which it ends up.\nTo illustrate this we return to the spam dataset given in Table 4.2[121]. Figure 4.7[128]shows\nhow the instances in the spam dataset are split when we partition it using each of the three\ndescriptive features. In Figure 4.7(a)[128], we can see that splitting the dataset based on the\nSUSPICIOUS WORDS feature provides a lot of information about whether an email is spam\nor ham. In fact, partitioning the data by this feature creates two pure sets: one containing\nonly instances with the target level spam and the other set containing only instances with\nthe target level ham. This indicates that the S USPICIOUS WORDS feature is a good feature\nto test if we are trying to decide whether a new email—not listed in the training dataset—is\nspam or not.\nWhat about the other features? Figure 4.7(b)[128]shows how the U NKNOWN SENDER\nfeature partitions the dataset. The resulting sets both contain a mixture of spam andham\ninstances. This indicates that the U NKNOWN SENDER feature is not as good at discrimi-\nnating between spam and ham emails as the S USPICIOUS WORDS feature. Although there\nis a mixture in each of these sets, however, it seems to be the case that when U NKNOWN\nSENDER =true, the majority of emails are spam , and when U NKNOWN SENDER =false ,\nthe majority of emails are ham. Therefore, although this feature doesn’t perfectly discrim-\ninate between spam and ham, it does give us some information that we might be able to\nuse in conjunction with other features to help decide whether a new email is spam or ham.\nFinally, if we examine the partitioning of the dataset based on the C ONTAINS IMAGES\nfeature, shown in Figure 4.7(c)[128], it looks like this feature is not very discriminatory for\nspam andham at all. Both the resulting sets contain a balanced mixture of spam and ham\ninstances.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":183,"page_label":"129","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 129\nWhat we need to do at this point is to develop a formal model that captures the intuitions\nabout the informativeness of these features. Unsurprisingly, we do this using Shannon’s\nentropy model. The measure of informativeness that we use is known as information\ngain, which is a measure of the reduction in the overall entropy of a set of instances that\nis achieved by testing on a descriptive feature. Computing information gain is a three-step\nprocess:\n1.Compute the entropy of the original dataset with respect to the target feature. This\ngives us a measure of how much information is required to organize the dataset into\npure sets.\n2.For each descriptive feature, create the sets that result by partitioning the instances in\nthe dataset using their feature values, and then sum the entropy scores of each of these\nsets. This gives a measure of the information that remains required to organize the\ninstances into pure sets after we have split them using the descriptive feature.\n3.Subtract the remaining entropy value (computed in step 2) from the original entropy\nvalue (computed in step 1) to give the information gain.\nWe need to deﬁne three equations to formally specify information gain (one for each step).\nThe ﬁrst equation calculates the entropy for a dataset with respect to a target feature6\nHpt,Dq“´ÿ\nlPlevelsptqpPpt“lqˆlog2pPpt“lqqq (4.2)\nwhere levelsptqis the set of levels in the domain of the target feature t; and Ppt“lqis the\nprobability of a randomly selected instance having the target feature level l.\nThe second equation deﬁnes how we compute the entropy remaining after we partition\nthe dataset using a particular descriptive feature d. When we partition the dataset Dusing\nthe descriptive feature d, we create a number of partitions (or sets) Dd“l1...Dd“lk, where\nl1...lkare the klevels that feature dcan take. Each partition, Dd“li, contains the instances\ninDthat have a value of level lifor the dfeature. The entropy remaining after we have\ntested dis a weighted sum of the entropy, still with respect to the target feature, of each\npartition. The weighting is determined by the size of each partition—so a large partition\nshould contribute more to the overall remaining entropy than a smaller partition. We use\nthe term rempd,Dqto denote this quantity and deﬁne it formally\nrempd,Dq“ÿ\nlPlevelspdq|Dd“l|\n|D|loomoon\nweightingˆHpt,Dd“lqlooooomooooon\nentropy of\npartition Dd“l(4.3)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":183,"page_label":"129","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the descriptive feature d, we create a number of partitions (or sets) Dd“l1...Dd“lk, where\nl1...lkare the klevels that feature dcan take. Each partition, Dd“li, contains the instances\ninDthat have a value of level lifor the dfeature. The entropy remaining after we have\ntested dis a weighted sum of the entropy, still with respect to the target feature, of each\npartition. The weighting is determined by the size of each partition—so a large partition\nshould contribute more to the overall remaining entropy than a smaller partition. We use\nthe term rempd,Dqto denote this quantity and deﬁne it formally\nrempd,Dq“ÿ\nlPlevelspdq|Dd“l|\n|D|loomoon\nweightingˆHpt,Dd“lqlooooomooooon\nentropy of\npartition Dd“l(4.3)\n6. This is almost identical to the deﬁnition of Shannon’s entropy model given in Equation (4.1)[125]. We have\nextended the deﬁnition to include an explicit parameter for the dataset Dfor which we are computing the entropy,\nand we have speciﬁed the base as 2.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":184,"page_label":"130","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"130 Chapter 4 Information-Based Learning\nUsing Equation (4.2)[129]and Equation (4.3)[129], we can now formally deﬁne information\ngain made from splitting the dataset Dusing the feature d\nIGpd,Dq“Hpt,Dq´rempd,Dq (4.4)\nTo illustrate how information gain is calculated and to check how well it models our intu-\nitions described at the beginning of this section, we compute the information gain for each\ndescriptive feature in the spam dataset. The ﬁrst step is to compute the entropy for the\nwhole dataset using Equation (4.2)[129]\nHpt,Dq“ ´ÿ\nlPtspam,hamupPpt“lqˆlog2pPpt“lqqq\n“ ´`\npPpt“spamqˆlog2pPpt“spamqq\n`pPpt“hamqˆlog2pPpt“hamqq˘\n“ ´``3{6ˆlog2p3{6q˘\n``3{6ˆlog2p3{6q˘˘\n“1bit\nThe next step is to compute the entropy remaining after we split the dataset using each of\nthe descriptive features. The computation for the S USPICIOUS WORDS feature is7\nrempWORDS,Dq\n“ˆ|DWORDS“true|\n|D|ˆHpt,DWORDS“trueq˙\n`ˆ|DWORDS“false|\n|D|ˆHpt,DWORDS“falseq˙\n“¨\n˝3{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n`¨\n˝3{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n“`3{6ˆ`\n´``3{3ˆlog2p3{3q˘\n``0{3ˆlog2p0{3q˘˘˘˘\n``3{6ˆ`\n´``0{3ˆlog2p0{3q˘\n``3{3ˆlog2p3{3q˘˘˘˘\n“0bits\n7. Note that we have shortened feature names in these calculations to save space.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":185,"page_label":"131","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.2 Fundamentals 131\nThe remaining entropy for the U NKNOWN SENDER feature is\nrempSENDER,Dq\n“ˆ|DSENDER“true|\n|D|ˆHpt,DSENDER“trueq˙\n`ˆ|DSENDER“false|\n|D|ˆHpt,DSENDER“falseq˙\n“¨\n˝3{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n`¨\n˝3{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n“`3{6ˆ`\n´``2{3ˆlog2p2{3q˘\n``1{3ˆlog2p1{3q˘˘˘˘\n``3{6ˆ`\n´``1{3ˆlog2p1{3q˘\n``2{3ˆlog2p2{3q˘˘˘˘\n“0.9183 bits\nThe remaining entropy for the C ONTAINS IMAGES feature is\nrempIMAGES,Dq\n“ˆ|DIMAGES“true|\n|D|ˆHpt,DIMAGES“trueq˙\n`ˆ|DIMAGES“false|\n|D|ˆHpt,DIMAGES“falseq˙\n“¨\n˝2{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n`¨\n˝4{6ˆ¨\n˝´ÿ\nlPtspam,hamuPpt“lqˆlog2pPpt“lqq˛\n‚˛\n‚\n“`2{6ˆ`\n´``1{2ˆlog2p1{2q˘\n``1{2ˆlog2p1{2q˘˘˘˘\n``4{6ˆ`\n´``2{4ˆlog2p2{4q˘\n``2{4ˆlog2p2{4q˘˘˘˘\n“1bit","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":186,"page_label":"132","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"132 Chapter 4 Information-Based Learning\nWe can now complete the information gain calculation for each descriptive feature\nIGpWORDS,Dq“HpCLASS,Dq´rempWORDS,Dq\n“1´0\n“1bit\nIGpSENDER,Dq“HpCLASS,Dq´rempSENDER,Dq\n“1´0.9183\n“0.0817 bits\nIGpIMAGES,Dq“HpCLASS,Dq´rempIMAGES,Dq\n“1´1\n“0bits\nThe information gain of the S USPICIOUS WORDS feature is 1 bit. This is equivalent to\nthe total entropy for the entire dataset. An information gain score for a feature that matches\nthe entropy for the entire dataset indicates that the feature is perfectly discriminatory with\nrespect to the target feature values. Unfortunately, in more realistic datasets, ﬁnding a fea-\nture as powerful as the S USPICIOUS WORDS feature is very rare. The feature U NKNOWN\nSENDER has an information gain of 0.0817 bits. An information gain score this low sug-\ngests that although splitting on this feature provides some information, it is not particularly\nuseful. Finally, the C ONTAINS IMAGES feature has an information gain score of 0bits.\nThis ranking of the features by information gain mirrors the intuitions that we developed\nabout the usefulness of these features during our previous discussion.\nWe started this section with the idea that if we could construct a sequence of tests that\nsplits the training data into pure sets with respect to the target feature values, then we could\ndo prediction by applying the same sequence of tests to the prediction queries and labeling\nthem with the target feature of the set in which they end up. A key part of doing this is\nbeing able to decide which tests should be included in the sequence and in what order. The\ninformation gain model we have developed allows us to decide which test we should add to\nthe sequence next because it enables us to select the best feature to use on a given dataset.\nIn the next section, we introduce the standard algorithm for growing decision trees in this\nway.\n4.3 Standard Approach: The ID3 Algorithm\nAssuming that we want to use shallow decision trees, is there a way in which we can auto-\nmatically create them from data? One of the best known decision tree induction algorithms","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":187,"page_label":"133","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.3 Standard Approach: The ID3 Algorithm 133\nis the Iterative Dichotomizer 3 (ID3) algorithm.8This algorithm attempts to create the\nshallowest decision tree that is consistent with the data given.\nThe ID3 algorithm builds the tree in a recursive, depth-ﬁrst manner, beginning at the\nroot node and working down to the leaf nodes. The algorithm begins by choosing the\nbest descriptive feature to test (i.e., the best question to ask ﬁrst). This choice is made\nby computing the information gain of the descriptive features in the training dataset. A\nroot node is then added to the tree and labeled with the selected test feature. The training\ndataset is then partitioned using the test. There is one partition created for each possible\ntest result, which contains the training instances that returned that result. For each partition\na branch is grown from the node. The process is then repeated for each branch using the\nrelevant partition of the training set in place of the full training set and with the selected\ntest feature excluded from further testing. This process is repeated until all the instances\nin a partition have the same target level, at which point a leaf node is created and labeled\nwith that level.\nThe design of the ID3 algorithm is based on the assumption that a correct decision tree for\na domain will classify instances from that domain in the same proportion as the target level\noccurs in the domain. So, given a dataset Drepresenting a domain with two target levels\nC1andC2, an arbitrary instance from the domain should be classiﬁed as being associated\nwith target level C1with the probability|C1|\n|C1|`|C2|and to target level C2with the probability\n|C2|\n|C1|`|C2|, where|C1|and|C2|refer to the number of instances in Dassociated with C1\nandC2, respectively. To ensure that the resulting decision tree classiﬁes in the correct\nproportions, the decision tree is constructed by repeatedly partitioning9the training dataset\nuntil every instance in a partition maps to the same target level.\nAlgorithm 1[134]lists a pseudocode description of the ID3 algorithm. Although the algo-\nrithm looks quite complex, it essentially does one of two things each time it is invoked:\neither it stops growing the current path in the tree by adding a leaf node to the tree, Lines\n1–6, or it extends the current path by adding an interior node to the tree and growing the\nbranches of this node by repeatedly rerunning the algorithm, Lines 7–13.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":187,"page_label":"133","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"andC2, respectively. To ensure that the resulting decision tree classiﬁes in the correct\nproportions, the decision tree is constructed by repeatedly partitioning9the training dataset\nuntil every instance in a partition maps to the same target level.\nAlgorithm 1[134]lists a pseudocode description of the ID3 algorithm. Although the algo-\nrithm looks quite complex, it essentially does one of two things each time it is invoked:\neither it stops growing the current path in the tree by adding a leaf node to the tree, Lines\n1–6, or it extends the current path by adding an interior node to the tree and growing the\nbranches of this node by repeatedly rerunning the algorithm, Lines 7–13.\nLines 1–6 of Algorithm 1[134]control when a new leaf node is created in the tree. We\npreviously mentioned that the ID3 algorithm constructs the decision tree by recursively\npartitioning the dataset. An important decision to be made in designing any recursive\nprocess is what the base cases that stop the recursion will be. In the ID3 algorithm the base\ncases are the situations in which we stop splitting the dataset and construct a leaf node with\nan associated target level. There are two important things to remember in designing these\nbase cases. First, the dataset of training instances considered at each of the interior nodes\nin the tree is not the complete dataset; rather, it is the subset of instances considered at its\n8. This algorithm was ﬁrst published in Quinlan (1986).\n9. Hence the name Iterative Dichotomizer .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":188,"page_label":"134","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"134 Chapter 4 Information-Based Learning\nAlgorithm 1 Pseudocode description of the ID3 algorithm.\nRequire: set of descriptive features d\nRequire: set of training instances D\n1:ifall the instances in Dhave the same target level Cthen return a decision tree\nconsisting of a leaf node with label C\n2:else if dis empty then return a decision tree consisting of a leaf node with the label\nof the majority target level in D\n3:else if Dis empty then return a decision tree consisting of a leaf node with the label\nof the majority target level of the dataset of the immediate parent node\n4:else\n5: drbestsÐarg max\ndPdIGpd,Dq\n6: make a new node, Node drbests, and label it with drbests\n7: partition Dusing drbests\n8: remove drbestsfrom d\n9: foreach partition DiofDdo\n10: grow a branch from Node drbeststo the decision tree created by rerunning ID3\nwithD“Di\n11: end for\n12:end if\nparent node that had the relevant feature value for the branch from the parent to the current\nnode. Second, once a feature has been tested, it is not considered for selection again along\nthat path in the tree. A feature will be tested only once on any path in the tree, but it may\noccur several times in the tree on different paths. On the basis of these constraints, the\nalgorithm deﬁnes three situations where the recursion stops and a leaf node is constructed:\n1.All the instances in the dataset have the same target feature level. In this situation, the\nalgorithm returns a single leaf node tree with that target level as its label (Algorithm\n1[134]Lines 1–2).\n2.The set of features left to test is empty. This means that we have already tested every\nfeature on the path between the root node and the current node. We have no more\nfeatures we can use to distinguish between the instances, so we return a single leaf\nnode tree with the majority target level of the dataset as its target level (Algorithm 1[134]\nLines 3–4).\n3.The dataset is empty. This can occur when, for a particular partition of the dataset,\nthere are no instances that have a particular feature value. In this case we return a\nsingle leaf node tree with the majority target level of the dataset at the parent node that\nmade the recursive call (Algorithm 1[134]Lines 5–6).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":189,"page_label":"135","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.3 Standard Approach: The ID3 Algorithm 135\nIf none of these cases holds, the algorithm continues to recursively create interior nodes,\nLines 7–13 of Algorithm 1[134]. The ﬁrst step in creating an interior node is to decide which\ndescriptive feature should be tested at this node (Line 8 of Algorithm 1[134]). When we ﬁrst\nmentioned the ID3 algorithm, we stated that it tries to create the shallowest decision tree\nthat is consistent with the data given. The feature of the ID3 algorithm that biases it toward\nshallow trees is the mechanism that it uses to determine which descriptive feature is the\nmost informative one to test at a new node. The ID3 algorithm uses the information gain\nmetric to choose the best feature to test at each node in the tree. Consequently, the selection\nof the best feature on which to split a dataset is based on the purity, or homogeneity, of the\nresulting partitions in the datasets. Again, remember that each node is constructed in a\ncontext consisting of a dataset of instances containing a subset of the instances used to\nconstruct its parent node and the set of descriptive features that have not been tested on\nthe path between the root node and parent node. As a result, the information gain for\na particular descriptive feature may be different at different nodes in the tree because it\nwill be computed on different subsets of the full training dataset. One consequence of\nthis is that a feature with a low information gain at the root node (when the full dataset is\nconsidered) may have a high information gain score at one of the interior nodes because it\nis predictive on the subset of instances that are considered at that interior node.\nOnce the most informative feature, drbests, has been chosen, the algorithm adds a new\nnode, labeled with the feature drbests, to the tree (Line 9). It then splits the dataset that\nwas considered at this node, D, into partitions, D1,...,Dk, according to the levels that\ndrbestscan take,tl1,..., lku(Line 10). Next, it removes the feature drbestsfrom the set\nof features considered for testing later on this path in the tree; this enforces the constraint\nthat a feature can be tested only once on any particular path in the tree (Line 11). Finally,\nin Lines 12 and 13, the algorithm grows a branch in the tree for each of the values in the\ndomain of drbestsby recursively calling itself for each of the partitions created at Line 10.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":189,"page_label":"135","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Once the most informative feature, drbests, has been chosen, the algorithm adds a new\nnode, labeled with the feature drbests, to the tree (Line 9). It then splits the dataset that\nwas considered at this node, D, into partitions, D1,...,Dk, according to the levels that\ndrbestscan take,tl1,..., lku(Line 10). Next, it removes the feature drbestsfrom the set\nof features considered for testing later on this path in the tree; this enforces the constraint\nthat a feature can be tested only once on any particular path in the tree (Line 11). Finally,\nin Lines 12 and 13, the algorithm grows a branch in the tree for each of the values in the\ndomain of drbestsby recursively calling itself for each of the partitions created at Line 10.\nEach of these recursive calls uses the partition it is called on as the dataset it considers and\nis restricted to selecting from the set of features that have not been tested so far on the path\nfrom the root node. The node returned by the recursive call to the algorithm may be the\nroot of a subtree or a leaf node. Either way, it is joined to the current node with a branch\nlabeled with the appropriate level of the selected feature.\n4.3.1 A Worked Example: Predicting Vegetation Distributions\nIn this section we work through an example to illustrate how the ID3 is used to induce a\ndecision tree. This example is based on ecological modeling , an area of scientiﬁc research\nthat applies statistical and analytical techniques to model ecological processes. One of the\nproblems faced by ecological management practitioners is that it is often too expensive\nto do large-scale, high-resolution land surveys. Using predictive analytics, however, the\nresults of small-scale surveys can be used to create predictive models that can be applied\nacross large regions. These models are used to inform resource management and conserva-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":190,"page_label":"136","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"136 Chapter 4 Information-Based Learning\nTable 4.3\nThe vegetation classiﬁcation dataset.\nID S TREAM SLOPE ELEVATION VEGETATION\n1 false steep high chapparal\n2 true moderate low riparian\n3 true steep medium riparian\n4 false steep medium chapparal\n5 false ﬂat high conifer\n6 true steep highest conifer\n7 true steep high chapparal\ntion activities,10such as managing the distribution of animal species and vegetation across\ngeographic regions. The descriptive features used by these models often can be automati-\ncally extracted from digitized maps, aerial photographs, or satellite imagery—for example,\nthe elevation, steepness, color, and spectral reﬂection of the terrain and the presence or ab-\nsence of features such as rivers, roads, or lakes.\nTable 4.3 lists an example dataset from the ecological modeling domain.11In this ex-\nample, the prediction task is to classify the type of vegetation that is likely to be growing\nin areas of land on the sole basis of descriptive features extracted from maps of the ar-\neas. Ecological modelers can use information about the type of vegetation that grows in a\nregion as a direct input into their animal species management and conservation programs\nbecause areas covered in different types of vegetation support different animal species. By\nusing a predictive model that requires only features from maps, the ecological modelers\ncan avoid expensive ground-based or aerial surveys. There are three types of vegetation\nthat should be recognized by this model. First, chapparal is a type of evergreen shrubland\nthat can be ﬁre-prone. The animal species typically found in this vegetation include gray\nfoxes, bobcats, skunks, and rabbits. Second, riparian vegetation occurs near streams and\nis characterized by trees and shrubs. It is usually home to small animals, including rac-\ncoons, frogs, and toads. Finally, conifer refers to forested areas that contain a variety of\ntree species (including pine, cedar, and ﬁr trees), with a mixture of shrubs on the forest\nﬂoor. The animals that may be found in these forests include bears, deer, and cougars. The\ntype of vegetation in an area is stored in the target feature, V EGETATION .\nThere are three descriptive features in the dataset. S TREAM is a binary feature that\ndescribes whether or not there is a stream in the area. S LOPE describes the steepness of the\n10. See Guisan and Zimmermann (2000) and Franklin (2009) for an introduction to uses of predictive analytics\nin ecological modeling.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":190,"page_label":"136","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"coons, frogs, and toads. Finally, conifer refers to forested areas that contain a variety of\ntree species (including pine, cedar, and ﬁr trees), with a mixture of shrubs on the forest\nﬂoor. The animals that may be found in these forests include bears, deer, and cougars. The\ntype of vegetation in an area is stored in the target feature, V EGETATION .\nThere are three descriptive features in the dataset. S TREAM is a binary feature that\ndescribes whether or not there is a stream in the area. S LOPE describes the steepness of the\n10. See Guisan and Zimmermann (2000) and Franklin (2009) for an introduction to uses of predictive analytics\nin ecological modeling.\n11. This artiﬁcially generated example dataset is inspired by the research reported in Franklin et al. (2000).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":191,"page_label":"137","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.3 Standard Approach: The ID3 Algorithm 137\nTable 4.4\nPartition sets (Part.), entropy, remainder (Rem.), and information gain (Info. Gain) by feature for the\ndataset in Table 4.3[136].\nSplit by Partition Info.\nFeature Level Part. Instances Entropy Rem. Gain\nSTREAMtrue D1 d2,d3,d6,d7 1.50001.2507 0.3060false D2 d1,d4,d5 0.9183\nSLOPEﬂat D3 d5 0.0\n0.9793 0.5774 moderate D4 d2 0.0\nsteep D5 d1,d3,d4,d6,d7 1.3710\nELEVATIONlow D6 d2 0.0\n0.6793 0.8774medium D7 d3,d4 1.0\nhigh D8 d1,d5,d7 0.9183\nhighest D9 d6 0.0\nterrain in an area and has the levels ﬂat,moderate , and steep . ELEVATION describes the\nelevation of an area and has the levels low,medium ,high, and highest .\nThe ﬁrst step in building the decision tree is to determine which of the three descriptive\nfeatures is the best one to split the dataset on at the root node. The algorithm does this by\ncomputing the information gain for each feature. The total entropy for this dataset, which\nis required to calculate information gain, is computed\nHpVEGETATION ,Dq\n“´ÿ\nlP#chapparal,\nriparian,\nconifer+PpVEGETATION“lqˆlog2pPpVEGETATION“lqq\n“´``3{7ˆlog2`3{7˘˘\n``2{7ˆlog2`2{7˘˘\n``2{7ˆlog2`2{7˘˘˘\n“1.5567 bits(4.5)\nTable 4.4[137]shows the calculation of the information gain for each feature using this result.\nWe can see from the information in Table 4.4[137]that E LEVATION has the largest infor-\nmation gain of the three features and therefore is selected by the algorithm at the root node\nof the tree. Figure 4.8[138]shows the state of the tree after the dataset is split using E LE-\nVATION . Notice that the full dataset has been split into four partitions (labeled D6,D7,\nD8, and D9in Table 4.4[137]) and that the feature E LEVATION is no longer listed in these\npartitions because it has already been used to split the data. The D6andD9partitions\neach contain just one instance. Consequently they are pure sets, and these partitions can\nbe converted into leaf nodes. The D7andD8partitions, however, contain instances with\na mixture of target feature levels, so the algorithm needs to continue splitting these parti-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":192,"page_label":"138","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"138 Chapter 4 Information-Based Learning\nElevation\n D6   ID    Stream    Slope    Vegetation  \n2 true moderate riparianlow\n D7   ID    Stream    Slope    Vegetation  \n3 true steep riparian\n4 false steep chaparralmedium\n D8   ID    Stream    Slope    Vegetation  \n1 false steep chaparral\n5 false ﬂat conifer\n7 true steep chaparralhigh  D9   ID    Stream    Slope    Vegetation  \n6 true steep coniferhighest\nFigure 4.8\nThe decision tree after the data has been split using E LEVATION .\nTable 4.5\nPartition sets (Part.), entropy, remainder (Rem.), and information gain (Info. Gain) by feature for the\ndataset D7in Figure 4.8[138].\nSplit by Partition Info.\nFeature Level Part. Instances Entropy Rem. Gain\nSTREAMtrue D10 d3 0.00.0 1.0false D11 d4 0.0\nSLOPEﬂat D12 0.0\n1.0 0.0 moderate D13 0.0\nsteep D14 d3,d4 1.0\ntions. To do this, the algorithm needs to decide which of the remaining descriptive features\nhas the highest information gain for each partition.\nTo address partition D7, ﬁrst the algorithm computes the entropy of D7\nHpVEGETATION ,D7q\n“´ÿ\nlP#chapparal,\nriparian,\nconifer+PpVEGETATION“lqˆlog2pPpVEGETATION“lqq\n“´``1{2ˆlog2p1{2q˘\n``1{2ˆlog2p1{2q˘\n``0{2ˆlog2p0{2q˘˘\n“1.0bits(4.6)\nThe information gained by splitting D7for using S TREAM and S LOPE is then computed\nas presented in Table 4.5[138].\nThe calculations in Table 4.5[138]show that S TREAM has a higher information gain than\nSLOPE and so is the best feature with which to split D7. Figure 4.9[139]depicts the state","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":193,"page_label":"139","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.3 Standard Approach: The ID3 Algorithm 139\nElevation\n D6   ID    Stream    Slope    Vegetation  \n2 true moderate riparianlow\nStreammedium\n D8   ID    Stream    Slope    Vegetation  \n1 false steep chaparral\n5 false ﬂat conifer\n7 true steep chaparral high  D9   ID    Stream    Slope    Vegetation  \n6 true steep coniferhighest\n D10   ID    Slope    Vegetation  \n3 steep ripariantrue\n D11   ID    Slope    Vegetation  \n4 steep chaparralfalse\nFigure 4.9\nThe state of the decision tree after the D7partition has been split using S TREAM .\nof the decision tree after the D7partition has been split. Splitting D7creates two new\npartitions ( D10andD11). Notice that S LOPE is the only descriptive feature that is listed in\nD10andD11. This reﬂects that E LEVATION and S TREAM have already been used on the\npath from the root node to each of these partitions and so cannot be used again. Both these\nnew partitions are pure sets with respect to the target feature (indeed, they contain only one\ninstance each), and consequently these sets do not need to be split any further and can be\nconverted into leaf nodes.\nAt this point D8is the only partition that is not a pure set. There are two descriptive\nfeatures that can be used to split D8: STREAM and S LOPE . The decision regarding which\nof these features to use for the split is made by calculating which feature has the higher\ninformation gain for D8. The overall entropy for D8is calculated\nHpVEGETATION ,D8q\n“´ÿ\nlP#chapparal,\nriparian,\nconifer+PpVEGETATION“lqˆlog2pPpVEGETATION“lqq\n“´``2{3ˆlog2p2{3q˘\n``0{3ˆlog2p0{3q˘\n``1{3ˆlog2p1{3q˘˘\n“0.9183 bits(4.7)\nTable 4.6[140]details the calculation of the information gain for each descriptive feature in\nD8using this result. It is clear from the information in Table 4.6[140]that in the context of\nD8, SLOPE has a higher information gain than S TREAM .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":194,"page_label":"140","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"140 Chapter 4 Information-Based Learning\nTable 4.6\nPartition sets (Part.), entropy, remainder (Rem.), and information gain (Info. Gain) by feature for the\ndataset D8in Figure 4.9[139].\nSplit by Partition Info.\nFeature Level Part. Instances Entropy Rem. Gain\nSTREAMtrue D15 d7 0.00.6666 0.2517false D16 d1,d5 1.0\nSLOPEﬂat D17 d5 0.0\n0.0 0.9183 moderate D18 0.0\nsteep D19 d1,d7 0.0\nElevation\n D6   ID    Stream    Slope    Vegetation  \n2 true moderate riparianlow\nStreammedium\nSlopehigh  D9   ID    Stream    Slope    Vegetation  \n6 true steep coniferhighest\n D10   ID    Slope    Vegetation  \n3 steep ripariantrue\n D11   ID    Slope    Vegetation  \n4 steep chaparralfalse\n D17   ID    Stream    Vegetation  \n5 false coniferﬂat\n D18   ID    Stream    Vegetation  \n- - -moderate\n D19   ID    Stream    Vegetation  \n1 false chaparral\n7 true chaparralsteep\nFigure 4.10\nThe state of the decision tree after the D8partition has been split using S LOPE .\nFigure 4.10[140]illustrates the state of the decision tree after D8has been split. Notice\nthat one of the partitions created by splitting D8on the basis of S LOPE is empty: D18.\nThis is the case because there were no instances in D8that had a value of moderate for the\nSLOPE feature. This empty partition will result in a leaf node that returns a prediction of\nthe majority target level in D8,chapparal . The other two partitions created by splitting D8\nare pure with respect to the target feature: D17contains one instance with a conifer target\nlevel, and D19contains two instances that both have a chapparal target level.\nAt this point all the remaining partitions are pure with respect to the target feature. Con-\nsequently, the algorithm converts each partition into a leaf node and returns the ﬁnal de-\ncision tree. Figure 4.11[141]shows this decision tree. If the prediction strategy encoded in\nthis tree is applied to the original dataset in Table 4.3[136], it will correctly classify all the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":195,"page_label":"141","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 141\nElevation\nriparianlow\nStreammedium\nSlopehigh\nconiferhighest\nripariantrue\nchaparralfalse\nconiferﬂat\nchaparralmoderate\nchaparralsteep\nFigure 4.11\nThe ﬁnal vegetation classiﬁcation decision tree.\ninstances in the dataset. In machine learning terms, the induced model is consistent with\nthe training data.\nOne ﬁnal point: remember that the empty partition in Figure 4.10[140](D18) has been con-\nverted into a leaf node that returns the chapparal target level. The reason is that chapparal\nis the majority target level in the partition at the parent node ( D8) of this leaf node. Con-\nsequently, this tree will return a prediction of V EGETATION =chapparal for the following\nquery:\nSTREAM =true, SLOPE =moderate , ELEVATION =high\nThis is interesting because there are no instances listed in Table 4.3[136]where S LOPE =\nmoderate and V EGETATION =chapparal . This example illustrates one way in which the\npredictions made by the model generalize beyond the dataset. Whether the generalizations\nmade by the model are correct will depend on whether the assumptions used in generating\nthe model (i.e., the inductive bias ) are appropriate.\nThe ID3 algorithm works in exactly the same way for larger, more complicated datasets;\nthere is simply more computation involved. Since it was ﬁrst proposed, there have been\nmany modiﬁcations to the original ID3 algorithm to handle variations that are common in\nreal-world datasets. We explore the most important of these modiﬁcations in the following\nsections.\n4.4 Extensions and Variations\nThe ID3 decision tree induction algorithm described in the previous section provides the\nbasic approach to decision tree induction: a top-down, recursive, depth-ﬁrst partitioning\nof the dataset beginning at the root node and ﬁnishing at the leaf nodes. Although this\nalgorithm works quite well as presented, it assumes categorical features and clean data. It\nis relatively easy, however, to extend the ID3 algorithm to handle continuous descriptive","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":196,"page_label":"142","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"142 Chapter 4 Information-Based Learning\nfeatures and continuous target features. A range of techniques can also be used to make a\ndecision tree more robust to noise in the data. In this section we describe the techniques\nused to address these issues as well as the use of ensemble methods that allow us to com-\nbine the predictions made by multiple models. We begin, however, by introducing some of\nthe metrics, other than entropy-based information gain, that can be used to select the next\nfeature to split on as we build the tree.\n4.4.1 Alternative Feature Selection and Impurity Metrics\nThe information gain measure described in Section 4.2.3[127]uses entropy to judge the impu-\nrity of the partitions that result from splitting a dataset using a particular feature. Entropy-\nbased information gain, however, does have some drawbacks. In particular, it preferences\nfeatures with many levels because these features split the data into many small subsets,\nwhich tend to be pure irrespective of any correlation between the descriptive feature and\nthe target feature. One way of addressing this issue is to use information gain ratio in-\nstead of entropy. The information gain ratio is computed by dividing the information gain\nof a feature by the amount of information used to determine the value of the feature\nGRpd,Dq“IGpd,Dq\n´ÿ\nlPlevelspdqpPpd“lqˆlog2pPpd“lqqq(4.8)\nwhere IGpd,Dqis the information gain of the feature dfor the dataset D(computed using\nEquation (4.4)[130]from Section 4.2.3[127]), and the divisor is the entropy of the dataset D\nwith respect to the feature d(note that levelspdqis the set of levels that the feature d\ncan take). This divisor biases information gain ratio away from features that take on a\nlarge number of values and as such counteracts the bias in information gain toward these\nfeatures.\nTo illustrate how information gain ratio is computed, we compute the information gain\nratio for the descriptive features S TREAM , SLOPE , and E LEVATION in the vegetation clas-\nsiﬁcation dataset in Table 4.3[136]. We already know the information gain for these features\n(see Table 4.4[137])\nIGpSTREAM,Dq“0.3060\nIGpSLOPE,Dq“0.5774\nIGpELEVATION,Dq“0.8774\nTo convert these information gain scores into information gain ratios, we need to compute\nthe entropy of each feature and then divide the information gain scores by the respective","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":197,"page_label":"143","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 143\nentropy values. The entropy calculations for these descriptive features are\nHpSTREAM,Dq\n“´ÿ\nlP!true,\nfalse)PpSTREAM“lqˆlog2pPpSTREAM“lqq\n“´``4{7ˆlog2`4{7˘˘\n``3{7ˆlog2`3{7˘˘˘\n“0.9852 bits\nHpSLOPE,Dq\n“´ÿ\nlP#ﬂat,\nmoderate,\nsteep+PpSLOPE“lqˆlog2pPpSLOPE“lqq\n“´``1{7ˆlog2`1{7˘˘\n``1{7ˆlog2`1{7˘˘\n``5{7ˆlog2`5{7˘˘˘\n“1.1488 bits\nHpELEVATION,Dq\n“´ÿ\nlP$\n&\n%low,\nmedium,\nhigh,\nhighest,\n.\n-PpELEVATION“lqˆlog2pPpELEVATION“lqq\n“´``1{7ˆlog2`1{7˘˘\n``2{7ˆlog2`2{7˘˘\n``3{7ˆlog2`3{7˘˘\n``1{7ˆlog2`1{7˘˘˘\n“1.8424 bits\nUsing these results, we can compute the information gain ratio for each descriptive fea-\nture by dividing the feature’s information gain by the entropy for that feature\nGRpSTREAM,Dq“0.3060\n0.9852“0.3106\nGRpSLOPE,Dq“0.5774\n1.1488“0.5026\nGRpELEVATION,Dq“0.8774\n1.8424“0.4762\nFrom these calculations we can see that S LOPE has the highest information gain ratio\nscore, even though E LEVATION has the highest information gain. The implication is that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":198,"page_label":"144","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"144 Chapter 4 Information-Based Learning\nSlope\nconiferﬂat\nriparianmoderate\nElevationsteep\nchaparrallow\nStreammedium\nchaparralhigh\nconiferhighest\nripariantrue\nchaparralfalse\nFigure 4.12\nThe vegetation classiﬁcation decision tree generated using information gain ratio.\nif we built a decision tree for the dataset in Table 4.3[136]using information gain ratio,\nthen S LOPE (rather than E LEVATION ) would be the feature chosen for the root of the tree.\nFigure 4.12[144]illustrates the tree that would be generated for this dataset using information\ngain ratio.\nNotice that there is a chapparal leaf node at the end of the branch E LEVATION =loweven\nthough there are no instances in the dataset where E LEVATION =lowand V EGETATION\n=chapparal . This leaf node is the result of an empty partition being generated when the\npartition at the E LEVATION node was split. This leaf node was assigned the target level\nchapparal because this was the majority target level in the partition at the E LEVATION\nnode.\nIf we compare this decision tree to the decision tree generated using information gain\n(see Figure 4.11[141]), it is obvious that the structure of the two trees is very different. This\ndifference illustrates the effect of the metric used to select which feature to split on during\ntree construction. Another interesting point of comparison between these two trees is that\neven though they are both consistent with the dataset given in Table 4.3[136], they do not\nalways return the same prediction. For example, given the following query:\nSTREAM =false , SLOPE =moderate , ELEVATION =highest\nthe tree generated using information gain ratio (Figure 4.12[144]) will return V EGETATION\n=riparian , whereas the tree generated using information gain (Figure 4.11[141]) will return\nVEGETATION =conifer . The combination of features listed in this query does not occur in\nthe dataset. Consequently, both the trees are attempting to generalize beyond the dataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":199,"page_label":"145","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 145\nThis illustrates how two different models that are both consistent with a dataset can make\ndifferent generalizations.12So, which feature selection metric should be used, information\ngain or information gain ratio? Information gain has the advantage that it is computation-\nally less expensive than information gain ratio. If there is variation across the number of\nvalues in the domain of the descriptive features in a dataset, however, information gain\nratio may be a better option. These factors aside, the effectiveness of descriptive feature\nselection metrics can vary from domain to domain. Therefore, we should experiment with\ndifferent metrics to ﬁnd which one results in the best models for each dataset.\nAnother commonly used measure of impurity is the Gini index\nGinipt,Dq“1´ÿ\nlPlevelsptqPpt“lq2(4.9)\nwhere Dis a dataset with a target feature t;levelsptqis the set of levels in the domain of\nthe target feature; and Ppt“lqis the probability of an instance of Dhaving the target\nlevel l. The Gini index can be understood as calculating how often the target levels of\ninstances in a dataset would be misclassiﬁed if predictions were made on the sole basis of\nthe distribution of the target levels in the dataset. For example, if there were two target\nlevels with equal likelihood in a dataset, then the expected rate of misclassiﬁcation would\nbe0.5, and if there were four target levels with equal likelihood, then the expected rate of\nmisclassiﬁcation would be 0.75. The Gini index is 0when all the instances in the dataset\nhave the same target level, and it is 1´1\nkwhen there are kpossible target levels with equal\nlikelihood. Indeed, a nice feature of the Gini index is that Gini index scores are always\nbetween 0and1, and in some contexts this may make it easier to compare Gini indexes\nacross features. We can calculate the Gini index for the dataset in Table 4.3[136]\nGinipVEGETATION ,Dq\n“1´ÿ\nlP#chapparal,\nriparian,\nconifer+PpVEGETATION“lq2\n“1´´`3{7˘2``2{7˘2``2{7˘2¯\n“0.6531\nThe information gain for a feature based on the Gini index can be calculated in the same\nway that it is using entropy: calculate the Gini index for the full dataset and then subtract\nthe sum of the weighted Gini index scores for the partitions created by splitting with the\nfeature. Table 4.7[146]shows the calculation of the information gain using the Gini index for\nthe descriptive features in the vegetation classiﬁcation dataset. Comparing these results to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":199,"page_label":"145","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"between 0and1, and in some contexts this may make it easier to compare Gini indexes\nacross features. We can calculate the Gini index for the dataset in Table 4.3[136]\nGinipVEGETATION ,Dq\n“1´ÿ\nlP#chapparal,\nriparian,\nconifer+PpVEGETATION“lq2\n“1´´`3{7˘2``2{7˘2``2{7˘2¯\n“0.6531\nThe information gain for a feature based on the Gini index can be calculated in the same\nway that it is using entropy: calculate the Gini index for the full dataset and then subtract\nthe sum of the weighted Gini index scores for the partitions created by splitting with the\nfeature. Table 4.7[146]shows the calculation of the information gain using the Gini index for\nthe descriptive features in the vegetation classiﬁcation dataset. Comparing these results to\n12. This is an example of how machine learning is an ill-posed problem , as discussed in Section 1.3[7].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":200,"page_label":"146","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"146 Chapter 4 Information-Based Learning\nTable 4.7\nPartition sets (Part.), entropy, Gini index, remainder (Rem.), and information gain (Info. Gain) by\nfeature for the dataset in Table 4.3[136].\nSplit by Partition Info.\nFeature Level Part. Instances Gini Index Rem. Gain\nSTREAMtrue D1 d2,d3,d6,d7 0.62500.5476 0.1054false D2 d1,d4,d5 0.4444\nSLOPEﬂat D3 d5 0.0\n0.4000 0.2531 moderate D4 d2 0.0\nsteep D5 d1,d3,d4,d6,d7 0.5600\nELEVATIONlow D6 d2 0.0\n0.3333 0.3198medium D7 d3,d4 0.5000\nhigh D8 d1,d5,d7 0.4444\nhighest D9 d6 0.0\nthe information gain calculated using entropy (see Table 4.4[137]), we can see that although\nthe resulting numbers are different, the relative ranking of the features is the same—in both\ncases E LEVATION has the highest information gain. Indeed, for the vegetation dataset, the\ndecision tree that will be generated using information gain based on the Gini index will be\nidentical to the one generated using information gain based on entropy (see Figure 4.11[141]).\nSo, which impurity measure should be used, Gini or entropy? The best advice that we\ncan give is that it is good practice in building decision tree models to try out different\nimpurity metrics and compare the results to see which suits a dataset best.\n4.4.2 Handling Continuous Descriptive Features\nThe easiest way to handle a continuous descriptive feature in a decision tree is to deﬁne\na threshold within the range of values that the continuous feature can take and to use this\nthreshold to partition the instances on the basis of whether their values for the feature are\nabove or below the threshold.13The only challenge is to determine the best threshold to\nuse. Ideally, we should use the threshold that results in the highest information gain when\nthe feature is used to split the dataset. The problem, however, is that with a continuous\nfeature, there is an inﬁnite number of thresholds to choose from.\nThere is, however, a simple way to ﬁnd the optimal threshold, which avoids testing an\ninﬁnite number of possible thresholds. First, the instances in the dataset are sorted ac-\ncording to the values of the continuous feature. The adjacent instances in the ordering\nthat have different target feature levels are then selected as possible threshold points. It\ncan be shown that the optimal threshold value must lie at one of the boundaries between\n13. This approach is related to binning , as described in Section 3.6.2[89]. Simply binning continuous features to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":200,"page_label":"146","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the feature is used to split the dataset. The problem, however, is that with a continuous\nfeature, there is an inﬁnite number of thresholds to choose from.\nThere is, however, a simple way to ﬁnd the optimal threshold, which avoids testing an\ninﬁnite number of possible thresholds. First, the instances in the dataset are sorted ac-\ncording to the values of the continuous feature. The adjacent instances in the ordering\nthat have different target feature levels are then selected as possible threshold points. It\ncan be shown that the optimal threshold value must lie at one of the boundaries between\n13. This approach is related to binning , as described in Section 3.6.2[89]. Simply binning continuous features to\nconvert them into categorical features is another valid approach to handling continuous features in decision trees.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":201,"page_label":"147","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 147\nTable 4.8\nDataset for predicting the vegetation in an area with a continuous E LEVATION feature (measured in\nfeet).\nID S TREAM SLOPE ELEVATION VEGETATION\n1 false steep 3,900 chapparal\n2 true moderate 300 riparian\n3 true steep 1,500 riparian\n4 false steep 1,200 chapparal\n5 false ﬂat 4,450 conifer\n6 true steep 5,000 conifer\n7 true steep 3,000 chapparal\nTable 4.9\nDataset for predicting the vegetation in an area sorted by the continuous E LEVATION feature.\nID S TREAM SLOPE ELEVATION VEGETATION\n2 true moderate 300 riparian\n4 false steep 1,200 chapparal\n3 true steep 1,500 riparian\n7 true steep 3,000 chapparal\n1 false steep 3,900 chapparal\n5 false ﬂat 4,450 conifer\n6 true steep 5,000 conifer\nadjacent instances with different target levels. The optimal threshold is found by comput-\ning the information gain for each of the target level transition boundaries and selecting the\nboundary with the highest information gain as the threshold. Once a threshold has been set,\nthe continuous feature can compete with the other categorical features for selection as the\nsplitting feature at any node. To illustrate how this is done, we use a modiﬁed version of\nthe vegetation classiﬁcation dataset given in Table 4.3[136]in which the E LEVATION feature\nnow contains actual elevations in feet. This dataset is listed in Table 4.8[147].\nTo select the best feature to use at the root of the tree, we need to calculate the infor-\nmation gain for each feature. We know from our earlier calculations that the entropy for\nthis dataset is 1.5567 bits (see Equation (4.5)[137]) and that the information gain for the cat-\negorical features are IGpSTREAM,Dq“0.3060 andIGpSLOPE,Dq“0.5774 (see Table\n4.4[137]). This leaves us with the tasks of calculating the best threshold on which to split the\nELEVATION feature, and calculating the information gain when we partition the dataset\nwith E LEVATION using this optimal threshold. Our ﬁrst task is to sort the dataset based on\nthe E LEVATION feature. The result is shown in Table 4.9[147].\nOnce the instances have been sorted, we look for adjacent pairs that have different target\nlevels. In Table 4.9[147]we can see that four pairs of adjacent instances have a transition","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":202,"page_label":"148","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"148 Chapter 4 Information-Based Learning\nTable 4.10\nPartition sets (Part.), entropy, remainder (Rem.), and information gain (Info. Gain) for the candidate\nELEVATION thresholds:ě750,ě1,350,ě2,250andě4,175.\nSplit by Partition Info.\nThreshold Part. Instances Entropy Rem. Gain\ně750D1 d2 0.01.2507 0.3060D2 d4,d3,d7,d1,d5,d6 1.4591\ně1,350D3 d2,d4 1.01.3728 0.1839D4 d3,d7,d1,d5,d6 1.5219\ně2,250D5 d2,d4,d3 0.91830.9650 0.5917D6 d7,d1,d5,d6 1.0\ně4,175D7 d2,d4,d3,d7,d1 0.97100.6935 0.8631D8 d5,d6 0.0\nbetween the target levels: instances d2andd4,d4andd3,d3andd7, and d1andd5. The\nboundary value between each of these pairs is simply the average of their E LEVATION\nvalues:\n‚The boundary between d2andd4is300`1,200\n2“750\n‚The boundary between d4andd3is1,200`1,500\n2“1,350\n‚The boundary between d3andd7is1,500`3,000\n2“2,250\n‚The boundary between d1andd5is3,900`4,450\n2“4,175\nThis results in four candidate thresholds: ě750,ě1,350,ě2,250, andě4,175. Table\n4.10[148]shows the computation of information gain for a split using each of these thresh-\nolds. The threshold ě4,175has the highest information gain of any of the candidate thresh-\nolds ( 0.8631 bits), and this information gain is also higher than the information gain for\neither of the other two descriptive features. Therefore, we should use E LEVATIONě4,175\nas the test at the root node of the tree, as shown in Figure 4.13[149].\nUnlike categorical features, continuous features can be used at multiple points along a\npath in a decision tree, although the threshold applied to the feature at each of these tests\nwill be different. This is important because it allows multiple splits within a range of\na continuous feature to be considered on a path. Consequently, as we build the rest of\nthe tree, we may reuse the E LEVATION feature. This is why that E LEVATION feature is\nlisted in both the partitions ( D7andD8) shown in Figure 4.13[149]. We can continue to\nbuild the tree by recursively extending each branch as we did in the previous decision tree\nexamples. Figure 4.14[150]shows the decision tree that is ultimately generated from this\nprocess. Notice that the tree uses a mixture of continuous and categorical features and that\nthe E LEVATION feature is used twice with different thresholds in each case.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":203,"page_label":"149","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 149\nElevation\n D7   ID    Stream    Slope    Elevation    Vegetation  \n2 true moderate    300 riparian\n4 false steep 1,200 chaparral\n3 true steep 1,500 riparian\n7 true steep 3,000 chaparral\n1 false steep 3,900 chaparral<4,175\n D8   ID    Stream    Slope    Elevation    Vegetation  \n5 false ﬂat 4,450 conifer\n6 true steep 5,000 conifer≥4,175\nFigure 4.13\nThe vegetation classiﬁcation decision tree after the dataset has been split using E LEVATIONě4,175.\n4.4.3 Predicting Continuous Targets\nWhen we use a decision tree to make predictions for a continuous target, we refer to the\ntree as a regression tree .14Typically, the value output by the leaf node of a regression tree\nis the mean of the target feature values of the instances from the training set that reached\nthat node. This means that the error of a regression tree in making a prediction for a query\ninstance is the difference between the mean of the training instances that reached the leaf\nnode that returns the prediction and the correct value that should have been returned for\nthat query. Assuming that the set of training instances reaching a leaf node are indicative\nof the queries that will be labeled by the node, it makes sense to construct regression trees\nin a manner that reduces the variance in the target feature values of the set of training\ninstances at each leaf node in the tree. We can do this by adapting the ID3 algorithm to\nuse a measure of variance15rather than a measure of entropy in selecting the best feature.\nUsing variance as our measure of impurity, the impurity at a node can be calculated\nvarpt,Dq“řn\ni“1pti´¯tq2\nn´1(4.10)\nwhere Dis the dataset that has reached the node; nis the number of instances in D;¯tis the\nmean of the target feature for the dataset D; and tiiterates across the target value of each\ninstance in D. Using variance as our measure of impurity, we can select which feature to\nsplit on at a node by selecting the feature that minimizes the weighted variance across the\nresulting partitions. The weighted variance is computed by summing the variance of the\ntarget feature within each partition created by splitting a dataset on a descriptive feature\n14. Sometimes the task of predicting a continuous target is referred to as a regression task .\n15. We introduce variance in Section A.1.2[746], and although we extend the formal deﬁnition of variance here to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":203,"page_label":"149","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"mean of the target feature for the dataset D; and tiiterates across the target value of each\ninstance in D. Using variance as our measure of impurity, we can select which feature to\nsplit on at a node by selecting the feature that minimizes the weighted variance across the\nresulting partitions. The weighted variance is computed by summing the variance of the\ntarget feature within each partition created by splitting a dataset on a descriptive feature\n14. Sometimes the task of predicting a continuous target is referred to as a regression task .\n15. We introduce variance in Section A.1.2[746], and although we extend the formal deﬁnition of variance here to\ninclude a dataset parameter D—we do this to explicitly highlight the fact that we are calculating the variance of a\nfeature within a particular dataset, usually the dataset at a node in the tree—the measure of variance we are using\nis identical to the variance deﬁned in Equation (A.3)[747].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":204,"page_label":"150","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"150 Chapter 4 Information-Based Learning\nElevation\nStream<4,175\nconifer ≥4,175\nElevationtrue\nchaparralfalse\nriparian<2,250\nchaparral ≥2,250\nFigure 4.14\nThe decision tree that would be generated for the vegetation classiﬁcation dataset listed in Table\n4.9[147]using information gain.\nmultiplied by the fraction of the dataset in each partition. So, at each node the algorithm\nwill choose the feature to split on by selecting the feature with the lowest weighted variance\nfor the target feature\ndrbests“arg min\ndPdÿ\nlPlevelspdq|Dd“l|\n|D|ˆvarpt,Dd“lq (4.11)\nwhere varpt,Dd“lqis the variance of the target feature in the partition of the dataset D\ncontaining the instances where d“l,|Dd“l|is the size of this partition and |D|is the\nsize of the dataset. This means that at each decision node, the algorithm will select the\nfeature that partitions the dataset to most reduce the weighted variance of the partitions.\nThis causes the algorithm to cluster instances with similar target feature values. As a result,\nleaf nodes with small variance in the target feature values across the set of instances at the\nnode are preferred over leaf nodes where the variance in the target feature values across\nthe set of instances at the node is large. To change the ID3 algorithm in Algorithm 1[134]to\nselect features to split on based on variance, we replace Line 5 with Equation 4.11[150].\nThe other change we need to make to Algorithm 1[134]to handle continuous targets relates\nto the base cases that cause the algorithm to stop processing data partitions and to create\na leaf node. In the ID3 algorithm we created a leaf node when there were no instances\nleft in the partition being processed (Line 3), when there were no features left on which to\nsplit the data (Line 2), or when we had created a pure partition of the dataset with respect","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":205,"page_label":"151","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 151\n- u u u u u u u\nTarget (a)\n- u u u u u u u\nUnder f itting (b)\n- u u u u u u u\nGoldilocks (c)\n- u u u u u u u\nOver f ittingh hh hhhh\n(d)\nFigure 4.15\n(a) A set of instances on a continuous number line; (b), (c), and (d) depict some of the potential\ngroupings that could be applied to these instances.\nto the target feature levels (Line 1). An algorithm to learn decision trees for a continuous\ntarget can use the ﬁrst two base cases. When these cases occur, the algorithm will create\na leaf node that returns the mean value of the target feature in a data partition, rather than\nthe majority level. For continuous targets there is no such thing as a pure split, so we will\nneed to change the ﬁnal base case.\nFigure 4.15[151]illustrates the type of partitioning we are trying to achieve when we use\na variance measure to select the features to split on in a decision tree. Figure 4.15(a)[151]\ndepicts a set of instances on the continuous number line. Figure 4.15(b)[151]depicts one\nof the extremes for grouping these instances, in which we treat them all as belonging to\none partition. The large gap between the two apparent clusters in this dataset results in\na large variance, which indicates that we are probably underﬁtting with this grouping.\nFigure 4.15(c)[151]shows that the instances have been gathered into two groups that have a\nrelatively low variance compared with the single group in Figure 4.15(b)[151]. Intuitively we\ncan see that this grouping is, as Goldilocks put it, just right and is the type of grouping we\nare trying to generate when we use a variance measure to select the splitting point.\nFigure 4.15(d)[151]depicts one of the problems that can arise when a variance measure\nis used to split a continuous target feature. In this example each instance has been put\ninto an individual partition, and although these partitions each have a variance of zero, this\nis indicative of overﬁtting the data. This extreme partitioning of the dataset into sets of\nsingle instances can happen if there are a lot of descriptive features in the dataset or if there\nare one or more continuous descriptive features that the algorithm is allowed to split on","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":206,"page_label":"152","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"152 Chapter 4 Information-Based Learning\nTable 4.11\nA dataset listing the number of bike rentals per day.\nWORK\nID S EASON DAY RENTALS\n1 winter false 800\n2 winter false 826\n3 winter true 900\n4 spring false 2,100\n5 spring true 4,740\n6 spring true 4,900WORK\nID S EASON DAY RENTALS\n7 summer false 3,000\n8 summer true 5,800\n9 summer true 6,200\n10 autumn false 2,910\n11 autumn false 2,880\n12 autumn true 2,820\nrepeatedly. The reason that partitioning the dataset into single instances is indicative of\noverﬁtting is that if there is any noise in the training data (something that is likely in real\napplications), then the leaf nodes generated due to noisy instances will result in unreliable\npredictions for queries. To avoid this kind of extreme partitioning, we introduce an early\nstopping criterion into the algorithm for building regression trees. The simplest early\nstopping criterion is to stop partitioning the dataset if the number of training instances in\nthe partition at the node we are processing is less than some threshold, usually around 5%\nof the overall dataset size.16This early stopping criterion replaces the base case on Line 1\nof the ID3 algorithm.\nThe change to the mechanism for selecting the best feature to split on (made on Line\n5) and the introduction of an early stopping criterion (which replaces Line 1) are the only\nmodiﬁcations we need to make to the ID3 algorithm (Algorithm 1[134]) to allow it to handle\ncontinuous target features. To see how this revised algorithm can induce a decision tree,\nwe use the example of predicting the number of bike rentals per day for a city bike sharing\nprogram based on the S EASON and whether it is a W ORK DAY. Predicting the number\nof bike rentals on a given day is useful because it can give the administrators of the bike\nsharing program an insight into the number of resources they need to have ready each day.\nTable 4.11[152]lists a small dataset from this domain.17\nTable 4.12[153]illustrates the computation of the weighted variance that results from par-\ntitioning the data by S EASON and W ORK DAY. It is evident from Table 4.12[153]that par-\ntitioning the data using S EASON results in a lower weighted variance than partitioning by\n16. It is also common to use a minimum partition variance as an early stopping criterion. If the variance in the\npartition being processed is below a set threshold, then the algorithm will not partition the data and will instead\ncreate a leaf node.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":206,"page_label":"152","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sharing program an insight into the number of resources they need to have ready each day.\nTable 4.11[152]lists a small dataset from this domain.17\nTable 4.12[153]illustrates the computation of the weighted variance that results from par-\ntitioning the data by S EASON and W ORK DAY. It is evident from Table 4.12[153]that par-\ntitioning the data using S EASON results in a lower weighted variance than partitioning by\n16. It is also common to use a minimum partition variance as an early stopping criterion. If the variance in the\npartition being processed is below a set threshold, then the algorithm will not partition the data and will instead\ncreate a leaf node.\n17. This example is inspired by the research reported in Fanaee-T and Gama (2014). The dataset presented\nhere is synthesized for this example; however, a real bike sharing dataset for this task is available through the\nUCI Machine Learning Repository (Bache and Lichman, 2013) at archive.ics.uci.edu/ml/datasets/Bike+Sharing+\nDataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":207,"page_label":"153","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 153\nTable 4.12\nThe partitioning of the dataset in Table 4.11[152]based on S EASON and W ORK DAYfeatures and the\ncomputation of the weighted variance for each partitioning.\nSplit by |Dd“l|\n|D|Weighted\nFeature Level Part. Instances varpt,Dq Variance\nSEASONwinter D1 d1,d2,d3 0.25 2 ,692\n1,379,331spring D2 d4,d5,d6 0.25 2,472,533\nsummer D3 d7,d8,d9 0.25 3,040,000\nautumn D4 d10,d11,d12 0.25 2 ,100\nWORK DAYtrue D5 d3,d5,d6,d8,d9,d12 0.50 4,026,3462,551,813false D6 d1,d2,d4,d7,d10,d11 0.50 1,077,280\nSeason\n D1   ID    Work Day    Rentals  \n1 false 800\n2 false 826\n3 true 900winter\n D2   ID    Work Day    Rentals  \n4 false 2,100\n5 true 4,740\n6 true 4,900spring\n D3   ID    Work Day    Rentals  \n7 false 3,000\n8 true 5,800\n9 true 6,200summer\n D4   ID    Work Day    Rentals  \n10 false 2,910\n11 false 2,880\n12 true 2,820 autumn\nFigure 4.16\nThe decision tree resulting from splitting the data in Table 4.11[152]using the feature S EASON .\nWORK DAY. This tells us that splitting by S EASON results in a better clustering of the\ntarget data than splitting by W ORK DAY. Figure 4.16[153]illustrates the state of the decision\ntree after the root node has been created using S EASON .\nFigure 4.17[154]illustrates the ﬁnal decision tree that will be generated for this dataset.\nThis tree will predict the mean target feature value of the leaf node indicated by the de-\nscriptive features of a query instance. For example, given a query instance with S EASON\n=summer and W ORK DAY=true, this decision tree will predict that there will be 6,000\nbike rentals on that day.\n4.4.4 Tree Pruning\nA predictive model overﬁts the training set when at least some of the predictions it re-\nturns are based on spurious patterns present in the training data used to induce the model.\nOverﬁtting happens for a number of reasons, including sampling variance18and noise in\n18. This means that the distribution over the target feature will be different between a training set sample and the\nfull population.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":208,"page_label":"154","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"154 Chapter 4 Information-Based Learning\nSeason\nWork Daywinter\nWork Dayspring\nWork Day summer Work Day autumn\nIDRentals Pred.\n3900 900true\nIDRentals Pred.\n18008132826 false\nIDRentals Pred.\n54,7404,82064,900true\nIDRentals Pred.\n42,100 2,100false\nIDRentals Pred.\n85,8006,00096,200 true\nIDRentals Pred.\n73,000 3,000 falseIDRentals Pred.\n122,820 2,820 true\nIDRentals Pred.\n102,9102,895112,880false\nFigure 4.17\nThe ﬁnal decision tree induced from the dataset in Table 4.11[152]. To illustrate how the tree generates\npredictions, this tree lists the instances that ended up at each leaf node and the prediction (PRED.)\nmade by each leaf node.\nthe training set.19The problem of overﬁtting can affect any machine learning algorithm;\nhowever, the fact that decision tree induction algorithms work by recursively splitting the\ntraining data means that they have a natural tendency to segregate noisy instances and to\ncreate leaf nodes around these instances. Consequently, decision trees overﬁt by split-\nting the data on irrelevant features that appear relevant only because of noise or sampling\nvariance in the training data. The likelihood of overﬁtting increases as a tree gets deeper\nbecause the resulting predictions are based on smaller and smaller subsets as the dataset is\npartitioned after each feature test in the path.\nTree pruning identiﬁes and removes subtrees within a decision tree that are likely to be\ndue to noise and sample variance in the training set used to induce it. In cases in which a\nsubtree is deemed to be overﬁtting, pruning the subtree means replacing the subtree with\na leaf node that makes a prediction on the basis of the majority target feature level (or\naverage target feature value) of the dataset created by merging the instances from all the\nleaf nodes in the subtree. Obviously, pruning will result in the creation of decision trees\nthat are not consistent with the training set used to build them. In general, however, we are\nmore interested in creating prediction models that generalize well to new data rather than\nthat are strictly consistent with training data, so it is common to sacriﬁce consistency for\ngeneralization capacity.\n19. For example, there might be errors in the target feature or descriptive feature values of one or more of the\ntraining instances.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":209,"page_label":"155","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 155\nThe simplest way to prune a decision tree is to introduce early stopping criteria (similar\nto the early stopping criterion discussed in the preceding section) into the tree induction\nalgorithm. This is often known as pre-pruning . There is a range of simple pre-pruning\nstrategies. For example, we can stop creating subtrees when the number of instances in\na partition falls below a threshold, when the information gain (or whatever other feature\nselection metric is being used) measured at a node is not deemed to be sufﬁcient to make\npartitioning the data worthwhile,20or when the depth of the tree goes beyond a predeﬁned\nlimit. More advanced approaches to pre-pruning use statistical signiﬁcance tests to deter-\nmine the importance of subtrees, for example, χ2pruning (pronounced chi-squared ).21\nPre-pruning approaches are computationally efﬁcient and can work well for small datasets.\nBy stopping the partitioning of the data early, however, induction algorithms that use pre-\npruning can fail to create the most effective trees because they miss interactions between\nfeatures that emerge within subtrees that are not obvious when the parent nodes are being\nconsidered. Pre-pruning can mean that these useful subtrees are never created.\nPost-pruning is an alternative approach to tree pruning in which the tree induction algo-\nrithm is allowed to grow a tree to completion, and then each branch on the tree is examined\nin turn. Branches that are deemed likely to be due to overﬁtting are pruned. Post-pruning\nrelies on a criterion that can distinguish between subtrees that model relevant aspects of the\ndata and subtrees that model irrelevant random patterns in the data. There is a range of dif-\nferent criteria that can be used, from a very simple threshold on the number of instances at\na node in the tree, to statistical signiﬁcance texts like χ2. We recommend the use of criteria\nthat compare the error rate in the predictions made by a decision tree when a given subtree\nis included and when it is pruned. To measure error rate, we set aside some of the training\ndata as a validation dataset22that is not used during tree induction. We can measure the\nperformance of a decision tree by presenting the instances in the validation to the decision\ntree and comparing the predictions made for these instances with the actual target feature\nvalues in the dataset. The error rate measures the number of predictions made by the tree","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":209,"page_label":"155","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ferent criteria that can be used, from a very simple threshold on the number of instances at\na node in the tree, to statistical signiﬁcance texts like χ2. We recommend the use of criteria\nthat compare the error rate in the predictions made by a decision tree when a given subtree\nis included and when it is pruned. To measure error rate, we set aside some of the training\ndata as a validation dataset22that is not used during tree induction. We can measure the\nperformance of a decision tree by presenting the instances in the validation to the decision\ntree and comparing the predictions made for these instances with the actual target feature\nvalues in the dataset. The error rate measures the number of predictions made by the tree\nthat are incorrect. A subtree is pruned if the error rate on the validation set of the decision\ntree with the subtree removed is no greater than the error rate of the decision tree when the\nsubtree is included. Because the instances in the validation set are not used during training,\nthe error rate on the validation set provides a good estimate of the generalization capability\nof a decision tree.\nReduced error pruning (Quinlan, 1987) is a popular version of post-pruning based on\nerror rates. In reduced error pruning, a decision tree is built to completion and then the tree\nis searched in an iterative, bottom-up, left-to-right manner for subtrees that can be pruned.\n20.Critical value pruning (Mingers, 1987) is a well-known version of this pruning technique.\n21. See Frank (2000) for a detailed discussion and analysis on the use of statistical tests in decision tree pruning.\n22. In the context of decision tree pruning, the validation set is often referred to as the pruning dataset .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":210,"page_label":"156","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"156 Chapter 4 Information-Based Learning\n  Core-Temp   \n   [icu]   \n  Gender   \n   [icu]   low\n  Stable-Temp   \n   [gen]   high\nicumale\ngenfemale\ngentrue\nicufalse\nFigure 4.18\nThe decision tree for the post-operative patient routing task.\nThe error rate resulting from predictions for the instances in the validation dataset made at\nthe root node of each subtree is compared to the error rate resulting from predictions made\nat the leaves of the subtree. If the error rate at the subtree root node is less than or equal to\nthe combined error rate at the leaves, the subtree is pruned.\nTo show how reduced error pruning works, we consider the task of predicting whether a\npost-operative patient should be sent to an intensive care unit (ICU) or to a general ward\nfor recovery.23Hypothermia is a major concern for post-operative patients, so many of\nthe descriptive features relevant to this domain relate to a patient’s body temperature. In\nour example, C ORE-TEMP describes the core temperature of the patient (which can be low\norhigh), and S TABLE -TEMP describes whether the patient’s current temperature is stable\n(true orfalse ). We also include the G ENDER of the patient ( male orfemale ). The target\nfeature in this domain, D ECISION , records the decision of whether the patient is sent to the\nicuor to a general ward ( gen) for recovery. Figure 4.18[156]illustrates a decision tree that has\nbeen trained for this post-operative patient routing task. The target level in square brackets\nat each interior node in the tree shows the majority target level for the data partition at that\nnode.\nTable 4.13[157]lists a validation dataset for this domain, and Figure 4.19[158]illustrates\nhow this validation dataset is used to perform reduced error pruning. In Figure 4.19(a)[158]\nthe pruning algorithm considers the subtree under the G ENDER node for pruning. The path\nthrough the tree to make predictions for instances d2,d5, and d6from the validation dataset\nleads to this subtree. The majority target level predicted at the root node of this subtree (the\nGENDER node) gives a correct prediction of icufor each of the three instances, so the error\n23. The example of predicting where post-operative patients should be sent is inspired by the research reported\nin Woolery et al. (1991). A real dataset related to this research is available through the UCI Machine Learning\nRepository (Bache and Lichman, 2013) at archive.ics.uci.edu/ml/datasets/Post-Operative+Patient/.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":211,"page_label":"157","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 157\nTable 4.13\nAn example validation set for the post-operative patient routing task.\nID C ORE-TEMP STABLE -TEMP GENDER DECISION\n1 high true male gen\n2 low true female icu\n3 high false female icu\n4 high false male icu\n5 low false female icu\n6 low true male icu\nrate on the validation set for the root node of the subtree is 0. In contrast, the predictions\nmade at the leaf nodes of this subtree are incorrect for d2andd5(because these patients\narefemale , the prediction made is genwhich does not match the validation dataset), so the\nerror rate for the leaf nodes of this subtree is 0`2“2. Because the error rate for the leaf\nnodes is higher than the error rate for the root node of the subtree, this subtree is pruned\nand replaced by a leaf node. The result of this pruning is visible on the left branch of the\ntree shown in Figure 4.19(b)[158].\nIn the second iteration of the algorithm, the subtree under the S TABLE -TEMP node is\nconsidered for pruning (highlighted in Figure 4.19(b)[158]). In this instance, the error rate\nfor the root node of this subtree (the S TABLE -TEMP node) is 2, whereas the error rate of\nthe leaf nodes of the tree is 0`0“0. As the error rate of the root node of the subtree\nis higher than the error rate of the leaf nodes, the tree is not pruned. Figure 4.19(c)[158]\nillustrates the ﬁnal iteration of the algorithm. In this iteration the subtree underneath the\nroot node of the decision tree (the C ORE-TEMP node) is considered for pruning (i.e., the\nfull decision tree). In this iteration, the error rate of the root node (1) is greater than the\nerror rate of the three leaf nodes, ( 0`0`0“0), so the tree is left unchanged.\nPost-pruning using an error rate criterion is probably the most popular way to prune\ndecision trees.24One of the advantages of pruning decision trees is that it keeps trees\nsmaller, which in turn makes them easier to interpret. Another advantage is that pruning\noften increases the accuracy of the trees when there is noise in the training dataset. The\nreason is that pruning typically affects the lower parts of the decision tree, where noisy\ntraining data is most likely to cause overﬁtting. As such, pruning can be viewed as a noise\ndampening mechanism that removes nodes that have been created because of a small set\nof noisy instances.\n24. See Esposito et al. (1997) and Mingers (1989) for overviews and empirical comparisons of a range of decision\ntree pruning methods based on error rate.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":212,"page_label":"158","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"158 Chapter 4 Information-Based Learning\n  Core-Temp   \n   [icu]   \n  Gender   \n   [icu] (0)    low\n  Stable-Temp   \n   [gen]    high\nicu (0)male\ngen (2)female\ngen true\nicu false\n(a)\n  Core-Temp   \n   [icu]   \niculow\n  Stable-Temp   \n   [gen] (2)   high\ngen (0) true\nicu (0) false (b)\n  Core-Temp   \n   [icu] (1)   \nicu (0)low\n  Stable-Temp   \n   [gen]   high\ngen (0)true\nicu (0)false (c)\nFigure 4.19\nThe iterations of reduced error pruning for the decision tree in Figure 4.18[156]using the validation set\nin Table 4.13[157]. The subtree that is being considered for pruning in each iteration is highlighted in\nblack. The prediction returned by each non-leaf node is listed in square brackets. The error rate for\neach node is given in parantheses.\n4.4.5 Model Ensembles\nMuch of the focus of machine learning is on developing the single most accurate prediction\nmodel possible for a given task. The techniques we introduce in this section take a slightly\ndifferent approach. Rather than creating a single model, they generate a set of models and\nthen make predictions by aggregating the outputs of these models. A prediction model that\nis composed of a set of models is called a model ensemble .\nThe motivation behind using ensemble methods is that a committee of experts working\ntogether on a problem are more likely to solve it successfully than a single expert working\nalone. As is always the case when a committee is working together, however, steps should\nbe taken to guard against group think . In the context of model ensembles, this means\nthat each model should make predictions independently of the other models in the ensem-\nble. Given a large population of independent models, an ensemble can be very accurate,\neven if the individual models in the ensemble perform only marginally better than random\nguessing.\nThere are two deﬁning characteristics of ensemble models:\n1.They build multiple different models from the same dataset by inducing each model\nusing a modiﬁed version of the dataset.\n2.They make a prediction by aggregating the predictions of the different models in the\nensemble. For categorical target features, this can be done using different types of\nvoting mechanisms; and for continuous target features, this can be done using a mea-\nsure of the central tendency of the different model predictions, such as the mean or the\nmedian.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":213,"page_label":"159","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 159\nThere are two standard approaches to creating ensembles: bagging andboosting . The\nremainder of this section explains each of these basic approaches. Commonly used, high-\nperforming extensions to both of the basic approaches are also described: random forests\nin the case of bagging and gradient boosting in the case of boosting.\n4.4.5.1 Bagging When we use bagging (orbootstrap aggregating ), each model in the\nensemble is trained on a random sample25of the dataset where, importantly, each random\nsample is the same size as the dataset, and sampling with replacement is used. These\nrandom samples are known as bootstrap samples , and one model is induced from each\nbootstrap sample. The reason that we sample with replacement is that this will result in\nduplicates within each of the bootstrap samples, and consequently every bootstrap sample\nwill be missing some of the instances from the dataset. As a result, each bootstrap sample\nwill be different, and this means that models trained on different bootstrap samples will\nalso be different.26\nDecision tree induction algorithms are particularly well suited to use with bagging. The\nreason is that decision trees are very sensitive to changes in the dataset: a small change in\nthe dataset can result in a different feature being selected to split the dataset at the root or\nhigh in the tree, and this can have a ripple effect throughout the subtrees under this node.\nFrequently, when bagging is used with decision trees, the sampling process is extended so\nthat each bootstrap sample uses only a randomly selected subset of the descriptive features\nin the dataset. This sampling of the feature set is known as subspace sampling . Subspace\nsampling further encourages the diversity of the trees within the ensemble and has the\nadvantage of reducing the training time for each tree.\nFigure 4.20[160]illustrates the process of creating a model ensemble using bagging and\nsubspace sampling. Model ensembles combining bagging, subspace sampling, and deci-\nsion trees are known as random forest models. Once the individual models in an ensemble\nhave been induced, the ensemble makes predictions by returning the majority vote or the\nmedian, depending on the type of prediction required. For continuous target features, the\nmedian is preferred to the mean because the mean is more heavily affected by outliers.27\n4.4.5.2 Boosting When we use boosting ,28each new model added to an ensemble is bi-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":213,"page_label":"159","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"advantage of reducing the training time for each tree.\nFigure 4.20[160]illustrates the process of creating a model ensemble using bagging and\nsubspace sampling. Model ensembles combining bagging, subspace sampling, and deci-\nsion trees are known as random forest models. Once the individual models in an ensemble\nhave been induced, the ensemble makes predictions by returning the majority vote or the\nmedian, depending on the type of prediction required. For continuous target features, the\nmedian is preferred to the mean because the mean is more heavily affected by outliers.27\n4.4.5.2 Boosting When we use boosting ,28each new model added to an ensemble is bi-\nased to pay more attention to instances that previous models misclassiﬁed. This is done by\nincrementally adapting the dataset used to train the models. To do this we use a weighted\n25. See Section 3.6.3[91].\n26. If we have a very large dataset, we may—for computational reasons—want to create bootstrap samples that\nare smaller than the original dataset. If this is the case, then sampling without replacement is preferred. This is\nknown as subagging .\n27. Question 5 in the Exercises section at the end of this chapter explores bagging and random forest ensemble\nmodels in more detail, and worked examples are provided in the solution.\n28. Schapire (1999) gives a readable introduction to boosting by one of the originators of the technique.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":214,"page_label":"160","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"160 Chapter 4 Information-Based Learning\nMODEL ENSEMBLEIDF1F2F3Target\n1--- -\n2--- -\n3--- -\n4--- -\nBagging and \n Subspace Sampling\nIDF1F3Target\n1-- -\n1-- -\n2-- -\n3-- -\nMachine Learning \n AlgorithmIDF2F3Target\n2-- -\n2-- -\n4-- -\n4-- -\nMachine Learning \n AlgorithmIDF1F3Target\n1-- -\n3-- -\n3-- -\n4-- -\nMachine Learning \n Algorithm\nF3 F2 F1\nF1 F3\nFigure 4.20\nThe process of creating a model ensemble using bagging and subspace sampling.\ndataset in which each instance has an associated weight wiě0, initially set to1\nnwhere\nnis the number of instances in the dataset. These weights are used as a distribution over\nwhich the dataset is sampled to create a replicated training set , in which the number of\ntimes an instance is replicated is proportional to its weight.\nBoosting works by iteratively creating models and adding them to the ensemble. The\niteration stops when a predeﬁned number of models have been added. During each iteration\nthe algorithm does the following:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":215,"page_label":"161","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 161\n1.Induces a model using the weighted dataset and calculates the total error, ϵ, in the\nset of predictions made by the model for the instances in the training dataset.29The\nϵvalue is calculated by summing the weights of the training instances for which the\npredictions made by the model are incorrect.\n2.Increases the weights for the instances misclassiﬁed by the model using\nwrisÐwrisˆˆ1\n2ˆϵ˙\n(4.12)\nand decreases the weights for the instances correctly classiﬁed by the model using30\nwrisÐwrisˆˆ1\n2ˆp1´ϵq˙\n(4.13)\n3.Calculates a conﬁdence factor ,α, for the model such that αincreases as ϵdecreases.\nA common way to calculate the conﬁdence factor is\nα“1\n2ˆlogeˆ1´ϵ\nϵ˙\n(4.14)\nOnce the set of models has been created, the ensemble makes predictions using a weighted\naggregate of the predictions made by the individual models. The weights used in this ag-\ngregation are the conﬁdence factors associated with each model. For categorical target\nfeatures, the ensemble returns the majority target level using a weighted vote. The boost-\ning algorithm presented here can be easily adapted to work with continuous targets by\nchanging the weight update rules to be based on error rather than misclassiﬁcation, and in\nthis case the aggregation calculates a weighted mean prediction from the base models in\nthe ensemble.31\nTo show how boosting works in practice, we use a modiﬁed version of the bike rentals\ndataset used in Section 4.4.3[149]. This time we will predict the expected rental demand on\nthe basis of a single descriptive feature, the forecasted temperature for a day. The ﬁrst\ncolumns of Table 4.14[162]detail a small sample dataset giving temperatures, T EMP, and\nrental demand, R ENTALS (which can be either Low orHigh ), for 10 days. The dataset\nis visualized in Figure 4.21(a)[163]. At the beginning of the boosting process the sampling\ndistribution is initialized so that each instance has an equal weight of1\n10(represented by\nthe equal segments for each instance in the visualization in Figure 4.21(c)[163]). In the ﬁrst\niteration of the algorithm a dataset is sampled using this distribution, and the number of\n29. Normally in machine learning, we do not test a model using the same dataset that we use to train it. Boosting,\nhowever, is an exception to this rule.\n30. Updating the weights using Equations (4.12)[161]and (4.13)[161]ensures that the weights always sum to 1.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":215,"page_label":"161","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"rental demand, R ENTALS (which can be either Low orHigh ), for 10 days. The dataset\nis visualized in Figure 4.21(a)[163]. At the beginning of the boosting process the sampling\ndistribution is initialized so that each instance has an equal weight of1\n10(represented by\nthe equal segments for each instance in the visualization in Figure 4.21(c)[163]). In the ﬁrst\niteration of the algorithm a dataset is sampled using this distribution, and the number of\n29. Normally in machine learning, we do not test a model using the same dataset that we use to train it. Boosting,\nhowever, is an exception to this rule.\n30. Updating the weights using Equations (4.12)[161]and (4.13)[161]ensures that the weights always sum to 1.\n31.Adaboost.R2 (Drucker, 1997) is a nice early example of a boosting algorithm designed to work with contin-\nuous targets.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":216,"page_label":"162","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"162 Chapter 4 Information-Based Learning\nTable 4.14\nA simple bicycle demand predictions dataset and the workings of the ﬁrst three iterations of training\nan ensemble model using boosting to predict R ENTALS given T EMP. For each iteration of the boost-\ning process the columns labeled Dist. give the weight in the sampling distribution for each training\ninstance, and the columns labeled Freq. give the number of times a training instance was included\nin the training set sampled using the sampling distribution. The columns labeled Mipdqgive the\npredictions made by the model trained at iteration ifor each instance in the training dataset.\nIteration 0 Iteration 1 Iteration 2\nID T EMP RENTALS Dist. Freq. M0pdq Dist. Freq. M1pdq Dist. Freq. M2pdq\n1 4 Low 0.100 2 Low 0.062 0 High 0.167 2 Low\n2 5 Low 0.100 1 Low 0.062 1 High 0.167 1 Low\n3 7 Low 0.100 0 Low 0.062 1 High 0.167 3 Low\n4 12 High 0.100 1 High 0.062 2 High 0.038 0 Low\n5 18 High 0.100 1 High 0.062 0 High 0.038 0 Low\n6 23 High 0.100 1 High 0.062 0 High 0.038 0 Low\n7 27 High 0.100 1 High 0.062 1 High 0.038 0 Low\n8 28 High 0.100 1 High 0.062 1 High 0.038 1 Low\n9 32 Low 0.100 2 High 0.250 3 Low 0.154 1 Low\n10 35 Low 0.100 0 High 0.250 1 Low 0.154 2 Low\ntimes each training instance is included in this sample is given in the Freq. column in\nTable 4.14[162]. Instances d1andd9are duplicated in this sample, and instances d3andd10\nare not included at all. A decision tree is then trained using the sample, and it is used to\nmake predictions for each instance in the complete training set. In this simple example\nthe decision tree is limited to a single root node with one split based on a value of T EMP\n(in this case the tree predicts Low rental demand for temperatures less than or equal to 8.5\ndegrees and High rental demand for all other temperatures). In Table 4.14[162]the column\nlabeled M0pdqshows the predictions made by this model.\nOn the basis of these predictions, ϵis calculated as 0.200, the sum of the weights of\nthe two instances misclassiﬁed by the model ( d9andd10). The weights of all correctly\nclassiﬁed instances are then updated using Equation (4.13)[161]. For example, the weight for\nd1is updated\nwr1sÐ0.100ˆˆ1\n2ˆp1´0.200q˙\nÐ0.0625\nSimilarly, the weights of all incorrectly classiﬁed instances are updated using Equation\n(4.12)[161]. For example, the weight for d9is updated\nwr9sÐ0.100ˆˆ1\n2ˆ0.200˙\nÐ0.250\nFigure 4.21(d)[163]illustrates the new distribution that arises from these changes, in which","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":216,"page_label":"162","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"degrees and High rental demand for all other temperatures). In Table 4.14[162]the column\nlabeled M0pdqshows the predictions made by this model.\nOn the basis of these predictions, ϵis calculated as 0.200, the sum of the weights of\nthe two instances misclassiﬁed by the model ( d9andd10). The weights of all correctly\nclassiﬁed instances are then updated using Equation (4.13)[161]. For example, the weight for\nd1is updated\nwr1sÐ0.100ˆˆ1\n2ˆp1´0.200q˙\nÐ0.0625\nSimilarly, the weights of all incorrectly classiﬁed instances are updated using Equation\n(4.12)[161]. For example, the weight for d9is updated\nwr9sÐ0.100ˆˆ1\n2ˆ0.200˙\nÐ0.250\nFigure 4.21(d)[163]illustrates the new distribution that arises from these changes, in which\nthe weight of the segments highlighted in black have increased, and those in gray have","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":217,"page_label":"163","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 163\n5 10 15 20 25 30 35\nTemperatureLowHighRentals\n(a) Training data\n5 10 15 20 25 30 35\nTemperatureLowHighRentals (b) The ﬁnal ensemble model, M\nd1\nd2\nd3\nd4\nd5 d6d7d8d9d10\n(c) Distribution 0\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8d9d10 (d) Distribution 1\nd1\nd2\nd3\nd4d5d6d7d8d9d10 (e) Distribution 2\nFigure 4.21\n(a) A plot of the bike rental dataset from Table 4.14[162]. (b) An illustration of the ﬁnal ensemble\nmodel trained using the boosting algorithm. The black circles show the training dataset, the gray\nsquares show the predictions made for the instances in the training dataset by the ensemble model,\nand the dotted line shows the predictions that would be made by the ensemble model for the full\nrange of input temperatures. (c)–(e) A representation of the changing weights used to generate\nsample datasets for the ﬁrst iterations of the boosting process. Black segments represent weights\nthat have increased, and gray segments represent weights that have decreased (all segments for the\ndistribution at the ﬁrst iteration are white because they have done neither).\ndecreased. The details of the next iteration of the boosting process are also detailed in\nTable 4.14[162]and Figure 4.21(e)[163].\nFigure 4.21(b)[163]shows an illustration of the ﬁnal ensemble model trained using ﬁve iter-\nations of the boosting algorithm. Even this simple example can nicely illustrate the power\nof boosting as, although all the individual decision trees in the ensemble are limited to a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":218,"page_label":"164","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"164 Chapter 4 Information-Based Learning\nsingle split at the root node of the tree, the ensemble is able to learn a more sophisticated\nprediction model that none of the individual models within it are capable of representing.\n4.4.5.3 Gradient boosting Gradient boosting is a more recently developed, and very\neffective, algorithm for training ensemble models using boosting. Like simpler boosting\nalgorithms, gradient boosting iteratively trains prediction models trying to make later mod-\nels specialize in areas that earlier models struggled with. Gradient boosting can be said to\ndo this in a more aggressive way than the boosting algorithm described previously. In gra-\ndient boosting later models are trained to directly correct errors made by earlier models,\nrather than the more subtle approach of simply changing weights in a sampling distribution\nto encourage this.\nThe gradient boosting algorithm is actually quite straightforward, and it is easiest to ex-\nplain in the context of a prediction problem with a continuous target. Given a training\ndataset, D, made up of descriptive feature and target feature pairs, pd,tq, the algorithm be-\ngins by training a very simple base model, M0. For problems with a continuous target, this\nmodel typically simply predicts the overall average target value from the training dataset\nM0pdq“1\nnÿ\niti (4.15)\nGradient boosting then iteratively adds new models to the ensemble. However, rather\nthan training these models to perform the original prediction task—to predict the target\nfeature tbased on the descriptive features d—these models are trained to predict the errors\nthat the previous model is likely to have made.32In this way the newly trained models\nare directly trying to correct, and therefore improve, the predictions made by the models\npreviously added to the ensemble. We write the model trained at the ﬁrst iteration of\ngradient boosting\nM1pdq“M0pdq`M∆1pdq (4.16)\nwhereM∆1is the model trained to predict the errors made by the base model, M0.\nThe gradient boosting algorithm proceeds by adding more and more models where each\nmodel is trained to improve the predictions of the previous ones. Equation (4.16)[164]can be\ngeneralized to capture this\nMipdq“Mi´1pdq`M∆ipdq (4.17)\nThe ﬁnal model is then a model that makes a basic prediction and adds a number of\nimprovements to this prediction. We can see this if we consider the model trained after\n32. These errors are often known as the model residuals .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":219,"page_label":"165","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 165\nfour iterations of gradient boosting33\nM4pdq“M3pdq`M∆4pdq\n“pM2pdq`M∆3pdqq`M∆4pdq\n“ppM1`M∆2pdqq`M∆3pdqq`M∆4pdq\n“pppM0pdq`M∆1pdqq`M∆2pdqq`M∆3pdqq`M∆4pdq\n“M0pdq`M∆1pdq`M∆2pdq`M∆3pdq`M∆4pdq (4.18)\nAlthough any model can be used at the model training steps in gradient boosting, it is\nmost common to use very shallow decision trees, or decision stumps —often just one level\ndeep. This is using the same approach taken in random forests, which aims to combine a\nlarge number of weak learners into an overall strong learner.\nTo illustrate gradient boosting, we use another modiﬁed version of the bike rentals dataset\nused in Section 4.4.3[149]. This will again predict the expected rental demand on the basis\nof a single descriptive feature, the forecasted temperature for a day. The ﬁrst columns of\nTable 4.15[166]detail a small sample dataset giving temperatures, T EMP, and rental demand,\nRENTALS (this time as actual numeric values), for 10 days. A visualization of the dataset\nis provided in Figure 4.22(a)[167].\nThe ﬁrst step in building a gradient boosting model for this problem is to train the initial\nmodel, M0. This always simply predicts the average value of the target feature from the\ndataset—in this case a rental demand of 1,287.1bicycles. On the basis of the predictions\nmade by this model, the ﬁrst set of errors can be calculated. These are shown in the column\nlabeled t´M0in Table 4.15[166]; they are simply the difference between the values predicted\nby the initial model and the target feature value for each instance in the dataset. The ﬁrst\nreal model trained for this ensemble, M∆1, is then trained to predict these errors on the\nbasis of the descriptive features in the training set. This is a simple one-level decision tree\ncomposed of a root note that performs one split on the basis of T EMPERATURE (because\nTEMPERATURE is a continuous target, the process for training regression trees described\nin Section 4.4.3[149]is used).\nThe simple nature of this ﬁrst trained model is evident from the predictions made by\nM∆1shown in Table 4.15[166]. The model predicts a correction value of ´460.9for input\ntemperatures less than or equal to 9.5degrees, and a correction value of 691.4for tempera-\ntures above this threshold. These corrections are combined with the M0predictions to give\nthe ensemble predictions after the ﬁrst iteration, M1. The predictions for this very simple","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":219,"page_label":"165","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"basis of the descriptive features in the training set. This is a simple one-level decision tree\ncomposed of a root note that performs one split on the basis of T EMPERATURE (because\nTEMPERATURE is a continuous target, the process for training regression trees described\nin Section 4.4.3[149]is used).\nThe simple nature of this ﬁrst trained model is evident from the predictions made by\nM∆1shown in Table 4.15[166]. The model predicts a correction value of ´460.9for input\ntemperatures less than or equal to 9.5degrees, and a correction value of 691.4for tempera-\ntures above this threshold. These corrections are combined with the M0predictions to give\nthe ensemble predictions after the ﬁrst iteration, M1. The predictions for this very simple\nensemble model are given in Table 4.15[166]and visualized in Figure 4.22(c)[167], from which\nthe characteristic step pattern of a decision tree is very obvious.\n33. Because of the way that they make a number of additions to a basic model, gradient boosting models can be\nconsidered an instance of a generic family of mathematical models known as additive models .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":220,"page_label":"166","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 4.15\nA simple bicycle demand predictions dataset and the workings of the ﬁrst iterations of training a gradient boosting model.\nID T EMP RENTALS M0pdq t´M0pdqM∆1pdqM1pdq t´M1pdqM∆2pdqM2pdq t´M2pdqM∆3pdqM3pdq\n1 4 602 1 287.1 -685.1 -460.9 826.2 -224.2 -167.2 659.0 -57.0 -34.1 624.9\n2 5 750 1 287.1 -537.1 -460.9 826.2 -76.2 -167.2 659.0 91.0 -34.1 624.9\n3 7 913 1 287.1 -374.1 -460.9 826.2 86.8 71.6 897.8 15.2 -34.1 863.7\n4 12 1229 1 287.1 -58.1 -460.9 826.2 402.8 71.6 897.8 331.2 -34.1 863.7\n5 18 1827 1 287.1 539.9 691.4 1 978.5 -151.5 71.6 2 050.1 -223.1 -34.1 2 016.1\n6 23 2246 1 287.1 958.9 691.4 1 978.5 267.5 71.6 2 050.1 195.9 136.4 2 186.5\n7 27 2127 1 287.1 839.9 691.4 1 978.5 148.5 71.6 2 050.1 76.9 136.4 2 186.5\n8 28 1714 1 287.1 426.9 691.4 1 978.5 -264.5 71.6 2 050.1 -336.1 -34.1 2 016.1\n9 32 838 1 287.1 -449.1 -460.9 826.2 11.8 71.6 897.8 -59.8 -34.1 863.7\n10 35 625 1 287.1 -662.1 -460.9 826.2 -201.2 -167.2 659.0 -34.0 -34.1 624.9","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":221,"page_label":"167","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.4 Extensions and Variations 167\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals\n(a) Training data\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals (b)M0\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals\n(c)M1\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals (d)M2\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals\n(e)M3\n0 5 10 15 20 25 30 35 40\nTemperature05001000150020002500Rentals (f)M20\nFigure 4.22\n(a) A plot of the bike rental dataset from Table 4.15[166]. (b)–(e) Visualizations of the prediction\nmodels trained in the early iterations of the gradient boosting process. (f) The ﬁnal ensemble model\ntrained after 20 iterations of gradient boosting. In the model visualizations black circles show the\ntraining dataset, gray squares show the predictions made for the instances in the training dataset by\nthe current model, and the dotted line shows the predictions that would be made by the current model\nfor the full range of input temperatures.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":222,"page_label":"168","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"168 Chapter 4 Information-Based Learning\nA new set of errors are then calculated by comparing the M1predictions to the target\nfeature values. A new model, M∆2, is trained to predict these errors on the basis of the\noriginal descriptive feature values. The outputs of this model are combined with the M1\npredictions to give the full model for this step in the ensemble building process, M2. The\noutputs of this model for the training dataset instances are shown in Table 4.15[166]and\nFigure 4.22(d)[167]. To complete the process of building the ensemble model, the algorithm\ncontinues to iteratively calculate errors and train new models to predict these errors, which\nare added as correction terms to the previous model output. Figure 4.22(f)[167]shows the\nmodel output after 20 iterations of model training have been completed.\nThe gradient boosting algorithm described here does not have an explicit aggregation\nstep, as do the bagging and boosting algorithms described previously (in which individual\nmodel predictions were combined through averaging or voting). Instead, the ﬁnal model\ntrained provides the overall output of the ensemble as it implicitly combines the outputs\nof all models trained—recall Equation (4.18)[165]. The simple version of gradient boosting\ndescribed here can be extended in many ways. For example, a learning rate34parameter,\nα, can be added to Equation (4.17)[164]to cause predictions of later errors to have less impact\non the overall model than earlier ones, which makes models more robust to outliers.35\nMipdq“Mi´1pdq`αˆM∆ipdq (4.19)\nAnother way to make models robust to outliers is to train the models to predict only the\nsign of the errors of the previous predictions rather than the magnitudes of the errors; this\nis a frequently used adjustment. Although the gradient boosting approach is most easily\nexplained in the context of predicting continuous targets, it can also be easily adapted\nto work with categorical targets. In this case the initial model predicts a likelihood that\nan instance belongs to each of the possible target levels, and subsequent models predict\ncorrections to these likelihoods.\nThe version of the gradient boosting algorithm described here searches for an ensemble\nmodel that minimizes the mean squared error between the target feature values from the\ntraining dataset and the model predictions. We often describe this as minimizing mean\nsquared error (or L2)loss.36Gradient boosting can be extended to learn to minimize other","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":222,"page_label":"168","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is a frequently used adjustment. Although the gradient boosting approach is most easily\nexplained in the context of predicting continuous targets, it can also be easily adapted\nto work with categorical targets. In this case the initial model predicts a likelihood that\nan instance belongs to each of the possible target levels, and subsequent models predict\ncorrections to these likelihoods.\nThe version of the gradient boosting algorithm described here searches for an ensemble\nmodel that minimizes the mean squared error between the target feature values from the\ntraining dataset and the model predictions. We often describe this as minimizing mean\nsquared error (or L2)loss.36Gradient boosting can be extended to learn to minimize other\nloss functions, which makes it an extremely effective and ﬂexible approach to predictive\nmodeling.\n34.Learning rates are discussed in much more detail in Chapter 7[311].\n35. Learning rate values are typically in p0,1s, and gradient boosting implementations that use a learning rate\noften refer to it as incremental shrinkage .\n36.Loss functions are discussed in much more detail in Chapters 7[311]and 8[381]. We can view the gradient\nboosting process as equivalent to the gradient descent process described in those chapters.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":223,"page_label":"169","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.5 Summary 169\n4.5 Summary\nWe have introduced information theory as a method of determining the shortest sequence\nof descriptive feature tests required to make a prediction. We have also introduced deci-\nsion tree models, which make predictions based on sequences of tests on the descriptive\nfeature values of a query. Consequently, decision trees naturally lend themselves to be-\ning trained using information-based metrics. We also introduced the ID3 algorithm as a\nstandard algorithm for inducing decision trees from a dataset. The ID3 algorithm uses a\ntop-down, recursive, depth-ﬁrst partitioning of the dataset to build a tree model beginning\nat the root node and ﬁnishing at the leaf nodes. Although this algorithm works quite well\nas presented, it assumes categorical features with no missing values and clean data. The\nalgorithm can, however, be extended to handle continuous descriptive features and contin-\nuous target features. We also discussed how tree pruning can be used to help with the\nproblem of overﬁtting.\nTheC4.5 algorithm is a well-known variant of the ID3 algorithm that uses these exten-\nsions to handle continuous and categorical descriptive features and missing features. It\nalso uses post-pruning to help with overﬁtting. J48is an open-source implementation of\nthe C4.5 algorithm that is used in many data analytics toolkits. Another well-known variant\nof the ID3 algorithm is the CART algorithm. The CART algorithm uses the Gini index\n(introduced in Section 4.4.1[142]) instead of information gain to select features to add to the\ntree. This algorithm can also handle continuous target features. The variant of the decision\ntree algorithm that should be used for a particular problem depends on the nature of the\nproblem and the dataset being used. Performing evaluation experiments using different\nmodel types is really the only way to determine which variant will work best for a speciﬁc\nproblem.\nThe main advantage of decision tree models is that they are interpretable. It is relatively\neasy to understand the sequences of tests a decision tree carried out in order to make a\nprediction. This interpretability is very important in some domains. For example, if a\nprediction model is being used as a diagnostic tool in a medical scenario, it is not sufﬁcient\nfor the system to simply return a diagnosis. In these contexts both the doctor and the patient\nwould want the system to provide some explanation of how it arrives at the predictions it","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":223,"page_label":"169","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"problem and the dataset being used. Performing evaluation experiments using different\nmodel types is really the only way to determine which variant will work best for a speciﬁc\nproblem.\nThe main advantage of decision tree models is that they are interpretable. It is relatively\neasy to understand the sequences of tests a decision tree carried out in order to make a\nprediction. This interpretability is very important in some domains. For example, if a\nprediction model is being used as a diagnostic tool in a medical scenario, it is not sufﬁcient\nfor the system to simply return a diagnosis. In these contexts both the doctor and the patient\nwould want the system to provide some explanation of how it arrives at the predictions it\nmakes. Decision tree models are ideal for these scenarios.\nDecision tree models can be used for datasets that contain both categorical and contin-\nuous descriptive features. A real advantage of the decision tree approach is that it has the\nability to model the interactions between descriptive features. This arises from the fact\nthat the tests carried out at each node in the tree are performed in the context of the re-\nsults of the tests on the other descriptive features that were tested at the preceding nodes\non the path from the root. Consequently, if there is an interaction effect between two\nor more descriptive features, a decision tree can model this. It is worth noting that this\nability is diminished if pre-pruning is employed, as pre-pruning may stop subtrees that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":224,"page_label":"170","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"170 Chapter 4 Information-Based Learning\ncapture descriptive feature interactions from forming. Finally, as noted earlier, decision\ntree induction is, relatively, robust to noise in the dataset if pruning is used.\nThere are, however, some situations where decision tree models are not the best option.\nAlthough decision trees can handle both categorical and continuous features, they tend to\nbecome quite large when dealing with continuous descriptive features. This can result in\ntrees becoming difﬁcult to interpret. Consequently, in dealing with purely continuous data,\nother prediction models may be more appropriate, for example, the error-based models\ndiscussed in Chapter 7[311].\nDecision trees also have difﬁculty with domains that have a large number of descrip-\ntive features, particularly if the number of instances in the training dataset is small. In\nthese situations overﬁtting becomes very likely. The probability-based models discussed\nin Chapter 6[243]do a better job of handling high-dimensional data.\nAnother potential issue with decision trees is that they are eager learners. As such, they\nare not suitable for modeling concepts that change over time, because they will need to be\nretrained. In these scenarios, the similarity-based prediction models that are the topic of\nChapter 5[181]perform better, as these models can be incrementally retrained.\nWe concluded by explaining model ensembles . We can build a model ensemble us-\ning any type of prediction model—or indeed, a mixture of model types. We don’t have\nto use decision trees. However, decision trees are often used in model ensembles due to\nthe sensitivity of tree induction to changes in the dataset, and this is why we introduced\nmodel ensembles in this chapter. Model ensembles are among the most powerful machine\nlearning algorithms; Caruana and Niculescu-Mizil (2006) report a large-scale compari-\nson between seven different types of prediction model in which bagged and boosted tree\nensembles are reported as among the best-performing. Which approach should we use?\nBagging is simpler to implement and parallelize than boosting, so it may be better with\nrespect to ease of use and training time. With respect to the general ability of bagging\nand boosting ensembles to make accurate predictions, the results reported in Caruana et al.\n(2008) indicate that boosted decision tree ensembles were the best-performing model of\nthose tested for datasets containing up to 4,000 descriptive features. For datasets con-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":224,"page_label":"170","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"learning algorithms; Caruana and Niculescu-Mizil (2006) report a large-scale compari-\nson between seven different types of prediction model in which bagged and boosted tree\nensembles are reported as among the best-performing. Which approach should we use?\nBagging is simpler to implement and parallelize than boosting, so it may be better with\nrespect to ease of use and training time. With respect to the general ability of bagging\nand boosting ensembles to make accurate predictions, the results reported in Caruana et al.\n(2008) indicate that boosted decision tree ensembles were the best-performing model of\nthose tested for datasets containing up to 4,000 descriptive features. For datasets con-\ntaining more that 4,000features, random forest ensembles (based on bagging) performed\nbetter. Caruana et al. (2008) suggest that a potential explanation for this pattern of results\nis that boosted ensembles are prone to overﬁtting, and in domains with large numbers of\nfeatures, overﬁtting becomes a serious problem. With all model ensembles, however, the\ncost of their high performance is increased learning and model complexity.\n4.6 Further Reading\nGleick (2011) provides an excellent and accessible introduction to information theory and\nits history. Shannon and Weaver (1949) is taken as the foundational book in information","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":225,"page_label":"171","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.6 Further Reading 171\ntheory, and Cover and Thomas (1991) is a well-regarded textbook on the topic. MacKay\n(2003) is an excellent textbook on information theory and machine learning.\nQuinlan (1986) originally described the ID3 algorithm, and Quinlan (1993) and Breiman\n(1993) are two of the best-known books on decision trees. Loh (2011) provides a good\noverview of more recent developments in tree induction algorithms.\nSchapire (1990) desscribed some of the early work on weak learners and computational\nlearning theory. Freund and Schapire (1995) introduced the AdaBoost algorithm, which is\none of the seminal boosting algorithms. Friedman et al. (2000) generalized the AdaBoost\nalgorithm and developed another popular boosting algorithm, the LogitBoost algorithm.\nBreiman (1996) developed the use of bagging for prediction, and Breiman (2001) intro-\nduced random forests . The original gradient boosting paper by Friedman (2001) gives a\ndetailed explanation on the fundamentals of gradient boosting. The XGBoost (Chen and\nGuestrin, 2016) gradient boosting implementation played a signiﬁcant role in populariz-\ning the approach and is a good example of how the basic algorithm can be optimized for\nperformance. Dietterich (2000) give an excellent explanation of the motivations behind\nusing ensembles, and Kuncheva (2004) and Zhou (2012) both provide good overviews of\nensemble learning methods.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":226,"page_label":"172","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"172 Chapter 4 Information-Based Learning\n4.7 Exercises\n1.The image below shows a set of eight Scrabble pieces.\n(a)What is the entropy in bits of the letters in this set?\n(b)What would be the reduction in entropy (i.e., the information gain ) in bits if we\nsplit these letters into two sets, one containing the vowels and the other containing\nthe consonants?\n(c)What is the maximum possible entropy in bits for a set of eight Scrabble pieces?\n(d)In general, which is preferable when you are playing Scrabble: a set of letters with\nhigh entropy or a set of letters with low entropy?\n2.A convicted criminal who reoffends after release is known as a recidivist . The follow-\ning table lists a dataset that describes prisoners released on parole and whether they\nreoffended within two years of release.37\nGOOD DRUG\nID B EHAVIOR AGEă30 DEPENDENT RECIDIVIST\n1 false true false true\n2 false false false false\n3 false true false true\n4 true false false false\n5 true false true true\n6 true false false false\nThis dataset lists six instances in which prisoners were granted parole. Each of these\ninstances is described in terms of three binary descriptive features (G OOD BEHAV -\nIOR, AGEă30, DRUG DEPENDENT ) and a binary target feature (R ECIDIVIST ). The\nGOOD BEHAVIOR feature has a value of true if the prisoner had not committed any\ninfringements during incarceration, the A GEă30has a value of true if the prisoner\nwas under 30 years of age when granted parole, and the D RUG DEPENDENT feature\nistrue if the prisoner had a drug addiction at the time of parole. The target feature,\n37. This example of predicting recidivism is based on a real application of machine learning: parole boards do\nrely on machine learning prediction models to help them when they are making their decisions. See Berk and\nBleich (2013) for a recent comparison of different machine learning models used for this task. Datasets dealing\nwith prisoner recidivism are available online, for example, catalog.data.gov/dataset/prisoner-recidivism/. The\ndataset presented here is not based on real data.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":227,"page_label":"173","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.7 Exercises 173\nRECIDIVIST , has a true value if the prisoner was arrested within two years of being\nreleased; otherwise it has a value of false .\n(a)Using this dataset, construct the decision tree that would be generated by the ID3\nalgorithm, using entropy-based information gain.\n(b)What prediction will the decision tree generated in Part (a) of this question return\nfor the following query?\nGOOD BEHAVIOR =false ,AGEă30=false ,\nDRUG DEPENDENT =true\n(c)What prediction will the decision tree generated in Part (a) of this question return\nfor the following query?\nGOOD BEHAVIOR =true,AGEă30=true,\nDRUG DEPENDENT =false\n3.The following table lists a sample of data from a census.38\nMARITAL ANNUAL\nID A GE EDUCATION STATUS OCCUPATION INCOME\n1 39 bachelors never married transport 25K–50K\n2 50 bachelors married professional 25K–50K\n3 18 high school never married agriculture ă25K\n4 28 bachelors married professional 25K–50K\n5 37 high school married agriculture 25K–50K\n6 24 high school never married armed forces ă25K\n7 52 high school divorced transport 25K–50K\n8 40 doctorate married professional ą50K\nThere are four descriptive features and one target feature in this dataset, as follows:\n‚AGE, a continuous feature listing the age of the individual;\n‚EDUCATION , a categorical feature listing the highest education award achieved by\nthe individual ( high school ,bachelors ,doctorate );\n‚MARITAL STATUS (never married ,married ,divorced );\n‚OCCUPATION (transport = works in the transportation industry; professional =\ndoctor, lawyer, or similar; agriculture = works in the agricultural industry; armed\nforces = is a member of the armed forces); and\n‚ANNUAL INCOME , the target feature with 3 levels ( ă25K,25K–50K,ą50K).\n(a)Calculate the entropy for this dataset.\n38. This census dataset is based on the Census Income Dataset (Kohavi, 1996), which is available from the UCI\nMachine Learning Repository (Bache and Lichman, 2013) at archive.ics.uci.edu/ml/datasets/Census+Income/.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":228,"page_label":"174","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"174 Chapter 4 Information-Based Learning\n(b)Calculate the Gini index for this dataset.\n(c)In building a decision tree, the easiest way to handle a continuous feature is to\ndeﬁne a threshold around which splits will be made. What would be the opti-\nmal threshold to split the continuous A GEfeature (use information gain based on\nentropy as the feature selection measure)?\n(d)Calculate information gain (based on entropy) for the E DUCATION , M ARITAL\nSTATUS , and O CCUPATION features.\n(e)Calculate the information gain ratio (based on entropy) for E DUCATION , MAR-\nITAL STATUS , and O CCUPATION features.\n(f)Calculate information gain using the Gini index for the E DUCATION , MARITAL\nSTATUS , and O CCUPATION features.\n4.The following diagram shows a decision tree for the task of predicting heart disease.39\nThe descriptive features in this domain describe whether the patient suffers from chest\npain (C HEST PAIN) and the blood pressure of the patient (B LOOD PRESSURE ). The\nbinary target feature is H EART DISEASE . The table beside the diagram lists a pruning\nset from this domain.\n  Chest Pain   \n   [true]    \n  Blood Pressure   \n   [false]    false\ntruetrue\ntruehigh\nfalselow\nCHEST BLOOD HEART\nID P AIN PRESSURE DISEASE\n1 false high false\n2 true low true\n3 false low false\n4 true high true\n5 false high false\nUsing the pruning set, apply reduced error pruning to the decision tree. Assume\nthat the algorithm is applied in a bottom-up, left-to-right fashion. For each iteration of\nthe algorithm, indicate the subtrees considered as pruning candidates, explain why the\nalgorithm chooses to prune or leave these subtrees in the tree, and illustrate the tree\nthat results from each iteration.\n39. This example is inspired by the research reported in Palaniappan and Awang (2008).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":229,"page_label":"175","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.7 Exercises 175\n5.The following table40lists a dataset containing the details of ﬁve participants in a heart\ndisease study, and a target feature R ISK, which describes their risk of heart disease.\nEach patient is described in terms of four binary descriptive features\n‚EXERCISE , how regularly do they exercise\n‚SMOKER , do they smoke\n‚OBESE , are they overweight\n‚FAMILY , did any of their parents or siblings suffer from heart disease\nID E XERCISE SMOKER OBESE FAMILY RISK\n1 daily false false yes low\n2 weekly true false yes high\n3 daily false false no low\n4 rarely true true yes high\n5 rarely true true no high\n(a)As part of the study, researchers have decided to create a predictive model to\nscreen participants based on their risk of heart disease. You have been asked to\nimplement this screening model using a random forest . The three tables below\nlist three bootstrap samples that have been generated from the above dataset. Us-\ning these bootstrap samples, create the decision trees that will be in the random\nforest model (use entropy-based information gain as the feature selection crite-\nrion).\nID E XERCISE FAMILY RISK\n1 daily yes low\n2 weekly yes high\n2 weekly yes high\n5 rarely no high\n5 rarely no high\nBootstrap Sample AID S MOKER OBESE RISK\n1 false false low\n2 true false high\n2 true false high\n4 true true high\n5 true true high\nBootstrap Sample BID O BESE FAMILY RISK\n1 false yes low\n1 false yes low\n2 false yes high\n4 true yes high\n5 true no high\nBootstrap Sample C\n(b)Assuming the random forest model you have created uses majority voting, what\nprediction will it return for the following query:\nEXERCISE =rarely , SMOKER =false , OBESE =true, FAMILY =yes\n40. The data in this table has been artiﬁcially generated for this question, but is inspired by the results from the\nFramingham Heart Study: www.framinghamheartstudy.org.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":230,"page_label":"176","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"176 Chapter 4 Information-Based Learning\n˚6.The following table lists a dataset containing the details of six patients. Each pa-\ntient is described in terms of three binary descriptive features (O BESE , SMOKER , and\nDRINKS ALCOHOL ) and a target feature (C ANCER RISK).41\nDRINKS CANCER\nID O BESE SMOKER ALCOHOL RISK\n1 true false true low\n2 true true true high\n3 true false true low\n4 false true true high\n5 false true false low\n6 false true true high\n(a)Which of the descriptive features will the ID3 decision tree induction algorithm\nchoose as the feature for the root node of the decision tree?\n(b)In designing a dataset, it is generally a bad idea if all the descriptive features are\nindicators of the target feature taking a particular value. For example, a potential\ncriticism of the design of the dataset in this question is that all the descriptive\nfeatures are indicators of the C ANCER RISKtarget feature taking the same level,\nhigh. Can you think of any descriptive features that could be added to this dataset\nthat are indicators of the lowtarget level?\n˚7.The following table lists a dataset collected in an electronics shop showing details of\ncustomers and whether they responded to a special offer to buy a new laptop.\nID A GE INCOME STUDENT CREDIT BUYS\n1ă31 high no bad no\n2ă31 high no good no\n3 31´40 high no bad yes\n4ą40 med no bad yes\n5ą40 low yes bad yes\n6ą40 low yes good no\n7 31´40 low yes good yes\n8ă31 med no bad no\n9ă31 low yes good yes\n10ą40 med yes bad yes\n11ă31 med yes good yes\n12 31´40 med no good yes\n13 31´40 high yes bad yes\n14ą40 med no good no\n41. The data in this table has been artiﬁcially generated for this question. The American Cancer Society does,\nhowever, provide information on the causes of cancer: www.cancer.org/cancer/cancercauses/.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":231,"page_label":"177","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.7 Exercises 177\nThis dataset has been used to build a decision tree to predict which customers will\nrespond to future special offers. The decision tree, created using the ID3algorithm, is\nthe following:\n  Age  \n  Student    < 31\nyes 31—40\n  Credit    > 40\nno no\nyes yes\nyes bad\nno good\n(a)The information gain (calculated using entropy) of the feature A GEat the root\nnode of the tree is 0.247. A colleague has suggested that the S TUDENT feature\nwould be better at the root node of the tree. Show that this is not the case.\n(b)Yet another colleague has suggested that the ID feature would be a very effective\nat the root node of the tree. Would you agree with this suggestion?\n˚8.This table lists a dataset of the scores students achieved on an exam described in terms\nof whether the student studied for the exam (S TUDIED ) and the energy level of the\nlecturer when grading the student’s exam (E NERGY ).\nID S TUDIED ENERGY SCORE\n1 yes tired 65\n2 no alert 20\n3 yes alert 90\n4 yes tired 70\n5 no tired 40\n6 yes alert 85\n7 no tired 35\nWhich of the two descriptive features should we use as the testing criterion at the root\nnode of a decision tree to predict students’ scores?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":232,"page_label":"178","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"178 Chapter 4 Information-Based Learning\n˚9.Calculate the probability of a model ensemble that uses simple majority voting mak-\ning an incorrect prediction in the following scenarios. (Hint: Understanding how to\nuse the binomial distribution will be useful in answering this question.)\n(a)The ensemble contains 11independent models, all of which have an error rate of\n0.2.\n(b)The ensemble contains 11independent models, all of which have an error rate of\n0.49.\n(c)The ensemble contains 21independent models, all of which have an error rate of\n0.49.\n˚10.The following table shows the target feature, O UTCOME , for a set of instances in a\nsmall dataset. An ensemble model is being trained using this dataset using boosting .\nThe table also shows the instance distribution weights, w4, for this dataset used at\nthe ﬁfth iteration of the boosting process. The last column of the table shows the\npredictions made by the model trained at the ﬁfth iteration of boosting, M4.\nID O UTCOME w4 M4\n1 Bad 0.167 Bad\n2 Good 0.047 Good\n3 Bad 0.167 Bad\n4 Good 0.071 Bad\n5 Good 0.047 Good\n6 Bad 0.047 Bad\n7 Bad 0.047 Bad\n8 Good 0.047 Good\n9 Bad 0.167 Bad\n10 Good 0.071 Bad\n11 Bad 0.047 Bad\n12 Good 0.071 Bad\n(a)Calculate the error, ϵ, associated with the set of predictions made by the model M4\ngiven in the table above.\n(b)Calculate the conﬁdence factor ,α, associated with M4.\n(c)Calculate the updated instance distribution, wr5s, based on the predictions made\nbyM4.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":233,"page_label":"179","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"4.7 Exercises 179\n˚11.The following table shows a set of predictions made by six models in an ensemble and\nthe ground truth of the target feature in a small test dataset, P ROGNOSIS .\nID P ROGNOSIS M0 M1 M2 M3 M4 M5\n1 Bad Bad Bad Good Bad Bad Good\n2 Good Good Good Good Bad Good Bad\n3 Good Bad Good Bad Good Good Good\n4 Bad Bad Bad Bad Bad Bad Good\n5 Bad Good Bad Good Bad Good Good\n(a)Assuming that these models are part of an ensemble training using bagging , cal-\nculate the overall output of the ensemble for each instance in the test dataset.\n(b)Measure the performance of this bagged ensemble using misclassiﬁcation rate\n(misclassiﬁcation rate is discussed in detail in Section 9.3[535]; it is simply the\npercentage of instances in the test dataset that a model has incorrectly classiﬁed).\n(c)Assuming that these models are part of an ensemble trained using boosting and\nthat the conﬁdence factors, α, for the models are as follows:\nM0M1M2M3M4M5\n0.114 0.982 0.653 0.912 0.883 0.233\ncalculate the overall output of the ensemble for each instance in the test dataset.\n(d)Measure the performance of this boosted ensemble using misclassiﬁcation rate .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":235,"page_label":"181","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5 Similarity-Based Learning\n“When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird\na duck. ”\n—James Whitcomb Riley\nSimilarity-based approaches to machine learning come from the idea that the best way to\nmake predictions is to simply look at what has worked well in the past and predict the same\nthing again. The fundamental concepts required to build a system on the basis of this idea\narefeature spaces andmeasures of similarity , and these are covered in the fundamentals\nsection of this chapter. These concepts allow us to understand the standard approach to\nbuilding similarity-based models: the nearest neighbor algorithm . After covering the\nstandard algorithm, we then look at extensions and variations that allow us to handle noisy\ndata (the knearest neighbor , ork-NN, algorithm), to make predictions more efﬁciently\n(k-dtrees ), to predict continuous targets, and to handle different kinds of descriptive fea-\ntures with varying measures of similarity . We also take the opportunity to introduce the\nuse of data normalization andfeature selection in the context of similarity-based learn-\ning. These techniques are generally applicable to all machine learning algorithms but are\nespecially important when similarity-based approaches are used.\n5.1 Big Idea\nThe year is 1798, and you are Lieutenant-Colonel David Collins of HMS Calcutta explor-\ning the region around Hawkesbury River, in New South Wales. One day, after an expedition\nup the river has returned to the ship, one of the men from the expedition tells you that he\nsaw a strange animal near the river. You ask him to describe the animal to you, and he\nexplains that he didn’t see it very well because, as he approached it, the animal growled at\nhim, so he didn’t approach too closely. However, he did notice that the animal had webbed\nfeet and a duck-billed snout.\nIn order to plan the expedition for the next day, you decide that you need to classify\nthe animal so that you can determine whether it is dangerous to approach it or not. You\ndecide to do this by thinking about the animals you can remember coming across before","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":236,"page_label":"182","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"182 Chapter 5 Similarity-Based Learning\nTable 5.1\nMatching animals you remember to the features of the unknown animal described by the sailor.\nGrrr!\n Score\n\u0013 \u0017 \u0017 1\n\u0017 \u0013 \u0017 1\n\u0017 \u0013 \u0013 2\nImages created by Jan Gillbank, English for the Australian Curriculum website (www.e4ac.edu.au). Used under\nCreative Commons Attribution 3.0 license.\nand comparing the features of these animals with the features the sailor described to you.\nTable 5.1[182]illustrates this process by listing some of the animals you have encountered\nbefore and how they compare with the growling, web-footed, duck-billed animal that the\nsailor described. For each known animal, you count how many features it has in common\nwith the unknown animal. At the end of this process, you decide that the unknown animal\nis most similar to a duck, so that is what it must be. A duck, no matter how strange, is not\na dangerous animal, so you tell the men to get ready for another expedition up the river the\nnext day.\nThe process of classifying an unknown animal by matching the features of the animal\nagainst the features of animals you have encountered before neatly encapsulates the big\nidea underpinning similarity-based learning: if you are trying to make a prediction for a\ncurrent situation, then you should search your memory to ﬁnd situations that are similar to\nthe current one and make a prediction based on what was true for the most similar situation\nin your memory. In this chapter we are going to see how this type of reasoning can be\nimplemented as a machine learning algorithm.\n5.2 Fundamentals\nAs the term similarity-based learning suggests, a key component of this approach to pre-\ndiction is deﬁning a computational measure of similarity between instances. Often this\nmeasure of similarity is actually some form of distance measure. A consequence of this,\nand a somewhat less obvious requirement of similarity-based learning, is that if we are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":237,"page_label":"183","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.2 Fundamentals 183\nTable 5.2\nThe S PEED and A GILITY ratings for 20 college athletes and whether they were drafted by a profes-\nsional team.\nID S PEED AGILITY DRAFT\n1 2.50 6.00 no\n2 3.75 8.00 no\n3 2.25 5.50 no\n4 3.25 8.25 no\n5 2.75 7.50 no\n6 4.50 5.00 no\n7 3.50 5.25 no\n8 3.00 3.25 no\n9 4.00 4.00 no\n10 4.25 3.75 noID S PEED AGILITY DRAFT\n11 2.00 2.00 no\n12 5.00 2.50 no\n13 8.25 8.50 no\n14 5.75 8.75 yes\n15 4.75 6.25 yes\n16 5.50 6.75 yes\n17 5.25 9.50 yes\n18 7.00 4.25 yes\n19 7.50 8.00 yes\n20 7.25 5.75 yes\ngoing to compute distances between instances, we need to have a concept of space in the\nrepresentation of the domain used by our model. In this section we introduce the concept\nof a feature space as a representation for a training dataset and then illustrate how we can\ncompute measures of similarity between instances in a feature space.\n5.2.1 Feature Space\nTable 5.2[183]lists an example dataset containing two descriptive features, the S PEED and\nAGILITY ratings for college athletes (both measures out of 10), and one target feature that\nlists whether the athletes were drafted to a professional team.1We can represent this dataset\nin afeature space by taking each of the descriptive features to be the axes of a coordinate\nsystem . We can then place each instance within the feature space based on the values of its\ndescriptive features. Figure 5.1[184]is a scatter plot to illustrate the resulting feature space\nwhen we do this using the data in Table 5.2[183]. In this ﬁgure, S PEED has been plotted on\nthe horizontal axis, and A GILITY has been plotted on the vertical axis. The value of the\nDRAFT feature is indicated by the shape representing each instance as a point in the feature\nspace: triangles for noand crosses for yes.\nThere is always one dimension for every descriptive feature in a dataset. In this example,\nthere are only two descriptive features, so the feature space is two-dimensional. Feature\nspaces can, however, have many more dimensions—in document classiﬁcation tasks, for\nexample, it is not uncommon to have thousands of descriptive features and therefore thou-\nsands of dimensions in the associated feature space. Although we can’t easily draw feature\nspaces beyond three dimensions, the ideas underpinning them remain the same.\n1. This example dataset is inspired by the use of analytics in professional and college sports, often referred to as\nsabremetrics . Two accessible introductions to this ﬁeld are Lewis (2004) and Keri (2007).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":238,"page_label":"184","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"184 Chapter 5 Similarity-Based Learning\n2 3 4 5 6 7 82 4 6 8\nSpeedAgility\nFigure 5.1\nA feature space plot of the college athlete data in Table 5.2[183].\nWe can formally deﬁne a feature space as an abstract m-dimensional space that is created\nby making each descriptive feature in a dataset an axis of an m-dimensional coordinate\nsystem and mapping each instance in the dataset to a point in this coordinate system based\non the values of its descriptive features.\nFor similarity-based learning, the nice thing about the way feature spaces work is that if\nthe values of the descriptive features of two or more instances in the dataset are the same,\nthen these instances will be mapped to the same point in the feature space. Also, as the\ndifferences between the values of the descriptive features of two instances grows, so too\ndoes the distance between the points in the feature space that represent these instances. So\nthe distance between two points in the feature space is a useful measure of the similarity\nof the descriptive features of the two instances.\n5.2.2 Measuring Similarity Using Distance Metrics\nThe simplest way to measure the similarity between two instances, aandb, in a dataset is\nto measure the distance between the instances in a feature space. We can use a distance\nmetric to do this: metricpa,bqis a function that returns the distance between two instances\naandb. Mathematically, a metric must conform to the following four criteria:\n1.Non-negativity :metricpa,bqě0\n2.Identity :metricpa,bq“0ðñ a“b\n3.Symmetry :metricpa,bq“metricpb,aq\n4.Triangular Inequality :metricpa,bqďmetricpa,cq`metricpb,cq","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":239,"page_label":"185","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.2 Fundamentals 185\nOne of the best-known distance metrics is Euclidean distance , which computes the\nlength of the straight line between two points. Euclidean distance between two instances a\nandbin an m-dimensional feature space is deﬁned as\nEuclideanpa,bq“gffemÿ\ni“1paris´brisq2(5.1)\nThe descriptive features in the college athlete dataset are both continuous, which means\nthat the feature space representing this data is technically known as a Euclidean coor-\ndinate space , and we can compute the distance between instances in it using Euclidean\ndistance. For example, the Euclidean distance between instances d12(SPEED“5.00,\nAGILITY“2.50) and d5(SPEED“2.75, AGILITY“7.50) from Table 5.2[183]is\nEuclideanpd12,d5q“b\np5.00´2.75q2`p2.50´7.50q2\n“?\n30.0625“5.4829\nAnother, less well-known, distance metric is the Manhattan distance .2The Manhattan\ndistance between two instances aandbin a feature space with mdimensions is deﬁned as\nManhattanpa,bq“mÿ\ni“1absparis´brisq (5.2)\nwhere the abspqfunction returns the absolute value. For example, the Manhattan dis-\ntance between instances d12(SPEED“5.00, AGILITY“2.50) and d5(SPEED“2.75,\nAGILITY“7.50) in Table 5.2[183]is\nManhattanpd12,d5q“absp5.00´2.75q`absp2.5´7.5q\n“2.25`5“7.25\nFigure 5.2(a)[186]illustrates the difference between the Manhattan and Euclidean distances\nbetween two points in a two-dimensional feature space. If we compare Equation (5.1)[185]\nand Equation (5.2)[185], we can see that both distance metrics are essentially functions of\nthe differences between the values of the features. Indeed, the Euclidean and Manhattan\ndistances are special cases of the Minkowski distance , which deﬁnes a family of distance\nmetrics based on differences between features.\n2. The Manhattan distance, or taxi-cab distance , is so called because it is the distance that a taxi driver would\nhave to cover if going from one point to another on a road system that is laid out in blocks, like the Manhattan\nroad system.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":240,"page_label":"186","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"186 Chapter 5 Similarity-Based Learning\n/uni25CF/uni25CF\nEuclidean\nManhattan\n(a)\n2 3 4 5 6 7 82 4 6 8\nSpeedAgilityManhattan\nEuclidean\n12517 (b)\nFigure 5.2\n(a) A generalized illustration of the Manhattan and Euclidean distances between two points; and (b)\na plot of the Manhattan and Euclidean distances between instances d12andd5, and between d12and\nd17from Table 5.2[183].\nTheMinkowski distance between two instances aandbin a feature space with mde-\nscriptive features is deﬁned as\nMinkowskipa,bq“˜mÿ\ni“1absparis´brisqp¸1\np\n(5.3)\nwhere the parameter pis typically set to a positive value and deﬁnes the behavior of the\ndistance metric. Different distance metrics result from adjusting the value of p. For ex-\nample, the Minkowski distance with p“1is the Manhattan distance, and with p“2is\nthe Euclidean distance. Continuing in this manner, we can deﬁne an inﬁnite number of\ndistance metrics.\nThe fact that we can deﬁne an inﬁnite number of distance metrics is not merely an aca-\ndemic curiosity. In fact, the predictions produced by a similarity-based model will change\ndepending on the exact Minkowski distance used (i.e., p“1,2,...,8). Larger values of p\nplace more emphasis on large differences between feature values than smaller values of p\nbecause all differences are raised to the power of p. Consequently, the Euclidean distance\n(with p“2) is more strongly inﬂuenced by a single large difference in one feature than\nthe Manhattan distance (with p“1).3\n3. In the extreme case with p“8 , the Minkowski metric simply returns the maximum difference between any\nof the features. This is known as the Chebyshev distance but is also sometimes called the chessboard distance\nbecause it is the number of moves a king must make in chess to go from one square on the board to any other.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":241,"page_label":"187","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.3 Standard Approach: The Nearest Neighbor Algorithm 187\nWe can see this if we compare the Euclidean and Manhattan distances between instances\nd12andd5with the Euclidean and Manhattan distances between instances d12andd17\n(SPEED“5.25, AGILITY“9.50). Figure 5.2(b)[186]plots the Manhattan and Euclidean\ndistances between these pairs of instances.\nThe Manhattan distances between both pairs of instances are the same: 7.25. It is strik-\ning, however, that the Euclidean distance between d12andd17is8.25, which is greater\nthan the Euclidean distance between d12andd5, which is just 5.48. This is because the\nmaximum difference between d12andd17for any single feature is 7units (for A GILITY ),\nwhereas the maximum difference between d12andd5on any single feature is just 5units\n(for A GILITY ). Because these differences are squared in the Euclidean distance calcula-\ntion, the larger maximum single difference between d12andd17results in a larger overall\ndistance being calculated for this pair of instances. Overall the Euclidean distance weights\nfeatures with larger differences in values more than features with smaller differences in\nvalues. This means that the Euclidean difference is more inﬂuenced by a single large dif-\nference in one feature rather than a lot of small differences across a set of features, whereas\nthe opposite is true of Manhattan distance.\nAlthough we have an inﬁnite number of Minkowski-based distance metrics to choose\nfrom, Euclidean distance and Manhattan distance are the most commonly used of these.\nThe question of which is the best one to use, however, still remains. From a computational\nperspective, the Manhattan distance has a slight advantage over the Euclidean distance—\nthe computation of the squaring and the square root is saved—and computational con-\nsiderations can become important when dealing with very large datasets. Computational\nconsiderations aside, Euclidean distance is often used as the default.\n5.3 Standard Approach: The Nearest Neighbor Algorithm\nWe now understand the two fundamental components of similarity-based learning: a fea-\nture space representation of the instances in a dataset and a measure of similarity be-\ntween instances. We can put these components together to deﬁne the standard approach\nto similarity-based learning: the nearest neighbor algorithm . The training phase needed\nto build a nearest neighbor model is very simple and just involves storing all the training","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":241,"page_label":"187","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the computation of the squaring and the square root is saved—and computational con-\nsiderations can become important when dealing with very large datasets. Computational\nconsiderations aside, Euclidean distance is often used as the default.\n5.3 Standard Approach: The Nearest Neighbor Algorithm\nWe now understand the two fundamental components of similarity-based learning: a fea-\nture space representation of the instances in a dataset and a measure of similarity be-\ntween instances. We can put these components together to deﬁne the standard approach\nto similarity-based learning: the nearest neighbor algorithm . The training phase needed\nto build a nearest neighbor model is very simple and just involves storing all the training\ninstances in memory. In the standard version of the algorithm, the data structure used to\nstore training data is a simple list. In the prediction stage, when the model is used to make\npredictions for new query instances, the distance in the feature space between the query in-\nstance and each instance in memory is computed, and the prediction returned by the model\nis the target feature level of the instance that is nearest to the query in the feature space.\nThe default distance metric used in nearest neighbor models is Euclidean distance. Algo-\nrithm 2[188]provides a pseudocode deﬁnition of the algorithm for the prediction stage. The\nalgorithm really is very simple, so we can move straight to looking at a worked example\nof it in action.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":242,"page_label":"188","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"188 Chapter 5 Similarity-Based Learning\nAlgorithm 2 Pseudocode description of the nearest neighbor algorithm.\nRequire: a set of training instances\nRequire: a query instance\n1:Iterate across the instances in memory to ﬁnd the nearest neighbor—this is the instance\nwith the shortest distance across the feature space to the query instance.\n2:Make a prediction for the query instance that is equal to the value of the target feature\nof the nearest neighbor.\n5.3.1 A Worked Example\nAssume that we are using the dataset in Table 5.2[183]as our labeled training dataset, and\nwe want to make a prediction to tell us whether a query instance with S PEED“6.75and\nAGILITY“3.00is likely to be drafted or not. Figure 5.3[188]illustrates the feature space of\nthe training dataset with the query, represented by the ?marker.\n2 3 4 5 6 7 82 4 6 8\nSpeedAgility\n?\nFigure 5.3\nA feature space plot of the data in Table 5.2[183], with the position in the feature space of the query\nrepresented by the ? marker.\nJust by visually inspecting Figure 5.3[188], we can see that the nearest neighbor to the\nquery instance has a target level of yes, so this is the prediction that the model should re-\nturn. However, let’s step through how the algorithm makes this prediction. Remember that\nduring the prediction stage, the nearest neighbor algorithm iterates across all the instances\nin the training dataset and computes the distance between each instance and the query.\nThese distances are then ranked from lowest to highest to ﬁnd the nearest neighbor. Table","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":243,"page_label":"189","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.3 Standard Approach: The Nearest Neighbor Algorithm 189\nTable 5.3\nThe distances (Dist.) between the query instance with S PEED“6.75and A GILITY“3.00and each\ninstance in Table 5.2[183].\nID S PEED AGILITY DRAFT Dist.\n18 7.00 4.25 yes 1.27\n12 5.00 2.50 no 1.82\n10 4.25 3.75 no 2.61\n20 7.25 5.75 yes 2.80\n9 4.00 4.00 no 2.93\n6 4.50 5.00 no 3.01\n8 3.00 3.25 no 3.76\n15 4.75 6.25 yes 3.82\n7 3.50 5.25 no 3.95\n16 5.50 6.75 yes 3.95ID S PEED AGILITY DRAFT Dist.\n11 2.00 2.00 no 4.85\n19 7.50 8.00 yes 5.06\n3 2.25 5.50 no 5.15\n1 2.50 6.00 no 5.20\n13 8.25 8.50 no 5.70\n2 3.75 8.00 no 5.83\n14 5.75 8.75 yes 5.84\n5 2.75 7.50 no 6.02\n4 3.25 8.25 no 6.31\n17 5.25 9.50 yes 6.67\n5.3[189]shows the distances between our query instance and each instance from Table 5.2[183]\nranked from lowest to highest. Just as we saw in Figure 5.3[188], this shows that the nearest\nneighbor to the query is instance d18, with a distance of 1.2749 and a target level of yes.\nWhen the algorithm is searching for the nearest neighbor using Euclidean distance, it is\npartitioning the feature space into what is known as a Voronoi tessellation ,4and it is trying\nto decide which Voronoi region the query belongs to. From a prediction perspective, the\nV oronoi region belonging to a training instance deﬁnes the set of queries for which the\nprediction will be determined by that training instance. Figure 5.4(a)[190]illustrates the\nV oronoi tessellation of the feature space using the training instances from Table 5.2[183]and\nshows the position of our sample query instance within this decomposition. We can see\nin this ﬁgure that the query is inside a V oronoi region deﬁned by an instance with a target\nlevel of yes. As such, the prediction for the query instance should be yes.\nThe nearest neighbor prediction algorithm creates a set of local models , or neighbor-\nhoods, across the feature space where each model is deﬁned by a subset of the training\ndataset (in this case, one instance). Implicitly, however, the algorithm is also creating a\nglobal prediction model based on the full dataset. We can see this if we highlight the deci-\nsion boundary within the feature space. The decision boundary is the boundary between\nregions of the feature space in which different target levels will be predicted. We can\ngenerate the decision boundary by aggregating the neighboring local models (in this case,\nV oronoi regions) that make the same prediction. Figure 5.4(b)[190]illustrates the decision","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":243,"page_label":"189","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"The nearest neighbor prediction algorithm creates a set of local models , or neighbor-\nhoods, across the feature space where each model is deﬁned by a subset of the training\ndataset (in this case, one instance). Implicitly, however, the algorithm is also creating a\nglobal prediction model based on the full dataset. We can see this if we highlight the deci-\nsion boundary within the feature space. The decision boundary is the boundary between\nregions of the feature space in which different target levels will be predicted. We can\ngenerate the decision boundary by aggregating the neighboring local models (in this case,\nV oronoi regions) that make the same prediction. Figure 5.4(b)[190]illustrates the decision\nboundary within the feature space for the two target levels in the college athlete dataset.\n4. A V oronoi tessellation is a way of decomposing a space into regions in which each region belongs to an\ninstance and contains all the points in the space whose distance to that instance is less than the distance to any\nother instance.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":244,"page_label":"190","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"190 Chapter 5 Similarity-Based Learning\nGiven that the decision boundary is generated by aggregating the V oronoi regions, it is not\nsurprising that the query is on the side of the decision boundary representing the yestarget\nlevel. This illustrates that a decision boundary is a global representation of the predictions\nmade by the local models associated with each instance in the training set. It also high-\nlights the fact that the nearest neighbor algorithm uses multiple local models to create an\nimplicit global model to map from the descriptive feature values to the target feature.\n(a) V oronoi tessellation\n (b) Decision boundary ( k“1)\nFigure 5.4\n(a) The V oronoi tessellation of the feature space for the dataset in Table 5.2[183], with the position of\nthe query represented by the ? marker; and (b) the decision boundary created by aggregating the\nneighboring V oronoi regions that belong to the same target level.\nOne of the advantages of the nearest neighbor approach to prediction is that it is relatively\nstraightforward to update the model when new labeled instances become available—we\nsimply add them to the training dataset. Table 5.4[191]lists the updated dataset when the ex-\nample query instance with its prediction of yesis included.5Figure 5.5(a)[192]illustrates the\nV oronoi tessellation of the feature space that results from this update, and Figure 5.5(b)[192]\npresents the updated decision boundary. Comparing Figure 5.5(b)[192]with Figure 5.4(b)[190],\nwe can see that the main difference is that the decision boundary in the bottom-right region\nof the feature space has moved to the left. This reﬂects the extension of the yesregion due\nto the inclusion of the new instance.\n5. Instances should be added to the training dataset only if we have determined after making the prediction that\nthe prediction was, in fact, correct. In this example, we assume that at the draft, the query player was drafted.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":245,"page_label":"191","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 191\nTable 5.4\nThe extended version of the college athletes dataset.\nID S PEED AGILITY DRAFT\n1 2.50 6.00 no\n2 3.75 8.00 no\n3 2.25 5.50 no\n4 3.25 8.25 no\n5 2.75 7.50 no\n6 4.50 5.00 no\n7 3.50 5.25 no\n8 3.00 3.25 no\n9 4.00 4.00 no\n10 4.25 3.75 no\n11 2.00 2.00 noID S PEED AGILITY DRAFT\n12 5.00 2.50 no\n13 8.25 8.50 no\n14 5.75 8.75 yes\n15 4.75 6.25 yes\n16 5.50 6.75 yes\n17 5.25 9.50 yes\n18 7.00 4.25 yes\n19 7.50 8.00 yes\n20 7.25 5.75 yes\n21 6.75 3.00 yes\nIn summary, the inductive bias underpinning similarity-based machine learning algo-\nrithms is that things that are similar (i.e., instances that have similar descriptive features)\nalso have the same target feature values. The nearest neighbor algorithm creates an implicit\nglobal predictive model by aggregating local models, or neighborhoods. The deﬁnition of\nthese neighborhoods is based on similarity within the feature space to the labeled training\ninstances. Predictions are made for a query instance using the target level of the training\ninstance deﬁning the neighborhood in the feature space that contains the query.\n5.4 Extensions and Variations\nWe now understand the standard nearest neighbor algorithm. The algorithm, as presented,\ncan work well with clean, reasonably sized datasets containing continuous descriptive fea-\ntures. Often, however, datasets are noisy, very large, and may contain a mixture of different\ndata types. As a result, a lot of extensions and variations of the algorithm have been devel-\noped to address these issues. In this section we describe the most important of these.\n5.4.1 Handling Noisy Data\nThroughout our worked example using the college athlete dataset, the top-right corner of\nthe feature space contained a noregion (see Figure 5.4[190]). This region exists because one\nof the noinstances occurs far away from the rest of the instances with this target level.\nConsidering that all the immediate neighbors of this instance are associated with the yes\ntarget level, it is likely that either this instance has been incorrectly labeled and should\nhave a target feature value of yes, or one of the descriptive features for this instance has an\nincorrect value and hence it is in the wrong location in the feature space. Either way, this\ninstance is likely to be an example of noise in the dataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":246,"page_label":"192","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"192 Chapter 5 Similarity-Based Learning\n(a) V oronoi tessellation\n (b) Decision boundary ( k“1)\nFigure 5.5\n(a) The V oronoi tessellation of the feature space when the dataset has been updated to include the\nquery instance; and (b) the updated decision boundary reﬂecting the addition of the query instance\nin the training set.\nFundamentally, the nearest neighbor algorithm is a set of local models, each deﬁned\nusing a single instance. Consequently, the algorithm is sensitive to noise because any\nerrors in the description or labeling of training data results in erroneous local models and\nhence incorrect predictions. The most direct way of mitigating against the impact of noise\nin the dataset on a nearest neighbor algorithm is to dilute the dependency of the algorithm\non individual (possibly noisy) instances. To do this we simply modify the algorithm to\nreturn the majority target level within the set of knearest neighbors to the query q:\nMkpqq“arg max\nlPlevelsptqkÿ\ni“1δpti,lq (5.4)\nwhereMkpq) is the prediction of the model Mfor the query qgiven the parameter of\nthe model k;levelsptqis the set of levels in the domain of the target feature, and lis an\nelement of this set; iiterates over the instances diin increasing distance from the query\nq;tiis the value of the target feature for instance di; andδpti,lqis the Kronecker delta\nfunction, which takes two parameters and returns 1if they are equal and 0otherwise.\nFigure 5.6(a)[193]demonstrates how this approach can regularize the decision boundary for\nthe dataset in Table 5.4[191]. In this ﬁgure we have set k“3, and this modiﬁcation has\nresulted in the noregion in the top right corner of the feature space disappearing.\nAlthough, in our example, increasing the set of neighbors from 1 to 3 removed the noise\nissue, k“3does not work for every dataset. There is always a trade-off in setting the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":247,"page_label":"193","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 193\n(a) Decision boundary ( k“3)\n (b) Decision boundary ( k“5)\nFigure 5.6\nThe decision boundary using majority vote of the nearest 3 and 5 instances.\nvalue of k. If we set ktoo low, we run the risk of the algorithm being sensitive to noise in\nthe data and overﬁtting. Conversely, if we set ktoo high, we run the risk of losing the true\npattern of the data and underﬁtting. For example, Figure 5.6(b)[193]illustrates what happens\nto the decision boundary in our example feature space when k“5. Here we can see that\nthe decision boundary may have been pushed too far back into the yesregion (one of the\ncrosses is now on the wrong side of the decision boundary). So, even a small increase in k\ncan have a signiﬁcant impact on the decision boundary.\nThe risks associated with setting kto a high value are particularly acute when we are\ndealing with an imbalanced dataset . An imbalanced dataset is a dataset that contains\nsigniﬁcantly more instances of one target level than another. In these situations, as kin-\ncreases, the majority target level begins to dominate the feature space. The dataset in the\ncollege athlete example is imbalanced—there are 13 noinstances and only 7 yesinstances.\nAlthough this differential between the target levels in the dataset may not seem substantial,\nit does have an impact as kincreases. Figure 5.7(a)[194]illustrates the decision boundary\nwhen k“15. Clearly, large portions of the yesregion are now on the wrong side of the\ndecision boundary. Moreover, if kis set to a value larger than 15, the majority target level\ndominates the entire feature space. Given the sensitivity of the algorithm to the value of k,\nhow should we set this parameter? The most common way to tackle this issue is to perform\nevaluation experiments to investigate the performance of models with different values for k\nand to select the one that performs best. We return to these kinds of evaluation experiments\nin Chapter 9[533].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":248,"page_label":"194","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"194 Chapter 5 Similarity-Based Learning\n(a) Decision boundary ( k“15)\n (b) Weighted decision boundary ( k“21)\nFigure 5.7\n(a) The decision boundary using majority vote of the nearest 15 neighbors; and (b) the weighted k\nnearest neighbor model decision boundary (with k“21).\nAnother way to address the problem of how to set kis to use a weighted knearest\nneighbor approach. The problem with setting kto a high value arises because the algorithm\nstarts taking into account neighbors that are far away from the query instance in the feature\nspace. As a result, the algorithm tends toward the majority target level in the dataset. One\nway of counterbalancing this tendency is to use a distance weighted knearest neighbor\napproach. When a distance weighted knearest neighbor approach is used, the contribution\nof each neighbor to the prediction is a function of the inverse distance between the neighbor\nand the query. So when calculating the overall majority vote across the knearest neighbors,\nthe votes of the neighbors that are close to the query get a lot of weight, and the votes of\nthe neighbors that are farther away from the query get less weight. The easiest way to\nimplement this weighting scheme is to weight each neighbor by the reciprocal6of the\nsquared distance between the neighbor dand the query q:\n1\ndistpq,dq2(5.5)\nUsing the distance weighted knearest neighbor approach, the prediction returned for\na given query is the target level with the highest score when we sum the weights of the\nvotes of the instances in the neighborhood of knearest neighbors for each target level. The\n6. When using the reciprocal of the squared distance as a weighting function, we need to be careful to avoid\ndivision by zero in the case in which the query is exactly the same as its nearest neighbor. Typically this problem\ncase is handled by assigning the query the target level of the training instance dthat it exactly matches.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":249,"page_label":"195","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 195\nweighted knearest neighbor model is deﬁned as\nMkpqq“arg max\nlPlevelsptqkÿ\ni“11\ndistpq,diq2ˆδpti,lq (5.6)\nwhereMkpq) is the prediction of the model Mfor the query qgiven the parameter of the\nmodel k;levelsptqis the set of levels in the domain of the target feature, and lis an element\nof this set; iiterates over the instances diin increasing distance from the query q;tiis\nthe value of the target feature for instance di; andδpti,lqis the Kronecker delta function,\nwhich takes two parameters and returns 1 if they are equal and 0 otherwise. The reason we\nmultiply by the Kronecker delta function is to ensure that in calculating the score for each\nof the candidate target levels, we include only the weights for the instances whose target\nfeature value matches that level.\nWhen we weight the contribution to a prediction of each of the neighbors by the recipro-\ncal of the distance to the query, we can actually set kto be equal to the size of the training\nset and therefore include all the training instances in the prediction process. The issue of\nlosing the true pattern of the data is less acute now because the training instances that are\nvery far away from the query naturally won’t have much of an effect on the prediction.\nFigure 5.7(b)[194]shows the decision boundary for a weighted knearest neighbor model\nfor the dataset in Table 5.4[191]with k“21(the size of the dataset) and weights computed\nusing the reciprocal of the squared distance. One of the most striking things about this plot\nis that the top-right region of the feature space again belongs to the noregion. This may\nnot be a good thing if this instance is due to noise in the data, and this demonstrates that\nthere is no silver bullet solution to handling noise in datasets. This is one of the reasons\nwhy creating a data quality report7and spending time on cleaning the dataset is such an\nimportant part of any machine learning project. That said, there are some other features of\nthis plot that are encouraging. For example, the size of a noregion in the top right of the\nfeature space is smaller than the corresponding region for the nearest neighbor model with\nk“1(see Figure 5.4(b)[190]). So by giving all the instances in the dataset a weighted vote,\nwe have at least reduced the impact of the noisy instance. Also, the decision boundary is\nmuch smoother than the decision boundaries of the other models we have looked at in this","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":249,"page_label":"195","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"there is no silver bullet solution to handling noise in datasets. This is one of the reasons\nwhy creating a data quality report7and spending time on cleaning the dataset is such an\nimportant part of any machine learning project. That said, there are some other features of\nthis plot that are encouraging. For example, the size of a noregion in the top right of the\nfeature space is smaller than the corresponding region for the nearest neighbor model with\nk“1(see Figure 5.4(b)[190]). So by giving all the instances in the dataset a weighted vote,\nwe have at least reduced the impact of the noisy instance. Also, the decision boundary is\nmuch smoother than the decision boundaries of the other models we have looked at in this\nsection. This may indicate that the model is doing a better job of modeling the transition\nbetween the different target levels.\nUsing a weighted knearest neighbor model does not require that we set kequal to the\nsize of the dataset, as we did in this example. It may be possible to ﬁnd a value for k—using\nevaluation experiments—that eliminates, or further reduces, the effect of the noise on the\nmodel. As is so often the case in machine learning, ﬁtting the parameters of a model is as\nimportant as selecting which model to use.\n7. See Section 3.1[54].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":250,"page_label":"196","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"196 Chapter 5 Similarity-Based Learning\nFinally, it is worth mentioning two situations where this weighted knearest neighbor\napproach can be problematic. The ﬁrst is if the dataset is very imbalanced, then even with\na weighting applied to the contribution of the training instances, the majority target level\nmay dominate. The second is when the dataset is very large, which means that computing\nthe reciprocal of squared distance between the query and all the training instances can\nbecome too computationally expensive to be feasible.\n5.4.2 Efﬁcient Memory Search\nThe fact that the nearest neighbor algorithm stores the entire training dataset in memory has\na negative effect on the time complexity of the algorithm. In particular, if we are working\nwith a large dataset, the time cost in computing the distances between a query and all the\ntraining instances and retrieving the knearest neighbors may be prohibitive. Assuming that\nthe training set will remain relatively stable, this time issue can be offset by investing in\nsome one-off computation to create an index of the instances that enables efﬁcient retrieval\nof the nearest neighbors without doing an exhaustive search of the entire dataset.\nThek-dtree,8which is short for k-dimensional tree, is one of the best known of these\nindices. A k-dtree is a balanced binary tree9in which each of the nodes in the tree\n(both interior and leaf nodes) index one of the instances in a training dataset. The tree is\nconstructed so that nodes that are nearby in the tree index training instances that are nearby\nin the feature space.\nTo construct a k-dtree, we ﬁrst pick a feature and split the data into two partitions using\nthe median value of this feature.10We then recursively split each of the two new partitions,\nstopping the recursion when there are fewer than two instances in a partition. The main\ndecision to be made in this process is how to select the feature to split on. The most\ncommon way to do this is to deﬁne an arbitrary order over the descriptive features before\nwe begin building the tree. Then, using the feature at the start of the list for the ﬁrst split,\nwe select the next feature in the list for each subsequent split. If we get to a point where\nwe have already split on all the features, we go back to the start of the feature list.\nEvery time we partition the data, we add a node with two branches to the k-dtree. The\nnode indexes the instance that had the median value of the feature, the left branch holds","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":250,"page_label":"196","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"stopping the recursion when there are fewer than two instances in a partition. The main\ndecision to be made in this process is how to select the feature to split on. The most\ncommon way to do this is to deﬁne an arbitrary order over the descriptive features before\nwe begin building the tree. Then, using the feature at the start of the list for the ﬁrst split,\nwe select the next feature in the list for each subsequent split. If we get to a point where\nwe have already split on all the features, we go back to the start of the feature list.\nEvery time we partition the data, we add a node with two branches to the k-dtree. The\nnode indexes the instance that had the median value of the feature, the left branch holds\nall the instances that had values less than the median, and the right branch holds all the\n8. The primary papers introducing k-dtrees are Bentley (1975) and Friedman et al. (1977). Also note that the\nkhere has no relationship with the kused in knearest neighbor. It simply speciﬁes the number of levels in the\ndepth of the tree, which is arbitrary and typically determined by the algorithm that constructs the tree.\n9. A binary tree is simply a tree in which every node in the tree has at most two branches.\n10. We use the median value as the splitting threshold because it is less susceptible to the inﬂuence of outliers than\nthe mean, and this helps keep the tree as balanced as possible—having a balanced tree helps with the efﬁciency\nin retrieval. If more than one instance in a dataset has the median value for a feature we are splitting on, then we\nselect one of these instances to represent the median and place the other instances with the median value in the\nset containing the instances whose values are greater than the median.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":251,"page_label":"197","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 197\ninstances that had values greater than the median. The recursive partitioning then grows\neach of these branches in a depth-ﬁrst manner.\nEach node in a k-dtree deﬁnes a boundary that partitions the feature space along the\nmedian value of the feature the data was split on at that node. Technically these boundaries\narehyperplanes11and, as we shall see, play an important role when we are using the k-d\ntree to ﬁnd the nearest neighbor for a query. In particular, the hyperplane at a node deﬁnes\nthe boundary between the instances stored on each of the subtrees below the node. We will\nﬁnd this useful when we are trying to decide whether to search both branches of a node\nwhen we are looking for the nearest neighbor or whether we can prune one of them.\nFigure 5.8[198]illustrates the creation of the ﬁrst two nodes of a k-dtree for the college\nathlete dataset in Table 5.4[191]. In generating this ﬁgure we have assumed that the algorithm\nselected the features to split on using the following ordering over the features: S PEED ,\nAGILITY . The non-leaf nodes in the trees list the ID of the instance the node indexes and\nthe feature and value pair that deﬁne the hyperplane partition on the feature space deﬁned\nby the node. Figure 5.9(a)[199]shows the complete k-dtree generated for the dataset, and\nFigure 5.9(b)[199]shows the partitioning of the feature space as deﬁned by the k-dtree. The\nlines in this ﬁgure indicate the hyperplanes partitioning the feature space that were created\nby the splits encoded in the non-leaf nodes in the tree. The heavier the weight of the line\nused to plot the hyperplane, the earlier in the tree the split occurred.\nOnce we have stored the instances in a dataset in a k-dtree, we can use the tree to quickly\nretrieve the nearest neighbor for a query instance. Algorithm 3[200]lists the algorithm we\nuse to retrieve the nearest neighbor for a query. The algorithm starts by descending through\nthe tree from the root node, taking the branch at each interior node that matches the value\nof the query for the feature tested at that node, until it comes to a leaf node (Line 3 of the\nalgorithm). The algorithm stores the instance indexed by the leaf node in the best variable\nand sets the best-distance variable to the distance between the instance indexed by the\nleaf node and the query instance (Lines 5, 6, and 7). Unfortunately, there is no guarantee","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":251,"page_label":"197","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Once we have stored the instances in a dataset in a k-dtree, we can use the tree to quickly\nretrieve the nearest neighbor for a query instance. Algorithm 3[200]lists the algorithm we\nuse to retrieve the nearest neighbor for a query. The algorithm starts by descending through\nthe tree from the root node, taking the branch at each interior node that matches the value\nof the query for the feature tested at that node, until it comes to a leaf node (Line 3 of the\nalgorithm). The algorithm stores the instance indexed by the leaf node in the best variable\nand sets the best-distance variable to the distance between the instance indexed by the\nleaf node and the query instance (Lines 5, 6, and 7). Unfortunately, there is no guarantee\nthat this instance will be the nearest neighbor, although it should be a good approximate\nneighbor for the query. So the algorithm then searches the tree looking for instances that\nare closer to the query than the instance stored in best(Lines 4-11 of the algorithm control\nthis search).\nAt each node encountered in the search, the algorithm does three things. First, it checks\nthat the node is not NULL . If this is the case, then the algorithm has reached the parent node\nof the root of the tree and should terminate (Line 4) by returning the instance stored in best\n(Line 12). Second, the algorithm checks if the instance indexed by the node is closer to the\nquery than the instance at the current best node. If it is, best andbest-distance are updated\n11. A hyperplane is a geometric concept that generalizes the idea of a plane into different dimensions. For\nexample, a hyperplane in 2D space is a line and in a 3D space is a plane.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":252,"page_label":"198","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"198 Chapter 5 Similarity-Based Learning\nID=6\nSpeed :4.5\nIDs= 1,2,3,4,5,7,\n8,9,10,11   Speed<4.5\nIDs= 12,13,14,15,16,\n17,18,19,20,21Speed ≥4.5\n(a)\n2 4 6 82 4 6 8\nSpeedAgility (b)\nID=6\nSpeed :4.5\nID=3\nAgility :5.5Speed<4.5\nIDs= 12,13,14,15,16,\n17,18,19,20,21Speed ≥4.5\nIDs=7,8,9,10,11Agility<5.5\nIDs=1,2,4,5Agility ≥5.5\n(c)\n2 4 6 82 4 6 8\nSpeedAgility (d)\nFigure 5.8\n(a) The k-dtree generated for the dataset in Table 5.4[191]after the initial split using the S PEED feature\nwith a threshold of 4.5; (b) the partitioning of the feature space by the k-dtree in (a); (c) the k-dtree\nafter the dataset at the left child of the root has been split using the A GILITY feature with a threshold\nof5.5; and (d) the partitioning of the feature space by the k-dtree in (c).\nto reﬂect this (Lines 5, 6, and 7). Third, the algorithm chooses which node it should move\nto next: the parent of the node or a node in the subtree under the other branch of the node\n(Lines 8, 9, 10, and 11).\nThe decision of which node to move to next is made by checking if any instances in-\ndexed by nodes in the subtree on the other branch of the current node could be the nearest","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":253,"page_label":"199","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 199\nID=6\nSpeed:4.5\nID=3\nAgility:5.5Speed<4.5\nID=16\nAgility:6.75Speed>=4.5\nID=7\nSpeed:3.5Agility<5.5\nID=4\nSpeed:3.25Agility>=5.5\nID=8\nAgility:3.25Speed<3.5\nID=9\nAgility:4.0Speed>=3.5\nID=11Agility<3.25\nID=10Agility<4.0ID=5\nAgility:7.5Speed<3.25\nID=2Speed>=3.25\nID=1Agility<7.5ID=21\nSpeed:6.75Agility<6.75\nID=19\nSpeed:7.5Agility>=6.75\nID=15\nAgility:6.25Speed<6.75\nID=20\nAgility:5.75Speed>=6.75\nID=12Agility<6.25\nID=18Agility<5.75ID=17\nAgility:9.5Speed<7.5\nID=13Speed>=7.5\nID=14Agility<9.5\n(a)\n2 4 6 82 4 6 8\nSpeedAgility\n(b)\nFigure 5.9\n(a) The ﬁnal k-dtree generated for the dataset in Table 5.4[191]; and (b) the partitioning of the feature\nspace deﬁned by this k-dtree.\nneighbor. The only way that this can happen is if there is at least one instance on the other\nside of the hyperplane boundary that bisects the node that is closer to the query than the\ncurrent best-distance . Fortunately, because the hyperplanes created by the k-dtree are all\naxis-aligned, the algorithm can test for this condition quite easily. The hyperplane bound-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":254,"page_label":"200","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"200 Chapter 5 Similarity-Based Learning\nAlgorithm 3 Pseudocode description of the k-dtree nearest neighbor retrieval algorithm.\nRequire: query instance qand a k-dtreekdtree\n1:best = null\n2:best-distance = 8\n3:node = descendTree(kdtree,q)\n4:while node! = NULL do\n5: ifdistance(q,node) ăbest-distance then\n6: best = node\n7: best-distance = distance(q,node)\n8: end if\n9: ifboundaryDist(q, node) ăbest-distance then\n10: node = descendtree(node,q)\n11: else\n12: node = parent(node)\n13: end if\n14:end while\n15:return best\nary bisecting a node is deﬁned by the value used to split the descriptive feature at the node.\nThis means that we only need to test whether the difference between the value for this\nfeature for the query instance and the value for this feature that deﬁnes the hyperplane is\nless than the best-distance (Line 8). If this test succeeds, the algorithm descends to a leaf\nnode of this subtree, using the same process it used to ﬁnd the original leaf node (Line\n9). If this test fails, the algorithm ascends the tree to the parent of the current node and\nprunes the subtree containing the region on the other side of the hyperplane without testing\nthe instances in that region (Line 11). In either case, the search continues from the new\nnode as before. The search ﬁnishes when it reaches the root node and both its branches\nhave been either searched or pruned. The algorithm returns the instance stored in the best\nvariable as the nearest neighbor.\nWe can demonstrate how this retrieval algorithm works by showing how the algorithm\nﬁnds the nearest neighbor for a query instance with S PEED“6.00and A GILITY“3.50.\nFigure 5.10(a)[201]illustrates the ﬁrst stage of the retrieval of the nearest neighbor. The\nbold lines show the path taken to descend the tree from the root to a leaf node based on\nthe values of the query instance (use Figure 5.9(a)[199]to trace this path in detail). This\nleaf node indexes instance d12(SPEED“5.00, AGILITY“2.50). Because this is the\ninitial descent down the tree, bestis automatically set to d12, and best-distance is set to the\ndistance between instance d12and the query, which is 1.4142 (we use Euclidean distance\nthroughout this example). At this point the retrieval process will have executed Lines 1–7\nof the algorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":255,"page_label":"201","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 201\nID=12\n(a)\n2 4 6 82 4 6 8\nSpeedAgility\n?\n1215\n2118 (b)\nFigure 5.10\n(a) The path taken from the root node to a leaf node when we search the tree with a query S PEED“\n6.00, AGILITY“3.50; and (b) the ? marks the location of the query, and the dashed circle plots the\nextent of the target, and for convenience in the discussion, we have labeled some of the nodes with\nthe IDs of the instances they index (12, 15, 18, and 21).\nFigure 5.10(b)[201]illustrates the location of the query in the feature space (the ?). The\ndashed circle centered on the query location has a radius equal to the best-distance . We\ncan see in Figure 5.10(b)[201]that this circle intersects with the triangle marking the location\nofd12, which is currently stored in best (i.e., it is our current best guess for the nearest\nneighbor). This circle covers the area in the feature space that we know must contain all\nthe instances that are closer to the query than best. Although this example is just two\ndimensional, the k-dtree algorithm can work in a many dimensional feature space, so we\nwill use the term target hypersphere12to denote the region around the query that is inside\nthebest-distance . We can see in Figure 5.10(b)[201]that instance d12is not the true nearest\nneighbor to the query—several other instances are inside the target hypersphere.\nThe search process must now move to a new node (Lines 8, 9, 10, and 11). This move is\ndetermined by Line 8, which checks if the distance between the query and the hyperplane13\ndeﬁned by the current node is less than the value of best-distance . In this case, however,\n12. Similar to a hyperplane, a hypersphere is a generalization of the geometric concept of a sphere across multiple\ndimensions. Hence, in a 2D space the term hypersphere denotes a circle, in 3D it denotes a sphere, and so on.\n13. Recall that each non-leaf node in the tree indexes an instance in the dataset and also deﬁnes a hyperplane that\npartitions the feature space. For example, the horizontal and vertical lines in Figure 5.9(b)[199]plot the hyperplanes\ndeﬁned by the non-leaf nodes of the k-dtree shown in Figure 5.9(a)[199].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":256,"page_label":"202","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"202 Chapter 5 Similarity-Based Learning\nthe current node is a leaf node, so it does not deﬁne a hyperplane on the feature space. As\na result, the condition checked in Line 8, fails and the search moves to the parent node of\nthe current node (Line 11).\nThis new node indexes d15. The node is not NULL , so the while loop on Line 4 suc-\nceeds. The distance between instance d15and the query instance is 3.0208 , which is not\nless than the current value of best-distance , so the ifstatement on Line 5 will fail. We can\nsee this easily in Figure 5.10(b)[201], asd15is well outside the target hypersphere. The search\nwill then move to a new node (Lines 8, 9, 10, and 11). To calculate the distance between\nthe query instance and the hyperplane deﬁned by the node indexing d15(the boundaryDist\nfunction on Line 8), we use only the A GILITY feature, as it is the splitting feature at this\nnode. This distance is 2.75, which is greater than best-distance (we can see this in Fig-\nure 5.10(b)[201], as the hyperplane deﬁned at the node indexing d15does not intersect with\nthe target hypersphere). This means that the ifstatement on Line 8 fails, and the search\nmoves to the parent of the current node (Line 11).\nThis new node indexes d21, which is not NULL , so the while loop on Line 4 succeeds.\nThe distance between the query instance and d21is0.9014 , which is less than the value\nstored in best-distance (we can see this in Figure 5.10(b)[201], asd21is inside the target\nhypersphere). Consequently, the ifstatement on Line 5 succeeds, and best is set to d21,\nandbest-distance is set to 0.9014 (Lines 6 and 7). Figure 5.11(a)[203]illustrates the extent\nof the revised target hypersphere once these updates have been made.\nTheifstatement on Line 8, which tests the distance between the query and the hyper-\nplane deﬁned by the current best node, is executed next. The distance between the query\ninstance and the hyperplane deﬁned by the node that indexes instance d21is0.75(recall\nthat because the hyperplane at this node is deﬁned by the S PEED value of 6.75, we only\ncompare this to the S PEED value of the query instance, 6.00). This distance is less than the\ncurrent best-distance (in Figure 5.11(a)[203], the hyperplane deﬁned by the node that indexes\ninstance d21intersects with the target hypersphere). The ifstatement on line 8 will suc-\nceed, and the search process will descend down the other branch of the current node (Line","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":256,"page_label":"202","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Theifstatement on Line 8, which tests the distance between the query and the hyper-\nplane deﬁned by the current best node, is executed next. The distance between the query\ninstance and the hyperplane deﬁned by the node that indexes instance d21is0.75(recall\nthat because the hyperplane at this node is deﬁned by the S PEED value of 6.75, we only\ncompare this to the S PEED value of the query instance, 6.00). This distance is less than the\ncurrent best-distance (in Figure 5.11(a)[203], the hyperplane deﬁned by the node that indexes\ninstance d21intersects with the target hypersphere). The ifstatement on line 8 will suc-\nceed, and the search process will descend down the other branch of the current node (Line\n9), because there is possibly an instance closer than the current best instance stored down\nthis branch.\nIt is obvious from Figure 5.11(a)[203]that the search process will not ﬁnd any instances\ncloser to the query than d21, nor are there any other hyperplanes that intersect with the\ntarget hypersphere. So the rest of the search process will involve a descent down to the\nnode, indexing d18and a direct ascent to the root node where the search process will then\nterminate and return d21as the nearest neighbor (we will skip the details of these steps).\nFigure 5.11(b)[203]illustrates the parts of the k-dtree that were checked or pruned during\nthe search process.\nIn this example, using a k-dtree saved us calculating the distance between the query\nnode and 14 of the instances in the dataset. This is the beneﬁt of using a k-dtree and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":257,"page_label":"203","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 203\n2 4 6 82 4 6 8\nSpeedAgility\n?\n1215\n2118\n(a)\nID=21 (b)\nFigure 5.11\n(a) The target hypersphere after instance d21has been stored as best, and best-distance has been\nupdated; and (b) the extent of the search process: white nodes were checked by the search process,\nand the node with the bold outline indexed instance d21, which was returned as the nearest neighbor\nto the query. Grayed-out branches indicate the portions of the k-dtree pruned from the search.\nbecomes especially apparent when datasets are very large. However, using a k-dtree is not\nalways appropriate; k-dtrees are reasonably efﬁcient when there are a lot more instances\nthan there are features. As a rough rule of thumb, we should have around 2minstances\nformdescriptive features. Once this ratio drops, the efﬁciency of the k-dtree diminishes.\nOther approaches to efﬁcient memory access have been developed, for example, locality\nsensitivity hashing, R-Trees, B-Trees, M-Trees, and V oRTrees, among others. All these\napproaches are similar to k-dtrees in that they are trying to set up indexes that enable\nefﬁcient retrieval from a dataset. Obviously, the differences between them make them\nmore or less appropriate for a given dataset, and it often requires some experiments to\nﬁgure out which is the best one for a given problem.\nWe can extend this algorithm to retrieve the knearest neighbors by modifying the search\nto use distance of the kthclosest instance found as best-distance . We can also add instances\nto the tree after if has been created. This is important because one of the key advantages of\na nearest neighbor approach is that it can be updated with new instances as more labeled\ndata arrive. To add a new instance to the tree, we start at the root node and descend to\na leaf node, taking the left or right branch of each node depending on whether the value\nof the instance’s feature is less than or greater than the splitting value used at the node.\nOnce we get to a leaf node, we simply add the new instance as either the left or the right\nchild of the leaf node. Unfortunately, adding nodes in this way results in the tree becoming","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":258,"page_label":"204","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"204 Chapter 5 Similarity-Based Learning\nTable 5.5\nA dataset listing salary and age information for customers and whether they purchased a product.\nID S ALARY AGE PURCH\n1 53,700 41 no\n2 65,300 37 no\n3 48,900 45 yes\n4 64,800 49 yes\n5 44,200 30 noID S ALARY AGE PURCH\n6 55,900 57 yes\n7 48,600 26 no\n8 72,800 60 yes\n9 45,300 34 no\n10 73,200 52 yes\nunbalanced, which can have a detrimental effect on the efﬁciency of the tree. So if we\nadd a lot of new instances, we may ﬁnd that the tree has become too unbalanced and that\nwe will need to construct a new tree from scratch using the extended dataset to restore the\nefﬁciency of the retrieval process.\n5.4.3 Data Normalization\nA ﬁnancial institution is planning a direct marketing campaign to sell a pension product to\nits customer base. In preparation for this campaign, the ﬁnancial institution has decided\nto create a nearest neighbor model using a Euclidean distance metric to predict which\ncustomers are most likely to respond to direct marketing. This model will be used to\ntarget the marketing campaign only to those customers who are most likely to purchase the\npension product. To train the model, the institution has created a dataset from the results\nof previous marketing campaigns that list customer information—speciﬁcally the annual\nsalary (S ALARY ) and age (A GE) of the customer—and whether the customer bought a\nproduct after they had been contacted via a direct marketing message (P URCH ). Table\n5.5[204]lists a sample from this dataset.\nUsing this nearest neighbor model, the marketing department wants to decide whether\nthey should contact a customer with the following proﬁle: S ALARY“56,000and A GE“\n35. Figure 5.12(a)[205]presents a plot of the feature space deﬁned by the S ALARY and A GE\nfeatures, containing the dataset in Table 5.5[204]. The location of the query customer in the\nfeature space is indicated by the ?. From inspecting Figure 5.12(a)[205], it would appear as if\ninstance d1—which has a target level no—is the closest neighbor to the query. So we would\nexpect that the model would predict noand that the customer would not be contacted.\nThe model, however, will actually return a prediction of yes, indicating that the customer\nshould be contacted. We can analyze why this happens if we examine the Euclidean dis-\ntance computations between the query and the instances in the dataset. Table 5.6[206]lists\nthese distances when we include both the S ALARY and A GEfeatures, only the S ALARY","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":258,"page_label":"204","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"features, containing the dataset in Table 5.5[204]. The location of the query customer in the\nfeature space is indicated by the ?. From inspecting Figure 5.12(a)[205], it would appear as if\ninstance d1—which has a target level no—is the closest neighbor to the query. So we would\nexpect that the model would predict noand that the customer would not be contacted.\nThe model, however, will actually return a prediction of yes, indicating that the customer\nshould be contacted. We can analyze why this happens if we examine the Euclidean dis-\ntance computations between the query and the instances in the dataset. Table 5.6[206]lists\nthese distances when we include both the S ALARY and A GEfeatures, only the S ALARY\nfeatures, and only the A GEfeature in the distance calculation. The nearest neighbor model\nuses both the S ALARY and A GEfeatures when it calculates distances to ﬁnd the nearest\nneighbor to the query. The S ALARY and A GEsection of Table 5.6[206]lists these distances","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":259,"page_label":"205","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 205\n45000 55000 65000 7500025 30 35 40 45 50 55 60\nSalaryAge1\n234\n56\n78\n910\n?\n(a)\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nRange No rmalized Sala ryRange No rmalized Age\n1\n234\n56\n78\n910\n? (b)\nFigure 5.12\n(a) The feature space deﬁned by the S ALARY and A GEfeatures in Table 5.5[204]; and (b) the normal-\nized S ALARY and A GEfeature space based on the normalized data in Table 5.7[208]. The instances\nare labeled with their IDs; triangles represent instances with the notarget level; and crosses repre-\nsent instances with the yestarget level. The location of the query S ALARY = 56,000, A GE= 35 is\nindicated by the ?.\nand the ranking that the model applies to the instances in the dataset using them. From the\nrankings we can see that the nearest neighbor to the query is instance d6(indicated by its\nrank of 1). Instance d6has a target value of yes, and this is why the model will return a\npositive prediction for the query.\nConsidering the distribution of the instances in the feature space as depicted in Figure\n5.12(a)[205], the result that instance d6is the nearest neighbor to the query is surprising.\nSeveral other instances appear to be much closer to the query, and importantly, several of\nthese instances have a target level of no, for example, instance d1. Why do we get this\nstrange result?\nWe can get a hint about what is happening by comparing the distances computed using\nboth the S ALARY and A GEfeatures with the distances computed using the S ALARY feature\nonly, that listed in the S ALARY Only section of Table 5.6[206]. The distances calculated using\nonly the S ALARY feature are almost exactly the same as the distances calculated using both\nthe S ALARY and A GEfeatures. This is happening because the salary values are much larger\nthan the age values. Consequently, the S ALARY feature dominates the computation of the\nEuclidean distance whether we include the A GEfeature or not. As a result, A GEis being\nvirtually ignored by the metric. This dominance is reﬂected in the ranking of the instances\nas neighbors. In Table 5.6[206], if we compare the rankings based on S ALARY and A GEwith","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":260,"page_label":"206","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"206 Chapter 5 Similarity-Based Learning\nTable 5.6\nThe dataset from Table 5.5[204]with the Euclidean distance between each instance and the query\nSALARY = 56,000, A GE= 35 when we use both the S ALARY and A GEfeatures, just the S ALARY\nfeature, and just the A GEfeature.\nDataset SALARY and A GE SALARY Only AGEOnly\nID S ALARY AGE PURCH Dist. Rank Dist. Rank Dist. Rank\n1 53,700 41 no 2,300.0078 2 2,300 2 6 4\n2 65,300 37 no 9,300.0002 6 9,300 6 2 2\n3 48,900 45 yes 7,100.0070 3 7,100 3 10 6\n4 64,800 49 yes 8,800.0111 5 8,800 5 14 7\n5 44,200 30 no 11,800.0011 8 11,800 8 5 5\n6 55,900 57 yes 102.3914 1 100 1 22 9\n7 48,600 26 no 7,400.0055 4 7,400 4 9 3\n8 72,800 60 yes 16,800.0186 9 16,800 9 25 10\n9 45,300 34 no 10,700.0000 7 10,700 7 1 1\n10 73,200 52 yes 17,200.0084 10 17,200 10 17 8\nThe Rank columns rank the distances of each instance to the query (1 is closest, 10 is farthest away).\nthe rankings based solely on S ALARY , we see that the values in these two columns are\nidentical. The model is using only the S ALARY feature and is ignoring the A GEfeature\nwhen it makes predictions.\nThis dominance of the distance computation by a feature based solely on the fact that it\nhas a larger range of values than other features is not a good thing. We do not want our\nmodel to bias toward a particular feature simply because the values of that feature happen\nto be large relative to the other features in the dataset. If we allowed this to happen, then\nour model will be affected by accidental data collection factors, such as the units used to\nmeasure something. For example, in a model that is sensitive to the relative size of the\nfeature values, a feature that was measured in millimeters would have a larger effect on the\nresulting model predictions than a feature that was measured in meters.14Clearly we need\nto address this issue.\nFortunately, we have already discussed the solution to this problem. The problem is\ncaused by features having different variance . In Section 3.6.1[87]we discussed variance\nand introduced a number of normalization techniques that normalize the variances in a set\nof features. The basic normalization technique we introduced was range normalization ,15\n14. Figure 5.12(a)[205]further misleads us because when we draw scatter plots, we scale the values to make the\nplot ﬁt into a square-shaped image. If we were to plot the axis for the S ALARY feature to the same scale as the\nAGEfeature in Figure 5.12(a)[205], it would stretch over almost 400 pages.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":260,"page_label":"206","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"to address this issue.\nFortunately, we have already discussed the solution to this problem. The problem is\ncaused by features having different variance . In Section 3.6.1[87]we discussed variance\nand introduced a number of normalization techniques that normalize the variances in a set\nof features. The basic normalization technique we introduced was range normalization ,15\n14. Figure 5.12(a)[205]further misleads us because when we draw scatter plots, we scale the values to make the\nplot ﬁt into a square-shaped image. If we were to plot the axis for the S ALARY feature to the same scale as the\nAGEfeature in Figure 5.12(a)[205], it would stretch over almost 400 pages.\n15. For convenience, we repeat Equation (3.7)[87]for range normalization\na1\ni“ai´minpaq\nmaxpaq´minpaqˆphigh´lowq`low","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":261,"page_label":"207","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 207\nand we can apply it to the pension plan prediction dataset to normalize the variance in the\nSALARY and A GEfeatures. For example, range normalization using the range r0,1sis\napplied to instance d1from Table 5.5[204]as follows:\nSALARY :ˆ53,700´44,200\n73,200´44,200˙\nˆp1.0´0.0q`0“0.3276\nAGE:ˆ41´26\n60´26˙\nˆp1.0´0.0q`0“0.4412\nTable 5.7[208]lists the dataset from Table 5.5[204]after we have applied range normalization\nusing a range of r0,1sto the S ALARY and A GEfeatures. When we normalize the features\nin a dataset, we also need to normalize the features in any query instances using the same\nnormalization process and parameters. We normalize the query instance with S ALARY\n=56,000and A GE=35as follows:\nSALARY :ˆ56,000´44,200\n73,200´44,200˙\nˆp1.0´0.0q`0“0.4069\nAGE:ˆ35´26\n60´26˙\nˆp1.0´0.0q`0“0.2647\nFigure 5.12(b)[205]shows a plot of the feature space after the features have been normalized.\nThe major difference between Figure 5.12(a)[205]and Figure 5.12(b)[205]is that the axes are\nscaled differently. In Figure 5.12(a)[205]the S ALARY axis ranged from 45,000to75,000,\nand the A GEaxis ranged from 25to60. In Figure 5.12(b)[205], however, both axes range\nfrom 0to1. Although this may seem like an insigniﬁcant difference, the fact that both\nfeatures now cover the same range has a huge impact on the performance of a similarity-\nbased prediction model that uses this data.\nTable 5.7[208]also repeats the calculations from Table 5.6[206]using the normalized dataset\nand the normalized query instance. In contrast with Table 5.6[206], where there was a close\nmatch between the S ALARY and A GEdistances and the S ALARY only distances and related\nrankings, in Table 5.7[208]there is much more variation between the S ALARY and A GE\ndistances and the S ALARY only distances. This increased variation is mirrored in the fact\nthat the rankings based on the distances calculated using the S ALARY and A GEfeatures\nare quite different from the rankings based on the distances calculated using S ALARY only.\nThese changes in the rankings of the instances is a direct result of normalizing the features\nand reﬂects the fact that the distance calculations are no longer dominated by the S ALARY\nfeature. The nearest neighbor model is now factoring both S ALARY and A GEinto the\nranking of the instances. The net effect of this is that instance d1is now ranked as the\nnearest neighbor to the query—this is in line with the feature space representation in Figure","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":261,"page_label":"207","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"distances and the S ALARY only distances. This increased variation is mirrored in the fact\nthat the rankings based on the distances calculated using the S ALARY and A GEfeatures\nare quite different from the rankings based on the distances calculated using S ALARY only.\nThese changes in the rankings of the instances is a direct result of normalizing the features\nand reﬂects the fact that the distance calculations are no longer dominated by the S ALARY\nfeature. The nearest neighbor model is now factoring both S ALARY and A GEinto the\nranking of the instances. The net effect of this is that instance d1is now ranked as the\nnearest neighbor to the query—this is in line with the feature space representation in Figure\n5.12(b)[205]. Instance d1has a target level of no, so the nearest neighbor model now predicts\na target level of nofor the query, meaning that the marketing department won’t include the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":262,"page_label":"208","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"208 Chapter 5 Similarity-Based Learning\nTable 5.7\nThe updated version of Table 5.6[206]once we have applied range normalization to the S ALARY and\nAGEfeatures in the dataset and to the query instance.\nNormalized Dataset SALARY and A GE SALARY Only AGEOnly\nID S ALARY AGE PURCH Dist. Rank Dist. Rank Dist. Rank\n1 0.3276 0.4412 no 0.1935 1 0.0793 2 0.17647 4\n2 0.7276 0.3235 no 0.3260 2 0.3207 6 0.05882 2\n3 0.1621 0.5588 yes 0.3827 5 0.2448 3 0.29412 6\n4 0.7103 0.6765 yes 0.5115 7 0.3034 5 0.41176 7\n5 0.0000 0.1176 no 0.4327 6 0.4069 8 0.14706 3\n6 0.4034 0.9118 yes 0.6471 8 0.0034 1 0.64706 9\n7 0.1517 0.0000 no 0.3677 3 0.2552 4 0.26471 5\n8 0.9862 1.0000 yes 0.9361 10 0.5793 9 0.73529 10\n9 0.0379 0.2353 no 0.3701 4 0.3690 7 0.02941 1\n10 1.0000 0.7647 yes 0.7757 9 0.5931 10 0.50000 8\nThe Rank columns rank the distances of each instance to the query (1 is closest, 10 is farthest away).\ncustomer in their list of direct marketing prospects. This is the opposite of the prediction\nmade using the original dataset.\nIn summary, distance computations are sensitive to the value ranges of the features in\nthe dataset. This is something we need to control for when we are creating a model, as\notherwise we are allowing an unwanted bias to affect the learning process. When we nor-\nmalize the features in a dataset, we control for the variation across the variances of features\nand ensure that each feature can contribute equally to the distance metric. Normalizing the\ndata is an important thing to do for almost all machine learning algorithms, not just nearest\nneighbor.\n5.4.4 Predicting Continuous Targets\nIt is relatively easy to adapt the knearest neighbor approach to handle continuous target\nfeatures. To do this we simply change the approach to return a prediction of the average\ntarget value of the nearest neighbors, rather than the majority target level. The prediction\nfor a continuous target feature by a knearest neighbor model is therefore\nMkpqq“1\nkkÿ\ni“1ti (5.7)\nwhereMkpqqis the prediction returned by the model using parameter value kfor the query\nq,iiterates over the knearest neighbors to qin the dataset, and tiis the value of the target\nfeature for instance i.\nLet’s look at an example. Imagine that we are dealers in rare whiskey, and we would\nlike some assistance in setting the reserve price for bottles of whiskey that we are selling at","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":263,"page_label":"209","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 209\nTable 5.8\nA dataset of whiskeys listing the age (in years), the rating (between 1 and 5, with 5 being the best),\nand the bottle price of each whiskey.\nID A GE RATING PRICE\n1 0 2 30.00\n2 12 3.5 40.00\n3 10 4 55.00\n4 21 4.5 550.00\n5 12 3 35.00\n6 15 3.5 45.00\n7 16 4 70.00\n8 18 3 85.00\n9 18 3.5 78.00\n10 16 3 75.00ID A GE RATING PRICE\n11 19 5 500.00\n12 6 4.5 200.00\n13 8 3.5 65.00\n14 22 4 120.00\n15 6 2 12.00\n16 8 4.5 250.00\n17 10 2 18.00\n18 30 4.5 450.00\n19 1 1 10.00\n20 4 3 30.00\nauction. We can use a knearest neighbor model to predict the likely sale price of a bottle\nof whiskey based on the prices achieved by similar bottles at previous auctions.16Table\n5.8[209]lists a dataset of whiskeys described by the R ATING they were given in a popular\nwhiskey enthusiasts magazine and their A GE(in years). The P RICE achieved at auction by\nthe each bottle is also included.\nOne thing that is immediately apparent in Table 5.8[209]is that the A GEand R ATING fea-\ntures have different ranges. We should normalize these features before we build a model.\nTable 5.9[210]lists the whiskey dataset after the descriptive features have been normalized,\nusing range normalization to the range r0,1s.\nLet’s now make a prediction using this model for a two-year-old bottle of whiskey that\nreceived a magazine rating of 5. Having normalized the dataset, we ﬁrst need to normalize\nthe descriptive feature values of this query instance using the same normalization process.\nThis results in a query with A GE“0.0667 and R ATING“1.00. For this example we set\nk“3. Figure 5.13[211]shows the neighborhood that this deﬁnes around the query instance.\nThe three closest neighbors to the query are instances d12,d16andd3. Consequently, the\nmodel will return a price prediction that is the average price of these three neighbors:\nM3p⟨0.0667,1.00⟩q“200.00`250.00`55.00\n3“168.33\nWe can also use a weighted knearest neighbor model to make predictions for contin-\nuous targets that take into account the distance from the query instance to the neighbors\n(just like we did for categorical target features in Section 5.4.1[191]). To do this, the model\n16. The example given here is based on artiﬁcial data generated for the purposes of this book. Predicting the prices\nof assets such as whiskey or wine using machine learning is, however, done in reality. For example, Ashenfelter\n(2008) deals with predicting wine prices and was covered in Ayres (2008).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":264,"page_label":"210","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"210 Chapter 5 Similarity-Based Learning\nTable 5.9\nThe whiskey dataset after the descriptive features have been normalized.\nID A GE RATING PRICE\n1 0.0000 0.25 30.00\n2 0.4000 0.63 40.00\n3 0.3333 0.75 55.00\n4 0.7000 0.88 550.00\n5 0.4000 0.50 35.00\n6 0.5000 0.63 45.00\n7 0.5333 0.75 70.00\n8 0.6000 0.50 85.00\n9 0.6000 0.63 78.00\n10 0.5333 0.50 75.00ID A GE RATING PRICE\n11 0.6333 1.00 500.00\n12 0.2000 0.88 200.00\n13 0.2667 0.63 65.00\n14 0.7333 0.75 120.00\n15 0.2000 0.25 12.00\n16 0.2667 0.88 250.00\n17 0.3333 0.25 18.00\n18 1.0000 0.88 450.00\n19 0.0333 0.00 10.00\n20 0.1333 0.50 30.00\nprediction equation in Equation (5.7)[208]is changed to\nMkpqq“kÿ\ni“11\ndistpq,diq2ˆti\nkÿ\ni“11\ndistpq,diq2(5.8)\nwhere distpq,diqis the distance between the query instance and its ithnearest neighbor.\nThis is a weighted average of the target values of the knearest neighbors, as opposed to the\nsimple average in Equation (5.7)[208].\nTable 5.10[212]shows the calculation of the numerator and denominator of Equation (5.8)[210]\nfor our whiskey bottle example, using the normalized dataset with kset to 20(the full size\nof the dataset). The ﬁnal prediction for the price of the bottle of whiskey we plan to sell is\n16,249.85\n99.2604“163.71\nThe predictions using the k“3nearest neighbor model and the weighted knearest\nneighbor model with kset to the size of the dataset are quite similar: 168.33and163.71.\nSo, which model is making the better prediction? In this instance, to ﬁnd out which model\nis best, we would really need to put the bottle of whiskey up for auction and see which\nmodel predicted the closest price. In situations where we have a larger dataset, how-\never, we could perform evaluation experiments17to see which value of kleads to the\nbest-performing model. In general, standard knearest neighbor models and weighted k\nnearest neighbor models will produce very similar results when a feature space is well\n17. These are covered in Section 9.4.1[540].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":265,"page_label":"211","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 211\n/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nAgeRating312 16?\nFigure 5.13\nThe A GEand R ATING feature space for the whiskey dataset. The location of the query instance\nis indicated by the ? symbol. The circle plotted with a dashed line demarcates the border of the\nneighborhood around the query when k“3. The three nearest neighbors to the query are labeled\nwith their ID values.\npopulated. For datasets that only sparsely populate the feature space, however, weighted k\nnearest neighbor models usually make more accurate predictions, as they take into account\nthe fact that some of the nearest neighbors can actually be quite far away.\n5.4.5 Other Measures of Similarity\nSo far we have discussed and used the Minkowski-based Euclidean and Manhattan distance\nmetrics to compute the similarity between instances in a dataset. There are, however, many\nother ways in which the similarity between instances can be measured. In this section we\nintroduce some alternative measures of similarity and discuss when it is appropriate to use\nthem. Any of these measures of similarity can simply replace the Euclidean measure we\nused in our demonstrations of the nearest neighbor algorithm.\nThroughout this section we use the terms similarity and distance almost interchange-\nably, because we often judge the similarity between two instances in terms of the distance\nbetween them in a feature space. The only difference to keep in mind is that when we\nuse distances, smaller values mean that instances are closer together in a feature space,\nwhereas when we use similarities, larger values indicate this. We will, however, be speciﬁc\nin distinguishing between metrics andindexes . Recall that in Section 5.2.2[184]we deﬁned\nfour criteria that a metric must satisfy: non-negativity ,identity ,symmetry , and trian-\ngular inequality . It is possible, however, to successfully use measures of similarity in\nsimilarity-based models that do not satisfy all four of these criteria. We refer to measures","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":266,"page_label":"212","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"212 Chapter 5 Similarity-Based Learning\nTable 5.10\nThe calculations for the weighted knearest neighbor prediction.\nPRICEˆ\nID P RICE Distance Weight Weight\n1 30.00 0.7530 1.7638 52.92\n2 40.00 0.5017 3.9724 158.90\n3 55.00 0.3655 7.4844 411.64\n4 550.00 0.6456 2.3996 1319.78\n5 35.00 0.6009 2.7692 96.92\n6 45.00 0.5731 3.0450 137.03\n7 70.00 0.5294 3.5679 249.75\n8 85.00 0.7311 1.8711 159.04\n9 78.00 0.6520 2.3526 183.50\n10 75.00 0.6839 2.1378 160.33\n11 500.00 0.5667 3.1142 1557.09\n12 200.00 0.1828 29.9376 5987.53\n13 65.00 0.4250 5.5363 359.86\n14 120.00 0.7120 1.9726 236.71\n15 12.00 0.7618 1.7233 20.68\n16 250.00 0.2358 17.9775 4494.38\n17 18.00 0.7960 1.5783 28.41\n18 450.00 0.9417 1.1277 507.48\n19 10.00 1.0006 0.9989 9.99\n20 30.00 0.5044 3.9301 117.90\nTotals: 99.2604 16,249.85\nof similarity of this type as indexes . Most of the time the technical distinction between a\nmetric and an index is not that important; we simply focus on choosing the right measure\nof similarity for the type of instances we are comparing. It is important, however, to know\nif a measure is a metric or an index, as there are some similarity-based techniques that\nstrictly require measures of similarity to be metrics. For example, the k-dtrees described\nin Section 5.4.2[196]require that the measure of similarity used be a metric (in particular that\nthe measure conform to the triangular inequality constraint).\n5.4.5.1 Similarity indexes for binary descriptive features There are lots of datasets\nthat contain binary descriptive features—categorical features that have only two levels. For\nexample, a dataset may record whether or not someone liked a movie, a customer bought\na product, or someone visited a particular webpage. If the descriptive features in a dataset\nare binary, it is often a good idea to use a similarity index that deﬁnes similarity between\ninstances speciﬁcally in terms of co-presence orco-absence of features, rather than an\nindex based on distance.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":267,"page_label":"213","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 213\nTable 5.11\nA binary dataset listing the behavior of two individuals on a website during a trial period and whether\nthey subsequently signed up for the website.\nID P ROFILE FAQ H ELPFORUM NEWSLETTER LIKED SIGNUP\n1 true true true false true yes\n2 true false false false false no\nTo illustrate a series of similarity indexes for binary descriptive features, we will use an\nexample of predicting upsell in an online service. A common business model for online\nservices is to allow users a free trial period after which time they have to sign up to a paid\naccount to continue using the service. These businesses often try to predict the likelihood\nthat users coming to the end of the trial period will accept the upsell offer to move to\nthe paid service. This insight into the likely future behavior of a customer can help a\nmarketing department decide which customers coming close to the end of their trial period\nthe department should contact to promote the beneﬁts of signup to the paid service.\nTable 5.11[213]lists a small binary dataset that a nearest neighbor model could use to\nmake predictions for this scenario. The descriptive features in this dataset are all binary\nand record the following information about the behavior of past customers:\n‚PROFILE : Did the user complete the proﬁle form when registering for the free trial?\n‚FAQ: Did the user read the frequently asked questions page?\n‚HELPFORUM : Did the user post a question on the help forum?\n‚NEWSLETTER : Did the user sign up for the weekly newsletter?\n‚LIKED : Did the user Like the website on Facebook?\nThe target feature, S IGNUP , indicates whether the customers ultimately signed up to the\npaid service or not ( yesorno).\nThe business has decided to use a nearest neighbor model to predict whether a current\ntrial user whose free trial period is about to end is likely to sign up for the paid service.\nThe query instance, q, describing this user is:\nPROFILE =true, FAQ = false , HELPFORUM =true,\nNEWSLETTER =false , LIKED =false\nTable 5.12[214]presents a pairwise analysis of similarity between the current trial user, q,\nand the two customers in the dataset in Table 5.11[213]in terms of\n‚co-presence (CP), how often a true value occurred for the same feature in both the query\ndataqand the data for the comparison user ( d1ord2)\n‚co-absence (CA), how often a false value occurred for the same feature in both the query\ndataqand the data for the comparison user ( d1ord2)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":268,"page_label":"214","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"214 Chapter 5 Similarity-Based Learning\nTable 5.12\nThe similarity between the current trial user, q, and the two users in the dataset, d1andd2, in terms\nof co-presence (CP), co-absence (CA), presence-absence (PA), and absence-presence (AP).\nq\nPres. Abs.\nd1Pres. CP = 2 PA = 0\nAbs. AP = 2 CA = 1q\nPres. Abs.\nd2Pres. CP = 1 PA = 1\nAbs. AP = 0 CA = 3\n‚presence-absence (PA), how often a true value occurred in the query data qwhen a false\nvalue occurred in the data for the comparison user ( d1ord2) for the same feature\n‚absence-presence (AP), how often a false value occurred in the query data qwhen a true\nvalue occurred in the data for the comparison user ( d1ord2) for the same feature\nOne way of judging similarity is to focus solely on co-presence. For example, in an\nonline retail setting, co-presence could capture what two users jointly viewed, liked, or\nbought. The Russel-Rao similarity index focuses on this and is measured in terms of the\nratio between the number of co-presences and the total number of binary features consid-\nered:\nsim RRpq,dq“CPpq,dq\n|q|(5.9)\nwhere qanddare two instances, |q|is the total number of features in the dataset, and\nCPpq,dqmeasures the total number of co-presences between qandd. Using Russel-Rao,\nqhas a higher similarity to d1than to d2:\nsim RRpq,d1q“2\n5“0.4\nsim RRpq,d2q“1\n5“0.2\nThis means that the current trial user is judged to be more similar to the customer repre-\nsented by instance d1than the customer represented by instance d2.\nIn some domains co-absence is important. For example, in a medical domain when\njudging the similarity between two patients, it may be as important to capture the fact that\nneither patient had a particular symptom as it is to capture the symptoms that the patients\nhave in common. The Sokal-Michener similarity index takes this into account and is\ndeﬁned as the ratio between the total number of co-presences and co-absences and the\ntotal number of binary features considered:\nsim S Mpq,dq“CPpq,dq`CApq,dq\n|q|(5.10)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":269,"page_label":"215","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 215\nUsing Sokal-Michener for our online services example q, is judged to be more similar to\ninstance d2than instance d1:\nsim S Mpq,d1q“3\n5“0.6\nsim S Mpq,d2q“4\n5“0.8\nSometimes, however, co-absences aren’t that meaningful. For example, we may be in\na retail domain in which there are so many items that most people haven’t seen, listened\nto, bought, or visited the vast majority of them, and as a result, the majority of features\nwill be co-absences. The technical term to describe a dataset in which most of the features\nhave zero values is sparse data . In these situations we should use a metric that ignores co-\nabsences. The Jaccard similarity index is often used in these contexts. This index ignores\nco-absences and is deﬁned as the ratio between the number of co-presences and the total\nnumber of features, excluding those that record a co-absence between a pair of instances:18\nsim Jpq,dq“CPpq,dq\nCPpq,dq`PApq,dq`APpq,dq(5.11)\nUsing Jaccard similarity, the current trial user in the online retail example is judged to be\nequally similar to instance d1andd2:\nsim Jpq,d1q“2\n4“0.5\nsim Jpq,d2q“1\n2“0.5\nThe fact that the judgment of similarity between current trial user and the other users\nin the dataset changed dramatically depending on which similarity index was employed\nillustrates the importance of choosing the correct index for the task. Unfortunately, beyond\nhighlighting that the Jaccard index is useful for sparse binary data, we cannot give a hard\nand fast rule for how to choose between these indexes. As is so often the case in predictive\nanalytics, making the right choice requires an understanding of the requirements of the\ntask that we are trying to accomplish and matching these requirements with the features\nwe want to emphasize in our model.\n18. One note of caution: the Jaccard similarity index is undeﬁned for pairs of instances in which all the features\nmanifest co-absence, because this leads to a division by zero.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":270,"page_label":"216","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"216 Chapter 5 Similarity-Based Learning\n5.4.5.2 Cosine similarity Cosine similarity is an index that can be used as a measure of\nthe similarity between instances with continuous descriptive features. The cosine similarity\nbetween two instances is the cosine of the inner angle between the two vectors that extend\nfrom the origin of a feature space to each instance. Figure 5.14(a)[218]illustrates the inner\nangle,θ, between the vector from the origin to two instances in a feature space deﬁned by\ntwo descriptive features, SMS and V OICE .\nCosine similarity is an especially useful measure of similarity when the descriptive fea-\ntures describing instances in a dataset are related to each other. For example, in a mobile\ntelecoms scenario, we could represent customers with just two descriptive features: the\naverage number of SMS messages a customer sends per month, and the average number\nof V OICE calls a customer makes per month. In this scenario it is interesting to take a\nperspective on the similarity between customers that focuses on the mix of these two types\nof services they use, rather than the volumes of the services they use. Cosine similarity\nallows us to do this. The instances shown in Figure 5.14(a)[218]are based on this mobile\ntelecoms scenario. The descriptive feature values for d1are SMS = 97 and V OICE = 21,\nand for d2are SMS = 181 and V OICE = 184.\nWe compute the cosine similarity between two instances as the normalized dot product\nof the descriptive feature values of the instances. The dot product is normalized by the\nproduct of the lengths of the descriptive feature value vectors.19The dot product of two\ninstances, aandb, deﬁned by mdescriptive features is\na¨b“mÿ\ni“1parisˆbrisq“par1sˆbr1sq`¨¨¨`parmsˆbrmsq (5.12)\nGeometrically, the dot product can be interpreted as equivalent to the cosine of the angle\nbetween the two vectors multiplied by the length of the two vectors:\na¨b“gffemÿ\ni“1aris2ˆgffemÿ\ni“1bris2ˆcospθq (5.13)\nWe can rearrange Equation (5.13)[216]to calculate the cosine of the inner angle between two\nvectors as the normalized dot product\na¨bdmÿ\ni“1aris2ˆdmÿ\ni“1bris2“cospθq\n(5.14)\n19. The length of a vector, |a|, is computed as the square root of the sum of the elements of the vector squared:\n|a|“řm\ni“1aris2.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":271,"page_label":"217","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 217\nSo, in an m-dimensional feature space, the cosine similarity between two instances aand\nbis deﬁned as\nsimCOS INEpa,bq“a¨bdmÿ\ni“1aris2ˆdmÿ\ni“1bris2\n“mÿ\ni“1parisˆbrisq\ndmÿ\ni“1aris2ˆdmÿ\ni“1bris2(5.15)\nThe cosine similarity between instances will be in the range r0,1s, where 1indicates max-\nimum similarity and 0indicates maximum dissimilarity.20We can calculate the cosine\nsimilarity between d1andd2from Figure 5.14(a)[218]as\nsimCOS INEpd1,d1q“p97ˆ181q`p21ˆ184q?\n972`212ˆ?\n1812`1842\n“0.8362\nFigure 5.14(b)[218]highlights the normalization of descriptive feature values that takes\nplace as part of calculating cosine similarity. This is different from the normalization\nwe have looked at elsewhere in this chapter as it takes place within an instance rather than\nacross all the values of a feature. All instances are normalized so as to lie on a hypersphere\nof radius 1.0with its center at the origin of the feature space. This normalization is what\nmakes cosine similarity so useful in scenarios in which we are interested in the relative\nspread of values across a set of descriptive features rather than the magnitudes of the values\nthemselves. For example, if we have a third instance with SMS “194and V OICE“42,\nthe cosine similarity between this instance and d1will be 1.0, because even though the\nmagnitudes of their feature values are different, the relationship between the feature values\nfor both instances is the same: both customers use about four times as many SMS messages\nas V OICE calls. Cosine similarity is also an appropriate similarity index for sparse data\nwith non-binary features (i.e., datasets with lots of zero values) because the dot product\nwill essentially ignore co-absences in its computation ( 0ˆ0“0).\n5.4.5.3 Mahalanobis distance The ﬁnal measure of similarity that we will introduce\nis the Mahalanobis distance , which is a metric that can be used to measure the similarity\nbetween instances with continuous descriptive features. The Mahalanobis distance is dif-\n20. If either vector used to calculate a cosine similarity contains negative feature values, then the cosine similarity\nwill actually be in the range r´1,1s. As previously, 1indicates high similarity, and 0indicates dissimilarity, but\nit can be difﬁcult to interpret negative similarity scores. Negative similarity values can be avoided, however, if we\nuse range normalization (see Section 3.6.1[87]) to ensure that descriptive feature values always remain positive.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":272,"page_label":"218","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"218 Chapter 5 Similarity-Based Learning\n/uni25CF\n0 50 100 150 2000 50 100 150 200\nSMSVoice\n 1 2\n/uni25CFθ\n(a)\n/uni25CF\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nSMSVoice\n 1 2\n/uni25CFθ (b)\nFigure 5.14\n(a) Theθrepresents the inner angle between the vector emanating from the origin to instance d1and\nthe vector emanating from the origin to instance d2; and (b) shows d1andd2normalized to the unit\ncircle.\nferent from the other distance metrics we have looked at because it allows us to take into\naccount how spread out the instances in a dataset are when judging similarities. Figure\n5.15[219]illustrates why this is important. This ﬁgure shows scatter plots for three bivariate\ndatasets that have the same central tendency, marked Aand located in the feature space\natp50,50q, but whose instances are spread out differently across the feature space. In all\nthree cases the question we would like to answer is, are instance B, located at atp30,70q,\nand instance C, located atp70,70q, likely to be from the same population from which the\ndataset has been sampled? In all three ﬁgures, BandCare equidistant from Abased on\nEuclidean distance.\nThe dataset in Figure 5.15(a)[219]is equally distributed in all directions around A, and\nas a result, we can say that BandCare equally likely to be from the same population\nas the dataset. The dataset in Figure 5.15(b)[219], however, demonstrates a strong negative\ncovariance21between the features. In this context, instance Bis much more likely to be\na member of the dataset than instance C. Figure 5.15(c)[219]shows a dataset with a strong\npositive covariance, and for this dataset, instance Cis much more likely to be a member\nthan instance B. What these examples demonstrate is that when we are trying to decide\nwhether a query belongs to a group, we need to consider not only the central tendency\n21. Covariance between features means that knowing the value of one feature tells us something about the value\nof the other feature. See Section 3.5.2[81]for more information.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":273,"page_label":"219","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 219\n0 20 40 60 80 1000 20 40 60 80 100\nXY\n /uni25CF\nA/uni25CFB /uni25CFC\n(a)\n0 20 40 60 80 1000 20 40 60 80 100\n/uni25CFB /uni25CFC\n/uni25CFA\nXY (b)\n0 20 40 60 80 1000 20 40 60 80 100\n/uni25CFB\n /uni25CFC\n/uni25CFA\nXY (c)\nFigure 5.15\nScatter plots of three bivariate datasets with the same center point Aand two queries BandCboth\nequidistant from A; (a) a dataset uniformly spread around the center point; (b) a dataset with negative\ncovariance; and (c) a dataset with positive covariance.\nof the group, but also how spread out the members in a group are. These examples also\nhighlight that covariance is one way of measuring the spread of a dataset.\nThe Mahalanobis distance uses covariance to scale distances so that distances along a\ndirection where the dataset is very spread out are scaled down, and distances along direc-\ntions where the dataset is tightly packed are scaled up. For example, in Figure 5.15(b)[219]\nthe Mahalanobis distance between BandAwill be less than the Mahalanobis distance be-\ntween CandA, whereas in Figure 5.15(c)[219]the opposite will be true. The Mahalanobis\ndistance is deﬁned as\nMahalanobispa,bq“gfffffe“\nar1s´br1s,..., arms´brms‰\nˆÿ´1ˆ»\n——–ar1s´br1s\n...\narms´brmsﬁ\nﬃﬃﬂ(5.16)\nLet’s step through Equation (5.16)[219]bit by bit. First, this equation computes a distance\nbetween two instances aandb, each with mdescriptive features. The ﬁrst big term we\ncome to in the equation is rar1s´br1s,..., arms´brmss. This is a row vector that is\ncreated by subtracting each descriptive feature value of instance bfrom the corresponding\nfeature values of a. The next term in the equation,ř´1, represents the inverse covariance\nmatrix22computed across all instances in the dataset. Multiplying the difference in feature\n22. We explain covariance matrices in Section 3.5.2[81]. The inverse covariance matrix is the matrix such that\nwhen the covariance matrix is multiplied by its inverse, the result is the identity matrix :řˆř´1“I. The","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":274,"page_label":"220","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"220 Chapter 5 Similarity-Based Learning\nvalues by the inverse covariance matrix has two effects. First, the larger the variance of\na feature, the less weight the difference between the values for that feature will contribute\nto the distance calculation. Second, the larger the correlation between two features, the\nless weight they contribute to the distance. The ﬁnal element of the equation is a column\nvector that is created in the same way as the row vector at the beginning of the equation—\nby subtracting each feature value from bfrom the corresponding feature value from a.\nThe motivation for using a row vector to hold one copy of the feature differences and a\ncolumn vector to hold the second copy of the features differences is to facilitate matrix\nmultiplication. Now that we know that the row and column vector both contain the dif-\nference between the feature values of the two instances, it should be clear that, similar to\nEuclidean distance, the Mahalanobis distance squares the differences of the features. The\nMahalanobis distance, however, also rescales the differences between feature values (using\nthe inverse covariance matrix) so that all the features have unit variance, and the effects of\ncovariance are removed.\nThe Mahalanobis distance can be understood as deﬁning an orthonormal coordinate sys-\ntem with (1) an origin at the instance we are calculating the distance from ( ain Equa-\ntion (5.16)[219]); (2) a primary axis aligned with the direction of the greatest spread in the\ndataset; and (3) the units of all the axes scaled so that the dataset has unit variance along\neach axis. The rotation and scaling of the axes are the result of the multiplication by the\ninverse covariance matrix of the dataset (ř´1). So, if the inverse covariance matrix is the\nidentity matrix I, then no scaling or rotation occurs. This is why for datasets such as the\none depicted in Figure 5.15(a)[219], where there is no covariance between the features, the\nMahalanobis distance is simply the Euclidean distance.23\nFigure 5.16[221]illustrates how the Mahalanobis distance deﬁnes this coordinate system,\nwhich is translated, rotated, and scaled with respect to the standard coordinates of a feature\nspace. The three scatter plots in this image are of the dataset in Figure 5.15(c)[219]. In each\ncase we have overlaid the coordinate system deﬁned by the Mahalanobis distance from a\ndifferent origin. The origins used for the ﬁgures were (a) p50,50q, (b)p63,71q, and (c)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":274,"page_label":"220","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"identity matrix I, then no scaling or rotation occurs. This is why for datasets such as the\none depicted in Figure 5.15(a)[219], where there is no covariance between the features, the\nMahalanobis distance is simply the Euclidean distance.23\nFigure 5.16[221]illustrates how the Mahalanobis distance deﬁnes this coordinate system,\nwhich is translated, rotated, and scaled with respect to the standard coordinates of a feature\nspace. The three scatter plots in this image are of the dataset in Figure 5.15(c)[219]. In each\ncase we have overlaid the coordinate system deﬁned by the Mahalanobis distance from a\ndifferent origin. The origins used for the ﬁgures were (a) p50,50q, (b)p63,71q, and (c)\np42,35q. The dashed lines plot the axes of the coordinate system, and the ellipses plot the\n1,3, and 5unit distance contours. Notice how the orientation of the axes and the scaling\nidentity matrix is a square matrix in which all the elements of the main diagonal are 1, and all other elements are\n0. Multiplying any matrix by the identity matrix leaves the original matrix unchanged—this is the equivalent of\nmultiplying by 1for real numbers. So the effect of multiplying feature values by an inverse covariance matrix is\nto rescale the variances of all features to 1and to set the covariance between all feature pairs to 0. Calculating\nthe inverse of a matrix involves solving systems of linear equations and requires the use of techniques from linear\nalgebra such as Gauss-Jordan elimination orLU decomposition . We do not cover these techniques here, but\nthey are covered in most standard linear algebra textbooks such as Anton and Rorres (2010).\n23. The inverse of the identity matrix IisI. Therefore, if there is no covariance between the features, both the\ncovariance and the inverse covariance matrix will be equal to I.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":275,"page_label":"221","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 221\nof the distance contours are consistent across the ﬁgures. This is because the same inverse\ncovariance matrix based on the entire dataset was used in each case.\n20406080\n20 40 60 80135\n/uni25CF\nXY\n(a)\n20406080\n20 40 60 801\n3\n5/uni25CF\nXY (b)\n20406080\n20 40 60 80135\n/uni25CF\nXY (c)\nFigure 5.16\nThe coordinate systems deﬁned by the Mahalanobis distance using the covariance matrix for the\ndataset in Figure 5.15(c)[219]using three different origins: (a) p50,50q; (b)p63,71q; and (c)p42,35q.\nThe ellipses in each ﬁgure plot the 1,3, and 5unit distance contours.\nLet’s return to the original question depicted in Figure 5.15[219]: Are BandClikely to be\nfrom the same population from which the dataset has been sampled? Focusing on Figure\n5.15(c)[219], for this dataset it appears reasonable to conclude that instance Cis a member\nof the dataset but that Bis probably not. To conﬁrm this intuition we can calculate the\nMahalanobis distance between AandBandAandCusing Equation (5.16)[219]as\nMahalanobispA,Bq\n“gffer50´30,50´70sˆ«\n0.059´0.521\n´0.521 0.0578ﬀ\nˆ«\n50´30\n50´70ﬀ\n“9.4049\nMahalanobispA,Cq\n“gffer50´70,50´70sˆ«\n0.059´0.521\n´0.521 0.0578ﬀ\nˆ«\n50´70\n50´70ﬀ\n“2.2540","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":276,"page_label":"222","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"222 Chapter 5 Similarity-Based Learning\nwhere the inverse covariance matrix used in the calculations is based on the covariance\nmatrix24calculated directly from the dataset:\n«\n82.39 74.26\n74.26 84.22ﬀ\nFigure 5.17[222]shows a contour plot of these Mahalanobis distances. In this ﬁgure, A\nindicates the central tendency of the dataset in Figure 5.15(c)[219], and the ellipses plot the\nMahalanobis distance contours that the distances from Ato the instances BandClie on.\nThese distance contours were calculated using the inverse covariance matrix for the dataset\nand point Aas the origin. The result is that instance Cis much closer to Athan Band so\nshould be considered a member of the same population as this dataset.\nXY\n20406080\n20 40 60 80/uni25CF/uni25CF /uni25CF 2.259.4\nAB C\nFigure 5.17\nThe effect of using a Mahalanobis versus Euclidean distance. Amarks the central tendency of the\ndataset in Figure 5.15(c)[219]. The ellipses plot the Mahalanobis distance contours from AthatBandC\nlie on. In Euclidean terms, BandCare equidistant from A; however, using the Mahalanobis distance,\nCis much closer to Athan B.\n24. Section 3.5.2[81]describes the calculation of covariance matrices. The inverse covariance matrix was calcu-\nlated using the solve function from the Rprogramming language.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":277,"page_label":"223","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 223\nTo use Mahalanobis distance in a nearest neighbor model, we simply use the model\nin exactly the same way as described previously but substitute Mahalanobis distance for\nEuclidean distance.\n5.4.5.4 Summary In this section we have introduced a number of commonly used met-\nrics and indexes for judging similarity between instances in a feature space. These are\ntypically used in situations where a Minkowski distance is not appropriate. For example,\nif we are dealing with binary features, it may be more appropriate to use the Russel-Rao,\nSokal-Michener, or Jaccard similarity metric. Or it may be that the features in the dataset\nare continuous—typically indicating that a Minkowski distance metric is appropriate—but\nthat the majority of the descriptive features for each instance have zero values,25in which\ncase we may want to use a similarity index that ignores descriptive features with zero values\nin both features, for example, cosine similarity . Alternatively, we may be dealing with a\ndataset where there is covariance between the descriptive features, in which case we should\nconsider using the Mahalanobis distance as our measure of similarity. There are many\nother indexes and metrics we could have presented, for example, Tanimoto similarity\n(which is a generalization of the Jaccard similarity to non-binary data), and correlation -\nbased approaches such as the Pearson correlation . The key things to remember, however,\nare that it is important to choose a similarity metric or index that is appropriate for the\nproperties of the dataset we are using (be it binary, non-binary, sparse, covariant, etc.) and\nsecond, experimentation is always required to determine which measure of similarity will\nbe most effective for a speciﬁc prediction model.\n5.4.6 Feature Selection\nIntuitively, adding more descriptive features to a dataset provides more information about\neach instance and should result in more accurate predictive models. Surprisingly, however,\nas the number of descriptive features in a dataset increases, there often comes a point at\nwhich continuing to add new features to the dataset results in a decrease in the predictive\npower of the induced models. The reason for this phenomenon is that, fundamentally, the\npredictive power of an induced model is based on one of the following:\n1.Partitioning the feature space into regions based on clusters of training instances with","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":277,"page_label":"223","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"be most effective for a speciﬁc prediction model.\n5.4.6 Feature Selection\nIntuitively, adding more descriptive features to a dataset provides more information about\neach instance and should result in more accurate predictive models. Surprisingly, however,\nas the number of descriptive features in a dataset increases, there often comes a point at\nwhich continuing to add new features to the dataset results in a decrease in the predictive\npower of the induced models. The reason for this phenomenon is that, fundamentally, the\npredictive power of an induced model is based on one of the following:\n1.Partitioning the feature space into regions based on clusters of training instances with\nthe same target value, and assigning a query located in a region the target value of the\ncluster that deﬁnes that region.\n25. Recall that a dataset in which the majority of descriptive features have zero as the value is known as sparse\ndata . This often occurs in document classiﬁcation problems, when a bag-of-words representation is used to\nrepresent documents as the frequency of occurrence of each word in a dictionary (the eponymous bag-of-words).\nThe bag-of-words representation is covered more in Question 2[236]at the end of this chapter. One problem with\nsparse data is that with so few non-zero values, the variation between two instances may be dominated by noise.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":278,"page_label":"224","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"224 Chapter 5 Similarity-Based Learning\n2.Assigning a query a target value interpolated (for instance, by majority vote or aver-\nage) from the target values of individual training instances that are near the query in\nthe feature space.\nBoth of these strategies depend on a reasonable sampling density of the training in-\nstances across the feature space. The sampling density is the average density of training\ninstances across the feature space. If the sampling density is too low, then large regions of\nthe feature space do not contain any training instances, and it doesn’t make sense to asso-\nciate such a region with any cluster of training instances or to look for training instances\nthat are nearby. In such instances a model is essentially reduced to guessing predictions.\nWe can measure the sampling density across a feature space in terms of the average den-\nsity of a unit hypercube26in the feature space. The density of a unit hypercube is equal\nto\ndensity“kp1\nmq(5.17)\nwhere kis the number of instances inside the hypercube, and mis the number of dimensions\nof the feature space.\nFigure 5.18[226]provides a graphical insight into the relationship between the number of\ndescriptive features in a dataset and the sampling density of the feature space. Figure\n5.18(a)[226]plots a one-dimensional dataset consisting of 29 instances spread evenly be-\ntween 0.0and3.0. We have marked the unit hypercube covering the interval 0 to 1 in this\nﬁgure. The density of this unit hypercube is 101\n1“10(there are 10instances inside the\nhypercube). If we increase the number of descriptive features, the dimensionality of the\nfeature space increases. Figures 5.18(b)[226]and 5.18(c)[226]illustrate what happens if we\nincrease the number of descriptive features in a dataset but do not increase the number of\ninstances. In Figure 5.18(b)[226]we have added a second descriptive feature, Y, and assigned\neach of the instances in the dataset a random Yvalue in the range r0.0,3.0s. The instances\nhave moved away from each other, and the sampling density has decreased. The density of\nthe marked unit hypercube is now 41\n2“2(there are only 4 instances inside the hypercube).\nFigure 5.18(c)[226]illustrates the distribution of the original 29 instances when we move to a\nthree-dimensional feature space (each instance has been given a random value in the range\nr0.0,3.0sfor the Z feature). It is evident that the instances are getting farther and farther","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":278,"page_label":"224","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"increase the number of descriptive features in a dataset but do not increase the number of\ninstances. In Figure 5.18(b)[226]we have added a second descriptive feature, Y, and assigned\neach of the instances in the dataset a random Yvalue in the range r0.0,3.0s. The instances\nhave moved away from each other, and the sampling density has decreased. The density of\nthe marked unit hypercube is now 41\n2“2(there are only 4 instances inside the hypercube).\nFigure 5.18(c)[226]illustrates the distribution of the original 29 instances when we move to a\nthree-dimensional feature space (each instance has been given a random value in the range\nr0.0,3.0sfor the Z feature). It is evident that the instances are getting farther and farther\naway from each other, and the feature space is becoming very sparsely populated, with\nrelatively large areas where there are no or very few instances. This is reﬂected in a further\ndecrease in the sampling density. The density of the marked hypercube is 21\n3“1.2599 .\nFigures 5.18(d)[226]and 5.18(e)[226]illustrate the cost we would have to incur in extra in-\nstances if we wished to maintain the sampling density in the dataset in line with each\n26. A hypercube is a generalization of the geometric concept of a cube across multiple dimensions. Hence in a\ntwo-dimensional space, the term hypercube denotes a square; in three-dimensional space, it denotes a cube; and\nso on. A unit hypercube is a hypercube in which the length of every side is 1unit.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":279,"page_label":"225","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 225\nincrease in the dimensionality of the feature space. In the two-dimensional feature space\nin Figure 5.18(d)[226], we have maintained the sampling density (the density of the marked\nunit hypercube is 1001\n2“10) at the expense of a very large increase in the number of\ninstances—there are 29ˆ29“841instances plotted in this ﬁgure. This is quite a dra-\nmatic increase; however, it gets even more dramatic when we increase from two to three\ndescriptive features. In Figure 5.18(e)[226]we have, again, maintained the sampling density\n(the density of the marked unit hypercube is 10001\n3“10) at the expense of a very large\nincrease in the number of instances—there are 29ˆ29ˆ29“24,389instances in this\nﬁgure!\nSo, in order to maintain the sampling density of the feature space as the number of de-\nscriptive features increases, we need to dramatically, indeed, exponentially, increase the\nnumber of instances. If we do not do this, then as we continue to increase the dimension-\nality of the feature space, the instances will continue to spread out until we reach a point\nin a high-dimensional feature space where most of the feature space is empty. When this\nhappens, most of the queries will be in locations where none of the training instances are\nnearby, and as a result, the predictive power of the models based on these training instances\nwill begin to decrease. This trade-off between the number of descriptive features and the\ndensity of the instances in the feature space is known as the curse of dimensionality .\nTypically, we are not able to increase the number of instances in our dataset, and we face\nthe scenario of a sparsely populated feature space,27as illustrated in Figures 5.18(b)[226]\nand 5.18(c)[226]. Fortunately, several features of real data can help us to induce reasonable\nmodels in high-dimensional feature spaces.28First, although real data does spread out,\nit doesn’t spread out quite as randomly and quickly as we have illustrated here. Real\ninstances tend to cluster. The net effect of this is that the distribution of real data tends to\nhave a lower effective dimensionality than the dimensionality of the feature space. Second,\nwithin any small region or neighborhood of the feature space, real data tends to manifest\na smooth correlation between changes in descriptive feature values and the values of the\ntarget feature. In other words, small changes in descriptive features result in small changes","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":279,"page_label":"225","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"and 5.18(c)[226]. Fortunately, several features of real data can help us to induce reasonable\nmodels in high-dimensional feature spaces.28First, although real data does spread out,\nit doesn’t spread out quite as randomly and quickly as we have illustrated here. Real\ninstances tend to cluster. The net effect of this is that the distribution of real data tends to\nhave a lower effective dimensionality than the dimensionality of the feature space. Second,\nwithin any small region or neighborhood of the feature space, real data tends to manifest\na smooth correlation between changes in descriptive feature values and the values of the\ntarget feature. In other words, small changes in descriptive features result in small changes\nin the target feature. This means that we can generate good predictions for queries by\ninterpolating from nearby instances with known target values.\nAnother factor that can help us deal with the curse of dimensionality is that some learn-\ning algorithms have a natural resistance to the problem. For example, the decision tree\nlearning algorithms we looked at in the last chapter worked by selecting subsets of features\nfrom which to build predictive trees and so naturally reduce dimensionality. Even these al-\ngorithms, however, do eventually succumb to the curse as the dimensionality grows. Other\n27. This should not be confused with the concept of sparse data that was introduced earlier.\n28. The discussion relating to the features of real data that help with the induction of models in high-dimensional\nspaces is based on Bishop (2006), pp. 33–38.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":280,"page_label":"226","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"226 Chapter 5 Similarity-Based Learning\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nX\n(a)\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.00.51.01.52.02.53.0\nY\nX (b)\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.5 1.0 1.5 2.0 2.5 3.0\n0.00.51.01.52.02.53.0\nXZY (c)\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.00.51.01.52.02.53.0\nY\nX\n(d)\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.5 1.0 1.5 2.0 2.5 3.0\n0.00.51.01.52.02.53.0\nXZY (e)\nFigure 5.18\nA set of scatter plots illustrating the curse of dimensionality. Across (a), (b), and (c), the number of\ninstances remains the same, so the density of the marked unit hypercubes decreases as the number\nof dimensions increases; (d) and (e) illustrate the cost we must incur, in terms of the number of extra\ninstances required, if we wish to maintain the density of the instances in the feature space as its\ndimensionality increases.\nalgorithms, such as the nearest neighbor algorithm, which use all the descriptive features\nwhen making a prediction, are particularly sensitive to the curse. The moral here is that\nthe curse of dimensionality is a problem for all inductive learning approaches, and given\nthat acquiring new labeled instances is typically not an option, the best way to avoid it is\nto restrict the number of descriptive features in a dataset to the smallest set possible, while\nstill providing the learning algorithm with enough information about the instances to be\nable to build a useful model. This is difﬁcult, however, because when we design descrip-\ntive features, we tend not to know exactly which ones will be predictive and which ones\nwill not.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":281,"page_label":"227","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 227\nFortunately, we can use feature selection29to help reduce the number of descriptive\nfeatures in a dataset to just the subset that is most useful. Before we begin our discussion\nof approaches to feature selection, it is useful to distinguish between different types of\ndescriptive features.\n‚Predictive: a predictive descriptive feature provides information that is useful in esti-\nmating the correct value of a target feature.\n‚Interacting: by itself, an interacting descriptive feature is not informative about the\nvalue of the target feature. In conjunction with one or more other features, however, it\nbecomes informative.\n‚Redundant: a descriptive feature is redundant if it has a strong correlation with another\ndescriptive feature.\n‚Irrelevant: an irrelevant descriptive feature does not provide information that is useful\nin estimating the value of the target feature.\nThe goal of any feature selection approach is to identify the smallest subset of descriptive\nfeatures that maintains overall model performance. Ideally, a feature selection approach\nwill return the subset of features that includes the predictive and interacting features while\nexcluding the irrelevant and redundant features.\nThe most popular and straightforward approach to feature selection is to rank and\nprune . In this approach the features are ranked using a measure of their predictiveness,\nand any feature outside the top X%of the features in the list is pruned. The measures of\npredictiveness are called ﬁlters because they are used to ﬁlter apparently irrelevant features\nbefore learning occurs. Technically, a ﬁlter can be deﬁned as a heuristic rule that assesses\nthe predictiveness of a feature using only the intrinsic properties of the data, independently\nof the learning algorithm that will use the features to induce the model. For example, we\ncan use information gain30as a ﬁlter in a rank and prune approach.\nAlthough rank and prune approaches using ﬁlters are computationally efﬁcient, they suf-\nfer from the fact that the predictiveness of each feature is evaluated in isolation from the\nother features in the dataset. This leads to the undesirable result that ranking and pruning\ncan exclude interacting features and include redundant features.\nTo ﬁnd the ideal subset of descriptive features to use to train a model, we could attempt\nto build a model using every possible subset, evaluate the performance of all these models,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":281,"page_label":"227","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of the learning algorithm that will use the features to induce the model. For example, we\ncan use information gain30as a ﬁlter in a rank and prune approach.\nAlthough rank and prune approaches using ﬁlters are computationally efﬁcient, they suf-\nfer from the fact that the predictiveness of each feature is evaluated in isolation from the\nother features in the dataset. This leads to the undesirable result that ranking and pruning\ncan exclude interacting features and include redundant features.\nTo ﬁnd the ideal subset of descriptive features to use to train a model, we could attempt\nto build a model using every possible subset, evaluate the performance of all these models,\nand select the feature subset that leads to the best model. This is unfeasible, however,\nas for dfeatures, there are 2ddifferent possible feature subsets, which is far too many to\nevaluate unless dis very small. For example, with just 20 descriptive features, there are\n220“1,048,576possible feature subsets. Instead, feature selection algorithms often frame\n29. Feature selection is sometimes also known as variable selection .\n30. See Section 4.2.3[127].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":282,"page_label":"228","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"228 Chapter 5 Similarity-Based Learning\n \n \n X\n \n \n \nY\n \n \n \nZX\nY\n \nX\n \nZ\n \nY\nZX\nY\nZBackward Selection Forward Selection\nFigure 5.19\nFeature subset space for a dataset with three features X, Y, and Z.\nfeature selection as a greedy local search problem , where each state in the search space\nspeciﬁes a subset of possible features. For example, Figure 5.19[228]illustrates a feature\nsubset space for a dataset with three descriptive features: X, Y, and Z. In this ﬁgure\neach rectangle represents a state in the search space that is a particular feature subset. For\ninstance, the rectangle on the very left represents the feature subset that includes no features\nat all, and the rectangle at the top of the second column from the left represents the feature\nsubset including just the feature X. Each state is connected to all the other states that can\nbe generated by adding or removing a single feature from that state. A greedy local search\nprocess moves across a feature subset space like this search in order to ﬁnd the best feature\nsubset.\nWhen framed as a greedy local search problem, feature selection is deﬁned in terms of\nan iterative process consisting of the following components:\n1.Subset Generation : This component generates a set of candidate feature subsets that\nare successors of the current best feature subset.\n2.Subset Selection : This component selects the feature subset from the set of candidate\nfeature subsets generated by the subset generation component that is the most desir-\nable for the search process to move to. One way to do this (similar to the ranking and\npruning approach described previously) is to use a ﬁlter to evaluate the predictiveness\nof each candidate set of features and select the most predictive one. A more com-\nmon approach is to use a wrapper . A wrapper evaluates a feature subset in terms of\nthe potential performance of the models that can be induced using that subset. This\ninvolves performing an evaluation experiment31for each candidate feature subset, in\n31. We discuss the design of evaluation experiments in detail in Chapter 9[533].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":283,"page_label":"229","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.4 Extensions and Variations 229\nwhich a model is induced using only the features in the subset, and its performance\nis evaluated. The candidate feature subset that leads to the best-performing model is\nthen selected. Wrapper approaches are more computationally expensive than ﬁlters,\nas they involve training multiple models during each iteration. The argument for using\na wrapper approach is that to get the best predictive accuracy, the inductive bias of the\nparticular machine learning algorithm that will be used should be taken into consider-\nation during feature selection. That said, ﬁlter approaches are faster and often result\nin models with good accuracy.\n3.Termination Condition : This component determines when the search process should\nstop. Typically we stop when the subset selection component indicates that none of\nthe feature subsets (search states) that can be generated from the current feature subset\nis more desirable than the current subset. Once the search process is terminated, the\nfeatures in the dataset that are not members of the selected feature subset are pruned\nfrom the dataset before the prediction model is induced.\nForward sequential selection is a commonly used implementation of the greedy local\nsearch approach to feature selection. In forward sequential selection, the search starts\nin a state with no features (shown on the left of Figure 5.19[228]). In the subset generation\ncomponent of forward sequential selection, the successors of the current best feature subset\nare the set of feature subsets that can be generated from the current best subset by adding\njust a single extra feature. For example, after beginning with the feature subset including\nno features, the forward sequential search process generates three feature subsets, each\ncontaining just one of X, Y, or Z (shown in the second column of Figure 5.19[228]). The\nsubset selection component in forward sequential selection can use any of the approaches\ndescribed above and moves the search process to a new feature subset. For example, after\nstarting with the feature subset including no features, the process will move to the most\ndesirable of the feature subsets containing just one feature. Forward sequential selection\nterminates when no accessible feature subset is better than the current subset.\nBackward sequential selection is a popular alternative to forward sequential selection.\nIn backward sequential selection, we start with a feature subset including all the possible","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":283,"page_label":"229","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"containing just one of X, Y, or Z (shown in the second column of Figure 5.19[228]). The\nsubset selection component in forward sequential selection can use any of the approaches\ndescribed above and moves the search process to a new feature subset. For example, after\nstarting with the feature subset including no features, the process will move to the most\ndesirable of the feature subsets containing just one feature. Forward sequential selection\nterminates when no accessible feature subset is better than the current subset.\nBackward sequential selection is a popular alternative to forward sequential selection.\nIn backward sequential selection, we start with a feature subset including all the possible\nfeatures in a dataset (shown on the right of Figure 5.19[228]). The successors of the current\nbest feature subset generated in backward sequential selection are the set of feature subsets\nthat can be generated from the current best subset by removing just a single extra feature.\nBackward sequential selection terminates when no accessible feature subset is better than\nor as good as the current subset.\nNeither forward nor backward sequential selection consider the effect of adding or re-\nmoving combinations of features, and as a result, they aren’t guaranteed to ﬁnd the abso-\nlute optimal subset of features. So which approach should we use? Forward sequential\nselection is a good approach if we expect lots of irrelevant features in the dataset, because\ntypically it results in a lower overall computational cost for feature selection due to the fact","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":284,"page_label":"230","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"230 Chapter 5 Similarity-Based Learning\nFigure 5.20\nThe process of model induction with feature selection.\nthat on average it generates smaller feature subsets. This efﬁciency gain, however, is at the\ncost of the likely exclusion of interacting features. Backward sequential selection has the\nadvantage that it allows for the inclusion of sets of interacting features that individually\nmay not be predictive (because all features are included at the beginning), with the extra\ncomputational cost of evaluating larger feature subsets. So if model performance is more\nimportant than computational considerations, backward sequential selection may be the\nbetter option; otherwise use forward sequential selection.\nFigure 5.20[230]illustrates how ﬁlter selection ﬁts into the model induction process. It\nis important to remember that feature selection can be used in conjunction with almost\nany machine learning algorithm, not just similarity-based approaches. Feature selection\nis appropriate when there are large numbers of features, so we do not present a worked\nexample here. We do, however, discuss the application of feature selection in the case\nstudy in Chapter 13[703].\n5.5 Summary\nSimilarity-based prediction models attempt to mimic a very human way of reasoning by\nbasing predictions for a target feature value on the most similar instances in memory. The","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":285,"page_label":"231","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.5 Summary 231\nfact that similarity-based models attempt to mimic a way of reasoning that is natural to\nhumans makes them easy to interpret and understand. This advantage should not be under-\nestimated. In a business context where people are using models to inform decision making,\nbeing able to understand how the model works gives people more conﬁdence in the model\nand, hence, in the insight that it provides.\nThe standard approach to implementing a similarity-based prediction model is the near-\nest neighbor algorithm . This algorithm is built on two fundamental concepts: (1) a fea-\nture space , and (2) measures of similarity between instances within the feature space. In\nthis chapter we presented a range of measures of similarity, including distance metrics\n(such as the Euclidean ,Manhattan , and Mahalanobis ) and similarity indexes (such as\ntheRussel-Rao ,Sokal-Michener ,Jaccard , and Cosine ). Each of these measures is suit-\nable for different types of data, and matching the appropriate measure to the data is an\nimportant step in inducing an accurate similarity-based prediction model.\nA point that we didn’t discuss in this chapter is that it is possible to create custom mea-\nsures for datasets with both continuous and categorical descriptive features by combining\nmeasures. For example, we might use a Euclidean distance metric to handle the continuous\nfeatures in a dataset and the Jaccard similarity index to handle the categorical features. The\noverall measure of similarity could then be based on a weighted combination of the two.\nBy combining measures in this way, we can apply nearest neighbor models to any dataset.\nCustom metrics aside, the standard distance metrics and similarity indexes weight all\nfeatures equally. Consequently, the predictions made by a nearest neighbor model are\nbased on the full set of descriptive features in a dataset. This is not true of all prediction\nmodels. For example, the predictions made by decision tree models are based on the subset\nof descriptive features tested on the path from the root of the tree to the leaf node that\nspeciﬁes the prediction. The fact that nearest neighbor models use the full set of descriptive\nfeatures when making a prediction makes them particularly sensitive to the occurrence of\nmissing descriptive feature values. In Section 3.4[69]we introduced a number of techniques\nfor handling missing values , and particular care should be taken to handle missing values","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":285,"page_label":"231","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"features equally. Consequently, the predictions made by a nearest neighbor model are\nbased on the full set of descriptive features in a dataset. This is not true of all prediction\nmodels. For example, the predictions made by decision tree models are based on the subset\nof descriptive features tested on the path from the root of the tree to the leaf node that\nspeciﬁes the prediction. The fact that nearest neighbor models use the full set of descriptive\nfeatures when making a prediction makes them particularly sensitive to the occurrence of\nmissing descriptive feature values. In Section 3.4[69]we introduced a number of techniques\nfor handling missing values , and particular care should be taken to handle missing values\nif a nearest neighbor model is being used. The same is true for large range variations across\nthe descriptive features in a dataset and normalization techniques (like those described in\nSection 3.6.1[87]) should almost always be applied when nearest neighbor models are used.\nNearest neighbor models are essentially a composition of a set of local models (recall\nour discussion on Voronoi tessellation ) with the predictions made being a function of the\ntarget feature value of the instance in the dataset closest to the query. As a result, these\nmodels are very sensitive to noise in the target feature. The easiest way to solve this\nproblem is to employ a knearest neighbor model, which uses a function of the target\nfeature values of the kclosest instances to a query. Care must be taken, however, when\nselecting the parameter k, particularly when working with imbalanced datasets.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":286,"page_label":"232","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"232 Chapter 5 Similarity-Based Learning\nNearest neighbor models are also sensitive to the presence of redundant and irrelevant\ndescriptive features in training data. Consequently, feature selection is a particularly im-\nportant process for nearest neighbor algorithms. Feature selection excludes redundant and\nirrelevant features from the induction process and by doing so alleviates the curse of di-\nmensionality . The fact that we have emphasized feature selection in this chapter does not\nmean that it is not important to predictive analytics in general. The issue with redundant\nand irrelevant features is inherent in any large dataset, and the feature selection techniques\ndescribed in this chapter are generally applicable when any type of machine learning algo-\nrithm is being used.\nFinally, the nearest neighbor algorithm is what is known as a lazy learner . This con-\ntrasts with eager learners , such as the information-based (Chapter 4[117]), probability-based\n(Chapter 6[243]), and error-based (Chapter 7[311]) approaches to machine learning described\nin other chapters in this book. The distinction between easy learners and lazy learners\nis based on when the algorithm abstracts from the data. The nearest neighbor algorithm\ndelays abstracting from the data until it is asked to make a prediction. At this point the\ninformation in the query is used to deﬁne a neighborhood in the feature space, and a pre-\ndiction is made based on the instances in this neighborhood. Eager learners abstract away\nfrom the data during training and use this abstraction to make predictions, rather than di-\nrectly comparing queries with instances in the dataset. The decision trees described in\nChapter 4[117]are an example of this type of abstraction. One consequence of abstracting\naway from the training data is that models induced using an eager learning algorithm are\ntypically faster at making predictions than models based on a lazy learner. In the case of\na nearest neighbor algorithm, as the number of instances becomes large, the model will\nbecome slower because it has more instances to check when deﬁning the neighborhood.\nTechniques such as the k-dtree can help with this issue by creating a fast index at the cost\nof some preprocessing. This means that a nearest neighbor model may not be appropriate\nin domains where speed of prediction is of the essence.\nAn advantage of the lazy learning strategy, however, is that similarity-based machine","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":286,"page_label":"232","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"away from the training data is that models induced using an eager learning algorithm are\ntypically faster at making predictions than models based on a lazy learner. In the case of\na nearest neighbor algorithm, as the number of instances becomes large, the model will\nbecome slower because it has more instances to check when deﬁning the neighborhood.\nTechniques such as the k-dtree can help with this issue by creating a fast index at the cost\nof some preprocessing. This means that a nearest neighbor model may not be appropriate\nin domains where speed of prediction is of the essence.\nAn advantage of the lazy learning strategy, however, is that similarity-based machine\nlearning approaches are robust to concept drift. Concept drift is a phenomenon that occurs\nwhen the relationship between the target feature and the descriptive features changes over\ntime. For example, the characteristics of spam emails change both cyclically through the\nyear (typical spam emails at Christmastime are different from typical spam at other times\nof the year) and also longitudinally (spam in 2014 is very different from spam in 1994).\nIf a prediction task is affected by concept drift, an eager learner may not be appropriate\nbecause the abstraction induced during training will go out of date, and the model will\nneed to be retrained at regular intervals, a costly exercise. A nearest neighbor algorithm\ncan be updated without retraining. Each time a prediction is made, the query instance","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":287,"page_label":"233","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.6 Further Reading 233\ncan be added into the dataset and used in subsequent predictions.32In this way, a nearest\nneighbor model can be easily updated, which makes it relatively robust to concept drift (we\nwill return to concept drift in Section 9.4.6[578]).\nTo conclude, the weaknesses of similarity-based learning approaches are that they are\nsensitive to the curse of dimensionality, they are slower than other models at making pre-\ndictions (particularly with very large datasets), and they may not be able to achieve the\nsame levels of accuracy as other learning approaches. The strengths of these models, how-\never, are that they are easy to interpret, they can handle different types of descriptive fea-\ntures, they are relatively robust to noise (when kis set appropriately), and they may be\nmore robust to concept drift than models induced by eager learning algorithms.\n5.6 Further Reading\nNearest neighbor models are based on the concepts of a feature space and measures of\nsimilarity within this feature space. We have claimed that this is a very natural way for hu-\nmans to think, and indeed, there is evidence from cognitive science to support a geometric\nbasis to human thought (G ¨adenfors, 2004). G ¨adenfors (2004) also provides an excellent\nintroduction and overview of distance metrics.\nChapter 13 of Hastie et al. (2009) gives an introduction to the statistical theory underpin-\nning nearest neighbor models. The measure used to judge similarity is a key element in a\nnearest neighbor model. In this chapter, we have described a number of different distance\nmetrics and similarity indexes. Cunningham (2009) provides a broader introduction to the\nrange of metrics and indexes that are available.\nEfﬁciently indexing and accessing memory is an important consideration in scaling near-\nest neighbor models to large datasets. In this chapter we have shown how k-dtrees (Bent-\nley, 1975; Friedman et al., 1977) can be used to speed up the retrieval of nearest neighbors.\nThere are, however, alternatives to k-dtrees. Samet (1990) gives an introduction to r-\ntrees and other related approaches. More recently, hash-based indexes, such as locality\nsensitive hashing , have been developed. Andoni and Indyk (2006) provide a survey of\nthese hash-based approaches. Another approach to scaling nearest neighbor models is to\nremove redundant or noisy instances from the dataset in which we search for neighbors.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":287,"page_label":"233","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Efﬁciently indexing and accessing memory is an important consideration in scaling near-\nest neighbor models to large datasets. In this chapter we have shown how k-dtrees (Bent-\nley, 1975; Friedman et al., 1977) can be used to speed up the retrieval of nearest neighbors.\nThere are, however, alternatives to k-dtrees. Samet (1990) gives an introduction to r-\ntrees and other related approaches. More recently, hash-based indexes, such as locality\nsensitive hashing , have been developed. Andoni and Indyk (2006) provide a survey of\nthese hash-based approaches. Another approach to scaling nearest neighbor models is to\nremove redundant or noisy instances from the dataset in which we search for neighbors.\nFor example, the condensed nearest neighbor approach (Hart, 1968) was one of the ear-\nliest attempts at this and removes the instances not near target level boundaries in a feature\nspace, as they are not required to make predictions. More recent attempts to do this include\nSegata et al. (2009) and Smyth and Keane (1995).\nNearest neighbor models are often used in text analytics applications. Daelemans and\nvan den Bosch (2005) discuss why nearest neighbor models are so suitable for text ana-\nlytics. Widdows (2004) provides a very readable and interesting introduction to geometry\n32. Obviously we must verify that the prediction made was correct before adding a new instance to the dataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":288,"page_label":"234","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"234 Chapter 5 Similarity-Based Learning\nFigure 5.21\nA duck-billed platypus. This platypus image was created by Jan Gillbank, English for the Australian\nCurriculum website (www.e4ac.edu.au). Used under Creative Commons Attribution 3.0 license.\nand linguistic meaning; see, in particular, Chapter 4 for an excellent introduction to sim-\nilarity and distance. For a more general textbook on natural language processing , we\nrecommend Jurafsky and Martin (2008). Finally, nearest neighbor models are the basis\nofcase-based reasoning (CBR ), which is an umbrella term for applications based on\nsimilarity-based machine learning. Richter and Weber (2013) is a good introduction, and\noverview, to CBR.\n5.7 Epilogue\nReturning to 1798 and HMS Calcutta , the next day you accompany your men on the expe-\ndition up the river, and you encounter the strange animal the sailor had described to you.\nThis time when you see the animal yourself, you realize that it deﬁnitely isn’t a duck! It\nturns out that you and your men are the ﬁrst Europeans to encounter a platypus.33\nThis epilogue illustrates two important, and related, aspects of supervised machine learn-\ning. First, supervised machine learning is based on the stationarity assumption , which\nstates that the data doesn’t change—it remains stationary—over time. One implication of\nthis assumption is that supervised machine learning assumes that new target levels—such\nas previously unknown animals—don’t suddenly appear in the data from which queries\nthat are input to the model are sampled. Second, in the context of predicting categorical\ntargets, supervised machine learning creates models that distinguish between the target lev-\nels that are present in the dataset from which they are induced. So if a prediction model is\ntrained to distinguish between lions, frogs, and ducks, the model will classify every query\ninstance as being either a lion, a frog, or a duck—even if the query is actually a platypus.\n33. The story recounted here of the discovery of the platypus is loosely based on real events. See Eco (1999) for\na more faithful account of what happened and for a discussion of the implications of this discovery for classi-\nﬁcation systems in general. The platypus is not the only animal from Australia whose discovery by Europeans\nhas relevance to predictive machine learning. See Taleb (2008) regarding the discovery of black swans and its\nrelevance to predictive models.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":289,"page_label":"235","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.7 Epilogue 235\nCreating models that can identify queries as being sufﬁciently different from what was\nin a training dataset so as to be considered a new type of entity is a difﬁcult research\nproblem. Some of the areas of research relevant to this problem include outlier detection\nandone-class classiﬁcation .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":290,"page_label":"236","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"236 Chapter 5 Similarity-Based Learning\n5.8 Exercises\n1.The table below lists a dataset that was used to create a nearest neighbor model that\npredicts whether it will be a good day to go surﬁng.\nID W AVE SIZE(FT) W AVE PERIOD (SECS ) W INDSPEED (MPH ) G OOD SURF\n1 6 15 5 yes\n2 1 6 9 no\n3 7 10 4 yes\n4 7 12 3 yes\n5 2 2 10 no\n6 10 2 20 no\nAssuming that the model uses Euclidean distance to ﬁnd the nearest neighbor, what\nprediction will the model return for each of the following query instances?\nID W AVE SIZE(FT) W AVE PERIOD (SECS ) W INDSPEED (MPH ) G OOD SURF\nQ1 8 15 2 ?\nQ2 8 2 18 ?\nQ3 6 11 4 ?\n2.Email spam ﬁltering models often use a bag-of-words representation for emails. In\na bag-of-words representation, the descriptive features that describe a document (in\nour case, an email) each represent how many times a particular word occurs in the\ndocument. One descriptive feature is included for each word in a predeﬁned dictio-\nnary. The dictionary is typically deﬁned as the complete set of words that occur in the\ntraining dataset. The table below lists the bag-of-words representation for the follow-\ning ﬁve emails and a target feature, S PAM, whether they are spam emails or genuine\nemails:\n‚“money, money, money ”\n‚“free money for free gambling fun ”\n‚“gambling for fun ”\n‚“machine learning for fun, fun, fun ”\n‚“free machine learning ”\nBag-of-Words\nID MONEY FREE FOR GAMBLING FUN MACHINE LEARNING SPAM\n1 3 0 0 0 0 0 0 true\n2 1 2 1 1 1 0 0 true\n3 0 0 1 1 1 0 0 true\n4 0 0 1 0 3 1 1 false\n5 0 1 0 0 0 1 1 false","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":291,"page_label":"237","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.8 Exercises 237\n(a)What target level would a nearest neighbor model using Euclidean distance return\nfor the following email: “ machine learning for free ”?\n(b)What target level would a k-NN model with k“3and using Euclidean distance\nreturn for the same query?\n(c)What target level would a weighted k-NN model with k“5and using a weighting\nscheme of the reciprocal of the squared Euclidean distance between the neighbor\nand the query, return for the query?\n(d)What target level would a k-NN model with k“3and using Manhattan distance\nreturn for the same query?\n(e)There are a lot of zero entries in the spam bag-of-words dataset. This is indicative\nofsparse data and is typical for text analytics. Cosine similarity is often a good\nchoice when dealing with sparse non-binary data. What target level would a 3-NN\nmodel using cosine similarity return for the query?\n3.The predictive task in this question is to predict the level of corruption in a country\nbased on a range of macroeconomic and social features. The table below lists some\ncountries described by the following descriptive features:\n‚LIFEEXP., the mean life expectancy at birth\n‚TOP-10 I NCOME , the percentage of the annual income of the country that goes to\nthe top 10% of earners\n‚INFANT MORT., the number of infant deaths per 1,000births\n‚MIL. SPEND , the percentage of GDP spent on the military\n‚SCHOOL YEARS , the mean number years spent in school by adult females\nThe target feature is the Corruption Perception Index (CPI). The CPI measures\nthe perceived levels of corruption in the public sector of countries and ranges from 0\n(highly corrupt) to 100(very clean).34\n34. The data listed in this table is real and is for 2010/11 (or the most recent year prior to 2010/11 when the data\nwas available). The data for the descriptive features in this table was amalgamated from a number of surveys\nretrieved from Gapminder (www.gapminder.org). The Corruption Perception Index is generated annually by\nTransparency International (www.transparency.org).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":292,"page_label":"238","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"238 Chapter 5 Similarity-Based Learning\nCOUNTRY LIFE TOP-10 I NFANT MIL. S CHOOL\nID E XP. I NCOME MORT. S PEND YEARS CPI\nAfghanistan 59.61 23.21 74.30 4.44 0.40 1.5171\nHaiti 45.00 47.67 73.10 0.09 3.40 1.7999\nNigeria 51.30 38.23 82.60 1.07 4.10 2.4493\nEgypt 70.48 26.58 19.60 1.86 5.30 2.8622\nArgentina 75.77 32.30 13.30 0.76 10.10 2.9961\nChina 74.87 29.98 13.70 1.95 6.40 3.6356\nBrazil 73.12 42.93 14.50 1.43 7.20 3.7741\nIsrael 81.30 28.80 3.60 6.77 12.50 5.8069\nUSA 78.51 29.85 6.30 4.72 13.70 7.1357\nIreland 80.15 27.23 3.50 0.60 11.50 7.5360\nUK 80.09 28.49 4.40 2.59 13.00 7.7751\nGermany 80.24 22.07 3.50 1.31 12.00 8.0461\nCanada 80.99 24.79 4.90 1.42 14.20 8.6725\nAustralia 82.09 25.40 4.20 1.86 11.50 8.8442\nSweden 81.43 22.18 2.40 1.27 12.80 9.2985\nNew Zealand 80.67 27.81 4.90 1.13 12.30 9.4627\nWe will use Russia as our query country for this question. The table below lists the\ndescriptive features for Russia.\nCOUNTRY LIFE TOP-10 I NFANT MIL. S CHOOL\nID E XP. I NCOME MORT. S PEND YEARS CPI\nRussia 67.62 31.68 10.00 3.87 12.90 ?\n(a)What value would a 3-nearest neighbor prediction model using Euclidean distance\nreturn for the CPI of Russia?\n(b)What value would a weighted k-NN prediction model return for the CPI of Rus-\nsia? Use k“16(i.e., the full dataset) and a weighting scheme of the reciprocal of\nthe squared Euclidean distance between the neighbor and the query.\n(c)The descriptive features in this dataset are of different types. For example, some\nare percentages, others are measured in years, and others are measured in counts\nper1,000. We should always consider normalizing our data, but it is particularly\nimportant to do this when the descriptive features are measured in different units.\nWhat value would a 3-nearest neighbor prediction model using Euclidean distance\nreturn for the CPI of Russia when the descriptive features have been normalized\nusing range normalization?\n(d)What value would a weighted k-NN prediction model—with k“16(i.e., the full\ndataset) and using a weighting scheme of the reciprocal of the squared Euclidean\ndistance between the neighbor and the query—return for the CPI of Russia when\nit is applied to the range-normalized data?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":293,"page_label":"239","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.8 Exercises 239\n(e)The actual 2011 CPI for Russia was 2.4488 . Which of the predictions made was\nthe most accurate? Why do you think this was?\n˚4.You have been given the job of building a recommender system for a large online shop\nthat has a stock of over 100,000items. In this domain the behavior of customers is\ncaptured in terms of what items they have bought or not bought. For example, the\nfollowing table lists the behavior of two customers in this domain for a subset of the\nitems that at least one of the customers has bought.\nITEM ITEM ITEM ITEM ITEM\nID 107 498 7256 28063 75328\n1 true true true false false\n2 true false false true true\n(a)The company has decided to use a similarity-based model to implement the rec-\nommender system. Which of the following three similarity indexes do you think\nthe system should be based on?\nRussell-Rao(X,Y) “CPpX,Yq\nP\nSokal-Michener(X,Y) “CPpX,Yq`CApX,Yq\nP\nJaccard(X,Y)“CPpX,Yq\nCPpX,Yq`PApX,Yq`APpX,Yq\n(b)What items will the system recommend to the following customer? Assume that\nthe recommender system uses the similarity index you chose in the ﬁrst part of this\nquestion and is trained on the sample dataset listed above. Also assume that the\nsystem generates recommendations for query customers by ﬁnding the customer\nmost similar to them in the dataset and then recommending the items that this\nsimilar customer has bought but that the query customer has not bought.\nITEM ITEM ITEM ITEM ITEM\nID 107 498 7256 28063 75328\nQuery true false true false false\n˚5.You are working as an assistant biologist to Charles Darwin on the Beagle voyage.\nYou are at the Gal ´apagos Islands, and you have just discovered a new animal that\nhas not yet been classiﬁed. Mr. Darwin has asked you to classify the animal using a\nnearest neighbor approach, and he has supplied you the following dataset of already\nclassiﬁed animals.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":294,"page_label":"240","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"240 Chapter 5 Similarity-Based Learning\nID\nBIRTHS LIVE\nYOUNG\nLAYS EGGS\nFEEDS OFFSPRING\nOWNMILK\nWARM -BLOODED\nCOLD-BLOODED\nLAND AND WATER\nBASED\nHASHAIR\nHASFEATHERS\nCLASS\n1 true false true true false false true false mammal\n2 false true false false true true false false amphibian\n3 true false true true false false true false mammal\n4 false true false true false true false true bird\nThe descriptive features of the mysterious newly discovered animal are as follows:\nID\nBIRTHS LIVE\nYOUNG\nLAYS EGGS\nFEEDS OFFSPRING\nOWNMILK\nWARM -BLOODED\nCOLD-BLOODED\nLAND AND WATER\nBASED\nHASHAIR\nHASFEATHERS\nCLASS\nQuery false true false false false true false false ?\n(a)A good measure of distance between two instances with categorical features is\ntheoverlap metric (also known as the hamming distance ), which simply counts\nthe number of descriptive features that have different values. Using this measure\nof distance, compute the distances between the mystery animal and each of the\nanimals in the animal dataset.\n(b)If you used a 1-NN model, what class would be assigned to the mystery animal?\n(c)If you used a 4-NN model, what class would be assigned to the mystery animal?\nWould this be a good value for kfor this dataset?\n˚6.You have been asked by a San Francisco property investment company to create a\npredictive model that will generate house price estimates for properties they are con-\nsidering purchasing as rental properties. The table below lists a sample of properties\nthat have recently been sold for rental in the city. The descriptive features in this\ndataset are S IZE(the property size in square feet) and R ENT (the estimated monthly\nrental value of the property in dollars). The target feature, P RICE , lists the prices that\nthese properties were sold for in dollars.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":295,"page_label":"241","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5.8 Exercises 241\nID S IZE RENT PRICE\n1 2,700 9,235 2,000,000\n2 1,315 1,800 820,000\n3 1,050 1,250 800,000\n4 2,200 7,000 1,750,000\n5 1,800 3,800 1,450,500\n6 1,900 4,000 1,500,500\n7 960 800 720,000\n(a)Create a k-dtree for this dataset. Assume the following order over the features:\nRENT then S IZE.\n(b)Using the k-dtree that you created in the ﬁrst part of this question, ﬁnd the nearest\nneighbor to the following query: S IZE=1,000, RENT =2,200.\n˚7.A data analyst building a k-nearest neighbor model for a continuous prediction prob-\nlem is considering appropriate values to use for k.\n(a)Initially the analyst uses a simple average of the target variables for the knearest\nneighbors in order to make a new prediction. After experimenting with values for\nkin the range 0´10, it occurs to the analyst that they might get very good results\nif they set kto the total number of instances in the training set. Do you think that\nthe analyst is likely to get good results using this value for k?\n(b)If the analyst was using a distance weighted averaging function rather than a sim-\nple average for his or her predictions, would this have made the analyst’s idea any\nmore useful?\n˚8.The following table describes a set of individuals in terms of their W EIGHT in kilo-\ngrams, H EIGHT in meters, and whether or not they have D IABETES :\nID W EIGHT HEIGHT DIABETES\n1 68 1.7 true\n2 55 1.6 false\n3 65 1.6 true\n4 100 1.9 true\n5 65 1.5 false\n(a)A doctor has carried out a regular checkup on a patient and measured the patient’s\nWEIGHT to be 65 kilograms and their H EIGHT to be 1.7 meters. The doctor\ninputs these details into a k-NN classiﬁer to check whether the patient is at risk of\nDIABETES . Assuming that k“1, and that the model uses Euclidean distance as\nits similarity metric, will the model return trueorfalse for this patient?\n(b)Clinicians often use BMI as a combined measure of an individual’s W EIGHT and\nHEIGHT . BMI is deﬁned as an individual’s weight in kilograms divided by their\nheight in meters-squared. Assuming that the proﬁles of the ﬁve individuals in the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":296,"page_label":"242","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"242 Chapter 5 Similarity-Based Learning\nsystem were updated so that the features W EIGHT and H EIGHT were replaced by\na single feature BMI and also that the doctor entered the patient’s BMI into the\nsystem, what prediction would the system return for this patient?\n˚9.A lecturer is about to leave for the airport to go on vacation when they ﬁnd a script\nfor a student they forgot to mark. They don’t have time to manually grade the script\nbefore the ﬂight, so they decide to use a k-nearest neighbor model to grade it instead.\nThe model is designed to award a grade to a student on the basis of how similar they\nare to other students in the module in terms of their grades on other modules. The\nfollowing table describes a set of students in terms of their grades out of 100 on two\nother modules (M ODULE 1 and M ODULE 2) and the G RADE they got in the lecturer’s\nmodule: ﬁrst-class honors ,second-class honors ,pass, orfail.\nID M ODULE 1 M ODULE 2 G RADE\n1 55 85 ﬁrst\n2 45 30 fail\n3 40 20 fail\n4 35 35 fail\n5 55 75 pass\n6 50 95 second\n(a)Looking up the results on the other modules of the student whose script hasn’t been\ncorrected, the lecturer ﬁnds that the student got the following marks: M ODULE\n1=60, and M ODULE 2=85. Assuming that the k-nearest neighbor model uses k=1\nand Euclidean distance as its similarity metric, what G RADE would the model\nassign the student?\n(b)Reviewing the spread of marks for the other two modules, the lecturer notices that\nthere is a larger variance across students in the marks for Module 2 than there is for\nModule 1 . So, the lecturer decides to update the k-nearest neighbor model to use\ntheMahalanobis distance instead of Euclidean distance as its similarity measure.\nAssuming that the inverse covariance matrix for the Module 1 andModule 2\nresults is\n´1ÿ\n“«\n0.046´0.009\n´0.009 0.003ﬀ\nwhat G RADE would the k-nearest neighbor model assign the student?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":297,"page_label":"243","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6 Probability-Based Learning\n“When my information changes, I alter my conclusions. What do you do, sir?”\n—John Maynard Keynes\nIn this chapter we introduce probability-based approaches to machine learning. Probability-\nbased prediction approaches are heavily based on Bayes’ Theorem , and the fundamentals\nsection of this chapter introduces this important cornerstone of computer science after cov-\nering some other fundamentals of probability theory . We then present the naive Bayes\nmodel , the standard approach to using probability-based approaches to machine learning.\nThe extensions and variations to this standard approach that we describe are the use of\nsmoothing to combat overﬁtting, the modiﬁcations required to the standard naive Bayes\nmodel to allow it to handle continuous features, and Bayesian network models that give us\nmore control than a naive Bayes model over the assumptions that are encoded in a model.\n6.1 Big Idea\nImagine that you are at a county fair and a stall owner is offering all comers a game of ﬁnd\nthe lady .Find the lady is a card game that hucksters have been using to earn money from\nunsuspecting marks for centuries.1In a game, the dealer holds three cards—one queen and\ntwo aces (as shown in Figure 6.1(a)[244])—and, typically with a little bit of ﬂair, quickly\ndrops these cards facedown onto a table. Faced with the backs of three cards (as shown\nin Figure 6.1(b)[244]), the player then has to guess where the queen has landed. Usually the\nplayer bets money on their ability to do this, and the dealer uses a little manual trickery to\nmisdirect the player toward the wrong card.\nWhen you ﬁrst see the game played, because the dealer lays out the three cards so quickly,\nyou think that there is no way to tell where the queen lands. In this case you can only\nassume that the queen is equally likely to be in any of the three possible positions: left,\n1. It is appropriate to use a game involving gambling to introduce probability-based machine learning. The\norigins of probability theory come from attempts to understand gambling and games of chance, in particular, the\nwork of Gerolamo Cardano and the later work of Pierre de Fermat and Blaise Pascal.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":298,"page_label":"244","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"244 Chapter 6 Probability-Based Learning\n(a)\n (b)\nLeft Center RightLikelihood\n(c)\nLeft Center RightLikelihood (d)\nFigure 6.1\nA game of ﬁnd the lady : (a) the cards used; (b) the cards dealt facedown on a table; (c) the initial\nlikelihoods of the queen ending up in each position; and (d) a revised set of likelihoods for the\nposition of the queen based on evidence collected.\ncenter , orright . This is shown in the bar plot in Figure 6.1(c)[244], which shows an equal\nlikelihood for each position. Not feeling quite brave enough to play a game, you decide to\ninstead study the dealer playing games with other people.\nAfter watching the dealer play 30games with other players, you notice that he has a\ntendency to drop the queen in the position on the right (19 times) more than the left (3\ntimes) or center (8 times). Based on this, you update your beliefs about where the queen\nis likely to land based on the evidence that you have collected. This is shown in Figure\n6.1(d)[244], where the bars have been redistributed to illustrate the revised likelihoods.\nConﬁdent that your study will help you, you lay a dollar down to play a game, ready to\nguess that the queen is in the position on the right. This time, however, as the dealer drops\nthe cards onto the table, a sudden gust of wind turns over the card on the right to reveal\nthat it is the ace of spades (shown in Figure 6.2(a)[245]). The extra piece of evidence means\nthat you need to revise your belief about the likelihoods of where the queen will be once\nagain. These revised likelihoods are shown in Figure 6.2(b)[245]. As you know that the card\nis not in the position on the right, the likelihood that you had associated with this position\nis redistributed among the other two possibilities. Based on the new likelihoods, you guess","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":299,"page_label":"245","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 245\n(a)\nLeft Center RightLikelihood (b)\n(c)\nFigure 6.2\n(a) The set of cards after the wind blows over the one on the right; (b) the revised likelihoods for the\nposition of the queen based on this new evidence; and (c) the ﬁnal positions of the cards in the game.\nthat the queen is in the center position, and happily, this turns out to be correct (see Figure\n6.2(c)[245]). The dealer encourages you to play again, but you know that you’ve got to know\nwhen to walk away, so you head off with an extra dollar in your pocket.\nThis illustrates the big idea underlying probability-based machine learning. We can use\nestimates of likelihoods to determine the most likely predictions that should be made. Most\nimportant, though, we revise these predictions based on data we collect and whenever extra\nevidence becomes available.\n6.2 Fundamentals\nIn this section we describe Bayes’ Theorem and the important fundamentals of probability\ntheory that are required to use it. This section assumes a basic understanding of probabil-\nity theory, including the basics of calculating probabilities based on relative frequencies ,\ncalculating conditional probabilities , the probability product rule , the probability chain\nrule, and the Theorem of Total Probability . Appendix B[757]provides a comprehensive in-\ntroduction to these aspects of probability theory, so we recommend that readers unfamiliar\nwith them review this appendix before continuing with this chapter.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":300,"page_label":"246","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"246 Chapter 6 Probability-Based Learning\nTable 6.1\nA simple dataset for a M ENINGITIS diagnosis with descriptive features that describe the presence or\nabsence of three common symptoms of the disease: H EADACHE , FEVER , and V OMITING .\nID H EADACHE FEVER VOMITING MENINGITIS\n1 true true false false\n2 false true false false\n3 true false true false\n4 true false true false\n5 false true false true\n6 true false true false\n7 true false true false\n8 true false true true\n9 false true false false\n10 true false true true\nWe will use the dataset2in Table 6.1[246]to illustrate how the terminology of probability\nis mapped into the language of machine learning for predictive data analytics. The target\nbeing predicted in this dataset is whether a patient is suffering from M ENINGITIS , and\nthe descriptive features are common symptoms associated with this disease (H EADACHE ,\nFEVER , and V OMITING ).\nFrom a probability point of view, each feature in a dataset is a random variable , and the\nsample space for the domain associated with a prediction problem is the set of all possible\ncombinations of assignments of values to features. Each row in a dataset represents an\nexperiment , which associates a target feature value with a set of descriptive feature values,\nand the assignment of a set of descriptive features with values is an event . So, for example,\neach row in Table 6.1[246]represents an experiment, and the assignment of the descriptive\nfeatures to the values shown in each row can be referred to as a distinct event.\nAprobability function ,Ppq, returns the probability of an event. For example, PpFEVER“\ntrueqreturns the probability of the F EVER feature taking the value true. This probability,\nwhich is 0.4, can be counted directly from the dataset. Probability functions for categor-\nical features are referred to as probability mass functions , while probability functions\nfor continuous features are known as probability density functions . In the early part\nof this chapter, we focus on categorical features, but we return to continuous features in\nSection 6.4[265]. Ajoint probability refers to the probability of an assignment of speciﬁc\nvalues to multiple different features, for example, PpMENINGITIS“true,HEADACHE“\ntrueq“0.2. Last, a conditional probability refers to the probability of one feature taking\n2. This data has been artiﬁcially generated for this example.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":301,"page_label":"247","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 247\na speciﬁc value given that we already know the value of a different feature, for example,\nPpMENINGITIS“true|HEADACHE“trueq“0.2857 .\nIt is often useful to talk about the probabilities for all the possible assignments to a\nfeature. To do this we use the concept of a probability distribution . A probability distri-\nbution is a data structure that describes the probability of each possible value a feature can\ntake. For example, the probability distribution for the binary feature M ENINGITIS from\nTable 6.1[246]isPpMENINGITISq“⟨0.3,0.7⟩(by convention we give the true probability\nﬁrst). We use bold notation to distinguish between a probability distribution, Ppq, and a\nprobability function, Ppq. The sum of a probability distribution must equal 1.0.\nAjoint probability distribution is a probability distribution over more than one feature\nassignment and is written as a multidimensional matrix in which each cell lists the proba-\nbility of a particular combination of feature values being assigned. The dimensions of the\nmatrix are dependent on the number of features and the number of values in the domains\nof the features. The sum of all the cells in a joint probability distribution must be 1.0. For\nexample, the joint probability distribution for the four binary features from Table 6.1[246]\n(HEADACHE , FEVER , VOMITING , and M ENINGITIS ) is written3\nPpH,F,V,Mq“»\n—————————————–Pph,f,v,mq, Pp␣h,f,v,mq\nPph,f,v,␣mq, Pp␣h,f,v,␣mq\nPph,f,␣v,mq, Pp␣h,f,␣v,mq\nPph,f,␣v,␣mq, Pp␣h,f,␣v,␣mq\nPph,␣f,v,mq, Pp␣h,␣f,v,mq\nPph,␣f,v,␣mq, Pp␣h,␣f,v,␣mq\nPph,␣f,␣v,mq, Pp␣h,␣f,␣v,mq\nPph,␣f,␣v,␣mq,Pp␣h,␣f,␣v,␣mqﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ(6.1)\nGiven a joint probability distribution, we can compute the probability of any event in the\ndomain that it covers by summing over the cells in the distribution where that event is true.\nFor example, to compute the probability of Pphqin the domain speciﬁed by the joint prob-\nability distribution PpH,F,V,Mq, we simply sum the values in the cells containing h(the\ncells in the ﬁrst column). Calculating probabilities in this way is known as summing out .\nWe can also use summing out to compute conditional probabilities from a joint probability\ndistribution. To calculate the probability Pph|fqfrom PpH,F,V,Mq, we sum the values\nin all the cells where handfare the case (the top four cells in the ﬁrst column).\nWe are now ready to take on Bayes’ Theorem!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":301,"page_label":"247","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"domain that it covers by summing over the cells in the distribution where that event is true.\nFor example, to compute the probability of Pphqin the domain speciﬁed by the joint prob-\nability distribution PpH,F,V,Mq, we simply sum the values in the cells containing h(the\ncells in the ﬁrst column). Calculating probabilities in this way is known as summing out .\nWe can also use summing out to compute conditional probabilities from a joint probability\ndistribution. To calculate the probability Pph|fqfrom PpH,F,V,Mq, we sum the values\nin all the cells where handfare the case (the top four cells in the ﬁrst column).\nWe are now ready to take on Bayes’ Theorem!\n3. To save space, throughout this chapter, named features are denoted by the uppercase initial letters of their\nnames (e.g., the M ENINGITIS feature is denoted M). If a named feature is binary, we use either the lowercase\ninitial letter of the feature name to denote that the feature is trueor the lowercase initial letter preceded by the ␣\nsymbol to denote that it is false (e.g., mdenotes M ENINGITIS“true, and␣mdenotes M ENINGITIS“false ).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":302,"page_label":"248","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"248 Chapter 6 Probability-Based Learning\n6.2.1 Bayes’ Theorem\nBayes’ Theorem4is so elegant and intuitive that it can be stated in one sentence of plain\nEnglish:\nthe probability that an event has happened given a set of evidence for it is equal to the\nprobability of the evidence being caused by the event multiplied by the probability of the\nevent itself\nor slightly more succinctly:\nPpan event given evidence q“Ppthe evidence given the event q\nˆPpthe eventq\nReading from left to right, the theorem shows us how to calculate the probability of an\nevent given the evidence we have of that event in terms of the likelihood of the event caus-\ning this evidence. This is useful because reasoning from the evidence to events ( inverse\nreasoning ) is often much more difﬁcult than reasoning from an event to the evidence it\ncauses ( forward reasoning ). Bayes’ Theorem allows us to easily swap back and forth\nbetween these two types of reasoning.\nThe formal deﬁnition of Bayes’ Theorem is\nPpX|Yq“PpY|XqPpXq\nPpYq(6.2)\nBayes’ Theorem deﬁnes the conditional probability of an event, X, given some evidence,\nY, in terms of the product of the inverse conditional probability, PpY|Xq, and the prior\nprobability of the event PpXq.\nFor an illustrative example of Bayes’ Theorem in action, imagine that after a yearly\ncheckup, a doctor informs a patient that there is both bad news and good news. The bad\nnews is that the patient has tested positive for a serious disease and that the test the doctor\nused is 99% accurate (i.e., the probability of testing positive when a patient has the disease\nis0.99, as is the probability of testing negative when a patient does not have the disease).\nThe good news, however, is that the disease is extremely rare, striking only 1in10,000\npeople. So what is the actual probability that the patient has the disease? And why is the\nrarity of the disease good news given that the patient has tested positive for it?\nWe can use Bayes’ Theorem to answer both of these questions. To calculate the probabil-\nity that the patient actually has the disease based on the evidence of the test result, Ppd|tq,\nwe apply Bayes’ Theorem:\nPpd|tq“Ppt|dqPpdq\nPptq\n4. Bayes’ Theorem is named after the Reverend Thomas Bayes, who wrote an essay that described how to\nupdate beliefs as new information arises. After Thomas Bayes died, this essay was edited and published by the\nReverend Richard Price (Bayes and Price, 1763). The modern mathematical form of Bayes’ Theorem, however,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":302,"page_label":"248","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"people. So what is the actual probability that the patient has the disease? And why is the\nrarity of the disease good news given that the patient has tested positive for it?\nWe can use Bayes’ Theorem to answer both of these questions. To calculate the probabil-\nity that the patient actually has the disease based on the evidence of the test result, Ppd|tq,\nwe apply Bayes’ Theorem:\nPpd|tq“Ppt|dqPpdq\nPptq\n4. Bayes’ Theorem is named after the Reverend Thomas Bayes, who wrote an essay that described how to\nupdate beliefs as new information arises. After Thomas Bayes died, this essay was edited and published by the\nReverend Richard Price (Bayes and Price, 1763). The modern mathematical form of Bayes’ Theorem, however,\nwas developed by Simon Pierre Laplace.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":303,"page_label":"249","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 249\nThe information about the scenario gives us the probability of having the disease as Ppdq“\n0.0001 and the probability of not having the disease as Pp␣dq“0.9999 . The accuracy of\nthe test is captured as Ppt|dq“0.99andPpt|␣dq“0.01. The overall probability of\nthe test returning a positive value, Pptq, is not given in the description above, but it can be\neasily calculated using the Theorem of Total Probability5as\nPptq “ Ppt|dqPpdq`Ppt|␣dqPp␣dq\n“ p 0.99ˆ0.0001q`p 0.01ˆ0.9999q\n“0.0101\nWe can insert these probabilities into the application of Bayes’ Theorem to give\nPpd|tq “0.99ˆ0.0001\n0.0101\n“0.0098\nSo, the probability of actually having the disease, in spite of the positive test result, is\nless than 1%. This is why the doctor said the rarity of the disease was such good news.\nOne of the important characteristics of Bayes’ Theorem is its explicit inclusion of the prior\nprobability of an event when calculating the likelihood of that event based on evidence.6\nLet’s look at Bayes’ Theorem in a little more detail. Bayes’ Theorem is easily derived\nfrom the product rule .7We know from the product rule and the logical symmetry of the\nandoperation8that\nPpY|XqPpXq“PpX|YqPpYq\nIf we divide both sides of this equation by the prior probability on the left-hand side, PpYq,\nwe get\nPpX|YqPpYq\nPpYq“PpY|XqPpXq\nPpYq\nThePpYqterms on the left-hand side of this equation, however, cancel each other out to\ngive us Bayes’ Theorem\nPpX|Yq\b\b\bPpYq\n\b\b\bPpYq“PpY|XqPpXq\nPpYq\nñPpX|Yq “PpY|XqPpXq\nPpYq\n5. The Theorem of Total Probability is explained in detail in Section B.3[762]of Appendix B[757].\n6. Famously, an experiment in which doctors were asked this question about the probability of the patient’s\nhaving the disease showed that most of them got this question wrong (Casscells et al., 1978).\n7. The product rule is explained in detail in Section B.3[762]of Appendix B[757].\n8. That is, aandb“banda.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":304,"page_label":"250","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"250 Chapter 6 Probability-Based Learning\nThere are two important observations regarding the division in Bayes’ Theorem by the\ndenominator PpYq. The ﬁrst is that this division functions as a normalization mechanism\nensuring that\n0ďPpX|Yqď1\nandÿ\niPpXi|Yq“1.0\nwhereř\niPpXiqshould be interpreted as summing over the set of events that are a complete\nassignment to the features in X. The reason that the division functions as a normalization\nmechanism is that the prior probability of the evidence, PpYq, is not conditional on Xi, and\nas a result, it is constant for all Xi.\nThe second interesting observation about the division of the right-hand side of Bayes’\nTheorem by PpYqis that we can calculate PpYqin two different ways. First, we can\ncalculate PpYqdirectly from a dataset as\nPpYq“|trows where Y is the case u|\n|trows in the dataset u|(6.3)\nAlternatively, we can use the Theorem of Total Probability to calculate PpYq:\nPpYq“ÿ\niPpY|XiqPpXiq (6.4)\nNotice that ignoring the subscripts, the expression we are summing in Equation (6.4)[250]\nis identical to the numerator in Bayes’ Theorem. This gives us a way to calculate the\nposterior probability distribution over the possible assignment of values to the features\nin event Xconditioned on the event Y, that is, PpX|Yq, which avoids explicitly calculating\nPpYq. If we let\nη“1ÿ\niPpY|XiqPpXiq(6.5)\nthen\nPpXi|Yq“ηˆPpY|XiqPpXiq (6.6)\nwhere the term ηexplicitly represents a normalization constant. Because Bayes’ Theorem\ncan be calculated in this way, it is sometimes written as\nPpX|Yq“ηˆPpY|XqPpXq (6.7)\nwhereηis as deﬁned in Equation (6.5)[250].\nSo we have two different deﬁnitions of Bayes’ Theorem (Equation (6.2)[248]and Equation\n(6.7)[250]), but which one should we use? The choice is really a matter of convenience. If\nwe are calculating the probability of a single event given some evidence, then calculating","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":305,"page_label":"251","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 251\nPpYqdirectly from the data using Equation (6.2)[248]is the easier option. If, however, we\nneed to calculate the posterior probability distribution over Xgiven Y, that is PpX|Yq,\nthen we will be actually calculating each of the PpY|XiqPpXiqvalues in Equation (6.5)[250]\nas part of this calculation, and it is more efﬁcient to use Equation (6.7)[250].\nWe are now ready to use Bayes’ Theorem to generate predictions based on a dataset. The\nnext section will examine how this is done.\n6.2.2 Bayesian Prediction\nTo make Bayesian predictions, we generate the probability of the event that a target feature,\nt, takes a speciﬁc level, l, given the assignment of values to a set of descriptive features,\nq, from a query instance. We can restate Bayes’ Theorem using this terminology and\ngeneralize the deﬁnition of Bayes’ Theorem so that it can take into account more than one\npiece of evidence (each descriptive feature value is a separate piece of evidence). The\nGeneralized Bayes’ Theorem is deﬁned as\nPpt“l|qr1s,..., qrmsq“Ppqr1s,..., qrms|t“lqPpt“lq\nPpqr1s,..., qrmsq(6.8)\nTo calculate a probability using the Generalized Bayes’ Theorem , we need to calculate\nthree probabilities:\n1.Ppt“lq, the prior probability of the target feature ttaking the level l\n2.Ppqr1s,..., qrmsq, the joint probability of the descriptive features of a query in-\nstance taking a speciﬁc set of values\n3.Ppqr1s,..., qrms|t“lq, the conditional probability of the descriptive features of\na query instance taking a speciﬁc set of values given that the target feature takes the\nlevel l\nThe ﬁrst two of these probabilities are easy to calculate. Ppt“lqis simply the relative\nfrequency with which the target feature takes the level lin a dataset. Ppqr1s,..., qrmsq\ncan be calculated as the relative frequency in a dataset of the joint event that the descriptive\nfeatures of an instance take on the values qr1s,..., qrms. As discussed in the previous\nsection, it can also be calculated using the Theorem of Total Probability (in this instance,\nsumming over all the target levelsř\nkPlevelsptqPpqr1s,..., qrms|t“kqPpt“kq), or\nreplaced entirely with a normalization constant ,η.\nThe ﬁnal probability that we need to calculate, Ppqr1s,..., qrms|t“lq, can be cal-\nculated either directly from a dataset (by calculating the relative frequency of the joint\nevent qr1s,..., qrmswithin the set of instances where t“l), or alternatively, it can be","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":305,"page_label":"251","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"can be calculated as the relative frequency in a dataset of the joint event that the descriptive\nfeatures of an instance take on the values qr1s,..., qrms. As discussed in the previous\nsection, it can also be calculated using the Theorem of Total Probability (in this instance,\nsumming over all the target levelsř\nkPlevelsptqPpqr1s,..., qrms|t“kqPpt“kq), or\nreplaced entirely with a normalization constant ,η.\nThe ﬁnal probability that we need to calculate, Ppqr1s,..., qrms|t“lq, can be cal-\nculated either directly from a dataset (by calculating the relative frequency of the joint\nevent qr1s,..., qrmswithin the set of instances where t“l), or alternatively, it can be\ncalculated using the probability chain rule .9The chain rule states that the probability of\na joint event can be rewritten as a product of conditional probabilities. So, we can rewrite\n9. The probability chain rule is explained in detail in Section B.3[762]of Appendix B[757].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":306,"page_label":"252","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"252 Chapter 6 Probability-Based Learning\nPpqr1s,..., qrmsqas\nPpqr1s,..., qrmsq“\nPpqr1sqˆPpqr2s|qr1sqˆ¨¨¨ˆ Ppqrms|qrm´1s,..., qr2s,qr1sq\nWe can use the chain rule for conditional probabilities by just adding the conditioning term\nto each term in the expression, so\nPpqr1s,..., qrms|t“lq“\nPpqr1s|t“lqˆPpqr2s|qr1s,t“lqˆ...\n¨¨¨ˆ Ppqrms|qrm´1s,..., qr3s,qr2s,qr1s,t“lq(6.9)\nThis transformation from a joint probability conditioned on a single event into a product\nof conditional probabilities with just one event being conditioned in each term may not\nappear to achieve much. We will see shortly, however, that this transformation is incredibly\nuseful.\nLet’s look at an example of how we can now use Bayes’ Theorem to make predic-\ntions based on the meningitis diagnosis dataset in Table 6.1[246]for a query instance with\nHEADACHE“true, FEVER“false , and V OMITING“true. Returning to the shortened\nnotation that we used previously, a predicted diagnosis for this query instance can be given\nusing Bayes’ Theorem as\nPpM|h,␣f,vq“Pph,␣f,v|MqˆPpMq\nPph,␣f,vq\nThere are two values in the domain of the M ENINGITIS feature, true andfalse , so we\nhave to do this calculation once for each. Considering ﬁrst the calculation for m, we need\nthe following probabilities, which can be computed directly from Table 6.1[246]\nPpmq“|td5,d8,d10u|\n|td1,d2,d3,d4,d5,d6,d7,d8,d9,d10u|“3\n10“0.3\nPph,␣f,vq“|td3,d4,d6,d7,d8,d10u|\n|td1,d2,d3,d4,d5,d6,d7,d8,d9,d10u|“6\n10“0.6\nWe also need to calculate the likelihood of the descriptive feature values of the query\ngiven that the target is true. We could calculate this directly from the dataset, but in this\nexample, we will illustrate the chain rule approach just described. Using the chain rule\napproach, we compute the overall likelihood of the descriptive feature values given a tar-\nget value of true as the product of a set of conditional probabilities that are themselves","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":307,"page_label":"253","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 253\ncalculated from the dataset\nPph,␣f,v|mq“Pph|mqˆPp␣f|h,mqˆPpv|␣f,h,mq\n“|td8,d10u|\n|td5,d8,d10u|ˆ|td8,d10u|\n|td8,d10u|ˆ|td8,d10u|\n|td8,d10u|\n“2\n3ˆ2\n2ˆ2\n2“0.6666\nWe can now combine the three probabilities just calculated to calculate the overall proba-\nbility of the target feature taking the level truegiven the query instance\nPpm|h,␣f,vq“Pph,␣f,v|mqˆPpmq\nPph,␣f,vq\n“0.6666ˆ0.3\n0.6“0.3333\nThe corresponding calculation for Pp␣m|h,␣f,vqis:\nPp␣m|h,␣f,vq“Pph,␣f,v|␣mqˆPp␣mq\nPph,␣f,vq\n“˜\nPph|␣mqˆPp␣f|h,␣mq\nˆPpv|␣f,h,␣mqˆPp␣mq¸\nPph,␣f,vq\n“0.7143ˆ0.8ˆ1.0ˆ0.7\n0.6“0.6667\nThese calculations tell us that it is twice as probable that the patient does not have menin-\ngitis as it is that the patient does. This might seem a little surprising given that the patient\nis suffering from a headache and is vomiting, two key symptoms of meningitis. Indeed, we\nhave a situation where the posterior for a given prediction given the evidence is quite low\n(here Ppm|h,␣f,vq“0.3333 ), even though the likelihood of the evidence if we assume\nthe prediction to be correct is quite high, Pph,␣f,v|mq“0.6666 .\nWhat is happening here is that, as Bayes’ Theorem states, when calculating a posterior\nprediction, we weight the likelihood of the evidence given the prediction by the prior of the\nprediction. In this case, although the likelihood of suffering from a headache and vomiting\nis quite high when someone has meningitis, the prior probability of having meningitis is\nquite low. So, even when we take the evidence into account, the posterior probability\nof having meningitis remains low. This can seem counterintuitive at ﬁrst. The mistake\nis to confuse the probability of a prediction given the evidence with the probability of","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":308,"page_label":"254","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"254 Chapter 6 Probability-Based Learning\nthe evidence given the prediction and is another example of the paradox of the false\npositive .10\nCalculating exact probabilities for each of the possible target levels is often very useful\nto a human decision maker, for example, a doctor. However, if we are trying to build a\npredictive model that automatically assigns a target level to a query instance, then we need\nto decide how the model will make a prediction based on the computed probabilities. The\nobvious way to do this is to have the model return the target level that has the highest\nposterior probability given the state of the descriptive features in the query. A prediction\nmodel that works in this way is making a maximum a posteriori (MAP ) prediction.11We\ncan formally deﬁne a Bayesian MAP prediction model as\nMpqq“arg max\nlPlevelsptqPpt“l|qr1s,..., qrmsq\n“arg max\nlPlevelsptqPpqr1s,..., qrms|t“lqˆPpt“lq\nPpqr1s,..., qrmsq(6.10)\nwhereMpqqis the prediction returned by the model Musing a MAP prediction mechanism\nfor a query, q, composed of qr1s,..., qrmsdescriptive features; levelsptqis the set of\nlevels the target feature can take; and arg maxlPlevelsptqspeciﬁes that we return the level, l,\nthat has the maximum value computed using the function on the right of the arg max term.\nNotice that the denominator in Equation (6.10)[254]is not dependent on the target feature,\nso it is functioning as a normalization constant. Furthermore, if we want to make a MAP\nprediction, we don’t necessarily have to calculate the actual probabilities for each level in\nthe target domain; we simply need to know which of the levels in the target domain has the\nlargest probability. Consequently, we don’t necessarily have to normalize the scores for\neach target level—something we would have to do if we wanted the actual probabilities.\nInstead, we can simply return the target level that has the highest score from the numerator\nterm. Using this simpliﬁcation, the Bayesian MAP prediction model can be restated as\nMpqq“arg max\nlPlevelsptqPpqr1s,..., qrms|t“lqˆPpt“lq (6.11)\nAlthough it might seem that we now have a good solution for building probability-based\nprediction models, we are not quite done yet. There is one fundamental ﬂaw with the\napproach that we have developed. To illustrate this, we will consider a second query\n10. The paradox of the false positive states that in order to make predictions about a rare event, the model has to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":308,"page_label":"254","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"each target level—something we would have to do if we wanted the actual probabilities.\nInstead, we can simply return the target level that has the highest score from the numerator\nterm. Using this simpliﬁcation, the Bayesian MAP prediction model can be restated as\nMpqq“arg max\nlPlevelsptqPpqr1s,..., qrms|t“lqˆPpt“lq (6.11)\nAlthough it might seem that we now have a good solution for building probability-based\nprediction models, we are not quite done yet. There is one fundamental ﬂaw with the\napproach that we have developed. To illustrate this, we will consider a second query\n10. The paradox of the false positive states that in order to make predictions about a rare event, the model has to\nbe at least as accurate as the event is rare (i.e., the probability of the model making an error has to be less than the\nprobability of the rare event occurring) or there is a signiﬁcant chance of false positive predictions (i.e., predicting\nthe event when it is not the case). Doctorow (2010) provides an interesting discussion of this phenomenon.\n11. The MAP prediction is the prediction mechanism that we assume throughout this book. An alternative mech-\nanism is the Bayesian optimal classiﬁer , but we won’t discuss it in this text. See Mitchell (1997) for more\ndetails.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":309,"page_label":"255","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 255\ninstance for the meningitis diagnosis problem, this time with descriptive feature values\nHEADACHE“true, FEVER“true, and V OMITING“false . The probability of M ENIN -\nGITIS =truegiven this query is\nPpm|h,f,␣vq“˜\nPph|mqˆPpf|h,mq\nˆPp␣v|f,h,mqˆPpmq¸\nPph,f,␣vq\n“0.6666ˆ0ˆ0ˆ0.3\n0.1“0\nand for M ENINGITIS =false\nPp␣m|h,f,␣vq“˜\nPph|␣mqˆPpf|h,␣mq\nˆPp␣v|f,h,␣mqˆPp␣mq¸\nPph,f,␣vq\n“0.7143ˆ0.2ˆ1.0ˆ0.7\n0.1“1.0\nThe calculated posterior probabilities indicate that it is a certainty that the patient does\nnot have meningitis! This is because as we progress along the sequence of conditional\nprobabilities speciﬁed by the chain rule, the size of the set of conditioning events for each\nterm increases. As a result, the set of events that fulﬁll the conditions for each conditional\nprobability in the sequence, and hence that are considered when we compute the probabil-\nity, get smaller and smaller as more and more conditions are added. The technical term for\nthis splitting of the data into smaller and smaller sets based on larger and larger sets of con-\nditions is data fragmentation . Data fragmentation is essentially an instance of the curse\nof dimensionality . As the number of descriptive features grows, the number of potential\nconditioning events grows. Consequently, an exponential increase is required in the size\nof the dataset as each new descriptive feature is added to ensure that for any conditional\nprobability, there are enough instances in the training dataset matching the conditions so\nthat the resulting probability is reasonable.\nReturning to our example query, in order to calculate Pph,f,␣v|mq, the chain rule\nrequires us to deﬁne three conditional probabilities, Pph|mq,Ppf|h,mq, and Pp␣v|\nf,h,mq. For the ﬁrst of these terms, Pph|mq, only three instances in the dataset fulﬁll the\ncondition of m(d5,d8andd10). In two out of these three rows ( d8andd10),his the case,\nso the conditional probability Pph|mq“0.6666 . These are also the only two rows that\nfulﬁll the conditions of the second term in the chain sequence, Ppf|h,mq. In neither of\nthese rows is fthe case, so the conditional probability for Ppf|h,mqis0. Because the\nchain rule speciﬁes the product of a sequence of probabilities, if any of the probabilities\nin the sequence is zero, then the overall probability will be zero. Even worse, because\nthere are no rows in the dataset where f,h, and mare true, there are no rows in the dataset","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":309,"page_label":"255","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"f,h,mq. For the ﬁrst of these terms, Pph|mq, only three instances in the dataset fulﬁll the\ncondition of m(d5,d8andd10). In two out of these three rows ( d8andd10),his the case,\nso the conditional probability Pph|mq“0.6666 . These are also the only two rows that\nfulﬁll the conditions of the second term in the chain sequence, Ppf|h,mq. In neither of\nthese rows is fthe case, so the conditional probability for Ppf|h,mqis0. Because the\nchain rule speciﬁes the product of a sequence of probabilities, if any of the probabilities\nin the sequence is zero, then the overall probability will be zero. Even worse, because\nthere are no rows in the dataset where f,h, and mare true, there are no rows in the dataset\nwhere the conditions for the third term Pp␣v|f,h,mqhold, so this probability is actually","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":310,"page_label":"256","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"256 Chapter 6 Probability-Based Learning\nundeﬁned, as calculating it involves a division by zero. Trying to compute the probability\nofPph,f,␣v|mqdirectly from the data rather than using the chain rule also suffers from\nthe same problem.\nIn summary, whether we compute the likelihood term for this example using the chain\nrule or directly from the dataset, we will end up with a probability of zero, or worse, an\nundeﬁned probability. This is because there are no instances in the dataset where a patient\nwho had meningitis was suffering from a headache and had a fever but wasn’t vomiting.\nConsequently, the probability for the M ENINGITIS feature being true given the evidence\nin the query using this dataset was zero.\nClearly, the probability of a patient who has a headache and a fever having meningitis\nshould be greater than zero. The problem here is that our dataset is not large enough to\nbe truly representative of the meningitis diagnosis scenario, and our model is overﬁtting\nto the training data. The problem is even more serious than this, however, as in practice,\nit is almost never possible to collect a dataset that is big enough to sufﬁciently cover all\nthe possible combinations of descriptive feature values that can occur in a dataset so as\nto avoid this. All is not lost, however, as the concepts of conditional independence and\nfactorization can help us overcome this ﬂaw of our current approach.\n6.2.3 Conditional Independence and Factorization\nSo far our treatment of probability has assumed that the evidence we have collected affects\nthe probability of the event we are trying to predict. This is not always the case. For\nexample, it would seem reasonable to argue that the behavior of an octopus in a swimming\ntank should not affect the outcome of a soccer match.12If knowledge of one event has no\neffect on the probability of another event, and vice versa, then the two events are said to be\nindependent of each other. If two events XandYare independent, then\nPpX|Yq“PpXq\nPpX,Yq“PpXqˆPpYq\nFull independence between events is quite rare. A more common phenomenon is that\ntwo or more events may be independent if we know that a third event has happened. This\nis known as conditional independence . The typical situation where conditional inde-\npendence holds between events is when the events share the same cause. For example,\nconsider the symptoms of meningitis. If we don’t know whether the patient has meningi-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":310,"page_label":"256","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tank should not affect the outcome of a soccer match.12If knowledge of one event has no\neffect on the probability of another event, and vice versa, then the two events are said to be\nindependent of each other. If two events XandYare independent, then\nPpX|Yq“PpXq\nPpX,Yq“PpXqˆPpYq\nFull independence between events is quite rare. A more common phenomenon is that\ntwo or more events may be independent if we know that a third event has happened. This\nis known as conditional independence . The typical situation where conditional inde-\npendence holds between events is when the events share the same cause. For example,\nconsider the symptoms of meningitis. If we don’t know whether the patient has meningi-\n12. During the European Soccer Championships in 2008 and the 2010 Soccer World Cup, an octopus in Germany,\ncalled Paul, was attributed with achieving an 85% success rate at predicting the results of the matches involving\nGermany. Paul’s impressive accuracy should not be taken to suggest that octopus behavior affects soccer matches\nbut rather that independent events may be correlated, at least for an interval of time, without the events actually\nbeing dependent. As the oft-quoted maxim states: correlation does not imply causation! (See Section 3.5.2[81]for\nfurther discussion.)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":311,"page_label":"257","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 257\ntis, then knowing that the patient has a headache may increase the probability we assign\nto the patient of suffering from a fever. This is because having a headache increases the\nprobability of the patient having meningitis, which in turn increases the probability of the\npatient having a fever. However, if we already know that the patient has meningitis, then\nalso knowing that the patient has a headache will not affect the probability of the patient\nhaving a fever. This is because the information we get from knowing that the patient has\na headache is already contained within the information that the patient has meningitis. In\nthis situation, knowing that someone has meningitis makes the events of them having a\nheadache and having a fever independent of each other. For two events, XandY, that are\nconditionally independent given knowledge of a third event, here Z, we can say that\nPpX|Y,Zq“PpX|Zq\nPpX,Y|Zq“PpX|ZqˆPpY|Zq\nThis allows us an important reformulation of the chain rule for situations in which con-\nditional independence applies. Recall that the chain rule for calculating the probability that\na set of descriptive features, qr1s,..., qrms, takes a speciﬁc set of values when a target\nfeature, t, takes a speciﬁc level, l, is\nPpqr1s,..., qrms|t“lq“\nPpqr1s|t“lqˆPpqr2s|qr1s,t“lqˆ...\n¨¨¨ˆ Ppqrms|qrm´1s,..., qr3s,qr2s,qr1s,t“lq(6.12)\nIf the event of the target feature ttaking the level lcauses the assignment of values to\nthe descriptive features, qr1s,..., qrms, then the events of each descriptive feature taking\na value are conditionally independent of each other given the value of the target feature.\nThis means that the chain rule deﬁnition can be simpliﬁed as follows:\nPpqr1s,..., qrms|t“lq\n“Ppqr1s|t“lqˆPpqr2s|t“lqˆ¨¨¨ˆ Ppqrms|t“lq\n“mź\ni“1Ppqris|t“lq(6.13)\nThe reason that this simpliﬁcation is so important is that it allows us to simplify the cal-\nculations in Bayes’ Theorem, under the assumption of conditional independence between\nthe descriptive features, given the level lof the target feature, from\nPpt“l|qr1s,..., qrmsq“¨\n˚˝Ppqr1s|t“lqˆPpqr2s|qr1s,t“lqˆ\n¨¨¨ˆ Ppqrms|qrm´1s,..., qr1s,t“lq\nˆPpt“lq˛\n‹‚\nPpqr1s,..., qrmsq","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":312,"page_label":"258","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"258 Chapter 6 Probability-Based Learning\nto the much simpler\nPpt“l|qr1s,..., qrmsq“˜mź\ni“1Ppqris|t“lq¸\nˆPpt“lq\nPpqr1s,..., qrmsq(6.14)\nWhere appropriate, conditional independence not only simpliﬁes the calculations but\nalso enables us to compactly represent the full joint probability distribution for a domain.\nRather than calculating and storing the probabilities of all the joint events in a domain,\nwe can break up the distribution into data structures called factors , which deﬁne distribu-\ntions over subsets of features. We can then compute any of the probabilities in the joint\nprobability distribution using the product of these factors.\nFor example, Equation (6.1)[247]listed the joint probability distribution for the four binary\nfeatures in the meningitis diagnosis dataset in Table 6.1[246]. This distribution contained 16\nentries. If, however, it is in fact the case that H EADACHE , FEVER , and V OMITING are\nconditionally independent of each other given M ENINGITIS , then we would need to store\nonly four factors: PpMq,PpH|Mq,PpF|Mq, and PpV|Mq. We can recalculate all the\nelements of the joint probability distribution using the product of these four factors:\nPpH,F,V,Mq“PpMqˆPpH|MqˆPpF|MqˆPpV|Mq\nBecause all the features in this example are binary, we need to store only the probabilities\nfor the events where the features are true under the different combinations of values for the\nconditioning cases, as the probabilities for the complementary events can be computed\nby subtracting the stored probabilities from 1.0. Consequently, under this factorization,\nwe need to calculate only seven probabilities directly from the data: Ppmq,Pph|mq,\nPph| ␣mq,Ppf|mq,Ppf| ␣mq,Ppv|mq, and Ppv| ␣mq. The four factors required\nto represent the full joint distribution over the features H EADACHE , FEVER , VOMITING ,\nand M ENINGITIS (when the ﬁrst three are assumed to be conditionally independent given\nMENINGITIS ) can be stated as\nFactor 1“⟨PpMq⟩\nFactor 2“⟨Pph|mq,Pph|␣mq⟩\nFactor 3“⟨Ppf|mq,Ppf|␣mq⟩\nFactor 4“⟨Ppv|mq,Ppv|␣mq⟩\nand the product required to calculate the probability of any joint event in the domain using\nthese four factors is\nPpH,F,V,Mq“PpMqˆPpH|MqˆPpF|MqˆPpV|Mq\nSo, the assumed conditional independence between the features permits us to factorize\nthe distribution and in doing so reduces the number of probabilities we need to calculate","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":313,"page_label":"259","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.2 Fundamentals 259\nand store from the data. The reduction from 16to7probabilities to represent this domain\nmay not seem to achieve much, but there are two things to bear in mind. First, individually,\nthe7probabilities have fewer constraints on them than the 16in the full joint probability\ndistribution. As a result, it is typically easier to collect the data required to calculate these\nprobabilities. Second, as the number of features in the domain grows, the difference be-\ntween the number of probabilities required for a factorized representation and the number\nof probabilities in the full joint probability distribution gets larger. For example, in a do-\nmain with one target feature and nine descriptive features, all of which are binary, the full\njoint probability distribution will contain 210“1,024probabilities. However, if all the de-\nscriptive features are conditionally independent given the target feature, we can factorize\nthe joint distribution and represent it using just 19probabilities (one for the prior of the\ntarget and two conditional probabilities for each descriptive feature).\nApart from making a model more compact, conditional independence and factorization\nalso increase the coverage of a probability-based prediction model by allowing the model\nto calculate reasonable probabilities for queries with combinations of evidence that do not\noccur in the training dataset. To illustrate this, let’s return to the example query instance\nfor the meningitis diagnosis problem, where H EADACHE“true, FEVER“true, and\nVOMITING“false . When we originally tried to calculate probabilities for this query, a\nproblem arose from the requirement that we have instances in the training dataset where\nallthe evidence events hold. If we treat the evidence events as conditionally independent\ngiven the target feature, however, then we can factorize the evidence into its component\nevents and calculate probabilities for each of these events separately. By doing this, we\nrelax the requirement that, to avoid probabilities of zero, all the evidence events must hold\nin at least one instance for each value in the domain of the target. Instead, to avoid zero\nprobabilities, we require only that for each value in the domain of the target feature, there\nbe at least one instance in the dataset where each event in the evidence holds. For example,\nthis allows us to use the probability of a patient having a fever given that the patient has","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":313,"page_label":"259","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"allthe evidence events hold. If we treat the evidence events as conditionally independent\ngiven the target feature, however, then we can factorize the evidence into its component\nevents and calculate probabilities for each of these events separately. By doing this, we\nrelax the requirement that, to avoid probabilities of zero, all the evidence events must hold\nin at least one instance for each value in the domain of the target. Instead, to avoid zero\nprobabilities, we require only that for each value in the domain of the target feature, there\nbe at least one instance in the dataset where each event in the evidence holds. For example,\nthis allows us to use the probability of a patient having a fever given that the patient has\nmeningitis, rather than the more constrained conditional probability of the patient having\na fever given that the patient has meningitis andis suffering from a headache.\nWe reiterate the factors required to represent the full joint distribution for the meningitis\ndiagnosis scenario when we assume that the descriptive features are conditionally inde-\npendent given the target, this time including the actual probabilities calculated from the\ndataset:\nFactor 1“⟨Ppmq“0.3⟩\nFactor 2“⟨Pph|mq“0.6666,Pph|␣mq“0.7413⟩\nFactor 3“⟨Ppf|mq“0.3333,Ppf|␣mq“0.4286⟩\nFactor 4“⟨Ppv|mq“0.6666,Ppv|␣mq“0.5714⟩(6.15)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":314,"page_label":"260","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"260 Chapter 6 Probability-Based Learning\nUsing the factors in Equation (6.15)[259], we calculate the posterior distribution for menin-\ngitis given the query instance using Equation (6.14)[258]as\nPpm|h,f,␣vq“Pph|mqˆPpf|mqˆPp␣v|mqˆPpmqÿ\niPph|MiqˆPpf|MiqˆPp␣v|MiqˆPpMiq\n“0.6666ˆ0.3333ˆ0.3333ˆ0.3\np0.6666ˆ0.3333ˆ0.3333ˆ0.3q`p 0.7143ˆ0.4286ˆ0.4286ˆ0.7q\n“0.1948\nPp␣m|h,f,␣vq“Pph|␣mqˆPpf|␣mqˆPp␣v|␣mqˆPp␣mqř\niPph|MiqˆPpf|MiqˆPp␣v|MiqˆPpMiq\n“0.7143ˆ0.4286ˆ0.4286ˆ0.7\np0.6666ˆ0.3333ˆ0.3333ˆ0.3q`p 0.7143ˆ0.4286ˆ0.4286ˆ0.7q\n“0.8052\nAs with our previous calculations, the posterior probabilities for meningitis, calculated\nunder the assumption of conditional independence of the evidence, indicates that the pa-\ntient probably does not have meningitis, and consequently, a MAP Bayesian model would\nreturn M ENINGITIS“false as the prediction for this query instance. However, the poste-\nrior probabilities are not as extreme as those calculated when we did not assume conditional\nindependence. What has happened is that asserting conditional independence has allowed\nthe evidence of the individual symptoms to be taken into account, rather than requiring\nan exact match across all the symptoms taken together. By doing this, the Bayesian pre-\ndiction model is able to calculate reasonable probabilities for queries with combinations of\nevidence that do not occur in the dataset. This results in the model having a higher coverage\nwith respect to the possible queries it can handle. Furthermore, the conditional indepen-\ndence assumption enables us to factorize the distribution of the domain, and consequently\nwe need fewer probabilities with fewer constraints to represent the domain. As we will\nsee, a fundamental component of creating probabilistic prediction models is deciding on\nthe conditional independence assumptions we wish to make and the resulting factorization\nof the domain.\nIn the next section we introduce the naive Bayes model, a probability-based machine\nlearning algorithm that asserts a global conditional independence between the descriptive\nfeatures given the target. As a result of this conditional independence assumption, naive\nBayes models are very compact and relatively robust to overﬁtting the data, making them\none of the most popular predictive modeling approaches.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":315,"page_label":"261","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.3 Standard Approach: The Naive Bayes Model 261\n6.3 Standard Approach: The Naive Bayes Model\nA naive Bayes model returns a MAP prediction where the posterior probabilities for the\nlevels of the target feature are computed under the assumption of conditional indepen-\ndence between the descriptive features in an instance given a target feature level. More\nformally, the naive Bayes model is deﬁned as\nMpqq“arg max\nlPlevelsptq˜˜mź\ni“1Ppqris|t“lq¸\nˆPpt“lq¸\n(6.16)\nwhere tis a target feature with a set of levels, levelsptq, and qis a query instance with a set\nof descriptive features, qr1s,..., qrms.\nIn Section 6.2[245]we described how a full joint probability distribution could be used to\ncompute the probability for any event in a domain. The problem with this, however, is that\ngenerating full joint probability distributions suffers from the curse of dimensionality, and\nas a result, this approach is not tractable for domains involving more than a few features.\nIn Section 6.2.3[256], however, we showed how conditional independence between features\nallows us to factorize the joint distribution, and this helps with the curse of dimensionality\nproblem by reducing the number of probabilities we are required to calculate from the\ndata as well as the number of conditioning constraints on these probabilities. The naive\nBayes model leverages conditional independence to the extreme by assuming conditional\nindependence between the assignment of all the descriptive feature values given the target\nlevel. This assumption allows a naive Bayes model to radically reduce the number of\nprobabilities it requires, resulting in a very compact, highly factored representation of a\ndomain.\nWe say that the naive Bayes model is naive because the assumption of conditional in-\ndependence between the features in the evidence given the target level is a simplifying\nassumption that is made whether or not it is incorrect. Despite this simplifying assump-\ntion, however, the naive Bayes approach has been found to be surprisingly accurate across\na large range of domains. This is partly because errors in the calculation of the posterior\nprobabilities for the different target levels do not necessarily result in prediction errors. As\nwe noted when we dropped the denominator of Bayes’ Theorem from the MAP prediction\nmodel (Equation (6.11)[254]), for a categorical prediction task, we are primarily interested in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":315,"page_label":"261","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"domain.\nWe say that the naive Bayes model is naive because the assumption of conditional in-\ndependence between the features in the evidence given the target level is a simplifying\nassumption that is made whether or not it is incorrect. Despite this simplifying assump-\ntion, however, the naive Bayes approach has been found to be surprisingly accurate across\na large range of domains. This is partly because errors in the calculation of the posterior\nprobabilities for the different target levels do not necessarily result in prediction errors. As\nwe noted when we dropped the denominator of Bayes’ Theorem from the MAP prediction\nmodel (Equation (6.11)[254]), for a categorical prediction task, we are primarily interested in\nthe relative size of the posterior probabilities for the different target levels rather than the\nexact probabilities. Consequently, the relative ranking of the likelihood of the target levels\nare, to a certain extent, robust to errors in the calculation of the exact probabilities.13\n13. One consequence of this, however, is that a naive Bayes model is not a good approach for predicting a\ncontinuous target, because errors in calculating posterior probabilities do directly affect the accuracy of the model.\nThis is the only modeling approach covered in this book for which we will not present a way to predict both\ncontinuous and categorical target features.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":316,"page_label":"262","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"262 Chapter 6 Probability-Based Learning\nThe assumption of conditional independence between the features in the evidence given\nthe level of the target feature also makes the naive Bayes model relatively robust to data\nfragmentation and the curse of dimensionality . This is particularly important in scenar-\nios with small datasets or with sparse data .14One application domain where sparse data\nis the norm rather than the exception is in text analytics (for example, spam ﬁltering ),\nand naive Bayes models are often successful in this domain.\nThe naive Bayes model can also be easily adapted to handle missing feature values:\nwe simply drop the conditional probabilities for the evidence events that specify features\ntaking values that are not in the data from the product of the evidence events. Obviously,\ndoing this may have a negative effect on the accuracy of posterior probabilities computed\nby the model, but again this may not translate directly into prediction errors.\nA ﬁnal advantage of the naive Bayes model is how simple it is to train. For a given\nprediction task, all that is required to train a naive Bayes model is to calculate the priors\nfor each target level and the conditional probability for each feature given each target level.\nAs a result, a naive Bayes model can be trained relatively quickly compared to many other\nprediction models. A further advantage that results from this simplicity is the compactness\nof the naive Bayes model with which a very large dataset can be represented.\nOverall, although naive Bayes models may not be as powerful as some other prediction\nmodels, they often provide reasonable accuracy results, for prediction tasks with categori-\ncal targets, while being robust to the curse of dimensionality and also being easy to train.\nAs a result, a naive Bayes model is often a good prediction model to use to deﬁne a baseline\naccuracy score or when working with limited data.\n6.3.1 A Worked Example\nWe will use the dataset presented in Table 6.2[263]to illustrate how to create and use a naive\nBayes model for a prediction problem. This dataset relates to a fraud detection scenario in\nwhich we would like to build a model that predicts whether loan applications are fraudulent\nor genuine. There are three categorical descriptive features in this dataset. C REDIT HIS-\nTORY captures the credit history of the applicant, and its levels are none (the applicant has","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":316,"page_label":"262","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"cal targets, while being robust to the curse of dimensionality and also being easy to train.\nAs a result, a naive Bayes model is often a good prediction model to use to deﬁne a baseline\naccuracy score or when working with limited data.\n6.3.1 A Worked Example\nWe will use the dataset presented in Table 6.2[263]to illustrate how to create and use a naive\nBayes model for a prediction problem. This dataset relates to a fraud detection scenario in\nwhich we would like to build a model that predicts whether loan applications are fraudulent\nor genuine. There are three categorical descriptive features in this dataset. C REDIT HIS-\nTORY captures the credit history of the applicant, and its levels are none (the applicant has\nno previous loans), paid (the applicant had loans previously and has paid them off), current\n(the applicant has existing loans and are current in repayments), and arrears (the applicant\nhas existing loans and are in arrears in repayments). The G UARANTOR /COAPPLICANT\nfeature records whether the loan applicant has a guarantor or coapplicant associated with\nthe application. The levels are none ,guarantor , and coapplicant . The A CCOMMODATION\nfeature refers to the applicant’s current accommodation, and the levels are own (the ap-\nplicant owns their accommodation), rent (the applicant rents their accommodation), and\n14. Recall that sparse data , discussed in Section 5.4.5[211], refers to datasets where the majority of descriptive\nfeatures have a value of zero.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":317,"page_label":"263","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.3 Standard Approach: The Naive Bayes Model 263\nTable 6.2\nA dataset from a loan application fraud detection domain.\nCREDIT GUARANTOR /\nID H ISTORY COAPPLICANT ACCOMMODATION FRAUD\n1 current none own true\n2 paid none own false\n3 paid none own false\n4 paid guarantor rent true\n5 arrears none own false\n6 arrears none own true\n7 current none own false\n8 arrears none own false\n9 current none rent false\n10 none none own true\n11 current coapplicant own false\n12 current none own true\n13 current none rent true\n14 paid none own false\n15 arrears none own false\n16 current none own false\n17 arrears coapplicant rent false\n18 arrears none free false\n19 arrears none own false\n20 paid none own false\nfree(the applicant has free accommodation). The binary target feature, F RAUD , tells us\nwhether the loan application turned out to be fraudulent ( trueorfalse ).\nTo train a naive Bayes model using this data, we need to compute the prior probabilities\nof the target feature taking each level in its domain, and the conditional probability of each\nfeature taking each level in its domain conditioned for each level that the target can take.\nThere are two levels in the target feature domain, four levels in the C REDIT HISTORY\ndomain, three in the G UARANTOR /COAPPLICANT domain, and three in the A CCOMMO -\nDATION domain. This means that we need to calculate 2`p2ˆ4q`p2ˆ3q`p2ˆ3q“22\nprobabilities. Although this sounds like a lot of probabilities considering the size of the\nexample dataset, it is worth noting that these 22 probabilities would sufﬁce no matter how\nmany new instances are added to the dataset, be it hundreds of thousands, or even millions.\nThis is an example of the compactness of a naive Bayes representation. Be aware, however,\nthat if new descriptive features were added to the dataset, then the number of probabilities\nrequired would grow by |domain of target |ˆ|domain of new feature |, and, furthermore, if\nan extra value were added to the domain of the target, then the number of probabilities\nwould grow exponentially. Once the required probabilities are calculated, our naive Bayes","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":318,"page_label":"264","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"264 Chapter 6 Probability-Based Learning\nmodel is ready to make predictions for queries. It is that simple! Table 6.3[264]lists the\nprobabilities we need for our naive Bayes fraud detection model.\nTable 6.3\nThe probabilities needed by a naive Bayes prediction model, calculated from the data in Table 6.2[263].\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“none|f rq “ 0.1666 PpCH“none|␣f rq “ 0\nPpCH“paid|f rq “ 0.1666 PpCH“paid|␣f rq “ 0.2857\nPpCH“current|f rq “ 0.5 PpCH“current|␣f rq “ 0.2857\nPpCH“arrears|f rq “ 0.1666 PpCH“arrears|␣f rq “ 0.4286\nPpGC“none|f rq “ 0.8334 PpGC“none|␣f rq “ 0.8571\nPpGC“guarantor|f rq “ 0.1666 PpGC“guarantor|␣f rq “ 0\nPpGC“coapplicant|f rq “ 0 PpGC“coapplicant|␣f rq “ 0.1429\nPpACC“own|f rq “ 0.6666 PpACC“own|␣f rq “ 0.7857\nPpACC“rent|f rq “ 0.3333 PpACC“rent|␣f rq “ 0.1429\nPpACC“free|f rq “ 0 PpACC“free|␣f rq “ 0.0714\nNotation key: FR = F RAUD , CH = C REDIT HISTORY , GC = G UARANTOR /COAPPLICANT , ACC\n=ACCOMMODATION .\nThe following is a query instance for the fraud detection domain:\nCREDIT HISTORY =paid, GUARANTOR /COAPPLICANT =none ,\nACCOMMODATION =rent\nTable 6.4[265]shows the relevant probabilities needed to make a prediction for this query\nand the calculation of the scores for each possible prediction. Each calculation applies\nEquation (6.16)[261]and can be understood as a product of the four factors that the naive\nBayes model represents: PpFRq,PpCH|FRq,PpGC|FRq, and PpACC|FRq. The\nscores are 0.0139 for a prediction of true and0.0245 for a prediction of false . It is worth\nemphasizing that the scores calculated are not the actual posterior probabilities for each\ntarget level given the query evidence (to get the actual probabilities we would need to\nnormalize these scores), but they do give us enough information to rank the different target\nlevels based on the relative posterior probabilities. A naive Bayes prediction model returns\nthe MAP prediction, so our naive Bayes model would make a prediction of false and so\nclassify this loan application query as not fraudulent.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":319,"page_label":"265","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 265\nTable 6.4\nThe relevant probabilities, from Table 6.3[264], needed by the naive Bayes prediction model to make\na prediction for a query with CH = paid, GC = none , and ACC = rent, and the calculation of the\nscores for each target level.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“paid|f rq “ 0.1666 PpCH“paid|␣f rq “ 0.2857\nPpGC“none|f rq “ 0.8334 PpGC“none|␣f rq “ 0.8571\nPpACC“rent|f rq “ 0.3333 PpACC“rent|␣f rq “ 0.1429\nˆmź\nk“1Ppqrks|f rq˙\nˆPpf rq“0.0139\nˆmź\nk“1Ppqrks|␣f rq˙\nˆPp␣f rq“0.0245\nThere is one, non-obvious, aspect of this example that is particularly interesting. If we\nlook for an instance in the dataset in Table 6.2[263]that matches all the descriptive feature\nvalues in the query, we won’t ﬁnd one. Despite the lack of any instances that perfectly\nmatch the evidence, the fact that we were still able to calculate a score for each target\nlevel and make a prediction for the query highlights how the conditional independence\nassumption between the evidence given the target level both increases the coverage of the\nmodel and allows the model to generalize beyond the data used to induce it.\n6.4 Extensions and Variations\nIn this section we discuss extensions and variations of the naive Bayes model that increase\nits ability to generalize and avoid overﬁtting (smoothing) and that allow it to handle con-\ntinuous descriptive features. We also describe Bayesian networks, which are a probability-\nbased modeling approach that allows us to include more subtle assumptions in a model\nthan the global assumption of conditional independence between all descriptive features\nthat the naive Bayes model makes.\n6.4.1 Smoothing\nAlthough the assumption of conditional independence extends the coverage of a naive\nBayes model and allows it to generalize beyond the contents of the training data, naive\nBayes models still do not have complete coverage of the set of all possible queries. We\ncan see the reason for this in Table 6.3[264], where there are still some probabilities equal\nto zero, for example, PpCH“none|␣f rq. These arise when there are no instances in\nthe training data that match a speciﬁc combination of target feature and descriptive feature\nlevels. Consequently, a model is likely to overﬁt the data for any query where one or more","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":320,"page_label":"266","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"266 Chapter 6 Probability-Based Learning\nTable 6.5\nThe relevant probabilities, from Table 6.3[264], needed by the naive Bayes prediction model to make\na prediction for the query with CH = paid, GC = guarantor , and ACC = free, and the calculation of\nthe scores for each possible target level.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“paid|f rq “ 0.1666 PpCH“paid|␣f rq “ 0.2857\nPpGC“guarantor|f rq “ 0.1666 PpGC“guarantor|␣f rq “ 0\nPpACC“free|f rq “ 0 PpACC“free|␣f rq “ 0.0714\nˆmź\nk“1Ppqrks|f rq˙\nˆPpf rq“0.0\nˆmź\nk“1Ppqrks|␣f rq˙\nˆPp␣f rq“0.0\nof the evidence events match the conditioned event of one of these zero probabilities. For\nexample, consider the following query:\nCREDIT HISTORY =paid, GUARANTOR /COAPPLICANT =guarantor ,\nACCOMMODATION =free\nTable 6.5[266]lists the relevant probabilities needed to make a prediction for this query, and\nthe calculation of the scores for each of the possible target levels. In this instance, both\npossible predictions have a score of zero! Both scores are set to zero because one of the\nconditional probabilities used to calculate them is zero. For f rthe probability PpACC“\nfree|f rqcauses the problem, and for ␣f rthe probability PpGC“guarantor|␣f rqis\nthe offender. As a result, the model is unable to return a prediction for this query.\nThe way to solve this problem is by smoothing the probabilities used by the model. We\nknow from the deﬁnition of probability that the sum of the probabilities of a feature taking\neach of its possible levels should equal 1.0:\nÿ\nlPlevelspfqPpf“lq “ 1.0\nwhere fis a feature and levelspfqis the set of levels in the domain of the feature. This\nmeans that we have a total probability mass of 1.0 that is shared out between the different\nassignments of a level to a feature based on their relative frequency. Smoothing involves\ntaking some of the probability mass from the assignments with probability greater than\naverage and spreading it across the probabilities that are below average, or even equal to\nzero.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":321,"page_label":"267","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 267\nTable 6.6\nThe posterior probability distribution for the G UARANTOR /COAPPLICANT feature under the condi-\ntion that F RAUD =false .\nPpGC“none|␣f rq “ 0.8571\nPpGC“guarantor|␣f rq “ 0\nPpGC“coapplicant|␣f rq “ 0.1429\nÿ\nlPlevelspGCqPpGC“l|␣f rq “ 1.0\nFor example, if we sum across the posterior probability distribution for the G UARAN -\nTOR/COAPPLICANT feature under the condition that F RAUD =false , we will get a value\nof 1.0 (see Table 6.6[267]). Notice that within this set, PpGC“none|␣f rqis quite large,\nand at the other extreme, PpGC“guarantor|␣f rq“is equal to zero. Smoothing takes\nsome of the probability mass from the events with high probability and shares this with\nthe events with low probabilities. If this is done correctly, then the total probability mass\nfor the set will remain equal to 1.0, but the spread of probabilities across the set will be\nsmoother (hence the name smoothing).\nThere are several different ways to smooth probabilities. We will use Laplace smooth-\ning. Note, that in general, it does not make sense to smooth the unconditional (prior)\nprobabilities for the different target feature levels,15so here we will focus on smoothing\nthe conditional probabilities for the features. Laplace smoothing for conditional probabili-\nties is deﬁned as\nPpf“l|tq “countpf“l|tq`k\ncountpf|tq`p kˆ|Domainpfq|q\nwhere countpf“l|tqis how often the event f“loccurs in the subset of rows in the\ndataset where the target level is t,countpf|tqis how often the feature, f, took any level in\nthe subset of rows in the dataset where the target level is t,|Domainpfq|is the number of\nlevels in the domain of the feature, and kis a predetermined parameter. Larger values of k\nmean that more smoothing occurs—that is, more probability mass is taken from the larger\nprobabilities and given to the small probabilities. Typically ktakes small values such as 1,\n2, or 3.\n15. The primary reason why we apply smoothing is to remove zero probabilities from a model’s representation\nof a domain, and in the vast majority of cases, all the unconditional target level probabilities will be non-zero\n(because there will be at least one instance with each target level in the training data). Even in cases where one\nof the target levels is very rare, it may not be appropriate to smooth the target level priors. See Bishop (2006, pp.\n45) for a discussion on how to train a probability-based prediction model in situations where one of the target\nlevels is rare.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":322,"page_label":"268","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"268 Chapter 6 Probability-Based Learning\nTable 6.7\nSmoothing the posterior probabilities for the G UARANTOR /COAPPLICANT feature conditioned on\nFRAUD =false .\nRaw PpGC“none|␣f rq “ 0.8571\nProbabilities PpGC“guarantor|␣f rq “ 0\nPpGC“coapplicant|␣f rq “ 0.1429\nSmoothing k“ 3\nParameters countpGC|␣f rq “ 14\ncountpGC“none|␣f rq “ 12\ncountpGC“guarantor|␣f rq “ 0\ncountpGC“coapplicant|␣f rq “ 2\n|DomainpGCq| “ 3\nSmoothed PpGC“none|␣f rq“12`3\n14`p3ˆ3q“ 0.6522\nProbabilities PpGC“guarantor|␣f rq“0`3\n14`p3ˆ3q“ 0.1304\nPpGC“coapplicant|␣f rq“2`3\n14`p3ˆ3q“ 0.2174\nTable 6.7[268]illustrates the steps in smoothing the posterior probabilities for the G UAR-\nANTOR /COAPPLICANT feature when conditioned on F RAUD =false . We can see that\nafter smoothing, the probability mass is more evenly distributed across the events in the\nset. Crucially, the posterior probability for PpGC“guarantor|␣f rqis no longer zero,\nand as a result, the coverage of the model has been extended to include queries with G UAR-\nANTOR /COAPPLICANT values of guarantor .\nTable 6.8[269]lists the prior and smoothed conditional probabilities for the fraud domain\nthat are relevant to a naive Bayes model. Notice that there are no zero probabilities, so the\nmodel will be able to return a prediction for any query in this domain. We can illustrate\nthe extended coverage of the model by returning to the query from the beginning of this\nsection:\nCREDIT HISTORY =paid, GUARANTOR /COAPPLICANT =guarantor ,\nACCOMMODATION =free\nTable 6.9[270]illustrates how a naive Bayes model would calculate the scores for each can-\ndidate target level for this query using the smoothed probabilities from Table 6.8[269]. Using\nour smoothed probabilities, we are able to calculate a score for both target levels: 0.0036","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":323,"page_label":"269","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 269\nTable 6.8\nThe Laplace smoothed (with k“3) probabilities needed by a naive Bayes prediction model, calcu-\nlated from the dataset in Table 6.2[263].\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“none|f rq “ 0.2222 PpCH“none|␣f rq “ 0.1154\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpCH“current|f rq “ 0.3333 PpCH“current|␣f rq “ 0.2692\nPpCH“arrears|f rq “ 0.2222 PpCH“arrears|␣f rq “ 0.3462\nPpGC“none|f rq “ 0.5333 PpGC“none|␣f rq “ 0.6522\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpGC“coapplicant|f rq “ 0.2 PpGC“coapplicant|␣f rq “ 0.2174\nPpACC“own|f rq “ 0.4667 PpACC“own|␣f rq “ 0.6087\nPpACC“rent|f rq “ 0.3333 PpACC“rent|␣f rq “ 0.2174\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nNotation key: FR =F RAUD , CH = C REDIT HISTORY , GC = G UARANTOR /COAPPLICANT ,\nACC =A CCOMMODATION .\nfortrueand0.0043 forfalse . The target level false has the highest score (if only marginally)\nand is the MAP prediction for this query. Therefore, our naive Bayes model will predict\nthat this loan application is not fraudulent.\n6.4.2 Continuous Features: Probability Density Functions\nTo calculate the probability of an event, we have simply counted how often the event oc-\ncurred and divided this number by how often the event could have occurred. A continuous\nfeature can have an inﬁnite number of values in its domain, so any particular value will oc-\ncur a negligible amount of the time. In fact, the relative frequency of any particular value\nfor a continuous feature will be indistinguishable from zero given a large dataset.\nThe way to solve the problem of zero probabilities is to think in terms of how the prob-\nability of a continuous feature taking a value is distributed across the range of values that\na continuous feature can take. A probability density function (PDF ) represents the prob-\nability distribution of a continuous feature using a mathematical function, and there are a\nlarge number of standard, well-deﬁned probability distributions—such as the normal dis-\ntribution —that we can use to model the probability of a continuous feature taking different\nvalues in its range.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":324,"page_label":"270","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"270 Chapter 6 Probability-Based Learning\nTable 6.9\nThe relevant smoothed probabilities, from Table 6.8[269], needed by the naive Bayes prediction model\nto make a prediction for the query with CH = paid, GC = guarantor , and ACC = free, and the\ncalculation of the scores for each target levels.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nˆmź\nk“1Ppqrms|f rq˙\nˆPpf rq“0.0036\nˆmź\nk“1Ppqrms|␣f rq˙\nˆPp␣f rq“0.0043\nValuesDensity\n(a) Normal/Student- t\nValuesDensity (b) Exponential\nValuesDensity (c) Mixture of Gaussians\nFigure 6.3\nPlots of some well-known probability distributions.\nTable 6.10[271]shows the deﬁnition of some of the standard probability distributions—the\nnormal ,exponential , and mixture of Gaussians distributions—that are commonly used\nin probabilistic prediction models, and Figure 6.3[270]illustrates the shapes of the density\ncurves of these distributions. All standard PDFs have parameters that alter the shape of the\ndensity curve deﬁning that distribution. The parameters required for the normal, exponen-\ntial, and mixture of Gaussians PDFs are shown in Table 6.10[271]. In order to use a PDF to\nrepresent the probability of a continuous feature taking different values, we need to choose\nthese parameters to ﬁt the characteristics of the data. We have already described the normal\ndistribution, in some detail, in Section 3.2.1[61], so we won’t repeat that introduction here,\nbut we will describe the other distributions in a little detail.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":325,"page_label":"271","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 271\nTable 6.10\nDeﬁnitions of some standard probability distributions.\nNormal\nNpx,µ,σq“1\nσ?\n2πe´px´µq2\n2σ2xPR\nµPR\nσPRą0\nStudent- t\nτpx,φ,ρ,κq“Γpκ`1\n2q\nΓpκ\n2qˆ?πκˆρˆˆ\n1`ˆ1\nκˆz2˙˙´κ`1\n2xPR\nφPR\nρPRą0\nκPRą0\nz“x´φ\nρ\nExponential\nEpx,λq“#\nλe´λxforxą0\n0 otherwisexPR\nλPRą0\nMixture of nGaussians\nNpx,µ1,σ1,ω1,...,µ n,σn,ωnq“nÿ\ni“1ωi\nσi?\n2πe´px´µiq2\n2σ2\nixPR\ntµ1,...,µ n|µiPRu\ntσ1,...,σ n|σiPRą0u\ntω1,...,ω n|ωiPRą0uřn\ni“1ωi“0\nThestudent- tdistribution is symmetric around a single peak. In fact, it looks very sim-\nilar to a normal distribution, as shown in Figure 6.3(a)[270]. The deﬁnition of the student- t\nprobability density function uses the gamma function ,Γpq, which is a standard statistical\nfunction.16The student- tdistribution is a member of the location-scale family of distribu-\ntions.17These distributions take two parameters: a location parameterφ, which speciﬁes\nthe position of the peak density of the distribution, and a non-negative scale parameterρ,\nwhich speciﬁes how spread out the distribution is; the higher the scale the more spread\nout the distribution. The normal distribution is a member of this location-scale family,\nwith the mean µspecifying the location, and the standard deviation σacting as the scale\n16. See Tijms (2012), or any good probability textbook, for an introduction to the gamma function.\n17. The student- tdistribution can be deﬁned in a number of ways. For example, it can be deﬁned so that it takes\nonly one parameter, degrees of freedom. In this text we use the extended location-scale deﬁnition.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":326,"page_label":"272","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"272 Chapter 6 Probability-Based Learning\nparameter. We use different notation for location and scale parameters, φandρ, than we do\nfor mean and standard deviation parameters of the normal, µandσ, because the values of\nthese parameters are estimated using different techniques: generally, the location and scale\nparameters for distributions are ﬁtted to the data using a guided search process.18The\nstudent- tdistribution, however, takes an extra parameter κ. This parameter is the degrees\nof freedom of the distribution. In statistics the degrees of freedom of a distribution is the\nnumber of variables in the calculation of the statistic that are free to vary. For the student- t\ndistribution, the degrees of freedom is always set to the sample size (number of rows in the\ndataset) minus one.\nFrom a distribution perspective, the main distinction between a normal distribution and\na student- tis that a normal distribution has light tails whereas the student- tdistribution\nhasfat tails . Figure 6.4[273]illustrates the distinction between fat and light tail distribu-\ntions using histograms of two datasets. The dataset in Figure 6.4(a)[273]follows a light tail\ndistribution—the bars at the extreme left and right of the distribution have zero height.\nThe dataset in Figure 6.4(b)[273]has a fat tail distribution—the bars on the extreme left\nand right of the distribution are still above zero, if only just. This distinction between fat\nand light tailed distributions is important because it highlights that when we use a normal\ndistribution, we are implicitly assuming that the likelihood of values that differ from the\nmean of the distribution drops quite dramatically as we move away from the mean. A\ncommon mistake made by many data analysts is to automatically default to modeling uni-\nmodally distributed data with a normal distribution.19There are statistical tests (such as the\nKolmogorov-Smirnov test ) that can be used to check whether or not a feature is normally\ndistributed, and in cases where the feature is not normally distributed, another unimodal\ndistribution, such as the student- tdistribution , may be a better ﬁt.\nAnother consequence of the normal distribution having light tails is that it is sensitive\nto outliers in the data. Figure 6.5[273]illustrates how outliers affect normal and student- t\ndistributions. Figure 6.5(a)[273]shows a histogram of a dataset that has been overlaid with","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":326,"page_label":"272","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"common mistake made by many data analysts is to automatically default to modeling uni-\nmodally distributed data with a normal distribution.19There are statistical tests (such as the\nKolmogorov-Smirnov test ) that can be used to check whether or not a feature is normally\ndistributed, and in cases where the feature is not normally distributed, another unimodal\ndistribution, such as the student- tdistribution , may be a better ﬁt.\nAnother consequence of the normal distribution having light tails is that it is sensitive\nto outliers in the data. Figure 6.5[273]illustrates how outliers affect normal and student- t\ndistributions. Figure 6.5(a)[273]shows a histogram of a dataset that has been overlaid with\nthe curves of a normal and a student- tdistribution that have been ﬁtted to the data. The\nnormal and the student- tdistributions are both very similar, and both do a good job of\nmatching the shape of the density histogram. Figure 6.5(b)[273]shows a histogram of the\nsame dataset after some outliers have been added to the extreme right of the distribution.\nAgain, we have overlaid the histogram with plots of the curves for a normal and a student- t\ndistribution that have been ﬁtted to the updated dataset. Comparing Figure 6.5(a)[273]and\nFigure 6.5(b)[273], we can see clearly that the introduction of outliers has a much larger effect\n18. This guided search process is similar to the gradient descent search we use to ﬁt our regression models in\nChapter 7[311]. Many data analytics packages and programming APIs provide functions that implement methods\nto ﬁt a distribution to a dataset.\n19. Taleb (2008) discusses the problems that arise when analysts use normal distributions to model social and\neconomic features, where the assumptions regarding light tails don’t hold.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":327,"page_label":"273","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 273\n(a)\n (b)\nFigure 6.4\nHistograms of two unimodal datasets: (a) the distribution has light tails; and (b) the distribution has\nfat tails.\nNormal\nStudent−t\n(a)\nNormal\nStudent−t (b)\nFigure 6.5\nIllustration of the robustness of the student- tdistribution to outliers: (a) a density histogram of a\nunimodal dataset overlaid with the density curves of a normal and a student- tdistribution that have\nbeen ﬁtted to the data; and (b) a density histogram of the same dataset with outliers added, overlaid\nwith the density curves of a normal and a student- tdistribution that have been ﬁtted to the data. (This\nﬁgure is inspired by Figure 2.16 in Bishop (2006).)\non the normal distribution than it does on the student- tdistribution. The robustness of the\nstudent- tto outliers is another reason to consider using this distribution, as opposed to a\nnormal distribution, to model unimodal data in situations with relatively small or possibly\nnoisy datasets.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":328,"page_label":"274","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"274 Chapter 6 Probability-Based Learning\nThe plot of the density curve for the exponential distribution (Figure 6.3(b)[270]) shows\nthat it assigns a high probability to values near the left of the distribution and that the\nprobability of a value occurring drops dramatically as we move to the right. The standard\nrange for the exponential distribution is from zero upward (i.e., the density assigned to\nvalues less than zero is zero). However, we can adjust this by offsetting the values input\ninto the distribution. The exponential distribution takes one parameter, λ, known as the\nrate. Varying the value λchanges the rate at which the density drops off. As λgets larger,\nthe peak of the distribution (on the left) gets larger, and the drop-off in density gets steeper.\nTo ﬁt an exponential distribution to a continuous feature, we set λequal to 1 divided by the\nmean of the feature. The exponential distribution is often used to model waiting times (for\nexample, how long it will take for a call to be answered at a help desk, how long you will\nhave to wait for a bus, or how long before a piece of hardware fails), where the parameter\nλis equal to 1 divided by the average time it takes for the event.\nAs the name suggests, the mixture of Gaussians distribution is the distribution that\nresults when a number of normal (or Gaussian) distributions are merged. Mixture of Gaus-\nsians distributions are used to represent data that is composed of multiple subpopulations.\nFigure 6.6(a)[275]illustrates the proﬁle typical of data with multiple subpopulations. The\nmultiple peaks in the density curve arise from the different subpopulations (a distribution\nwith multiple peaks is called multimodal ). Using a mixture of Gaussians distribution as-\nsumes that all the subpopulations in the data are distributed following a normal distribution,\nbut that each of these subpopulation normal distributions has a different mean and may also\nhave a different standard deviation.\nThe deﬁnition of the mixture of Gaussians distribution in Table 6.10[271]shows how the\nindividual normal distributions in a mixture of Gaussians distribution are combined using\na weighted sum. Each normal that is merged is known as a component of the mixture.\nThe weight of a component in the sum determines the contribution of the component to the\noverall density of the resulting mixture. A mixture of Gaussians distribution is deﬁned by\nthree parameters for each component: a mean, µ, a standard deviation, σ, and a weight, ω.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":328,"page_label":"274","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"but that each of these subpopulation normal distributions has a different mean and may also\nhave a different standard deviation.\nThe deﬁnition of the mixture of Gaussians distribution in Table 6.10[271]shows how the\nindividual normal distributions in a mixture of Gaussians distribution are combined using\na weighted sum. Each normal that is merged is known as a component of the mixture.\nThe weight of a component in the sum determines the contribution of the component to the\noverall density of the resulting mixture. A mixture of Gaussians distribution is deﬁned by\nthree parameters for each component: a mean, µ, a standard deviation, σ, and a weight, ω.\nThe set of weight parameters for the mixture must sum to 1.\nThere is no closed form solution to calculate the parameters to ﬁt a mixture of Gaussians\ndistribution to a set of feature values, as there is for the exponential and normal distribu-\ntions. Instead, given the set of values for a continuous feature, we ﬁt a mixture of Gaussians\ndistribution to this data by searching for the number of components and set of parameters\nfor each component that best matches the data. Guided search techniques, such as the\ngradient descent algorithm, are used for this task. Analysts will often input a suggested\nstarting point for this search based on their own analysis of the data in order to guide the\nprocess.\nIn Figure 6.6(b)[275]we can see the three normal distributions used to model the multi-\nmodal distribution in Figure 6.6(a)[275]. Each normal distribution has a different mean but","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":329,"page_label":"275","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 275\nValuesDensity\n(a)\nValuesDensity (b)\nValuesDensity (c)\nFigure 6.6\nIllustration of how a mixture of Gaussians model is composed of a number of normal distributions.\nThe curve plotted using a solid line is the mixture of Gaussians density curve, created using an\nappropriately weighted summation of the three normal curves, plotted using dashed and dotted lines.\nthe same standard deviation. The size of the individual normal density curves is propor-\ntional to the weight for that normal used in the mixture. Figure 6.6(c)[275]overlays the mul-\ntimodal density curve on top of the three weighted normals. It is clear from this ﬁgure that\nthe weighted sum of the three normals does an excellent job of modeling the multimodal\ndensity distribution.\nThe fact that we have a range of parameterized distributions to choose from means that\nin order to deﬁne a probability density function (PDF), we must\n1.Select which probability distribution we believe will best model the distribution of\nthe values of the feature. The simplest and most direct way to choose a distribution\nfor a feature is to create a density histogram of the feature’s values and compare the\nshape of this histogram to the shapes of the standard distributions. We should choose\nwhichever standard distribution best matches the shape of the histogram to model the\nfeature.\n2.Fit the parameters of the selected distribution to the feature values in the dataset. It\nis relatively straightforward to ﬁt the parameters, µandσ, of the normal distribution\nto a dataset by using the sample mean and standard deviation of the feature values\nin a dataset as estimates of µandσrespectively. Similar to the normal distribution,\ntheλparameter for the exponential distribution can be easily calculated by using the\nvalue of 1 divided by the mean of the data. However, for many of the other statistical\ndistributions, for example, the mixture of Gaussians distribution, we cannot deﬁne an\nequation over the data that estimates the parameters appropriately. For these distribu-\ntions, the parameters are set using guided search techniques such as gradient descent .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":330,"page_label":"276","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"276 Chapter 6 Probability-Based Learning\nFortunately, most data analytics packages and programming APIs provide functions\nthat implement methods to ﬁt a speciﬁed distribution to a given dataset.20\nA PDF is an abstraction over a density histogram and, as such, deﬁnes a density curve.\nThe shape of the curve is determined by (a) the statistical distribution that is used to deﬁne\nthe PDF, and (b) the values of the statistical distribution parameters. To use a PDF to\ncalculate a probability, we need to think in terms of the area under an interval of the PDF\ncurve. Consequently, to calculate a probability using a PDF, we need to ﬁrst decide on\nthe interval we wish to calculate the probability for, and then calculate the area under the\ndensity curve for that interval to give the probability of a value from that interval occurring.\nThere is no hard and fast rule for deciding on interval size . Instead, this decision is made\non a case-by-case basis and is dependent on the precision required in answering a question.\nIn some cases, the size of the interval is deﬁned as part of the problem we are trying to\nsolve, or there may be a natural interval to use because of the domain. For example, when\nwe are dealing with a ﬁnancial feature, we might use intervals that represent cents, while\nif we were dealing with temperature, we might deﬁne the interval to be 1 degree. Once we\nhave selected the interval size, we need to calculate the area under the density curve for\nthat interval.21\nWhen we use a PDF to represent the probability distribution of a descriptive feature in a\nnaive Bayes model, however, we don’t actually need to calculate exact probabilities. We\nonly need to calculate the relative likelihood of a continuous feature taking a value given\ndifferent levels of a target feature. The height of the density curve deﬁned by a PDF at a\nparticular feature value gives us this, so we can avoid the effort of calculating the actual\nprobability. We can use a value from a PDF as a relative measure of likelihood because\nwhen the interval is very small, the actual area under a PDF curve for that interval can be\napproximated (with a small error proportional to the width of the interval) by the height of\nthe PDF curve at the center of the interval multiplied by the width of the interval. Figure\n6.7[277]illustrates this approximation.\nIf we were to include the interval width when calculating conditional probabilities for a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":330,"page_label":"276","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"different levels of a target feature. The height of the density curve deﬁned by a PDF at a\nparticular feature value gives us this, so we can avoid the effort of calculating the actual\nprobability. We can use a value from a PDF as a relative measure of likelihood because\nwhen the interval is very small, the actual area under a PDF curve for that interval can be\napproximated (with a small error proportional to the width of the interval) by the height of\nthe PDF curve at the center of the interval multiplied by the width of the interval. Figure\n6.7[277]illustrates this approximation.\nIf we were to include the interval width when calculating conditional probabilities for a\ncontinuous descriptive feature in a naive Bayes prediction model, using Equation (6.16)[261],\nwe would multiply the value returned by the PDF by the same interval width each time we\ncalculated the likelihood score for a level of the target feature. Consequently, we can drop\nthis multiplication and just use the value returned by the PDF as a relative measure of the\nlikelihood that the feature takes a speciﬁc value.\n20. For example, the R language provides the ﬁtdistr() method, as part of the MASS package, that implements a\nmaximum-likelihood ﬁtting of a number of univariate distributions to a given dataset.\n21. We can do this either by consulting a probability table or by using integration to calculate the area under the\ncurve within the bounds of the interval. There are many excellent statistical textbooks that explain how to do both\nof these, for example, Montgomery and Runger (2010).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":331,"page_label":"277","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 277\nx−ε\n2x x+ε\n2PDF(x+ε\n2)PDF(x)PDF(x−ε\n2)\n(a)\nx−ε\n2x x+ε\n2PDF(x) (b)\nx−ε\n2x x+ε\n2PDF(x+ε\n2)PDF(x)PDF(x−ε\n2)\nA\nB (c)\nFigure 6.7\n(a) The area under a density curve between the limits x´ϵ\n2andx`ϵ\n2; (b) the approximation of\nthis area computed by PDFpxqˆϵ; and (c) the error in the approximation is equal to the difference\nbetween area A, the area under the curve omitted from the approximation, and area B, the area above\nthe curve erroneously included in the approximation. Both of these areas will get smaller as the\nwidth of the interval gets smaller, resulting in a smaller error in the approximation.\nTo ground our discussion of PDFs, and to illustrate how they can be used in making naive\nBayes prediction models, we will extend our loan application fraud detection scenario to\nhave two extra continuous features: A CCOUNT BALANCE , which speciﬁes the amount\nof money in the account of the loan applicant at the time of the application; and L OAN\nAMOUNT , which speciﬁes the amount of the loan being applied for. Table 6.11[278]lists this\nextended dataset. We ﬁrst use only the extra A CCOUNT BALANCE feature in the dataset\n(ignoring L OAN AMOUNT , which we return to later in this chapter) to demonstrate how\nPDFs allow us to include continuous features in a naive Bayes model.\nTo enable the naive Bayes model to handle the A CCOUNT BALANCE feature, we have\nto extend the set of probabilities used by the model to represent the domain to include the\nprobabilities for this feature. Recall that the naive Bayes domain representation deﬁnes a\nconditional probability for each possible value in the domain of a descriptive feature for\neach level in the domain of the target. In our example, the target feature, F RAUD , is binary,\nso we need to deﬁne two conditional probabilities for each value in the domain of the new\ndescriptive feature: PpAB“x|f rqandPpAB“x| ␣f rq. Because the descriptive\nfeature A CCOUNT BALANCE is continuous, there is an inﬁnite number of values in the\nfeature’s domain. However, we know that using an appropriately deﬁned PDF, we can\napproximate the probability of the feature taking any value in its domain. As a result,\nwe simply need to deﬁne two PDFs for the new feature with each PDF conditioned on\na different level of the target feature: PpAB“x|f rq “ PDF 1pAB“x|f rqand","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":332,"page_label":"278","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"278 Chapter 6 Probability-Based Learning\nTable 6.11\nThe dataset from the loan application fraud detection domain (from Table 6.2[263]) with two continu-\nous descriptive features added: A CCOUNT BALANCE and L OAN AMOUNT .\nCREDIT GUARANTOR / A CCOMMO - A CCOUNT LOAN\nID H ISTORY COAPPLICANT DATION BALANCE AMOUNT FRAUD\n1 current none own 56.75 900 true\n2 current none own 1,800.11 150,000 false\n3 current none own 1,341.03 48,000 false\n4 paid guarantor rent 749.50 10,000 true\n5 arrears none own 1,150.00 32,000 false\n6 arrears none own 928.30 250,000 true\n7 current none own 250.90 25,000 false\n8 arrears none own 806.15 18,500 false\n9 current none rent 1,209.02 20,000 false\n10 none none own 405.72 9,500 true\n11 current coapplicant own 550.00 16,750 false\n12 current none free 223.89 9,850 true\n13 current none rent 103.23 95,500 true\n14 paid none own 758.22 65,000 false\n15 arrears none own 430.79 500 false\n16 current none own 675.11 16,000 false\n17 arrears coapplicant rent 1,657.20 15,450 false\n18 arrears none free 1,405.18 50,000 false\n19 arrears none own 760.51 500 false\n20 current none own 985.41 35,000 false\nPpAB“x|␣f rq“PDF 2pAB“x|␣f rq. These two PDFs do not have to be deﬁned\nusing the same distribution. Once we have selected the distributions we wish to use, to\ndeﬁne a PDF for a descriptive feature that is conditioned on a particular target, we ﬁt the\nparameters of the selected distribution to the subset of the data where the target has that\nvalue.\nThe ﬁrst step in deﬁning the two PDFs is to decide which distribution we will use to de-\nﬁne the PDFs for each target feature level. To make this decision, we partition the training\ndata based on the target feature and generate histograms of the values of the descriptive\nfeature for each of the splits. We then select the statistical distribution that is most similar\nin shape to each of the resulting histograms. Figure 6.8[279]shows the histograms of the\nvalues of the A CCOUNT BALANCE feature partitioned on the two levels of the F RAUD\ntarget feature. It is clear from these histograms that the distribution of values taken by\nthe A CCOUNT BALANCE feature in the set of instances where F RAUD =true follows an\nexponential distribution; whereas, the distribution of the values taken by the A CCOUNT","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":333,"page_label":"279","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 279\nFeature V aluesDensity\n0 500 1000 1500 20000.0000 0.0005 0.0010 0.0015 0.0020\n(a)\nFeature V aluesDensity\n0 500 1000 1500 20000.0000 0.0005 0.0010 0.0015 0.0020 (b)\nFigure 6.8\nHistograms, using a bin size of 250 units, and density curves for the A CCOUNT BALANCE feature:\n(a) the fraudulent instances overlaid with a ﬁtted exponential distribution; and (b) the non-fraudulent\ninstances overlaid with a ﬁtted normal distribution.\nBALANCE feature in the set of instances where the F RAUD =false is similar to a normal\ndistribution.\nOnce we have selected the distributions, the next step is to ﬁt the distributions to the data.\nTo ﬁt the exponential distribution, we compute the sample mean of the A CCOUNT BAL-\nANCE feature in the set of instances where F RAUD =trueand set theλparameter equal to 1\ndivided by this value. To ﬁt the normal distribution to the set of instances where F RAUD =\nfalse , we compute the sample mean and sample standard deviation for the A CCOUNT BAL-\nANCE feature for this set of instances and set the parameters of the normal distribution to\nthese values. Table 6.12[280]shows how these values are calculated, and the dashed lines in\nFigure 6.8[279]plot the density curves that result from this process. Once distributions have\nbeen ﬁtted to the data, we can extend the naive Bayes domain representation to include the\nPDFs. Table 6.13[281]shows the extended domain representation.\nTo use the extended domain representation of the model to make a prediction for a query,\nwe calculate the product of the relevant descriptive feature probabilities and the priors for\nthe different target levels as before, but using PDFs to calculate the probabilities for the\ncontinuous feature. Table 6.14[282]shows how a prediction is made for the following query:\nCREDIT HISTORY =paid, GUARANTOR /COAPPLICANT =guarantor ,\nACCOMMODATION =free, ACCOUNT BALANCE =759.07","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":334,"page_label":"280","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"280 Chapter 6 Probability-Based Learning\nTable 6.12\nPartitioning the dataset based on the value of the target feature and ﬁtting the parameters of a statis-\ntical distribution to model the A CCOUNT BALANCE feature in each partition.\n(a) Instances where F RAUD =trueand the ﬁtted\nparameters for the exponential distribution\nACCOUNT\nID... BALANCE FRAUD\n1 56.75 true\n4 749.50 true\n6 928.30 true\n10... 405.72 true\n12 223.89 true\n13 103.23 true\nAB 411.22\nλ“1!{AB 0.0024(b) Instances where F RAUD =false and the ﬁt-\nted parameters for the normal distribution\nACCOUNT\nID... BALANCE FRAUD\n2 1,800.11 false\n3 1,341.03 false\n5 1,150.00 false\n7 250.90 false\n8 806.15 false\n9 1,209.02 false\n11... 550.00 false\n14 758.22 false\n15 430.79 false\n16 675.11 false\n17 1,657.20 false\n18 1,405.18 false\n19 760.51 false\n20 985.41 false\nAB 984.26\nsdpABq 460.94\nNote: A CCOUNT BALANCE has been shortened to AB in these tables.\nThe calculations for the probabilities for the A CCOUNT BALANCE feature are made using\nthe equations for the normal and exponential distributions in Table 6.10[271]. The result is\nthat F RAUD =false still has the highest score and will be returned as the prediction for this\nquery.\n6.4.3 Continuous Features: Binning\nA commonly used alternative to representing a continuous feature using a probability den-\nsity function is to convert the feature into a categorical feature using binning . In Section\n3.6.2[89]we explained two of the best known binning techniques, equal-width binning and\nequal-frequency binning , and discussed some of the general advantages and disadvan-\ntages of each technique. One feature of equal-width binning is that it can result in a very\nuneven distribution of instances across the bins, with some bins containing a large num-\nber of instances and other bins being nearly empty. This uneven distribution of instances\nacross bins can have dramatic and unwanted consequences for probability-based models.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":335,"page_label":"281","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 281\nTable 6.13\nThe Laplace smoothed (with k“3) probabilities needed by a naive Bayes prediction model, calcu-\nlated from the dataset in Table 6.11[278], extended to include the conditional probabilities for the new\nACCOUNT BALANCE feature, which are deﬁned in terms of PDFs.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“none|f rq “ 0.2222 PpCH“none|␣f rq “ 0.1154\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpCH“current|f rq “ 0.3333 PpCH“current|␣f rq “ 0.2692\nPpCH“arrears|f rq “ 0.2222 PpCH“arrears|␣f rq “ 0.3462\nPpGC“none|f rq “ 0.5333 PpGC“none|␣f rq “ 0.6522\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpGC“coapplicant|f rq “ 0.2 PpGC“coapplicant|␣f rq “ 0.2174\nPpACC“own|f rq “ 0.4667 PpACC“own|␣f rq “ 0.6087\nPpACC“rent|f rq “ 0.3333 PpACC“rent|␣f rq “ 0.2174\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nPpAB“x|f rq PpAB“x|␣f rq\n«E¨\n˚˝x,\nλ“0.0024˛\n‹‚ «N¨\n˚˚˚˚˝x,\nµ“984.26,\nσ“460.94˛\n‹‹‹‹‚\nNotation key: FR = F RAUD , CH = C REDIT HISTORY , GC = G UARANTOR /COAPPLICANT ,\nACC =A CCOMMODATION , AB =A CCOUNT BALANCE .\nBins that contain only a few instances may have extremely small or extremely large condi-\ntional probabilities (depending on how the instances are divided when conditioned on the\ntarget feature), and these extreme conditional probabilities may bias a model based on the\nparameters of the binning technique (for example, the number of bins we choose to have)\nrather than on real distributions in the data. For this reason, we recommend the use of\nequal-frequency binning to convert continuous features to categorical ones for probability-\nbased models.\nReturning to our loan application fraud detection example, we will show how binning\ncan be used to include the L OAN AMOUNT feature (see Table 6.11[278]) in a naive Bayes\nprediction model for this scenario. Table 6.15[283]shows the discretization of the L OAN\nAMOUNT feature into four equal-frequency bins. In this table, the instances in the dataset","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":336,"page_label":"282","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"282 Chapter 6 Probability-Based Learning\nTable 6.14\nThe probabilities, from Table 6.13[281], needed by the naive Bayes prediction model to make a pre-\ndiction for the query with CH = paid, GC = guarantor , ACC = free, and AB = 759.07, and the\ncalculation of the scores for each candidate prediction.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nPpAB“759.07|f rq PpAB“759.07|␣f rq\n«E¨\n˚˝759.07,\nλ“0.0024˛\n‹‚“ 0.00039 «N¨\n˚˚˚˚˝759.07,\nµ“984.26,\nσ“460.94˛\n‹‹‹‹‚“ 0.00077\nˆmź\nk“1Ppqrks|f rq˙\nˆPpf rq“0.0000014\nˆmź\nk“1Ppqrks|␣f rq˙\nˆPp␣f rq“0.0000033\nhave been reordered in ascending order based on their L OAN AMOUNT values. Even when\nusing equal-frequency binning, there is still chance that the partitioning of the data will\ngive rise to extreme conditional probabilities. For example, all the bin3 values have a\ntarget feature value of false . Consequently, the posterior probability of L OAN AMOUNT\n=bin3 conditioned on F RAUD =true will be 0.0 and L OAN AMOUNT =bin3 conditioned\nFRAUD =false will be 1.0. Smoothing should be used in conjunction with binning to help\nwith these extreme probabilities.\nOnce we have discretized the data using binning, we need to record the raw continuous\nfeature thresholds between the bins. The reason for this is that we need to be able to bin\nthe features of any query instances appropriately before we make predictions for them. To\ncalculate these thresholds, we take the midpoint in the feature range between the instance\nwith the highest feature value in one bin and the feature with the lowest feature value in the\nnext bin. For example, the instances in Table 6.15[283]are ordered in ascending order based\non the magnitude of their original L OAN AMOUNT value. So, the threshold between bin1\nandbin2 will be the midpoint between the L OAN AMOUNT values for d12(9,850) and d4\n(10,000) which is 9,925. The threshold boundaries for the four bins used to discretize the\nLOAN AMOUNT feature are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":337,"page_label":"283","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 283\nTable 6.15\nThe L OAN AMOUNT continuous feature discretized into four equal-frequency bins.\nBINNED\nLOAN LOAN\nID A MOUNT AMOUNT FRAUD\n15 500 bin1 false\n19 500 bin1 false\n1 900 bin1 true\n10 9,500 bin1 true\n12 9,850 bin1 true\n4 10,000 bin2 true\n17 15,450 bin2 false\n16 16,000 bin2 false\n11 16,750 bin2 false\n8 18,500 bin2 falseBINNED\nLOAN LOAN\nID A MOUNT AMOUNT FRAUD\n9 20,000 bin3 false\n7 25,000 bin3 false\n5 32,000 bin3 false\n20 35,000 bin3 false\n3 48,000 bin3 false\n18 50,000 bin4 false\n14 65,000 bin4 false\n13 95,500 bin4 true\n2 150,000 bin4 false\n6 250,000 bin4 true\nbin1ď9,925\n9,925ă bin2ď19,250\n19,225ă bin3ď49,000\n49,000ă bin4\nOnce we have discretized the continuous features and calculated the thresholds for bin-\nning query features, we are ready to create our predictive model. As before, for a naive\nBayes model, we calculate the prior probability distribution for the target feature and the\nposterior distribution for each descriptive feature conditioned on the target feature. Again,\nwe should smooth the resulting probabilities. Table 6.16[284]shows the Laplace smoothed\n(with k“3) probabilities required by a naive Bayes prediction model calculated from the\ndataset in Table 6.11[278]. Notice that in this domain representation, we blend different ap-\nproaches to continuous features: we are retaining the PDFs developed in Section 6.4.2[269]\nfor the A CCOUNT BALANCE feature and extend the representation with the binned version\nof the L OAN AMOUNT feature, B INNED LOAN AMOUNT .\nWe are now ready to process a query that has the continuous L OAN AMOUNT feature as\npart of the evidence:\nCREDIT HISTORY =paid, GUARANTOR /COAPPLICANT =guarantor ,\nACCOMMODATION =free, ACCOUNT BALANCE =759.07,\nLOAN AMOUNT =8,000\nThe L OAN AMOUNT value for this query ( 8,000) is below the threshold for bin1. Conse-\nquently, the query L OAN AMOUNT feature will be treated as being equal to bin1 during\nprediction. Table 6.17[285]lists the calculations of the naive Bayes scores for the candidate","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":338,"page_label":"284","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"284 Chapter 6 Probability-Based Learning\nTable 6.16\nThe Laplace smoothed (with k“3) probabilities needed by a naive Bayes prediction model, calcu-\nlated from the data in Tables 6.11[278]and 6.15[283].\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“none|f rq “ 0.2222 PpCH“none|␣f rq “ 0.1154\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpCH“current|f rq “ 0.3333 PpCH“current|␣f rq “ 0.2692\nPpCH“arrears|f rq “ 0.2222 PpCH“arrears|␣f rq “ 0.3462\nPpGC“none|f rq “ 0.5333 PpGC“none|␣f rq “ 0.6522\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpGC“coapplicant|f rq “ 0.2 PpGC“coapplicant|␣f rq “ 0.2174\nPpACC“own|f rq “ 0.4667 PpACC“own|␣f rq “ 0.6087\nPpACC“rent|f rq “ 0.3333 PpACC“rent|␣f rq “ 0.2174\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nPpAB“x|f rq PpAB“x|␣f rq\n«E¨\n˚˝x,\nλ“0.0024˛\n‹‚ «N¨\n˚˚˚˚˝x,\nµ“984.26,\nσ“460.94˛\n‹‹‹‹‚\nPpBLA“bin1|f rq “ 0.3333 PpBLA“bin1|␣f rq “ 0.1923\nPpBLA“bin2|f rq “ 0.2222 PpBLA“bin2|␣f rq “ 0.2692\nPpBLA“bin3|f rq “ 0.1667 PpBLA“bin3|␣f rq “ 0.3077\nPpBLA“bin4|f rq “ 0.2778 PpBLA“bin4|␣f rq “ 0.2308\nNotation key: FR =F RAUD , CH = C REDIT HISTORY , GC = G UARANTOR /COAPPLICANT ,\nACC =A CCOMMODATION , AB =A CCOUNT BALANCE , BLA =B INNED LOAN AMOUNT .\npredictions for this query: 0.000000462 fortrue and0.000000633 forfalse . The target\nlevel false has the highest score and will be the prediction made by the model.\n6.4.4 Bayesian Networks\nIn this chapter we have introduced two ways to represent the probabilities of events in\na domain, a full joint probability distribution and a naive Bayes model . A full joint","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":339,"page_label":"285","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 285\nTable 6.17\nThe relevant smoothed probabilities, from Table 6.16[284], needed by the naive Bayes model to make\na prediction for the query with CH = paid, GC = guarantor , ACC = free, AB = 759.07, and LA =\n8,000, and the calculation of the scores for each candidate prediction.\nPpf rq “ 0.3 Pp␣f rq “ 0.7\nPpCH“paid|f rq “ 0.2222 PpCH“paid|␣f rq “ 0.2692\nPpGC“guarantor|f rq “ 0.2667 PpGC“guarantor|␣f rq “ 0.1304\nPpACC“free|f rq “ 0.2 PpACC“free|␣f rq “ 0.1739\nPpAB“759.07|f rq PpAB“759.07|␣f rq\n«E¨\n˚˝759.07,\nλ“0.0024˛\n‹‚“ 0.00039 «N¨\n˚˚˚˚˝759.07,\nµ“984.26,\nσ“460.94˛\n‹‹‹‹‚“ 0.00077\nPpBLA“bin1|f rq “ 0.3333 PpBLA“bin1|␣f rq “ 0.1923\nˆmź\nk“1Ppqrks|f rq˙\nˆPpf rq“0.000000462\nˆnź\nk“1Ppqrks|␣f rq˙\nˆPp␣f rq“0.000000633\nprobability distribution encodes the probabilities for all joint events in the domain. Using\na full joint probability distribution, we can do probabilistic inference by summing out the\nfeatures we are not interested in. Full joint probability distributions, however, grow at an\nexponential rate as new features or feature levels are added to the domain. This exponential\ngrowth rate is partially due to the fact that a full joint probability distribution ignores the\nstructural relationships between features, such as direct inﬂuence and conditional indepen-\ndence relationships. As a result, full joint distributions are not tractable for any domain of\nreasonable complexity. By contrast, a naive Bayes model uses a very compact represen-\ntation of a domain. The reason for this is that the model assumes that all the descriptive\nfeatures are conditionally independent of each other given the value of the target feature.\nThe compactness of the representation is at the cost of making a naive assumption that may\nadversely affect the predictive accuracy of the model.\nBayesian networks use a graph-based representation to encode the structural relation-\nships (such as direct inﬂuence and conditional independence) between subsets of features\nin a domain. Consequently, a Bayesian network representation is generally more com-\npact than a full joint distribution (because it can encode conditional independence rela-\ntionships), yet it is not forced to assert a global conditional independence between all","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":340,"page_label":"286","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"286 Chapter 6 Probability-Based Learning\ndescriptive features. As such, Bayesian network models are an intermediary between full\njoint distributions and naive Bayes models and offer a useful compromise between model\ncompactness and predictive accuracy.\nA Bayesian network is a directed acyclical graph (there are no cycles in the graph) that\nis composed of three basic elements:\n‚nodes: each feature in a domain is represented by a single node in the graph.\n‚edges: nodes are connected by directed links; the connectivity of the links in a graph\nencodes the inﬂuence and conditional independence relationships between nodes.\n‚conditional probability tables: each node has a conditional probability table ( CPT )\nassociated with it. A CPT lists the probability distribution of the feature represented by\nthe node conditioned on the features represented by the other nodes to which a node is\nconnected by edges.\nFigure 6.9(a)[287]illustrates a simple Bayesian network. This network describes a domain\nconsisting of two features A and B. The directed link from A to B indicates that the value\nof A directly inﬂuences the value of B. In probability terms, the directed edge from A to B\nin Figure 6.9(a)[287]states that\nPpA,Bq“PpB|AqˆPpAq (6.17)\nFor example, the probability of the event aand␣bis\nPpa,␣bq“Pp␣b|aqˆPpaq“0.7ˆ0.4“0.28\nwhere the probabilities used in the calculation are read directly from the CPTs in Figure\n6.9(a)[287]. In the terminology of Bayesian networks, node A is a parent node of B, and\nnode B is a child node of A, because there is a direct edge from A into B. The CPT as-\nsociated with each node deﬁnes the probabilities of each feature taking a value given the\nvalue(s) of its parent node(s). Node A has no parents, so the CPT just lists the uncon-\nditional probability distribution for A. Notice that each row in the CPT tables sum to 1.\nConsequently, for a categorical feature with Nlevels, we need only N´1probabilities in\neach row, with the ﬁnal probability being understood as equal to 1 minus the sum of the\nother N´1probabilities. For example, when dealing with binary features, we need simply\nstate the probability of each feature being true, and the false value is understood as 1 minus\nthis probability. The network in Figure 6.9(a)[287]could be simpliﬁed in this way, and we\nwill use this simpliﬁcation for all networks drawn from now on. The standard approach for\nhandling continuous features in a Bayesian network is to use binning. As a result, the CPT","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":340,"page_label":"286","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ditional probability distribution for A. Notice that each row in the CPT tables sum to 1.\nConsequently, for a categorical feature with Nlevels, we need only N´1probabilities in\neach row, with the ﬁnal probability being understood as equal to 1 minus the sum of the\nother N´1probabilities. For example, when dealing with binary features, we need simply\nstate the probability of each feature being true, and the false value is understood as 1 minus\nthis probability. The network in Figure 6.9(a)[287]could be simpliﬁed in this way, and we\nwill use this simpliﬁcation for all networks drawn from now on. The standard approach for\nhandling continuous features in a Bayesian network is to use binning. As a result, the CPT\nrepresentation is sufﬁcient to handle both categorical and (binned) continuous features.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":341,"page_label":"287","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 287\nA\nBP(A=T)\n0.4P(A=F)\n0.6\nA\nT\nFP(B=T|A)\n0.3\n0.4P(B=F|A)\n0.7\n0.6\n(a)\nA\nBP(A=T)\n0.4D\nCP(D=T)\n0.4\nD\nT\nFP(C=T|D)\n0.2\n0.2\nA\nT\nT\nF\nFC\nT\nF\nT\nFP(B=T|A,C)\n0.2\n0.5\n0.4\n0.3 (b)\nFigure 6.9\n(a) A Bayesian network for a domain consisting of two binary features. The structure of the network\nstates that the value of feature A directly inﬂuences the value of feature B. (b) A Bayesian network\nconsisting of four binary features with a path containing three generations of nodes: D, C, and B.\nEquation (6.17)[286]can be generalized to the statement that for any network with Nnodes,\nthe probability of an event x1,..., xn, can be computed using the following formula:\nPpx1,..., xnq“nź\ni“1Ppxi|Parentspxiqq (6.18)\nwhere Parentspxiqdescribes the set of nodes in the graph that directly link into node xi.\nUsing this equation, we can compute any joint event in the domain represented by the\nBayesian network. For example, using the slightly more complex Bayesian network in\nFigure 6.9(b)[287], we can calculate the probability of the joint event Ppa,␣b,␣c,dqas\nPpa,␣b,␣c,dq“Pp␣b|a,␣cqˆPp␣c|dqˆPpaqˆPpdq\n“0.5ˆ0.8ˆ0.4ˆ0.4“0.064\nWhen we are computing a conditional probability, we need to be aware of the state of\nboth the parents of a node and the children of a node and their parents. This is because\nknowledge of the state of a child node can tell us something about the state of the parent\nnode. For example, returning to our simple Bayesian network in Figure 6.9(a)[287], we can","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":342,"page_label":"288","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"288 Chapter 6 Probability-Based Learning\nX\nM MM\nMM\nMM\nMM\nMM\nM M\nM M M\nFigure 6.10\nA depiction of the Markov blanket of a node. The gray nodes deﬁne the Markov blanket of the black\nnode. The black node is conditionally independent of the white nodes given the state of the gray\nnodes.\ncompute Ppa|␣bqusing Bayes’ Theorem as follows:\nPpa|␣bq“Pp␣b|aqˆPpaq\nPp␣bq“Pp␣b|aqˆPpaqř\niPp␣b|Aiq\n“Pp␣b|aqˆPpaq\npPp␣b|aqˆPpaqq`pPp␣b|␣aqˆPp␣aqq\n“0.7ˆ0.4\np0.7ˆ0.4q`p0.6ˆ0.6q“0.4375\nEssentially, here we are using Bayes’ Theorem to invert the dependencies between the\nnodes. So, for a conditional independence, we need to take into account not only the\nparents of a node but also the state of its children and their parents. If we have knowledge\nof these parent and children nodes, however, then the node is conditionally independent of\nthe rest of the nodes in the graph. The set of nodes in a graph that make a node independent\nof the rest of the graph are known as the Markov blanket of a node. Figure 6.10[288]\nillustrates the Markov blanket of a node.\nSo, the conditional probability of a node xiin a graph with nnodes can be deﬁned as\nPpxi|x1,..., xi´1,xi`1,..., xnq“\nPpxi|Parentspxiqqź\njPChildrenpxiqPpxj|Parentspxjqq (6.19)\nwhere Parentspxiqdescribes the set of nodes in the graph that directly link into node xi, and\nChildrenpxiqdescribes the set of nodes in the graph that xidirectly links into. Applying","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":343,"page_label":"289","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 289\nthis deﬁnition to the network in Figure 6.9(b)[287], we can calculate the probability of Ppc|\n␣a,b,dqas\nPpc|␣a,b,dq“Ppc|dqˆPpb|c,␣aq\n“0.2ˆ0.4“0.08\nWe already used Equation (6.19)[288]when we were making predictions for a naive Bayes\nclassiﬁer. A naive Bayes classiﬁer is a Bayesian network with a speciﬁc topological struc-\nture. Figure 6.11(a)[290]illustrates the network structure of a naive Bayes classiﬁer and how\nit encodes the conditional independence between the descriptive features given assumed\nknowledge of the target. Figure 6.11(b)[290]illustrates the network structure of the naive\nBayes model for predicting a fraudulent loan application that was built in Section 6.3.1[262].\nWe can see in this structure that the target feature, F RAUD , has no parents and is the single\nparent for all the descriptive feature nodes. This structure directly reﬂects the assumption,\nmade by naive Bayes models, of the conditional independence between descriptive fea-\ntures given knowledge of the target feature and is why the conditional probabilities of the\ndescriptive features in a naive Bayes model are conditioned only on the target feature.\nWhen we computed a conditional probability for the target feature using a naive Bayes\nmodel, we used the following calculation:\nPpt|dr1s,..., drnsq“Pptqź\njPChildrenptqPpdrjs|tq\nThis equation is equivalent to Equation (6.19)[288]. The fact that the probability Pptqis an\nunconditional probability simply reﬂects the structure of the naive Bayes’ network where\nthe target feature has no parent nodes (see Figure 6.11(a)[290]).\nComputing a conditional probability for a node becomes more complex if the value of\none or more of the parent nodes is unknown. In this situation the node becomes dependent\non the ancestors of its unknown parent. This is because if a parent node is unknown, then\nto compute the distribution for the node, we must sum out this parent. However, to do\nthis summing out, we must know the distribution for the unknown parent, which in turn\nrequires us to sum out the parents of the parent, and so on if necessary. As a result of\nthis recursive summing out, the distribution over a node is dependent on knowledge of\nthe ancestors of any of its parent nodes.22For example, in Figure 6.9(b)[287], if the status of\nnode C is not known, then node B becomes dependent on node D. For example, to compute\nPpb|a,dqwe would do the following calculations:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":343,"page_label":"289","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"on the ancestors of its unknown parent. This is because if a parent node is unknown, then\nto compute the distribution for the node, we must sum out this parent. However, to do\nthis summing out, we must know the distribution for the unknown parent, which in turn\nrequires us to sum out the parents of the parent, and so on if necessary. As a result of\nthis recursive summing out, the distribution over a node is dependent on knowledge of\nthe ancestors of any of its parent nodes.22For example, in Figure 6.9(b)[287], if the status of\nnode C is not known, then node B becomes dependent on node D. For example, to compute\nPpb|a,dqwe would do the following calculations:\n1.Compute the distribution for Cgiven D:Ppc|dq“0.2,Pp␣c|dq“0.8\n22. The conditional independence relationship between any two nodes in a Bayesian network can be speciﬁed\nusing the framework of d-separation (the “d” stands for directed ) (Pearl, 1988). We don’t discuss d-separation\nin this book as it is not required for our discussion.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":344,"page_label":"290","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"290 Chapter 6 Probability-Based Learning\nTarget\nDescriptive\nFeature 1Descriptive\nFeature 2…Descriptive\nFeature N\n(a)\nFraud\nCredit\nHistoryGuarantor\nCoapplicantAccommodationAccount\nBalanceLoan\nAmount\n(b)\nFigure 6.11\n(a) A Bayesian network representation of the conditional independence asserted by a naive Bayes\nmodel between the descriptive features given knowledge of the target feature; and (b) a Bayesian\nnetwork representation of the conditional independence assumption for the naive Bayes model in the\nfraud example.\n2.Compute Ppb|a,Cqby summing out C:Ppb|a,Cq“ř\niPpb|a,Ciq\nPpb|a,Cq“ÿ\niPpb|a,Ciq“ÿ\niPpb,a,Ciq\nPpa,Ciq\n“pPpb|a,cqˆPpaqˆPpcqq`p Ppb|a,␣cqˆPpaqˆPp␣cqq\npPpaqˆPpcqq`p PpaqˆPp␣cqq\n“p0.2ˆ0.4ˆ0.2q`p 0.5ˆ0.4ˆ0.8q\np0.4ˆ0.2q`p 0.4ˆ0.8q“0.44\nThis example illustrates the power of Bayesian networks. When complete knowledge of\nthe state of all the nodes in the network is not available, we clamp the values of nodes\nthat we do have knowledge of and sum out the unknown nodes. Furthermore, during these\ncalculations, we only need to condition a node on its Markov blanket, which dramatically\nreduces the number of probabilities required by the network.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":345,"page_label":"291","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 291\nA\nB\nCP(A=T)\n0.6\nA\nT\nFP(B=T|A)\n0.3333\n0.5\nA\nT\nT\nF\nFB\nT\nF\nT\nFP(C=T|A,B)\n0.25\n0.125\n0.25\n0.25\n(a)\nAC\nT\nT\nF\nFB\nT\nF\nT\nFP(A=T|B,C)\n0.5\n0.5\n0.5\n0.7P(C=T)\n0.2C\nBC\nT\nFP(B=T|C)\n0.5\n0.375 (b)\nFigure 6.12\nTwo different Bayesian networks, each deﬁning the same full joint probability distribution.\n6.4.4.1 Building Bayesian networks Bayesian networks can be constructed by hand or\nlearned from data. Learning both the topology of a Bayesian network and the parameters\nin the CPTs in the network is a difﬁcult computational task. One of the things that makes\nlearning the structure of a Bayesian network so difﬁcult is that it is possible to deﬁne\nseveral different Bayesian networks as representations for the same full joint probability\ndistribution. Consider, for example, a probability distribution for three binary features A,\nB, and C. The probability for a joint event in this domain PpA,B,Cqcan be decomposed\nusing the chain rule in the following way:\nPpA,B,Cq“PpC|A,BqˆPpB|AqˆPpAq (6.20)\nThe chain rule, however, doesn’t specify any constraints on which features in the domain\nwe choose to condition on. We could just as easily have decomposed the probability of the\njoint event as follows:\nPpA,B,Cq“PpA|C,BqˆPpB|CqˆPpCq (6.21)\nBoth of these decompositions are valid, and both deﬁne different Bayesian networks for\nthe domain. Figure 6.12(a)[291]illustrates the Bayesian network representing the decompo-\nsition deﬁned in Equation (6.20)[291], and Figure 6.12(b)[291]illustrates the Bayesian network\nrepresenting the decompositions deﬁned in Equation (6.21)[291].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":346,"page_label":"292","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"292 Chapter 6 Probability-Based Learning\nWe can show that both of the networks in Figure 6.12[291]represent the same joint proba-\nbility by using each of them to calculate the probability of an arbitrarily chosen joint event\nfrom the domain. We should get the same probability for the joint event from both of the\nnetworks. For this example, we will calculate the probability of the event ␣a,b,c. Using\nthe Bayesian network in Figure 6.12(a)[291], we would carry out the calculation as follows:\nPp␣a,b,cq“Ppc|␣a,bqˆPpb|␣aqˆPp␣aq\n“0.25ˆ0.5ˆ0.4“0.05\nUsing the network in Figure 6.12(b)[291], the calculation would be\nPp␣a,b,cq“Pp␣a|c,bqˆPpb|cqˆPpcq\n“0.5ˆ0.5ˆ0.2“0.05\nBoth networks return the same probability for the joint event. In fact, these networks will\nreturn identical probabilities for all events in this domain.\nThe basic approach to learning the structure of a Bayesian network is to use a local search\nalgorithm that moves through the space of possible networks and parameters, and searches\nfor the network topology and CPT parameters that best ﬁt with the data. To start the search,\nthe algorithm is given a seed network and then iteratively adapts this network by adding,\nremoving, or reversing links (and/or adding and removing hidden nodes), accompanied\nby iterations of parameter learning after each network structure adaptation. One of the\ndifﬁculties with learning a network structure is that we can always improve the likelihood\nof the data given a network by simply adding new links into the network. Each time we\nadd a link to a network, we increase the number of CPT entries in the network. The CPT\nentries are essentially parameters on the network, and the more parameters a network has,\nthe greater its ability to ﬁt (or overﬁt) the data. So, care must be taken to ensure that\nthe objective function used by the search process avoids overﬁtting the data by simply\ncreating a very highly connected graph. Consequently, the objective functions used by\nthese algorithms are often based on the minimum description length principle , which\nasserts that the solution with the fewest parameters (shortest description) is the best one.\nWe have already met the minimum description length principle in the more general form of\nOccam’s razor . A popular metric used by these algorithms is the Bayesian information\ncriterion (BIC):\nBICpG,Dq“loge`\nPpD|ˆP,Gq˘\n´ˆd\n2ˆlogepnq˙\n(6.22)\nwhere Gdenotes the network graph, Dis the training data, ˆPis the set of entries in the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":346,"page_label":"292","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the objective function used by the search process avoids overﬁtting the data by simply\ncreating a very highly connected graph. Consequently, the objective functions used by\nthese algorithms are often based on the minimum description length principle , which\nasserts that the solution with the fewest parameters (shortest description) is the best one.\nWe have already met the minimum description length principle in the more general form of\nOccam’s razor . A popular metric used by these algorithms is the Bayesian information\ncriterion (BIC):\nBICpG,Dq“loge`\nPpD|ˆP,Gq˘\n´ˆd\n2ˆlogepnq˙\n(6.22)\nwhere Gdenotes the network graph, Dis the training data, ˆPis the set of entries in the\nCPTs of G,dis the number of parameters of G(i.e., how many entries in the CPTs of G),\nandnis the number of instances in D. This metric contains a term describing how well\nthe model predicts the data PpD|ˆP,Gqas well as a term that punishes complex models","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":347,"page_label":"293","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 293\n´`d\n2ˆlogepnq˘\n. As such, it balances the search goals of model accuracy and simplicity.\nThe term PpD|ˆP,Gqcan be computed using metrics such as the Bayesian score or the K2\nscore .23The search space for these algorithms is exponential in the number of features.\nConsequently, developing algorithms to learn the structure of Bayesian networks is an\nongoing research challenge.24\nIt is much simpler to construct a Bayesian network using a hybrid approach, where the\ntopology of the network is given to the learning algorithm, and the learning task involves\ninducing the CPT entries from the data. This type of learning illustrates one of the real\nstrengths of the Bayesian network framework, namely, that it provides an approach to\nlearning that naturally accommodates human expert information. In this instance, the hu-\nman expert speciﬁes that topology of the network, and the learning algorithm induces the\nCPT entries for nodes in the topology in the same way that we computed the conditional\nprobabilities for the naive Bayes model.25\nGiven that there are multiple Bayesian networks for any domain, an obvious question\nto ask is, what is the best topological structure to give the algorithm as input? Ideally,\nwe would like to use the network whose structure most accurately reﬂects the causal re-\nlationships in the domain. Speciﬁcally, if the value of one feature directly inﬂuences, or\ncauses , the value taken by another feature, then this should be reﬂected in the structure\nof the graph by having a link from the cause feature to the effect feature. Bayesian net-\nworks whose topological structure correctly reﬂects the causal relationships between the\nfeatures in a dataset are called causal graphs . There are two advantages to using a causal\ngraph: (1) people ﬁnd it relatively easy to think in terms of causal relationships, and as a\nresult, networks that encode these relationships are relatively easy to understand; (2) often\nnetworks that reﬂect the causal structure of a domain are more compact in terms of the\nnumber of links between nodes and hence are more compact with respect to the number of\nCPT entries.\nWe will use an example from social science to illustrate how to construct a causal graph\nusing this hybrid approach. In this example, we will build a Bayesian network that enables\nus to predict the level of corruption in a country based on a number of macroeconomic and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":347,"page_label":"293","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"features in a dataset are called causal graphs . There are two advantages to using a causal\ngraph: (1) people ﬁnd it relatively easy to think in terms of causal relationships, and as a\nresult, networks that encode these relationships are relatively easy to understand; (2) often\nnetworks that reﬂect the causal structure of a domain are more compact in terms of the\nnumber of links between nodes and hence are more compact with respect to the number of\nCPT entries.\nWe will use an example from social science to illustrate how to construct a causal graph\nusing this hybrid approach. In this example, we will build a Bayesian network that enables\nus to predict the level of corruption in a country based on a number of macroeconomic and\n23. The K2 score is named after the K2 algorithm, one of the earliest and best-known algorithms for learning\nBayesian networks (Cooper and Herskovits, 1992).\n24. See Kollar and Friedman (2009) for a discussion of algorithms that seek to address this research challenge.\n25. In some cases we may not have data for all the features; and in these instances, the standard approach to\nlearning the CPT entries is to use a gradient descent approach (similar to the one we introduce in Chapter 7[311]),\nwhere the objective function of the local search algorithm is simply how well the product of the induced condi-\ntional probabilities match the relative frequency of each joint event in the data. In other words, we choose the set\nof conditional probabilities that maximize the likelihood of the training data.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":348,"page_label":"294","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"294 Chapter 6 Probability-Based Learning\nsocial descriptive features. Table 6.18[295]lists some countries described using the following\nfeatures:26\n‚GINICOEF measures the equality in a society, where a larger Gini coefﬁcient indicates\na more unequal society.\n‚LIFEEXPmeasures life expectancy at birth.\n‚SCHOOL YEARS refers to the mean number of years spent in school for adult females.\n‚CPI is the Corruption Perception Index (CPI), and it is the target feature. The CPI\nmeasures the perceived level of corruption in the public sector of a country and ranges\nfrom 0(highly corrupt) to 100(very clean).\nThe original feature values shown in Table 6.18[295]are continuous, so we use the standard\napproach of converting them to categorical features using equal-frequency binning , with\ntwo bins for each feature: lowandhigh. The columns labeled Binned Feature Values in\nTable 6.18[295]show the data after it has been binned.\nOnce the data has been prepared, there are two stages to building the Bayesian network.\nFirst, we deﬁne the topology of the network. Second, we create the network CPTs. The\ntopology of the network will be a causal graph that models this domain. In order to build\nthis, we must have a theory of the causal relationships between the features in the domain.\nA potential causal theory between the features in this dataset is that\nthe more equal a society, the higher the investment that society will make in health and\neducation, and this in turn results in a lower level of corruption\nFigure 6.13[296]illustrates a Bayesian network with a topology that encodes this causal\ntheory. Equality directly affects both health and education, so there are directed arcs from\nGINICOEF to both L IFEEXPand S CHOOL YEARS . Health and education directly affect\ncorruption, so there is a directed arc from L IFEEXPand from S CHOOL YEARS to CPI.\nTo complete the network, we need to add the CPTs. To do this, we compute the required\nconditional probabilities from the binned data in Table 6.18[295]. The CPTs are shown in\nFigure 6.13[296].\n6.4.4.2 Using a Bayesian network to make predictions Once a network has been\ncreated, it is relatively straightforward to use to make a prediction. We simply compute\nthe probability distribution for the target feature conditioned on the state of the descriptive\nfeatures in the query and return the target feature level with the maximum a posteriori","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":348,"page_label":"294","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"GINICOEF to both L IFEEXPand S CHOOL YEARS . Health and education directly affect\ncorruption, so there is a directed arc from L IFEEXPand from S CHOOL YEARS to CPI.\nTo complete the network, we need to add the CPTs. To do this, we compute the required\nconditional probabilities from the binned data in Table 6.18[295]. The CPTs are shown in\nFigure 6.13[296].\n6.4.4.2 Using a Bayesian network to make predictions Once a network has been\ncreated, it is relatively straightforward to use to make a prediction. We simply compute\nthe probability distribution for the target feature conditioned on the state of the descriptive\nfeatures in the query and return the target feature level with the maximum a posteriori\n26. The data listed in this table is real. The Gini coefﬁcient data is for 2013 (or the most recent year prior to\n2013 for which the data was available for a country) and was retrieved from the World Bank (data.worldbank.\norg/indicator/SI.POV .GINI); the life expectancy and mean years in school data was retrieved from Gapminder\n(www.gapminder.org) and is for 2010/11 (or the most recent year prior to 2010/11 for which the data was avail-\nable for a country); and the mean years in school were originally sourced from the Institute for Health Metrics and\nEvaluation (www.healthdata.org). The Corruption Perception Index is for 2011 and was retrieved from Trans-\nparency International (www.transparency.org).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":349,"page_label":"295","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 295\nTable 6.18\nSome socioeconomic data for a set of countries, and a version of the data after equal-frequency\nbinning has been applied.\nOriginal Feature Values Binned Feature Values\nCOUNTRY GINI SCHOOL LIFE GINI SCHOOL LIFE\nID COEF YEARS EXP CPI COEF YEARS EXP CPI\nAfghanistan 27.82 0.40 59.61 1.52 low low low low\nArgentina 44.49 10.10 75.77 3.00 high low low low\nAustralia 35.19 11.50 82.09 8.84 low high high high\nBrazil 54.69 7.20 73.12 3.77 high low low low\nCanada 32.56 14.20 80.99 8.67 low high high high\nChina 42.06 6.40 74.87 3.64 high low low low\nEgypt 30.77 5.30 70.48 2.86 low low low low\nGermany 28.31 12.00 80.24 8.05 low high high high\nHaiti 59.21 3.40 45.00 1.80 high low low low\nIreland 34.28 11.50 80.15 7.54 low high high high\nIsrael 39.2 12.50 81.30 5.81 low high high high\nNew Zealand 36.17 12.30 80.67 9.46 low high high high\nNigeria 48.83 4.10 51.30 2.45 high low low low\nRussia 40.11 12.90 67.62 2.45 high high low low\nSingapore 42.48 6.10 81.788 9.17 high low high high\nSouth Africa 63.14 8.50 54.547 4.08 high low low low\nSweden 25.00 12.80 81.43 9.30 low high high high\nUK 35.97 13.00 80.09 7.78 low high high high\nUSA 40.81 13.70 78.51 7.14 high high high high\nZimbabwe 50.10 6.7 53.684 2.23 high low low low\nprobability:\nMpqq“arg max\nlPlevelsptqBayesianNetwork pt“l,qq (6.23)\nwhereMpqqis the prediction made by the model for the query q,levelsptqis the set of\nlevels in the domain of the target feature t, and BayesianNetwork pt“l,qqreturns the\nprobability computed by the network for the event t“lgiven the evidence speciﬁed in the\nquery q.\nFor example, imagine we wanted to use the Bayesian network in Figure 6.13[296]to predict\nthe CPI for a country with the following proﬁle:\nGINICOEF =low, SCHOOL YEARS =high, LIFEEXP=high\nBecause both the parent nodes for CPI are known (S CHOOL YEARS and L IFEEXP), the\nprobability distribution for CPI is independent of the G INICOEF feature. Therefore, we\ncan read the relevant probability distribution for CPI directly from the CPT for the CPI","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":350,"page_label":"296","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"296 Chapter 6 Probability-Based Learning\nGini\nCoef\nSchool\nYearsLife\nExpP(GC=L)\nP(GC=H)0.5\n0.5\nCPIGC\nL\nHP(SY=L | GC)\n0.2\n0.8P(SY=H | GC)\n0.8\n0.2GC\nL\nHP(LE=L | GC)\n0.2\n0.8P(LE=H | GC)\n0.8\n0.2\nSY\nL\nL\nH\nHLE\nL\nH\nL\nHP(CPI=L | SY ,LE)\n1.0\n0\n1.0\n0P(CPI=H | SY ,LE)\n0\n1.0\n0\n1.0\nFigure 6.13\nA Bayesian network that encodes the causal relationships between the features in the corruption\ndomain. The CPT entries have been calculated using the binned data from Table 6.18[295].\nnode. From this CPT we can see that when S CHOOL YEARS =high, and L IFEEXP=high,\nthen the most likely level is CPI = high. As a result, CPI = high is the MAP CPI value\nfor this query, and this is the prediction the model will return. In other words, countries\nthat are relatively equal and that have good education and high life expectancy are likely\nto have a low level of corruption.\n6.4.4.3 Making predictions with missing descriptive feature values One real ad-\nvantage of Bayesian networks over the other predictive model types that we discuss in\nthis book is they a provide an elegant solution to making predictions for a target feature\nwhen one or more of the descriptive feature values in a query instance are missing.27For\nexample, we may wish to predict the CPI for a country with the following proﬁle:\nGINICOEF =high, SCHOOL YEARS =high\nwhere the value of the L IFEEXPfeature is unknown for the country. This means that in the\nnetwork, one of the parents of the target feature node, CPI, is unknown. Consequently, we\nneed to sum out this feature for each level of the target. We can calculate the probability\n27. The most common way to achieve this for the other model types covered in this book is to impute the missing\nvalues in the query instance using one of the techniques described in Section 3.4.1[69].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":351,"page_label":"297","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 297\nfor CPI = high as follows:28\nPpCPI“high|SY“high,GC“highq\n“PpCPI“high,SY“high,GC“highq\nPpSY“high,GC“highq\n“ÿ\niP!high,\nlow)PpCPI“high,SY“high,GC“high,LE“iq\nPpSY“high,GC“highq\nWe calculate the numerator in this term as follows:\nÿ\niP!high,\nlow)PpCPI“high,SY“high,GC“high,LE“iq\n“ÿ\niP!high,\nlow)¨\n˚˚˚˚˝PpCPI“high|SY“high,LE“iq\nˆPpSY“high|GC“highq\nˆPpLE“i|GC“highq\nˆPpGC“highq˛\n‹‹‹‹‚\n“´\nPpCPI“high|SY“high,LE“highq\nˆPpSY“high|GC“highqˆPpLE“high|GC“highqˆPpGC“highq¯\n`´\nPpCPI“high|SY“high,LE“lowq\nˆPpSY“high|GC“highqˆPpLE“low|GC“highqˆPpGC“highq¯\n“p1.0ˆ0.2ˆ0.2ˆ0.5q`p 0ˆ0.2ˆ0.8ˆ0.5q“0.02\nand denominator as:\nPpSY“high,GC“highq\n“PpSY“high|GC“highqˆPpGC“highq\n“0.2ˆ0.5“0.1\n28. In the following calculations we have abbreviated feature names as follows: GC = G INICOEF, LE = L IFE\nEXP, and SY = S CHOOL YEARS .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":352,"page_label":"298","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"298 Chapter 6 Probability-Based Learning\nWe can now calculate the probability for CPI = high as\nPpCPI“high|SY“high,GC“highq“0.02\n0.1“0.2\nWe know from this result that the probability for CPI = lowmust be 0.8. So, the network\nwill predict CPI = lowas the MAP target value for the query. This tells us that an unequal\nsociety that has a good education system but for which we have no evidence about the\nhealth system is still likely to suffer from corruption.\nThese calculations make it apparent that even in this small example domain, the calcula-\ntion of a probability becomes computationally complex very quickly, particularly when we\nneed to sum out one or more features. The complexity of the calculations can be reduced\nby being careful with the positioning of features with respect to summations and by using\ndynamic programming techniques to avoid repeated computations. A well-known algo-\nrithm that focuses on this approach to reducing the complexity is the variable elimination\nalgorithm (Zhang and Poole, 1994). However, even using the variable elimination algo-\nrithm, calculating exact probabilities from a Bayesian network when descriptive feature\nvalues are missing is prohibitively complex.\nGiven the complexity of exact probabilistic inference for Bayesian networks, a popular\nalternative is to approximate the probability distribution required for a prediction using\nMonte Carlo methods .29Monte Carlo methods generate a large number of sample events\nand then use the relative frequency of an event in the set of generated samples as the ap-\nproximation for the probability of that event in the real distribution. Monte Carlo methods\nwork well in conjunction with Bayesian networks because a Bayesian network models the\nprobability distribution over the features. More speciﬁcally, a Bayesian network can be\nviewed as deﬁning a Markov chain . A Markov chain is a system that has a set of ﬁnite\nstates and a set of transition probabilities that deﬁne the likelihood of the system moving\nfrom one state to another. When we view a Bayesian network as a Markov chain, a state\nis a complete assignment of values to all the nodes in the network (for example, G INI\nCOEF =high, SCHOOL YEARS =low, LIFEEXP=high,CPI =high would be a state in\nthe Markov chain deﬁned by the network in Figure 6.13[296]), and the CPTs of the network\nprovide a distributed representation of the transition probabilities of the Markov chain.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":352,"page_label":"298","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"probability distribution over the features. More speciﬁcally, a Bayesian network can be\nviewed as deﬁning a Markov chain . A Markov chain is a system that has a set of ﬁnite\nstates and a set of transition probabilities that deﬁne the likelihood of the system moving\nfrom one state to another. When we view a Bayesian network as a Markov chain, a state\nis a complete assignment of values to all the nodes in the network (for example, G INI\nCOEF =high, SCHOOL YEARS =low, LIFEEXP=high,CPI =high would be a state in\nthe Markov chain deﬁned by the network in Figure 6.13[296]), and the CPTs of the network\nprovide a distributed representation of the transition probabilities of the Markov chain.\nIf the distribution used to generate the samples for a Monte Carlo method is a Markov\nchain , then the speciﬁc algorithms we use to implement this approach come from a family\nknown as Markov chain Monte Carlo (MCMC ) algorithms. Gibbs sampling is one of\nthe best-known MCMC algorithms and is particularly suitable when we wish to generate\nprobabilities that are conditioned on some evidence, so this is the algorithm we discuss in\nthis section.\n29. Monte Carlo methods are named after the Mediterranean principality that is famous for its casino.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":353,"page_label":"299","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.4 Extensions and Variations 299\nThe Gibbs sampling algorithm initializes a Bayesian network by clamping the values\nof the evidence nodes and randomly assigning values to the non-evidence nodes. The\nalgorithm then iteratively generates samples by changing the value of one of the non-\nevidence nodes. The selection of which non-evidence node to change can be random or\nfollow a predeﬁned list through which the algorithm iterates. The new value for the selected\nnode is drawn from the distribution for the node (the CPT), conditioned on the current state\nof all the other nodes in the network. Each time a node is updated, a new sample state has\nbeen generated. More formally, for a network with three nodes x1,x2,x3, using a predeﬁned\nnode selection order of x1,x2,x3,x1,... and assuming that at iteration τeach node has the\nvalues xpτq\n1,xpτq\n2,xpτq\n3, the next four states generated will be\n1.⣨\nxpτ`1q\n1ÐPpx1|xpτq\n2,xpτq\n3q,xpτq\n2,xpτq\n3⟩\n2.⣨\nxpτ`1q\n1,xpτ`2q\n2ÐPpx2|xpτ`1q\n1,xpτq\n3q,xpτq\n3⟩\n3.⣨\nxpτ`1q\n1,xpτ`2q\n2,xpτ`3q\n3ÐPpx3|xpτ`1q\n1,xpτ`2q\n2q⟩\n4.⣨\nxpτ`4q\n1ÐPpx1|xpτ`2q\n2,xpτ`3q\n3q,xpτ`2q\n2,xpτ`3q\n3⟩\nThere are three technical requirements that must hold for distribution of states generated\nfrom Gibbs sampling to converge with the distribution that we are sampling from—in this\ncase, the distribution deﬁned by the Bayesian network. The ﬁrst is that the distribution we\nare sampling from must be a stationary distribution (also known as an invariant distri-\nbution ). A stationary distribution is a distribution that doesn’t change. The distribution\ndeﬁned by a Bayesian network doesn’t change during Gibbs sampling, so this requirement\nalways holds in this context. The second requirement is that the Markov chain used to gen-\nerate the samples must be ergodic . A Markov chain is ergodic if every state is reachable\nfrom every other state and there are no cycles in the chain. The Markov chain deﬁned by\na Bayesian network is ergodic if there are no zero entries in any of the CPTs.30The third\nrequirement is that the generated states should be independent of each other. As each gen-\nerated state is a modiﬁed version of the preceding state, it is clear that successive states will\nbe correlated with each other. So to obtain independent sample states, we often sub-sample\nfrom the sequence (sub-sampling in this way is also known as thinning ). Once these three\nconditions hold (stationary distribution, ergodicity, and independent states), the samples","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":353,"page_label":"299","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"erate the samples must be ergodic . A Markov chain is ergodic if every state is reachable\nfrom every other state and there are no cycles in the chain. The Markov chain deﬁned by\na Bayesian network is ergodic if there are no zero entries in any of the CPTs.30The third\nrequirement is that the generated states should be independent of each other. As each gen-\nerated state is a modiﬁed version of the preceding state, it is clear that successive states will\nbe correlated with each other. So to obtain independent sample states, we often sub-sample\nfrom the sequence (sub-sampling in this way is also known as thinning ). Once these three\nconditions hold (stationary distribution, ergodicity, and independent states), the samples\ngenerated will eventually converge with the distribution, and it is appropriate to use Gibbs\nsampling.\nBecause we start sampling from a random state, however, we do not know whether the\ninitial state is an appropriate state from which to start generating samples. It may, for\nexample, be a state that has a very low probability in the distribution. As a result, it\nis a good idea to run the network for a number of iterations before the generated states\n30. If there are one or more zero entries in the CPTs, then the Markov chain may still be ergodic, but it is non-\ntrivial to prove ergodicity in these cases.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":354,"page_label":"300","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"300 Chapter 6 Probability-Based Learning\nare recorded as samples. This burn-in time is to allow the Markov chain to settle into\na state that is independent of the initial random state and that is a probable state for the\ndistribution we are sampling from. The time it takes for the Markov chain to forget the\ninitial random state is called the mixing time . Unfortunately, estimating how long the\nburn-in should be is difﬁcult. For some Markov chains, mixing may require only a few\niterations, but for others, it may require hundreds or thousands of iterations. The topology\nof the network can provide some insight into this problem. Larger graphs will tend to have\nlonger mixing times. Also, an evenly connected network typically has a relatively short\nmixing time (for the size of the graph). If, however, a graph is composed of a number\nof clusters connected via bottleneck nodes, this would typically indicate a longer mixing\ntime. Another approach used to determine the appropriate burn-in time is to start several\nMarkov chains with different initial states and wait until all the chains are generating states\nwith similar distribution characteristics (mean state, mode state, etc.). When this happens,\nit indicates that all the chains are sampling from the same distribution and, hence, that\nit is likely that they have all forgotten their starting states. Once this happens, the target\nprobability can be computed by calculating the relative frequency of the event within the\nselected subset of generated states.\nTable 6.19[301]lists a some of the samples generated using Gibbs sampling for the Bayesian\nnetwork in Figure 6.13[296]for the query\nGINICOEF =high, SCHOOL YEARS =high\nA burn-in of 30 iterations was used, and the samples were thinned by sub-sampling every\n7thiteration. When the algorithm was used to generate 500samples, the relative frequency\nof CPI = high was0.196. When 2,000samples were generated, the relative frequency\nrose to 0.1975 . This rise in relative frequency illustrates that, as the number of samples\ngenerated increases, the resulting distribution approaches the actual distribution. Recall\nthat when we did an exact calculation for this query the probability of CPI = high was0.2.\nWe can make predictions using Gibbs sampling in the same way that we made predic-\ntions using exact probabilistic inference by predicting the target level with the maximum a\nposteriori probability:\nMpqq“arg max\nlPlevelsptqGibbspt“l,qq (6.24)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":354,"page_label":"300","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7thiteration. When the algorithm was used to generate 500samples, the relative frequency\nof CPI = high was0.196. When 2,000samples were generated, the relative frequency\nrose to 0.1975 . This rise in relative frequency illustrates that, as the number of samples\ngenerated increases, the resulting distribution approaches the actual distribution. Recall\nthat when we did an exact calculation for this query the probability of CPI = high was0.2.\nWe can make predictions using Gibbs sampling in the same way that we made predic-\ntions using exact probabilistic inference by predicting the target level with the maximum a\nposteriori probability:\nMpqq“arg max\nlPlevelsptqGibbspt“l,qq (6.24)\nwhereMpqqis the prediction made by the model for the query q,levelsptqis the set of\nlevels in the domain of the target feature t, and Gibbspt“l,qqreturns the probability for\nthe event t“lgiven the evidence speciﬁed in the query qusing Gibbs sampling.\n6.5 Summary\nThere are two ways to reason with probabilities forward and inverse. Forward probability\nreasons from causes to effects: if we know that a particular causal event has happened,\nthen we increase the probability associated with the known effects that it causes. Inverse","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":355,"page_label":"301","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.5 Summary 301\nTable 6.19\nExamples of the samples generated using Gibbs sampling.\nSample Gibbs Feature G INI SCHOOL LIFE\nNumber Iteration Updated C OEF YEARS EXP CPI\n1 37 CPI high high high low\n2 44 L IFEEXP high high high low\n3 51 CPI high high high low\n4 58 L IFEEXP high high low high\n5 65 CPI high high high low\n6 72 L IFEEXP high high high low\n7 79 CPI high high low high\n8 86 L IFEEXP high high low low\n9 93 CPI high high high low\n10 100 L IFEEXP high high high low\n11 107 CPI high high low high\n12 114 L IFEEXP high high high low\n13 121 CPI high high high low\n14 128 L IFEEXP high high high low\n15 135 CPI high high high low\n16 142 L IFEEXP high high low low\n...\nprobability reasons from effects to causes: if we know that a particular event has occurred,\nthen we can increase the probability that one or more of the events that could cause the\nobserved event have also happened. Bayes’ Theorem relates these two views of proba-\nbility by using the notion of a prior probability. Put in subjective terms, Bayes’ Theorem\ntells us that by modifying our initial beliefs about what has happened (our prior beliefs\nabout the world) proportionally with how our observations relate to their potential causes\n(inverse probability), we can update our beliefs regarding what has happened to cause our\nobservations (forward probability). Put more formally:\nPpt|dq“Ppd|tqˆPptq\nPpdq(6.25)\nThe use of prior probabilities in Bayes’ Theorem is what distinguishes between Bayesian\nandmaximum likelihood approaches to probability.\nBayesian prediction is a very intuitive approach to predicting categorical targets. In order\nto make a prediction, we have to learn two things:\n1.the probability of an instance having a particular set of descriptive feature values given\nthat it has a particular target level Ppd|tq\n2.the prior probability of that target level Pptq","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":356,"page_label":"302","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"302 Chapter 6 Probability-Based Learning\nGiven these two pieces of information, we can compute the relative likelihood of a partic-\nular instance having a particular target level as\nPpt|dq“Ppd|tqˆPptq (6.26)\nOnce the relative likelihoods for each target level have been calculated, we simply return\nthe maximum a posteriori (MAP) prediction.\nThe biggest challenge in creating a Bayesian prediction model is overcoming the expo-\nnential growth in the number of probabilities (model parameters) that are required as the\ndimensionality of the feature space increases. The standard approach to addressing this\nproblem is to use the independence andconditional independence relationships between\nthe features in a domain to factorize thefull joint distribution of the domain. Factoriz-\ning the domain representation reduces the number of interactions between the features and\nreduces the number of model parameters.\nA naive Bayes model addresses this problem by naively assuming that each of the de-\nscriptive features in a domain is conditionally independent of all the other descriptive fea-\ntures, given the state of the target feature. This assumption, although often wrong, en-\nables the naive Bayes model to maximally factorize the representation that it uses of the\ndomain—in other words, to use the smallest possible number of probabilities to represent\nthe domain.\nSurprisingly, given the naivete and strength of the assumption it depends upon, naive\nBayes models often perform well. This is partly because naive Bayes models are able\nto make correct predictions even if the probabilities that they calculate are incorrect, so\nlong as the error in the calculated probabilities does not affect the relative rankings of the\ndifferent target levels. One consequence of this observation is that naive Bayes models are\nnot really suitable for predicting continuous targets. When predicting a continuous target,\nevery error in the calculation of a probability is reﬂected in reduced model performance.\nThe conditional independence assumption means that naive Bayes models use very few\nparameters to represent a domain. One consequence of this is that naive Bayes models can\nbe trained using a relatively small dataset: with so few parameters and so few conditions\non each parameter—only the state of the target feature—it is possible to make reasonable\nestimates for the parameters using a small dataset. Another beneﬁt of the reduced repre-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":356,"page_label":"302","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"different target levels. One consequence of this observation is that naive Bayes models are\nnot really suitable for predicting continuous targets. When predicting a continuous target,\nevery error in the calculation of a probability is reﬂected in reduced model performance.\nThe conditional independence assumption means that naive Bayes models use very few\nparameters to represent a domain. One consequence of this is that naive Bayes models can\nbe trained using a relatively small dataset: with so few parameters and so few conditions\non each parameter—only the state of the target feature—it is possible to make reasonable\nestimates for the parameters using a small dataset. Another beneﬁt of the reduced repre-\nsentation of the model is that the behavior of the model is relatively easy to interpret. It is\npossible to look at the probabilities for each descriptive feature and analyze how that value\ncontributed to the ﬁnal prediction. This information can be useful in informing the devel-\nopment of more powerful models later in a project. Consequently, a naive Bayes model is\noften a good model to begin with: it is easy to train and has the potential to provide both\na baseline accuracy score and some insight into the problem structure. The major draw-\nback of naive Bayes models is the inability of the model to handle the interactions between\nfeatures.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":357,"page_label":"303","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.6 Further Reading 303\nBayesian networks provide a more ﬂexible representation for encoding the conditional\nindependence assumptions between the features in a domain. Ideally, the topology of a\nnetwork should reﬂect the causal relationships between the entities in a domain. Prop-\nerly constructed Bayesian networks are relatively powerful models that can capture the\ninteractions between descriptive features in determining a prediction. Although the task\nof inducing the optimal network structure from data is strictly intractable, algorithms that\nencode various assumptions exist that allow good models to be learned. Also, in domains\nwhere the causal relationships between features are known, Bayesian networks have the\nadvantage of providing a natural framework for integrating expert human knowledge with\ndata-driven induction. Bayesian networks have been successfully applied across a range of\nﬁelds, including medical diagnosis, object recognition, and natural language understand-\ning.\nSeveral parallels can be drawn between probability-based learning and the other ap-\nproaches to machine learning that we present in this book. Intuitively, the prior probability\nof anearest neighbor model predicting a particular target level is simply the relative fre-\nquency of that target level in the dataset. For this reason, in general it is wrong to artiﬁcially\nbalance the dataset used by a nearest neighbor model,31and doing so biases the target level\npriors used by the model.\nThe relationship between probability-based and information-based learning is simply\nthat the amount of information provided by an observation—such as a descriptive feature\ntaking a particular value—is reﬂected in the difference between the prior and posterior\nprobabilities caused by the observation. If the prior and posterior probabilities are sim-\nilar, then the information content in the observation was low. If the prior and posterior\nprobabilities are very different, then the information content in the observation was high.\nFinally, it can be shown that, under some assumptions, any learning algorithm that min-\nimizes the squared error of the model over the data will output a maximum likelihood\nprediction.32The relevance of this ﬁnding is that it provides a probabilistic justiﬁcation for\nthe approach to learning we present in Chapter 7[311].\n6.6 Further Reading\nMcGrayne (2011) is an accessible book on the development and history of Bayes’ Theo-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":357,"page_label":"303","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"probabilities caused by the observation. If the prior and posterior probabilities are sim-\nilar, then the information content in the observation was low. If the prior and posterior\nprobabilities are very different, then the information content in the observation was high.\nFinally, it can be shown that, under some assumptions, any learning algorithm that min-\nimizes the squared error of the model over the data will output a maximum likelihood\nprediction.32The relevance of this ﬁnding is that it provides a probabilistic justiﬁcation for\nthe approach to learning we present in Chapter 7[311].\n6.6 Further Reading\nMcGrayne (2011) is an accessible book on the development and history of Bayes’ Theo-\nrem. All data analysts should have at least one good textbook on statistics and probability.\nWe would recommend either Montgomery and Runger (2010) or Tijms (2012) (or both).\nJaynes (2003) deals with the use of probability theory in science and is a suitable text for\npostgraduate students.\n31. See Davies (2005, pp. 693–696).\n32. See Mitchell (1997, pp. 164–167).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":358,"page_label":"304","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"304 Chapter 6 Probability-Based Learning\nChapter 6 of Mitchell (1997) provides an excellent overview of Bayesian learning. Bar-\nber (2012) is a more recent machine learning textbook that adopts a Bayesian approach to\nlearning and inference.\nJudea Pearl is recognized as one of the key pioneers in developing the use of Bayesian\nnetworks in the ﬁeld of artiﬁcial intelligence , and his books (Pearl, 1988, 2000) are acces-\nsible and provide good introductions to the theory and methods of Bayesian networks, as\nwell as the more general ﬁeld of graphical models . Neapolitan (2004) is a good textbook\non Bayesian networks. Kollar and Friedman (2009) is a comprehensive text on the theory\nand methods of graphical models and is a good reference text for postgraduate students\nwho are doing research using graphical models.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":359,"page_label":"305","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.7 Exercises 305\n6.7 Exercises\n1.(a)Three people ﬂip a fair coin. What is the probability that exactly two of them will\nget heads?\n(b)Twenty people ﬂip a fair coin. What is the probability that exactly eight of them\nwill get heads?\n(c)Twenty people ﬂip a fair coin. What is the probability that at least four of them\nwill get heads?\n2.The table below gives details of symptoms that patients presented and whether they\nwere suffering from meningitis.\nID H EADACHE FEVER VOMITING MENINGITIS\n1 true true false false\n2 false true false false\n3 true false true false\n4 true false true false\n5 false true false true\n6 true false true false\n7 true false true false\n8 true false true true\n9 false true false false\n10 true false true true\nUsing this dataset, calculate the following probabilities:\n(a)PpVOMITING“trueq\n(b)PpHEADACHE“falseq\n(c)PpHEADACHE“true,VOMITING“falseq\n(d)PpVOMITING“false|HEADACHE“trueq\n(e)PpMENINGITIS|FEVER“true,VOMITING“falseq\n3.Predictive data analytics models are often used as tools for process quality control and\nfault detection. The task in this question is to create a naive Bayes model to monitor\na wastewater treatment plant.33The table below lists a dataset containing details of\nactivities at a wastewater treatment plant for 14 days. Each day is described in terms\nof six descriptive features that are generated from different sensors at the plant. S S-\nINmeasures the solids coming into the plant per day; S ED-INmeasures the sediment\ncoming into the plant per day; C OND -INmeasures the electrical conductivity of the\n33. The dataset in this question is inspired by the Waste Water Treatment Dataset that is available from the UCI\nMachine Learning repository (Bache and Lichman, 2013) at archive.ics.uci.edu/ml/machine-learning-databases/\nwater-treatment. The creators of this dataset reported their work in Bejar et al. (1991).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":360,"page_label":"306","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"306 Chapter 6 Probability-Based Learning\nwater coming into the plant.34The features S S-OUT, SED-OUT, and C OND -OUTare\nthe corresponding measurements for the water ﬂowing out of the plant. The target\nfeature, S TATUS , reports the current situation at the plant: ok, everything is working\ncorrectly; settler , there is a problem with the plant settler equipment; or solids , there\nis a problem with the amount of solids going through the plant.\nSS SED COND SS SED COND\nID -I N -IN -IN -OUT -OUT -OUT STATUS\n1 168 3 1,814 15 0.001 1,879 ok\n2 156 3 1,358 14 0.01 1,425 ok\n3 176 3.5 2,200 16 0.005 2,140 ok\n4 256 3 2,070 27 0.2 2,700 ok\n5 230 5 1,410 131 3.5 1,575 settler\n6 116 3 1,238 104 0.06 1,221 settler\n7 242 7 1,315 104 0.01 1,434 settler\n8 242 4.5 1,183 78 0.02 1,374 settler\n9 174 2.5 1,110 73 1.5 1,256 settler\n10 1,004 35 1,218 81 1,172 33.3 solids\n11 1,228 46 1,889 82.4 1,932 43.1 solids\n12 964 17 2,120 20 1,030 1,966 solids\n13 2,008 32 1,257 13 1,038 1,289 solids\n(a)Create a naive Bayes model that uses probability density functions to model the\ndescriptive features in this dataset (assume that all the descriptive features are\nnormally distributed).\n(b)What prediction will the naive Bayes model return for the following query?\nSS-IN= 222, S ED-IN= 4.5, C OND -IN= 1,518, S S-OUT= 74 S ED-OUT= 0.25,\nCOND -OUT= 1,642\n4.The following is a description of the causal relationship between storms, the behavior\nof burglars and cats, and house alarms:\nStormy nights are rare. Burglary is also rare, and if it is a stormy night, burglars are\nlikely to stay at home (burglars don’t like going out in storms). Cats don’t like storms either,\nand if there is a storm, they like to go inside. The alarm on your house is designed to be\ntriggered if a burglar breaks into your house, but sometimes it can be set off by your cat\ncoming into the house, and sometimes it might not be triggered even if a burglar breaks in\n(it could be faulty or the burglar might be very good).\n(a)Deﬁne the topology of a Bayesian network that encodes these causal relationships.\n34. The conductivity of water is affected by inorganic dissolved solids and organic compounds, such as oil.\nConsequently, water conductivity is a useful measure of water purity.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":361,"page_label":"307","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.7 Exercises 307\n(b)The table below lists a set of instances from the house alarm domain. Using the\ndata in this table, create the conditional probability tables (CPTs) for the network\nyou created in Part (a) of this question.\nID S TORM BURGLAR CAT ALARM\n1 false false false false\n2 false false false false\n3 false false false false\n4 false false false false\n5 false false false true\n6 false false true false\n7 false true false false\n8 false true false true\n9 false true true true\n10 true false true true\n11 true false true false\n12 true false true false\n13 true true false true\n(c)What value will the Bayesian network predict for A LARM , given that there is both\na burglar and a cat in the house but there is no storm?\n(d)What value will the Bayesian network predict for A LARM , given that there is a\nstorm but we don’t know if a burglar has broken in or where the cat is?\n˚5.The table below lists a dataset containing details of policyholders at an insurance com-\npany. The descriptive features included in the table describe each policy holders’ ID,\noccupation, gender, age, type of insurance policy, and preferred contact channel. The\npreferred contact channel is the target feature in this domain.\nPOLICY PREF\nID O CCUPATION GENDER AGE TYPE CHANNEL\n1 lab tech female 43 planC email\n2 farmhand female 57 planA phone\n3 biophysicist male 21 planA email\n4 sheriff female 47 planB phone\n5 painter male 55 planC phone\n6 manager male 19 planA email\n7 geologist male 49 planC phone\n8 messenger male 51 planB email\n9 nurse female 18 planC phone\n(a)Using equal-frequency binning , transform the A GEfeature into a categorical\nfeature with three levels: young ,middle-aged ,mature .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":362,"page_label":"308","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"308 Chapter 6 Probability-Based Learning\n(b)Examine the descriptive features in the dataset and list the features that you would\nexclude before you would use the dataset to build a predictive model. For each\nfeature you decide to exclude, explain why you have made this decision.\n(c)Calculate the probabilities required by a naive Bayes model to represent this do-\nmain.\n(d)What target level will a naive Bayes model predict for the following query:\nGENDER =female , AGE= 30, P OLICY =planA\n˚6.Imagine that you have been given a dataset of 1,000documents that have been clas-\nsiﬁed as being about entertainment oreducation . There are 700entertainment doc-\numents in the dataset and 300education documents in the dataset. The tables below\ngive the number of documents from each topic that a selection of words occurred in.\nWord-document counts for the entertainment dataset\nfun is machine christmas family learning\n415 695 35 0 400 70\nWord-document counts for the education dataset\nfun is machine christmas family learning\n200 295 120 0 10 105\n(a)What target level will a naive Bayes model predict for the following query docu-\nment: “ machine learning is fun ”?\n(b)What target level will a naive Bayes model predict for the following query docu-\nment: “ christmas family fun ”?\n(c)What target level will a naive Bayes model predict for the query document in Part\n(b) of this question, if Laplace smoothing with k“10and a vocabulary size of 6\nis used?\n˚7.Anaive Bayes model is being used to predict whether patients have a high risk of\nstroke in the next ﬁve years (S TROKE =true) or a low risk of stroke in the next ﬁve\nyears (S TROKE =false ). This model uses two continuous descriptive features A GE\nand W EIGHT (in kilograms). Both of these descriptive features are represented by\nprobability density functions, speciﬁcally normal distributions. The table below shows\nthe representation of the domain used by this model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":363,"page_label":"309","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"6.7 Exercises 309\nPpS troke“trueq “ 0.25 PpS troke“f alseq “ 0.75\nPpAGE“x|S troke“trueq PpAGE“x|S troke“f alseq\n«N¨\n˝x,\nµ“65,\nσ“15˛\n‚ «N¨\n˝x,\nµ“20,\nσ“15˛\n‚\nPpWEIGHT“x|S troke“trueq PpWEIGHT“x|S troke“f alseq\n«N¨\n˝x,\nµ“88,\nσ“8˛\n‚ «N¨\n˝x,\nµ“76,\nσ“6˛\n‚\n(a)What target level will the naive Bayes model predict for the following query:\nAGE= 45, W EIGHT = 80\n˚8.The table below lists a dataset of books and whether or not they were purchased by an\nindividual (i.e., the feature P URCHASED is the target feature in this domain).\nID S ECONDHAND GENRE COST PURCHASED\n1 false romance expensive true\n2 false science cheap false\n3 true romance cheap true\n4 false science cheap true\n5 false science expensive false\n6 true romance reasonable false\n7 true literature cheap false\n8 false romance reasonable false\n9 frue science cheap false\n10 true literature reasonable true\n(a)Calculate the probabilities (to four places of decimal) that a naive Bayes classiﬁer\nwould use to represent this domain.\n(b)Assuming conditional independence between features given the target feature value,\ncalculate the probability (rounded to four places of decimal) of each outcome\n(PURCHASED =true, and P URCHASED =false) for the following book:\nSECONDHAND =false, G ENRE =literature, C OST=expensive\n(c)What prediction would a naive Bayes classiﬁer return for the above book?\n˚9.The following is a description of the causal relationship between storms, the behavior\nof burglars and cats, and house alarms:\nJim and Martha always go shopping separately. If Jim does the shopping he buys wine,\nbut not always. If Martha does the shopping, she buys wine, but not always. If Jim tells\nMartha that he has done the shopping, then Martha doesn’t go shopping, but sometimes Jim\nforgets to tell Martha, and so sometimes both Jim and Martha go shopping.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":364,"page_label":"310","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"310 Chapter 6 Probability-Based Learning\n(a)Deﬁne the topology of a Bayesian network that encodes these causal relationships\nbetween the following Boolean variables: J IM(Jim has done the shopping, trueor\nfalse ), M ARTHA (Martha has done the shopping, true orfalse ), W INE(wine has\nbeen purchased, trueorfalse ).\n(b)The table below lists a set of instances from the house alarm domain. Using the\ndata in this table, create the conditional probability tables (CPTs) for the network\nyou created in the ﬁrst part of this question, and round the probabilities to two\nplaces of decimal.\nID J IM MARTHA WINE\n1 false false false\n2 false false false\n3 true false true\n4 true false true\n5 true false false\n6 false true true\n7 false true false\n8 false true false\n9 true true true\n10 true true true\n11 true true true\n12 true true false\n(c)What value will the Bayesian network predict for W INEif:\nJIM=trueand M ARTHA =false\n(d)What is the probability that J IMwent shopping given that W INE=true?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":365,"page_label":"311","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7 Error-Based Learning\n“Ever tried. Ever failed. No matter. Try again. Fail again. Fail better. ”\n—Samuel Beckett\nIn error-based machine learning, we perform a search for a set of parameters for a parame-\nterized model that minimizes the total error across the predictions made by that model with\nrespect to a set of training instances. The Fundamentals section of this chapter introduces\nthe key ideas of a parameterized model , measuring error , and an error surface . We then\npresent the standard approach to building error-based predictive models: multivariable\nlinear regression with gradient descent . The extensions and variations to this standard\napproach that we describe are how to handle categorical descriptive features, the use of lo-\ngistic regression to make predictions for categorical target features, ﬁne-tuning regression\nmodels, techniques for building non-linear andmultinomial models, and support vec-\ntor machines , which take a slightly different approach to using error to build prediction\nmodels.\n7.1 Big Idea\nAnyone who has learned a new sport will have had the sometimes painful experience of\ntaking an error-based approach to learning. Take surﬁng, for example. One of the key\nskills the novice surfer has to learn is how to successfully catch a wave . This involves\nﬂoating on your surfboard until a wave approaches and then paddling furiously to gain\nenough momentum for the wave to pick up both you and your board. The position of your\nbody on the board is key to doing this successfully. If you lie too far toward the back of\nthe board, the board will sink and create so much drag that even big waves will pass by,\nleaving you behind. If you lie too far forward on your board, you will begin to make great\nprogress before the surfboard tilts nose down into the water and launches you head over\nheels into the air. Only when you are positioned at the sweet spot in the middle of the\nboard—neither too far forward nor too far back—will you be able to use your paddling\nefforts to successfully catch a wave.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":366,"page_label":"312","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"312 Chapter 7 Error-Based Learning\nAt their ﬁrst attempt, new surfers will typically position themselves either too far forward\nor too far backward on their board when they attempt to catch their ﬁrst wave, resulting\nin a bad outcome. The outcome of an attempt to catch a wave is a judgment on how well\nthe surfer is doing, so an attempt constitutes an error function: lying too far back on the\nboard leads to a medium error, lying too far forward on the board leads to a more dramatic\nerror, while successfully catching a wave means really no error at all. Armed with the\nunsuccessful outcome of their ﬁrst attempt, surfers usually overcompensate on the second\nattempt, resulting in the opposite problem. On subsequent attempts, surfers will slowly\nreduce their error by slightly adjusting their position until they home in on the sweet spot\nat which they can keep their board perfectly balanced to allow a seamless transition to\ntickling the face of an awesome toob !\nA family of error-based machine learning algorithms takes the same approach. A pa-\nrameterized prediction model is initialized with a set of random parameters, and an error\nfunction is used to judge how well this initial model performs when making predictions for\ninstances in a training dataset. Based on the value of the error function, the parameters are\niteratively adjusted to create a more and more accurate model.\n7.2 Fundamentals\nIn this section we introduce a simple model of linear regression, some metrics for measur-\ning the error of a model, and the concept of an error surface. The discussion in this section,\nand in the rest of this chapter, assumes that you have a basic understanding of differen-\ntiation, in particular, what a derivative is, how to calculate a derivative for a continuous\nfunction, the chain rule for differentiation, and what a partial derivative is. If you don’t\nunderstand any of these concepts, see Appendix C[765]for the necessary introduction.\n7.2.1 Simple Linear Regression\nTable 7.1[313]shows a simple dataset recording the rental price (in Euro per month) of Dublin\ncity-center ofﬁces (R ENTAL PRICE ), along with a number of descriptive features that are\nlikely to be related to rental price: the S IZEof the ofﬁce (in square feet), the F LOOR in the\nbuilding in which the ofﬁce space is located, the B ROADBAND rate available at the ofﬁce\n(in Mb per second), and the E NERGY RATING of the building in which the ofﬁce space is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":366,"page_label":"312","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"function, the chain rule for differentiation, and what a partial derivative is. If you don’t\nunderstand any of these concepts, see Appendix C[765]for the necessary introduction.\n7.2.1 Simple Linear Regression\nTable 7.1[313]shows a simple dataset recording the rental price (in Euro per month) of Dublin\ncity-center ofﬁces (R ENTAL PRICE ), along with a number of descriptive features that are\nlikely to be related to rental price: the S IZEof the ofﬁce (in square feet), the F LOOR in the\nbuilding in which the ofﬁce space is located, the B ROADBAND rate available at the ofﬁce\n(in Mb per second), and the E NERGY RATING of the building in which the ofﬁce space is\nlocated (ratings range from AtoC, where Ais the most efﬁcient). Over the course of this\nchapter, we look at the ways in which all these descriptive features can be used to train\nan error-based model to predict ofﬁce rental prices. Initially, though, we will focus on a\nsimpliﬁed version of this task in which just S IZEis used to predict R ENTAL PRICE .\nFigure 7.1(a)[314]shows a scatter plot of the ofﬁce rentals dataset with R ENTAL PRICE on\nthe vertical (or y) axis and S IZEon the horizontal (or x) axis. From this plot, it is clear that\nthere is a strong linear relationship between these two features: as S IZEincreases so does\nRENTAL PRICE by a similar amount. If we could capture this relationship in a model, we","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":367,"page_label":"313","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.2 Fundamentals 313\nTable 7.1\nA dataset that includes ofﬁce rental prices and a number of descriptive features for 10 Dublin city-\ncenter ofﬁces.\nBROADBAND ENERGY RENTAL\nID S IZE FLOOR RATE RATING PRICE\n1 500 4 8 C 320\n2 550 7 50 A 380\n3 620 9 7 A 400\n4 630 5 24 B 390\n5 665 8 100 C 385\n6 700 4 8 B 410\n7 770 10 7 B 480\n8 880 12 50 A 600\n9 920 14 8 C 570\n10 1,000 9 24 B 620\nwould be able to do two important things. First, we would be able to understand how ofﬁce\nsize affects ofﬁce rental price. Second, we would be able to ﬁll in the gaps in the dataset to\npredict ofﬁce rental prices for ofﬁce sizes that we have never actually seen in the historical\ndata—for example, how much would we expect a 730-square-foot ofﬁce to rent for? Both\nof these things would be of great use to real estate agents trying to make decisions about\nthe rental prices they should set for new rental properties.\nThere is a simple, well-known mathematical model that can capture the relationship\nbetween two continuous features like those in our dataset. Many readers will remember\nfrom high school geometry that the equation of a line can be written\ny“mx`b (7.1)\nwhere mis the slope of the line, and bis known as the y-intercept of the line (i.e., the\nposition at which the line meets the vertical axis when the value of xis set to zero). The\nequation of a line predicts a yvalue for every xvalue given the slope and the y-intercept,\nand we can use this simple model to capture the relationship between two features such as\nSIZEand R ENTAL PRICE . Figure 7.1(b)[314]shows the same scatter plot as shown in Figure\n7.1(a)[314]with a simple linear model added to capture the relationship between ofﬁce sizes\nand ofﬁce rental prices. This model is\nRENTAL PRICE“6.47`0.62ˆSIZE (7.2)\nwhere the slope of the line is 0.62and the y-intercept is 6.47.\nThis model tells us that for every increase of a square foot in S IZE, RENTAL PRICE\nincreases by 0.62Euro. We can also use this model to determine the expected rental price\nof the 730-square-foot ofﬁce mentioned previously by simply plugging this value for S IZE","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":368,"page_label":"314","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"314 Chapter 7 Error-Based Learning\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n(a)\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice (b)\nFigure 7.1\n(a) A scatter plot of the S IZEand R ENTAL PRICE features from the ofﬁce rentals dataset; and (b) the\nscatter plot from (a) with a linear model relating R ENTAL PRICE to S IZEoverlaid.\ninto the model\nRENTAL PRICE“6.47`0.62ˆ730\n“459.07\nHence, we can expect our 730-square-foot ofﬁce to rent for about 460Euro per month.\nThis kind of model is known as a simple linear regression model . This approach to mod-\neling the relationships between features is extremely common in both machine learning\nand statistics.\nFor consistency with the notation that we use in this book, we can rewrite the simple\nlinear regression model\nMwpdq“wr0s`wr1sˆdr1s (7.3)\nwhere wis the vector⟨wr0s,wr1s⟩; the parameters wr0sandwr1sare referred to as\nweights;1dis an instance deﬁned by a single descriptive feature dr1s; andMwpdqis the\nprediction output by the model for the instance d. The key to using simple linear regression\nmodels is determining the optimal values for the weights in the model. The optimal values\nfor the weights are the ones that allow the model to best capture the relationship between\n1. Weights are also known as model parameters , and so regression models are often known as parameterized\nmodels .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":369,"page_label":"315","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.2 Fundamentals 315\nthe descriptive features and a target feature. A set of weights that capture this relationship\nwell are said to ﬁtthe training data. In order to ﬁnd the optimal set of weights, we need\nsome way to measure how well a model deﬁned using a candidate set of weights ﬁts a\ntraining dataset. We do this by deﬁning an error function to measure the error between\nthe predictions a model makes on the basis of the descriptive features for each instance in\nthe training data and the actual target values for each instance in the training data.\n7.2.2 Measuring Error\nThe model shown in Equation (7.2)[313]is deﬁned by the weights wr0s“6.47andwr1s“\n0.62. What tells us that these weights suitably capture the relationship within the training\ndataset? Figure 7.2(a)[316]shows a scatter plot of the S IZEand R ENTAL PRICE descriptive\nfeatures from the ofﬁce rentals dataset and a number of different simple linear regression\nmodels that might be used to capture this relationship. In these models the value for wr0s\nis kept constant at 6.47, and the values for wr1sare set to 0.4,0.5,0.62,0.7, and 0.8from\ntop to bottom. Out of the candidate models shown, the third model from the top (with\nwr1sset to 0.62), passes most closely through the actual dataset and is the one that most\naccurately ﬁts the relationship between ofﬁce sizes and ofﬁce rental prices—but how do\nwe measure this formally?\nIn order to formally measure the ﬁt of a linear regression model with a set of training data,\nwe require an error function . An error function captures the error between the predictions\nmade by a model and the actual values in a training dataset.2There are many different\nkinds of error functions, but for measuring the ﬁt of simple linear regression models, the\nmost commonly used is the sum of squared errors error function, or L2. To calculate\nL2we use our candidate model Mwto make a prediction for each member of the training\ndataset, D, and then calculate the error (or residual ) between these predictions and the\nactual target feature values in the training set.\nFigure 7.2(b)[316]shows the ofﬁce rentals dataset and the candidate model with wr0s“\n6.47andwr1s“0.62and also includes error bars to highlight the differences between\nthe predictions made by the model and the actual R ENTAL PRICE values in the training\ndata. Notice that the model sometimes overestimates the ofﬁce rental price, and sometimes","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":369,"page_label":"315","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"kinds of error functions, but for measuring the ﬁt of simple linear regression models, the\nmost commonly used is the sum of squared errors error function, or L2. To calculate\nL2we use our candidate model Mwto make a prediction for each member of the training\ndataset, D, and then calculate the error (or residual ) between these predictions and the\nactual target feature values in the training set.\nFigure 7.2(b)[316]shows the ofﬁce rentals dataset and the candidate model with wr0s“\n6.47andwr1s“0.62and also includes error bars to highlight the differences between\nthe predictions made by the model and the actual R ENTAL PRICE values in the training\ndata. Notice that the model sometimes overestimates the ofﬁce rental price, and sometimes\nunderestimates the ofﬁce rental price. This means that some of the errors will be positive\nand some will be negative. If we were to simply add these together, the positive and\nnegative errors would effectively cancel each other out. This is why, rather than just using\nthe sum of the errors, we use the sum of the squared errors because this means all values\nwill be positive.\n2. Error functions are commonly referred to as loss functions because they represent what we loseby reducing\nthe training set to a simple model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":370,"page_label":"316","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"316 Chapter 7 Error-Based Learning\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n(a)\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice (b)\nFigure 7.2\n(a) A scatter plot of the S IZEand R ENTAL PRICE features from the ofﬁce rentals dataset. A collection\nof possible simple linear regression models capturing the relationship between these two features are\nalso shown. For all models wr0sis set to 6.47. From top to bottom, the models use 0.4,0.5,0.62,\n0.7, and 0.8, respectively, for wr1s. (b) A scatter plot of the S IZEand R ENTAL PRICE features from\nthe ofﬁce rentals dataset showing a candidate prediction model (with wr0s“6.47andwr1s“0.62)\nand the resulting errors.\nThe sum of squared errors error function, L2, is formally deﬁned as\nL2pMw,Dq“1\n2nÿ\ni“1pti´Mwpdiqq2(7.4)\nwhere the training set is composed of ntraining instances; each training instance is com-\nposed of descriptive features dand a target feature t;Mwpdiqis the prediction made by\na candidate model Mwfor a training instance with descriptive features di; and the candi-\ndate model Mwis deﬁned by the weight vector w. For our simple scenario in which each\ninstance is described with a single descriptive feature, Equation (7.4)[316]expands to\nL2pMw,Dq“1\n2nÿ\ni“1pti´pwr0s`wr1sˆdir1sqq2(7.5)\nTable 7.2[317]shows the calculation of the sum of squared errors for the candidate model\nwith wr0s“6.47andwr1s“0.62. In this case, the sum of squared errors is equal to\n2,837.08.\nIf we perform the same calculation for the other candidate models shown in Figure\n7.2(a)[316], we ﬁnd that with wr1sset to 0.4,0.5,0.7, and 0.8, the sums of squared errors are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":371,"page_label":"317","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.2 Fundamentals 317\nTable 7.2\nCalculating the sum of squared errors for the candidate model (with wr0s“6.47andwr1s“0.62)\nto make predictions for the ofﬁce rentals dataset.\nRENTAL Model Squared\nID S IZE PRICE Prediction Error Error\n1 500 320 316.47 3.53 12.46\n2 550 380 347.47 32.53 1,058.20\n3 620 400 390.87 9.13 83.36\n4 630 390 397.07 -7.07 49.98\n5 665 385 418.77 -33.77 1,140.41\n6 700 410 440.47 -30.47 928.42\n7 770 480 483.87 -3.87 14.98\n8 880 600 552.07 47.93 2,297.28\n9 920 570 576.87 -6.87 47.20\n10 1,000 620 626.47 -6.47 41.86\nSum 5,674.15\nSum of squared errors (Sum/2) 2,837.08\n136,218,42,712,20,092, and 90,978respectively. The fact that the sums of squared errors\nfor these models are larger than for the model with wr1sset to 0.62demonstrates that our\nprevious visual intuition that this model most accurately ﬁts the training data was correct.\nThe sum of squared errors function can be used to measure how well any combination\nof weights ﬁts the instances in a training dataset. The next section explains how the values\nof an error function for many different potential models can be combined to form an error\nsurface across which we can search for the optimal weights with the minimum sum of\nsquared errors.3\n7.2.3 Error Surfaces\nFor every possible combination of weights, wr0sandwr1s, there is a corresponding sum\nof squared errors value. We can think about all these error values joined to make a surface\ndeﬁned by the weight combinations, as shown in Figure 7.3(a)[318]. Here, each pair of\nweights wr0sandwr1sdeﬁnes a point on the x-yplane, and the sum of squared errors\nfor the model using these weights determines the height of the error surface above the x-y\nplane for that pair of weights. The x-yplane is known as a weight space , and the surface\nis known as an error surface . The model that best ﬁts the training data is the model\ncorresponding to the lowest point on the error surface.\n3. One of the best-known and earliest applications of solving a problem by reducing the sum of squared errors\noccurred in 1801, when Carl Friedrich Gauss used it to minimize the measurement error in astronomical data\nand by doing so was able to extrapolate the position of the dwarf planet Ceres, which had recently been found but\nthen was lost behind the glare of the sun.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":372,"page_label":"318","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"318 Chapter 7 Error-Based Learning\n(a)\nw0w1\n−2−10123\n−5 05 10150.0e+005.0e+061.0e+071.5e+072.0e+072.5e+073.0e+073.5e+074.0e+07 (b)\nFigure 7.3\n(a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error surface generated by plotting\nthe sum of squared errors for the ofﬁce rentals training set for each possible combination of values\nforwr0s(from the range r´10,20s) and wr1s(from the range r´2,3s).\nAlthough for some simple problems, like those presented in our ofﬁce rentals dataset, it\nis possible to try out every reasonable combination of weights and through this brute-force\nsearch ﬁnd the best combination, for most real-world problems this is not feasible—the\ncomputation required would take far too long. Instead, we need a more efﬁcient way\nto ﬁnd the best combination of weights. Fortunately, for prediction problems like those\nposed by the ofﬁce rentals dataset, the associated error surfaces have two properties that\nhelp us ﬁnd the optimal combination of weights: they are convex , and they have a global\nminimum . By convex we mean that the error surfaces are shaped like a bowl. Having a\nglobal minimum means that on an error surface, there is a unique set of optimal weights\nwith the lowest sum of squared errors. The reason why the error surface always has these\nproperties is that its overall shape is determined by the linearity of the model, rather than\nthe properties of the data. If we can ﬁnd the global minimum of the error surface, we can\nﬁnd the set of weights deﬁning the model that best ﬁts the training dataset. This approach\nto ﬁnding weights is known as least squares optimization .\nBecause we can expect the error surface to be convex and possess a global minimum, we\ncan ﬁnd the optimal weights at the point where the partial derivatives of the error surface\nwith respect to wr0sandwr1sare equal to 0. The partial derivatives of the error surface","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":373,"page_label":"319","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 319\nwith respect to wr0sandwr1smeasure the slope of the error surface at the point wr0s\nandwr1s. The point on the error surface at which the partial derivatives with respect to\nwr0sandwr1sare equal to 0is simply the point at the very bottom of the bowl deﬁned by\nthe error surface—there is no slope at the bottom of the bowl. This point is at the global\nminimum of the error surface, and the coordinates of this point deﬁne the weights for the\nprediction model with the lowest sum of squared errors on the dataset. Using Equation\n(7.5)[316], we can formally deﬁne this point on the error surface as the point at which\nB\nBwr0s1\n2nÿ\ni“1pti´pwr0s`wr1sˆdir1sqq2“0 (7.6)\nand\nB\nBwr1s1\n2nÿ\ni“1pti´pwr0s`wr1sˆdir1sqq2“0 (7.7)\nThere are a number of different ways to ﬁnd this point. In this chapter we describe a\nguided search approach known as the gradient descent algorithm. This is one of the\nmost important algorithms in machine learning and, as we discuss in other chapters, can\nbe used for many different purposes. The next section describes how gradient descent\ncan be used to ﬁnd the optimal weights for linear regression models that handle multiple\ndescriptive features: multivariable linear regression models.\n7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent\nThe most common approach to error-based machine learning for predictive analytics is to\nusemultivariable linear regression with gradient descent to train a best-ﬁt model for\na given training dataset. This section explains how this works. First, we describe how\nwe extend the simple linear regression model described in the previous section to handle\nmultiple descriptive features, and then we describe the gradient descent algorithm.\n7.3.1 Multivariable Linear Regression\nThe simple linear regression model we looked at in Section 7.2.1[312]handled only a single\ndescriptive feature. Interesting problems in predictive analytics, however, are multivari-\nable4in nature. Fortunately, extending the simple linear regression model to a multivariable\nlinear regression model is straightforward. We can deﬁne a multivariable linear regression\n4. The words multivariable andmulti-feature are equivalent. The use of multivariable is a sign of the origins\nof regression in statistics rather than machine learning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":374,"page_label":"320","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"320 Chapter 7 Error-Based Learning\nmodel as\nMwpdq“wr0s`wr1sˆdr1s`¨¨¨` wrmsˆdrms\n“wr0s`mÿ\nj“1wrjsˆdrjs (7.8)\nwhere dis a vector of mdescriptive features, dr1s...drms, and wr0s...wrmsarepm`\n1qweights. We can make Equation (7.8)[320]look a little neater by inventing a dummy\ndescriptive feature, dr0s, that is always equal to 1. This then gives us\nMwpdq“wr0sˆdr0s`wr1sˆdr1s`¨¨¨` wrmsˆdrms\n“mÿ\nj“0wrjsˆdrjs\n“w¨d (7.9)\nwhere w¨dis the dot product of the vectors wandd. The dot product of two vectors is\nthe sum of the products of their corresponding elements.\nThe expansion of the sum of squared errors loss function, L2, that we gave in Equation\n(7.5)[316]changes slightly to reﬂect the new regression equation\nL2pMw,Dq“1\n2nÿ\ni“1pti´Mwpdiqq2\n“1\n2nÿ\ni“1pti´pw¨diqq2(7.10)\nwhere the training dataset is composed of ntraining instances pdi,tiq;Mwpdiqis the pre-\ndiction made by a model Mwfor a training instance with descriptive features di; and the\nmodelMwis deﬁned by the weight vector w.\nThis multivariable model allows us to include all but one of the descriptive features\nin Table 7.2[317]in a regression model to predict ofﬁce rental prices (we will see how to\ninclude the categorical E NERGY RATING in the model in Section 7.4.3[336]). The resulting\nmultivariable regression model equation is\nRENTAL PRICE“wr0s`wr1sˆSIZE`wr2sˆFLOOR\n`wr3sˆBROADBAND RATE\nWe will see in the next section how the best-ﬁt set of weights for this equation are found,\nbut for now we will set wr0s“´0.1513 ,wr1s“0.6270 ,wr2s“´0.1781 , and wr3s“\n0.0714 . This means that the model is rewritten\nRENTAL PRICE“´0.1513`0.6270ˆSIZE´0.1781ˆFLOOR\n`0.0714ˆBROADBAND RATE","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":375,"page_label":"321","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 321\nUsing this model, we can, for example, predict the expected rental price of a 690-square-\nfoot ofﬁce on the 11thﬂoor of a building with a broadband rate of 50Mb per second\nRENTAL PRICE“ ´ 0.1513`0.6270ˆ690´0.1781ˆ11`0.0714ˆ50\n“434.0896\nThe next section describes how the weights can be determined using the gradient descent\nalgorithm.\n7.3.2 Gradient Descent\nIn Section 7.2.3[317]we said that the best-ﬁt set of weights for a linear regression model can\nbe found at the global minimum of the error surface deﬁned by the weight space associated\nwith the relevant training dataset. We also mentioned that this global minimum can be\nfound at the point at which the partial derivatives of the error surface, with respect to\nthe weights, are equal to zero. Although it is possible to calculate this point directly for\nsome simpler problems, this approach is not computationally feasible for most interesting\npredictive analytics problems. The number of instances in the training set and the number\nof weights for which we need to ﬁnd values simply make the problem too large. The\nbrute-force search approach that was mentioned in Section 7.2.3[317]is not feasible either—\nespecially as the number of descriptive features, and subsequently the number of weights,\nincreases.\nThere is, however, a simple approach to learning weights that we can take based on the\nfacts that, even though they are hard to visualize, the error surfaces that correspond to these\nhigh-dimensional weight spaces still have the convex shape seen in Figure 7.3[318](albeit\nin multiple dimensions), and that a single global minimum exists. This approach uses a\nguided search from a random starting position and is known as gradient descent .\nTo understand how gradient descent works, imagine a hiker unlucky enough to be stranded\non the side of a valley on a foggy day. Because of the dense fog, it is not possible for her\nto see the way to her destination at the bottom of the valley. Instead, it is only possible to\nsee the ground at her feet to within about a three-foot radius. It might, at ﬁrst, seem like\nall is lost and that it will be impossible for the hiker to ﬁnd her way down to the bottom\nof the valley. There is, however, a reliable approach that the hiker can take that will guide\nher to the bottom (assuming, somewhat ideally, that the valley is convex and has a global","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":375,"page_label":"321","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"guided search from a random starting position and is known as gradient descent .\nTo understand how gradient descent works, imagine a hiker unlucky enough to be stranded\non the side of a valley on a foggy day. Because of the dense fog, it is not possible for her\nto see the way to her destination at the bottom of the valley. Instead, it is only possible to\nsee the ground at her feet to within about a three-foot radius. It might, at ﬁrst, seem like\nall is lost and that it will be impossible for the hiker to ﬁnd her way down to the bottom\nof the valley. There is, however, a reliable approach that the hiker can take that will guide\nher to the bottom (assuming, somewhat ideally, that the valley is convex and has a global\nminimum ). If the hiker looks at the slope of the ground at her feet, she will notice that in\nsome directions, the ground slopes up, and in other directions, the ground slopes down. If\nshe takes a small step in the direction in which the ground slopes most steeply downward\n(the direction of the gradient of the mountain), she will be headed toward the bottom of the\nmountain. If she repeats this process over and over again, she will make steady progress\ndown the mountain until eventually she arrives at the bottom. Gradient descent works in\nexactly the same way.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":376,"page_label":"322","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"322 Chapter 7 Error-Based Learning\nGradient descent starts by selecting a random point within the weight space (i.e., each\nweight in the multivariable linear regression equation is assigned a random value within\nsome sensible range) and calculating the sum of squared errors associated with this point\nbased on predictions made for each instance in the training set using the randomly selected\nweights (as shown in Section 7.2.2[315]). This deﬁnes one point on the error surface. Al-\nthough the error value at this point in the weight space can be calculated, we know little\nelse about the relative position of this point on the error surface. Just like our imagined\nmountain climber, the algorithm can use only very localized information. It is possible,\nhowever, to determine the slope of the error surface by determining the derivative of the\nfunction used to generate it, and then calculating the value of this derivative at the random\npoint selected in the weight space. This means that, again like our mountain climber, the\ngradient descent algorithm can use the direction of the slope of the error surface at the\ncurrent location in the weight space. Taking advantage of this information, the randomly\nselected weights are adjusted slightly in the direction of the error surface gradient to move\nto a new position on the error surface. Because the adjustments are made in the direction\nof the error surface gradient, this new point will be closer to the overall global minimum.\nThis adjustment is repeated over and over until the global minimum on the error surface\nis reached. Figure 7.4[323]shows an error surface (deﬁned over just two weights so that we\ncan visualize the error surface) and some examples of the path down this surface that the\ngradient descent algorithm would take from different random starting positions.5\nFor the simple version of the ofﬁce rentals example that uses only the S IZEdescriptive\nfeature, described in Section 7.2.1[312], it is easy to visualize how the gradient descent algo-\nrithm would move iteratively toward a model that best ﬁts the training data, making small\nadjustments each time—with each adjustment reducing the error of the model, just as our\nsurfer from Section 7.1[311]did. Figure 7.5[324]shows the journey across the error surface that\nis taken by the gradient descent algorithm when training this model. Figure 7.6[325]shows\na series of snapshots of the candidate models created at steps along this journey toward","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":376,"page_label":"322","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"gradient descent algorithm would take from different random starting positions.5\nFor the simple version of the ofﬁce rentals example that uses only the S IZEdescriptive\nfeature, described in Section 7.2.1[312], it is easy to visualize how the gradient descent algo-\nrithm would move iteratively toward a model that best ﬁts the training data, making small\nadjustments each time—with each adjustment reducing the error of the model, just as our\nsurfer from Section 7.1[311]did. Figure 7.5[324]shows the journey across the error surface that\nis taken by the gradient descent algorithm when training this model. Figure 7.6[325]shows\na series of snapshots of the candidate models created at steps along this journey toward\nthe best-ﬁt model for this dataset. Notice how the model gets closer and closer to a model\nthat accurately captures the relationship between S IZEand R ENTAL PRICE . This is also\napparent in the ﬁnal panel in Figure 7.6[325], which shows how the sum of squared errors\ndecreases as the model becomes more accurate.\nThe gradient descent algorithm for training multivariable regression models is formally\npresented in Algorithm 4[326]. Each weight is iteratively adjusted by a small amount based\non the error in the predictions made by the current candidate model so as to generate\nsubsequently more and more accurate candidate models. Eventually, the algorithm will\n5. In fact, this is the error surface that results from the ofﬁce rentals dataset when the descriptive features in the\ndataset are normalized to the range r´1,1susing range normalization before being used. We discuss normaliza-\ntion subsequently in the chapter.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":377,"page_label":"323","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 323\n(a)\n (b)\nFigure 7.4\n(a) A 3D plot of an error surface and (b) a bird’s-eye view contour plot of the same error surface. The\nlines indicate the path that the gradient descent algorithm would take across this error surface from\nfour different starting positions to the global minimum—marked as the white dot in the center.\nconverge to a point on the error surface where any subsequent changes to weights do not\nlead to a noticeably better model (within some tolerance). At this point we can expect the\nalgorithm to have found the global minimum of the error surface and, as a result, the most\naccurate predictive model possible.\nThe most important part to the gradient descent algorithm is the line on which the weights\nare updated, Line 4[326]. Each weight is considered independently, and for each one a\nsmall adjustment is made by adding a small value, called a delta value , to the current\nweight, wrjs. This adjustment should ensure that the change in the weight leads to a move\ndownward on the error surface. The learning rate ,α, determines the size of the adjust-\nments made to weights at each iteration of the algorithm and is discussed further in Section\n7.3.3[328].\nThe remainder of this section focuses on the error delta function, which calculates the\ndelta value that determines the direction (either positive or negative) and the magnitude of\nthe adjustments made to each weight. The direction and magnitude of the adjustment to be\nmade to a weight is determined by the gradient of the error surface at the current position\nin the weight space. Recalling that the error surface is deﬁned by the error function, L2","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":378,"page_label":"324","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"324 Chapter 7 Error-Based Learning\n(a)\n (b)\nFigure 7.5\n(a) A 3D surface plot and (b) a bird’s-eye view contour plot of the error surface for the ofﬁce rentals\ndataset showing the path that the gradient descent algorithm takes toward the best-ﬁt model.\n(given in Equation (7.10)[320]), the gradient at any point on this error surface is given by the\nvalue of the partial derivative of the error function with respect to a particular weight at\nthat point. The error delta function invoked on Line 4[326]of Algorithm 4[326]performs this\ncalculation to determine the delta value by which each weight should be adjusted.\nTo understand how to calculate the value of the partial derivative of the error function\nwith respect to a particular weight, let us imagine for a moment that our training dataset,\nD, contains just one training instance: pd,tq, where dis a set of descriptive features and t\nis a target feature. The gradient of the error surface is given as the partial derivative of L2\nwith respect to each weight, wrjs\nB\nBwrjsL2pMw,Dq“B\nBwrjsˆ1\n2pt´Mwpdqq2˙\n(7.11)\n“ p t´MwpdqqˆB\nBwrjspt´Mwpdqq (7.12)\n“ p t´MwpdqqˆB\nBwrjspt´pw¨dqq (7.13)\n“ p t´Mwpdqqˆ´ drjs (7.14)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":379,"page_label":"325","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 325\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n500 600 700 800 900 1000350 400 450 500 550 600\nSizeRental Pr ice\n0 20 40 60 80 1000e+00 2e+05 4e+05 6e+05 8e+05\nTraining Iter ationSum of Squared Errors\nFigure 7.6\nA selection of the simple linear regression models developed during the gradient descent process for\nthe ofﬁce rentals dataset. The bottom-right panel shows the sums of squared errors generated during\nthe gradient descent process.\nEquation (7.12)[324]is calculated from Equation (7.11)[324]by applying the differentiation\nchain rule .6To understand the move from Equation (7.13)[324]to Equation (7.14)[324], imag-\nine a problem with four descriptive features dr1s...dr4s. Remembering that we always\ninclude the dummy feature dr0swith a value of 1, the dot product w¨dbecomes\nw¨d“wr0sˆdr0s`wr1sˆdr1s`wr2sˆdr2s\n`wr3sˆdr3s`wr4sˆdr4s\n6. See Appendix C[765].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":380,"page_label":"326","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"326 Chapter 7 Error-Based Learning\nAlgorithm 4 The gradient descent algorithm for training multivariable linear regression\nmodels.\nRequire: set of training instances D\nRequire: a learning rate αthat controls how quickly the algorithm converges\nRequire: a function, errorDelta , that determines the direction in which to adjust a given\nweight, wrjs, so as to move down the slope of an error surface determined by the\ndataset, D\nRequire: a convergence criterion that indicates that the algorithm has completed\n1:wÐrandom starting point in the weight space\n2:repeat\n3: foreach wrjsinwdo\n4: wrjsÐwrjs`αˆerrorDeltapD,wrjsq\n5: end for\n6:until convergence occurs\nIf we take the partial derivative of this with respect to wr0s, all the terms that do not contain\nwr0sare treated as constants, so\nB\nBwr0sw¨d“B\nBwr0spwr0sˆdr0s`wr1sˆdr1s`wr2sˆdr2s\n`wr3sˆdr3s`wr4sˆdr4sq\n“dr0s`0`0`0`0“dr0s\nSimilarly, the partial derivative with respect to wr4sis\nB\nBwr4sw¨d“B\nBwr4spwr0sˆdr0s`wr1sˆdr1s`wr2sˆdr2s\n`wr3sˆdr3s`wr4sˆdr4sq\n“0`0`0`0`dr4s“dr4s\nSo, in the move between Equations (7.13)[324]and (7.14)[324],B\nBwrjspt´pw¨dqqbecomes\n´drjs(remember that in this equation, tis a constant and so becomes zero when differen-\ntiated).\nEquation (7.14)[324]calculates the gradient based only on a single training instance. To\ntake into account multiple training instances, we calculate the sum of the squared errors\nfor each training instance (as we did in all our previous examples). So, Equation (7.14)[324]\nbecomes\nB\nBwrjsL2pMw,Dq “nÿ\ni“1ppti´Mwpdiqqˆ´dirjsq (7.15)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":381,"page_label":"327","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 327\nwherepd1,t1q...pdn,tnqarentraining instances, and dirjsis the jthdescriptive feature\nof training instance pdi,tiq. The direction of the gradient calculated using this equation is\ntoward the highest values on the error surface. The error delta function from Line 4[326]\nof Algorithm 4[326]should return a small step toward a lower value on the error surface.\nTherefore, we move in the opposite direction of the calculated gradient, and the error delta\nfunction can be written\nerrorDeltapD,wrjsq“´B\nBwrjsL2pMw,Dq\n“nÿ\ni“1ppti´Mwpdiqqˆdirjsq (7.16)\nLine 4[326]of Algorithm 4[326]can therefore be rewritten as what is known as the weight\nupdate rule for multivariable linear regression with gradient descent\nwrjsÐwrjs`αˆnÿ\ni“1ppti´Mwpdiqqˆdirjsq\nloooooooooooooooomoooooooooooooooon\nerrorDeltapD,wrjsq(7.17)\nwhere wrjsis any weight, αis a constant learning rate, tiis the expected target feature value\nfor the ithtraining instance, Mwpdiqis the prediction made for this training instance by the\ncurrent candidate model deﬁned by the weight vector w, and dirjsis the jthdescriptive\nfeature of the ithtraining instance and corresponds with weight wrjsin the regression\nmodel.\nTo intuitively understand the weight update rule given in Equation (7.17)[327], it helps to\nthink in terms of what the weight update rule does to weights based on the error in the\npredictions made by the current candidate model:\n‚If the errors show that, in general, predictions made by the candidate model are too high,\nthenwrjsshould be decreased if dirjsis positive and increased if dirjsis negative.\n‚If the errors show that, in general, predictions made by the candidate model are too low,\nthenwrjsshould be increased if dirjsis positive and decreased if dirjsis negative.\nThe approach to training multivariable linear regression models described so far is more\nspeciﬁcally known as batch gradient descent . The word batch is used because only one\nadjustment is made to each weight at each iteration of the algorithm based on summing\nthe squared error made by the candidate model for each instance in the training dataset.7\n7.Stochastic gradient descent is a slightly different approach, in which an adjustment to each weight is made\non the basis of the error in the prediction made by the candidate model for each training instance individually.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":381,"page_label":"327","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"‚If the errors show that, in general, predictions made by the candidate model are too low,\nthenwrjsshould be increased if dirjsis positive and decreased if dirjsis negative.\nThe approach to training multivariable linear regression models described so far is more\nspeciﬁcally known as batch gradient descent . The word batch is used because only one\nadjustment is made to each weight at each iteration of the algorithm based on summing\nthe squared error made by the candidate model for each instance in the training dataset.7\n7.Stochastic gradient descent is a slightly different approach, in which an adjustment to each weight is made\non the basis of the error in the prediction made by the candidate model for each training instance individually.\nThis means that many more adjustments are made to the weights. We do not discuss stochastic gradient descent\nin any detail in this book, although the modiﬁcations that need to be made to the gradient descent algorithm for\nstochastic gradient descent are fairly simple.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":382,"page_label":"328","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"328 Chapter 7 Error-Based Learning\nBatch gradient descent is a straightforward, accurate, and reasonably efﬁcient approach\nto training multivariable linear regression models and is used widely in practice. The\ninductive bias encoded in this algorithm includes a preference bias to prefer models that\nminimize the sum of squared errors function and a restriction bias introduced by the facts\nthat we consider only linear combinations of descriptive features and that we take a single\npath through the error gradient from a random starting point.\n7.3.3 Choosing Learning Rates and Initial Weights\nThe values chosen for the learning rate and initial weights can have a signiﬁcant impact on\nhow the gradient descent algorithm proceeds. Unfortunately, there are no theoretical results\nthat help in choosing the optimal values for these parameters. Instead, these algorithm\nparameters must be chosen using rules of thumb gathered through experience.\nThe learning rate, α, in the gradient descent algorithm determines the size of the ad-\njustment made to each weight at each step in the process. We can illustrate this using\nthe simpliﬁed version of the R ENTAL PRICE prediction problem based only on ofﬁce size\n(SIZE). A linear regression model for the problem uses only two weights, wr0sandwr1s.\nFigure 7.7[329]shows how different learning rates— 0.002,0.08, and 0.18—result in very\ndifferent journeys across the error surface.8The changing sum of squared errors that result\nfrom these journeys are also shown.\nFigure 7.7(a)[329]shows the impact of a very small learning rate. Although the gradient\ndescent algorithm will converge to the global minimum eventually, it takes a very long time\nas tiny changes are made to the weights at each iteration of the algorithm. Figure 7.7(c)[329]\nshows the impact of a large learning rate. The large adjustments made to the weights during\ngradient descent cause it to jump completely from one side of the error surface to the other.\nAlthough the algorithm can still converge toward an area of the error surface close to the\nglobal minimum, there is a strong chance that the global minimum itself will be missed,\nand the algorithm will simply jump back and forth across it. In fact, if inappropriately large\nlearning rates are used, the jumps from one side of the error surface to the other can cause\nthe sum of squared errors to repeatedly increase rather than decrease, leading to a process","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":382,"page_label":"328","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"as tiny changes are made to the weights at each iteration of the algorithm. Figure 7.7(c)[329]\nshows the impact of a large learning rate. The large adjustments made to the weights during\ngradient descent cause it to jump completely from one side of the error surface to the other.\nAlthough the algorithm can still converge toward an area of the error surface close to the\nglobal minimum, there is a strong chance that the global minimum itself will be missed,\nand the algorithm will simply jump back and forth across it. In fact, if inappropriately large\nlearning rates are used, the jumps from one side of the error surface to the other can cause\nthe sum of squared errors to repeatedly increase rather than decrease, leading to a process\nthat will never converge. Figure 7.7(b)[329]shows that a well-chosen learning rate strikes a\ngood balance, converging quickly but also ensuring that the global minimum is reached.\nNote that even though the shape of the curve in Figure 7.7(e)[329]is similar to the shape in\nFigure 7.7(d)[329], it takes far fewer iterations to reach the global minimum.\nUnfortunately, choosing learning rates is not a well-deﬁned science. Although there are\nsome algorithmic approaches, most practitioners use rules of thumb and trial and error. A\ntypical range for learning rates is r0.00001,10s, and practitioners will usually try out higher\n8. Note that in this example, we have normalized the R ENTAL PRICE and S IZEfeatures to the range r´1,1s, so\nthe error surfaces shown look slightly different from those shown in Figure 7.3[318]and Figure 7.5[324].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":383,"page_label":"329","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 329\n(a)\n (b)\n (c)\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":383,"page_label":"329","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/un","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":383,"page_label":"329","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"5CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":383,"page_label":"329","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"i25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":383,"page_label":"329","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"0 100 200 300 400 5000 20 40 60 80\nTraining Iter ationSum of Squared Errors\n(d)\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n5 10 15 200 20 40 60 80\nTraining Iter ationSum of Squared Errors (e)\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n0 5 10 15 20 25 30 350 20 40 60 80\nTraining Iter ationSum of Squared Errors (f)\nFigure 7.7\nPlots of the journeys made across the error surface for the simple ofﬁce rentals prediction problem\nfor different learning rates: (a) a very small learning rate ( 0.002); (b) a medium learning rate ( 0.08);\nand (c) a very large learning rate ( 0.18). The changing sum of squared errors are also shown.\nvalues and observe the resulting learning graph. If the graph looks too much like Figure\n7.7(f)[329], a smaller value will be tested until something approaching Figure 7.7(e)[329]is\nfound.\nWhen the gradient descent algorithm is used to ﬁnd optimal weights for linear regression\nmodels, the initial weights are chosen randomly from a predeﬁned range that must be\nspeciﬁed as an input to the algorithm. The choice of the range from which these initial\nweights are selected affects how quickly the gradient descent algorithm will converge to a\nsolution. Unfortunately, as is the case with the learning rate, there are no well-established,\nproven methods for choosing initial weights. Normalization also has a part to play here. It\nis much easier to select initial weights for normalized feature values than for raw feature","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":384,"page_label":"330","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"330 Chapter 7 Error-Based Learning\nvalues, as the range in which weights for normalized feature values might reasonably fall\n(particularly for the intercept weight, wr0s) is much better deﬁned than the corresponding\nrange when raw feature values are used. The best advice we can give is that, based on\nempirical evidence, choosing random initial weights uniformly from the range r´0.2,0.2s\ntends to work well.\n7.3.4 A Worked Example\nWe are now in a position to build a linear regression model that uses all the continuous\ndescriptive features in the ofﬁce rentals dataset in Table 7.1[313](i.e., all features except for\nENERGY RATING ). The general structure of the model is\nRENTAL PRICE“wr0s`wr1sˆSIZE`wr2sˆFLOOR\n`wr3sˆBROADBAND RATE\nso there are four weights— wr0s,wr1s,wr2s, and wr3s—for which optimal values must\nbe found. For this example, let’s assume that the learning rate, α, is0.00000002 and the\ninitial weights are chosen from a uniform random distribution in the range r´0.2,0.2sto\nbewr0s“ ´ 0.146,wr1s“0.185,wr2s“ ´ 0.044, and wr3s“0.119. Table 7.3[331]\ndetails the important values from the ﬁrst two iterations of the gradient descent algorithm\nwhen applied to this data.9\nUsing the initial weights predictions are made for all the instances in the training dataset,\nas shown in the Predictions column (column 3) of Table 7.3[331]. By comparing these pre-\ndicted values with the actual R ENTAL PRICE (column 2), we can compute an error and a\nsquared error term for each training instance, columns 4 and 5 of the table.\nTo update the weights, we must ﬁrst calculate the delta value for each weight. This\nis calculated by summing over all the instances in the training set the prediction error\nmultiplied by the value of the relevant feature for that instance (see Equation (7.16)[327]).\nThe last four columns on the right of the table list for each instance the product of the\nprediction error and the feature value. Remember that dr0sis a dummy descriptive feature,\nadded to match wr0s, with a value of 1for all training instances. As a result, the values\nin column 6 are identical to the values in the error column. Focusing on the top cell\nof column 7, we see the value 113,370.05. This value was calculated by multiplying the\nprediction error for d1(226.74) by the S IZEvalue for this instance ( 500). The other cells in\nthese columns are populated with similar calculations. The errorDeltapD,wrjsqfor each","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":384,"page_label":"330","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"multiplied by the value of the relevant feature for that instance (see Equation (7.16)[327]).\nThe last four columns on the right of the table list for each instance the product of the\nprediction error and the feature value. Remember that dr0sis a dummy descriptive feature,\nadded to match wr0s, with a value of 1for all training instances. As a result, the values\nin column 6 are identical to the values in the error column. Focusing on the top cell\nof column 7, we see the value 113,370.05. This value was calculated by multiplying the\nprediction error for d1(226.74) by the S IZEvalue for this instance ( 500). The other cells in\nthese columns are populated with similar calculations. The errorDeltapD,wrjsqfor each\nweight is then the summation of the relevant column, for example, errorDeltapD,wr0sq“\n3,185.61anderrorDeltapD,wr1sq“2,412,073.90.\n9. All values in Table 7.3[331], and similar subsequent tables, are reported at a precision of two places of decimal.\nBecause of this, some error values and squared error values may appear inconsistent. This, however, is only due\nto rounding differences.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":385,"page_label":"331","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.3 Standard Approach: Multivariable Linear Regression with Gradient Descent 331\nTable 7.3\nDetails of the ﬁrst two iterations when the gradient descent algorithm is used to train a multivariable\nlinear regression model for the ofﬁce rentals dataset (using only the continuous descriptive features).\nInitial Weights\nwr0s: -0.146 wr1s: 0.185 wr2s: -0.044 wr3s: 0.119\nIteration 1\nRENTAL Squared errorDeltapD,wrjsq\nID P RICE Pred. Error Error wr0s wr1s wr2s wr3s\n1 320 93.26 226.74 51,411.08 226.74 113,370.05 906.96 1,813.92\n2 380 107.41 272.59 74,307.70 272.59 149,926.92 1,908.16 13,629.72\n3 400 115.15 284.85 81,138.96 284.85 176,606.39 2,563.64 1,993.94\n4 390 119.21 270.79 73,327.67 270.79 170,598.22 1,353.95 6,498.98\n5 385 134.64 250.36 62,682.22 250.36 166,492.17 2,002.91 25,036.42\n6 410 130.31 279.69 78,226.32 279.69 195,782.78 1,118.76 2,237.52\n7 480 142.89 337.11 113,639.88 337.11 259,570.96 3,371.05 2,359.74\n8 600 168.32 431.68 186,348.45 431.68 379,879.24 5,180.17 21,584.05\n9 570 170.63 399.37 159,499.37 399.37 367,423.83 5,591.23 3,194.99\n10 620 187.58 432.42 186,989.95 432.42 432,423.35 3,891.81 10,378.16\nSum 1,067,571.59 3,185.61 2,412,073.90 27,888.65 88,727.43\nSum of squared errors (Sum/2) 533,785.80\nNew Weights (after Iteration 1)\nwr0s: -0.146 wr1s: 0.233 wr2s: -0.043 wr3s: 0.121\nIteration 2\nRENTAL Squared errorDeltapD,wrjsq\nID P RICE Pred. Error Error wr0s wr1s wr2s wr3s\n1 320 117.40 202.60 41,047.92 202.60 101,301.44 810.41 1,620.82\n2 380 134.03 245.97 60,500.69 245.97 135,282.89 1,721.78 12,298.44\n3 400 145.08 254.92 64,985.12 254.92 158,051.51 2,294.30 1,784.45\n4 390 149.65 240.35 57,769.68 240.35 151,422.55 1,201.77 5,768.48\n5 385 166.90 218.10 47,568.31 218.10 145,037.57 1,744.81 21,810.16\n6 410 164.10 245.90 60,468.86 245.90 172,132.91 983.62 1,967.23\n7 480 180.06 299.94 89,964.69 299.94 230,954.68 2,999.41 2,099.59\n8 600 210.87 389.13 151,424.47 389.13 342,437.01 4,669.60 19,456.65\n9 570 215.03 354.97 126,003.34 354.97 326,571.94 4,969.57 2,839.76\n10 620 187.58 432.42 186,989.95 432.42 432,423.35 3,891.81 10,378.16\nSum 886,723.04 2,884.32 2,195,615.84 25,287.08 80,023.74\nSum of squared errors (Sum/2) 443,361.52\nNew Weights (after Iteration 2)\nwr0s: -0.145 wr1s: 0.277 wr2s: -0.043 wr3s: 0.123","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":386,"page_label":"332","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"332 Chapter 7 Error-Based Learning\nOnce the errorDeltapD,wrjsqfor a weight has been calculated, we can then update the\nweight using Equation (7.17)[327]. This weight update occurs on Line 4[326]of Algorithm\n4[326]. The update involves multiplying the errorDeltapD,wrjsqfor a given weight by the\nlearning rate and then adding this to the current weight to give a new, updated, weight. The\nnew set of weights is labeled New Weights (after Iteration 1) in Table 7.3[331].\nWe can see from Iteration 2 in the bottom half of Table 7.3[331]that the new set of predic-\ntions made using the updated set of weights calculated in iteration 1 result in a lower sum\nof squared errors, 443,361.52. Based on this error another new set of weights is calculated\nusing the error deltas shown. The algorithm then keeps iteratively applying the weight up-\ndate rule until it converges on a stable set of weights beyond which little improvement in\nmodel accuracy is possible. In our example, convergence occurred after 100iterations, and\nthe ﬁnal values for the weights were wr0s“´0.1513 ,wr1s“0.6270 ,wr2s“´0.1781 ,\nandwr3s“0.0714 . The sum of squared errors for the ﬁnal model was 2,913.5.10\nA last point to make about this example is that careful examination of Table 7.3[331]shows\nwhy such a low learning rate is used in this example. The large values of the R ENTAL\nPRICE feature,r320,620s, cause the squared errors and, in turn, the error delta values to\nbecome very large. This means that a very low learning rate is required in order to ensure\nthat the changes made to the weights at each iteration of the learning process are small\nenough for the algorithm to work effectively. Using normalization (see Section 3.6.1[87])\non the features can help avoid these large squared errors, and we do this in most examples\nfrom now on.\n7.4 Extensions and Variations\nIn this section we discuss common and useful extensions to the basic multivariable linear\nregression with gradient descent approach described in Section 7.3[319]. Topics covered in-\nclude interpreting a linear regression model, using weight decay to set the learning rate,\nhandling categorical descriptive and target features, using feature selection, using mul-\ntivariable linear regression models to model non-linear relationships, and using support\nvector machines (SVMs) as an alternative to linear regression models.\n7.4.1 Interpreting Multivariable Linear Regression Models","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":386,"page_label":"332","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"on the features can help avoid these large squared errors, and we do this in most examples\nfrom now on.\n7.4 Extensions and Variations\nIn this section we discuss common and useful extensions to the basic multivariable linear\nregression with gradient descent approach described in Section 7.3[319]. Topics covered in-\nclude interpreting a linear regression model, using weight decay to set the learning rate,\nhandling categorical descriptive and target features, using feature selection, using mul-\ntivariable linear regression models to model non-linear relationships, and using support\nvector machines (SVMs) as an alternative to linear regression models.\n7.4.1 Interpreting Multivariable Linear Regression Models\nA particularly useful feature of linear regression models is that the weights used by the\nmodel indicate the effect of each descriptive feature on the predictions returned by the\nmodel. First, the signs of the weights indicate whether different descriptive features have\na positive or a negative impact on the prediction. Table 7.4[333]repeats the ﬁnal weights for\n10. Because this is a higher-dimensional problem (three dimensions in the feature space and four dimensions in\nthe weight space), it is not possible to draw the same graphs of the error surfaces that were shown for the previous\nexamples.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":387,"page_label":"333","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 333\nTable 7.4\nWeights and standard errors for each feature in the ofﬁce rentals model.\nDescriptive Feature Weight Standard Error t-statistic p-value\nSIZE 0.6270 0.0545 11.504 ă0.0001\nFLOOR -0.1781 2.7042 -0.066 0.949\nBROADBAND RATE 0.071396 0.2969 0.240 0.816\nthe ofﬁce rentals model trained in Section 7.3.4[330]. We can see that increasing ofﬁce size\nleads to increasing rental prices; that lower building ﬂoors lead to higher rental prices; and\nthat rental prices increase with broadband rates. Second, the magnitudes of the weights\nshow how much the value of the target feature changes for a unit change in the value of\na particular descriptive feature. For example, for every increase of a square foot in ofﬁce\nsize, we can expect the rental price to go up by 0.6270 Euro per month. Similarly, for every\nﬂoor we go up in an ofﬁce building, we can expect the rental price to decrease by 0.1781\nEuro per month.\nIt is tempting to infer the relative importance of the different descriptive features in the\nmodel from the magnitude of the weights—that is, the descriptive features associated with\nhigher weights are more predictive than those with lower weights. This is a mistake, how-\never, when the descriptive features themselves have varying scales. For example, in the\nofﬁce rentals dataset, the values of the S IZEfeature range from 500to1,000, whereas the\nvalues for the F LOOR feature range from only 4to14. So, direct comparison of the weights\ntells us little about their relative importance. A better way to determine the importance of\neach descriptive feature in the model is to perform a statistical signiﬁcance test .\nA statistical signiﬁcance test works by stating a null hypothesis and then determining\nwhether there is enough evidence to accept or reject this hypothesis. This accept/reject\ndecision is carried out in three steps:\n1.Atest statistic is computed.\n2.The probability of a test-statistic value as big as or greater than the one computed\nbeing the result of chance is calculated. This probability is called a p-value .\n3.The p-value is compared to a predeﬁned signiﬁcance threshold, and if the p-value is\nless than or equal to the threshold (i.e., the p-value is small), the null hypothesis is\nrejected. These thresholds are typically the standard statistical thresholds of 5%or\n1%.\nThe statistical signiﬁcance test we use to analyze the importance of a descriptive feature","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":387,"page_label":"333","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"whether there is enough evidence to accept or reject this hypothesis. This accept/reject\ndecision is carried out in three steps:\n1.Atest statistic is computed.\n2.The probability of a test-statistic value as big as or greater than the one computed\nbeing the result of chance is calculated. This probability is called a p-value .\n3.The p-value is compared to a predeﬁned signiﬁcance threshold, and if the p-value is\nless than or equal to the threshold (i.e., the p-value is small), the null hypothesis is\nrejected. These thresholds are typically the standard statistical thresholds of 5%or\n1%.\nThe statistical signiﬁcance test we use to analyze the importance of a descriptive feature\ndrjsin a linear regression model is the t-test. The null hypothesis that we adopt for this\ntest is that the feature does not have a signiﬁcant impact on the model. The test statistic\nwe calculate is called the t-statistic. In order to calculate this test statistic, we ﬁrst have to\ncalculate the standard error for the overall model and the standard error for the descriptive\nfeature we are investigating the importance of. The standard error for the overall model is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":388,"page_label":"334","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"334 Chapter 7 Error-Based Learning\ncalculated as follows:\nse“gffffenÿ\ni“1pti´Mwpdiqq2\nn´2(7.18)\nwhere nis the number of instances in the training dataset. A standard error calculation is\nthen done for a descriptive feature as follows:\nsepdrjsq“sednÿ\ni“1´\ndirjs´drjs¯2(7.19)\nwhere drjsis some descriptive feature and drjsis the mean value of that descriptive feature\nin the training set.\nThet-statistic for this test is calculated as follows:\nt“wrjs\nsepdrjsq(7.20)\nwhere wrjsis the weight associated with descriptive feature drjs. Using a standard t-\nstatistic look-up table, we can then determine the p-value associated with this test (this is\na two tailed t-test with degrees of freedom set to the number of instances in the training\nset minus 2). If the p-value is less than the required signiﬁcance level, typically 0.05, we\nreject the null hypothesis and say that the descriptive feature has a signiﬁcant impact on\nthe model; otherwise we say that it does not. We can see from Table 7.4[333]that only the\nSIZEdescriptive feature has a signiﬁcant impact on the model. If a descriptive feature is\nfound to have a signiﬁcant impact on the model, this indicates that there is a signiﬁcant\nlinear relationship between it and the target feature.\n7.4.2 Setting the Learning Rate Using Weight Decay\nIn Section 7.3.3[328]we illustrated the impact of a learning rate parameter on the gradient\ndescent algorithm. In that section we also explained that most practitioners use rules of\nthumb and trial and error to set the learning rate. A more systematic approach is to use\nlearning rate decay , which allows the learning rate to start at a large value and then decay\nover time according to a predeﬁned schedule. Although there are different approaches in\nthe literature, a good approach is to use the following decay schedule:\nατ“α0c\nc`τ(7.21)\nwhereα0is an initial learning rate (this is typically quite large, e.g., 1.0),cis a constant\nthat controls how quickly the learning rate decays (the value of this parameter depends\non how quickly the algorithm converges, but it is often set to quite a large value, e.g.,\n100), andτis the current iteration of the gradient descent algorithm. Figure 7.8[335]shows","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":389,"page_label":"335","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 335\nthe journey across the error surface and related plot of the sums of squared errors for the\nofﬁce rentals problem—using just the S IZEdescriptive feature—when error decay is used\nwithα0“0.18andc“10(this is a pretty simple problem, so smaller values for these\nparameters are suitable). This example shows that the algorithm converges to the global\nminimum more quickly than any of the approaches shown in Figure 7.7[329].\nThe differences between Figures 7.7(f)[329]and 7.8(b)[335]most clearly show the impact of\nlearning rate decay as the initial learning rates are the same in these two instances. When\nlearning rate decay is used, there is much less thrashing back and forth across the error\nsurface than when the large static learning rate is used. Using learning rate decay can even\naddress the problem of inappropriately large error rates causing the sum of squared errors\nto increase rather than decrease. Figure 7.9[336]shows an example of this in which learning\nrate decay is used with α0“0.25andc“100. The algorithm starts at the position marked\n1on the error surface, and learning steps actually cause it to move farther and farther up the\nerror surface. This can be seen in the increasing sums of squared errors in Figure 7.9(b)[336].\nAs the learning rate decays, however, the direction of the journey across the error surface\nmoves back downward, and eventually the global minimum is reached. Although learning\n(a)\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF /uni25CF /uni25CF /uni25CF /uni25CF /uni25CF /uni25CF\n2 4 6 8 100 20 40 60 80\nTraining Iter ationSum of Squared Errors (b)\nFigure 7.8\n(a) The journey across the error surface for the ofﬁce rentals prediction problem when learning rate\ndecay is used ( α0“0.18,c“10); and (b) a plot of the changing sum of squared errors during this\njourney.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":390,"page_label":"336","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"336 Chapter 7 Error-Based Learning\nrate decay almost always leads to better performance than a ﬁxed learning rate, it still does\nrequire that problem-dependent values are chosen for α0andc.\n7.4.3 Handling Categorical Descriptive Features\nThe regression equation for a multivariable linear regression model for the full dataset\nshown in Table 7.1[313]would look like\nRENTAL PRICE“wr0s`wr1sˆSIZE`wr2sˆFLOOR\n`wr3sˆBROADBAND RATE\n`wr4sˆENERGY RATING\nThe multiplication of wr4sˆENERGY RATING causes a problem here. Energy rating is a\ncategorical feature, so multiplying the values of this feature by a numeric weight is simply\nnot sensible. The basic structure of the multivariable linear regression model allows for\nonly continuous descriptive features. Obviously, though, in real-world datasets, we often\nencounter categorical descriptive features, so for the linear regression approach to be really\nuseful, we need a way to handle these.\n\u0014\n\u0015\n\u0016\n\u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(a)\n/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n5 10 150 50 100 150 200 250\nTraining Iter ationSum of Squared Errors (b)\nFigure 7.9\n(a) The journey across the error surface for the ofﬁce rentals prediction problem when learning rate\ndecay is used ( α0“0.25,c“100); (b) a plot of the changing sum of squared errors during this\njourney.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":391,"page_label":"337","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 337\nTable 7.5\nThe ofﬁce rentals dataset from Table 7.1[313]adjusted to handle the categorical E NERGY RATING\ndescriptive feature in linear regression models.\nBROADBAND ENERGY ENERGY ENERGY RENTAL\nID S IZE FLOOR RATE RATING A R ATING B R ATING C P RICE\n1 500 4 8 0 0 1 320\n2 550 7 50 1 0 0 380\n3 620 9 7 1 0 0 400\n4 630 5 24 0 1 0 390\n5 665 8 100 0 0 1 385\n6 700 4 8 0 1 0 410\n7 770 10 7 0 1 0 480\n8 880 12 50 1 0 0 600\n9 920 14 8 0 0 1 570\n10 1,000 9 24 0 1 0 620\nThe most common approach to handling categorical features in linear regression models\nis to use a transformation that converts a single categorical descriptive feature into a num-\nber of continuous descriptive feature values that can encode the levels of the categorical\nfeature. This is done by creating one new binary descriptive feature for every level of the\ncategorical feature. These new features can then be used to encode a level of the original\ncategorical descriptive feature by setting the value of the new feature corresponding to the\nlevel of the categorical feature to 1and the other new continuous features to 0.\nFor example, if we were to use the E NERGY RATING descriptive feature from Table\n7.1[313]in a linear regression model, we would convert it into three new continuous descrip-\ntive features, as energy rating can have one of three distinct levels: A,B, orC. Table 7.5[337]\nshows this transformed dataset in which the energy rating feature has been replaced with\nENERGY RATING A, E NERGY RATING B, and E NERGY RATING C. For training instances\nin which the original E NERGY RATING feature had a value A, the new E NERGY RATING\nA feature has a value of 1, and the E NERGY RATING B and E NERGY RATING C are both\nset to 0. A similar rule is used for instances with the E NERGY RATING feature levels of B\nandC.\nReturning to our example, the regression equation for this R ENTAL PRICE model would\nchange to\nRENTAL PRICE“wr0s`wr1sˆSIZE`wr2sˆFLOOR\n`wr3sˆBROADBAND RATE\n`wr4sˆENERGY RATING A\n`wr5sˆENERGY RATING B\n`wr6sˆENERGY RATING C","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":392,"page_label":"338","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"338 Chapter 7 Error-Based Learning\nwhere the newly added categorical features allow the original E NERGY RATING feature to\nbe included. Everything else about using such a model is exactly the same as before.\nThe downside to this approach is that it introduces a number of extra weights for which\noptimal values must be found—in this simple example for only four descriptive features,\nwe need seven weights. This increases the size of the weight space through which we need\nto search when training the model. One way we can reduce the impact of this is that for\neach categorical feature we transform, we can reduce the number of newly added features\nby one by assuming that a zero in all the new features implies that the original feature had\nthe ﬁnal level. So, for example, for our E NERGY RATING feature, instead of adding three\nnew features (E NERGY RATING A, E NERGY RATING B, and E NERGY RATING C), we\ncould just add E NERGY RATING A and E NERGY RATING B and assume that whenever\nthey both have a value of 0, ENERGY RATING C is implicitly set.\n7.4.4 Handling Categorical Target Features: Logistic Regression\nIn Section 7.3[319]we described how a multivariable linear regression model trained using\ngradient descent can be used to make predictions for continuous target features. Although\nthis is useful for a range of real-world predictive analytics problems, we are also interested\nin prediction problems with categorical target features. This section covers the reasonably\nsimple adjustments that must be made to the multivariable linear regression with gradient\ndescent algorithm to handle categorical target features, in particular, logistic regression.\n7.4.4.1 Predicting categorical targets using linear regression Table 7.6[339]shows a\nsample dataset with a categorical target feature. This dataset contains measurements of\nthe revolutions per minute (RPM) that power station generators are running at, the amount\nof vibration in the generators (V IBRATION ), and an indicator to show whether the gener-\nators proved to be working or faulty the day after these measurements were taken. The\nRPM and V IBRATION measurements come from the day before the generators proved to\nbe operational or faulty. If power station administrators could predict upcoming generator\nfailures before the generators actually fail, they could improve power station safety and\nsave money on maintenance.11Using this dataset, we would like to train a model to distin-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":392,"page_label":"338","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sample dataset with a categorical target feature. This dataset contains measurements of\nthe revolutions per minute (RPM) that power station generators are running at, the amount\nof vibration in the generators (V IBRATION ), and an indicator to show whether the gener-\nators proved to be working or faulty the day after these measurements were taken. The\nRPM and V IBRATION measurements come from the day before the generators proved to\nbe operational or faulty. If power station administrators could predict upcoming generator\nfailures before the generators actually fail, they could improve power station safety and\nsave money on maintenance.11Using this dataset, we would like to train a model to distin-\nguish between properly operating power station generators and faulty generators using the\nRPM and V IBRATION measurements.\nFigure 7.10(a)[340]shows a scatter plot of this dataset in which we can see that there is a\ngood separation between the two types of generator. In fact, as shown in Figure 7.10(b)[340],\nwe can draw a straight line across the scatter plot that perfectly separates the good gen-\nerators from the faulty ones. This line is known as a decision boundary, and because we\ncan draw this line, this dataset is said to be linearly separable in terms of the two descrip-\n11. Gross et al. (2006) describes a real-world example of this kind of application of predictive analytics.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":393,"page_label":"339","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 339\nTable 7.6\nA dataset listing features for a number of generators.\nID RPM V IBRATION STATUS\n1 568 585 good\n2 586 565 good\n3 609 536 good\n4 616 492 good\n5 632 465 good\n6 652 528 good\n7 655 496 good\n8 660 471 good\n9 688 408 good\n10 696 399 good\n11 708 387 good\n12 701 434 good\n13 715 506 good\n14 732 485 good\n15 731 395 good\n16 749 398 good\n17 759 512 good\n18 773 431 good\n19 782 456 good\n20 797 476 good\n21 794 421 good\n22 824 452 good\n23 835 441 good\n24 862 372 good\n25 879 340 good\n26 892 370 good\n27 913 373 good\n28 933 330 goodID RPM V IBRATION STATUS\n29 562 309 faulty\n30 578 346 faulty\n31 593 357 faulty\n32 626 341 faulty\n33 635 252 faulty\n34 658 235 faulty\n35 663 299 faulty\n36 677 223 faulty\n37 685 303 faulty\n38 698 197 faulty\n39 699 311 faulty\n40 712 257 faulty\n41 722 193 faulty\n42 735 259 faulty\n43 738 314 faulty\n44 753 113 faulty\n45 767 286 faulty\n46 771 264 faulty\n47 780 137 faulty\n48 784 131 faulty\n49 798 132 faulty\n50 820 152 faulty\n51 834 157 faulty\n52 858 163 faulty\n53 888 91 faulty\n54 891 156 faulty\n55 911 79 faulty\n56 939 99 faulty\ntive features used. As the decision boundary is a linear separator, it can be deﬁned using\nthe equation of the line (remember Equation (7.2.1)[313]). In Figure 7.10(b)[340]the decision\nboundary is deﬁned as\nVIBRATION“830´0.667ˆRPM (7.22)\nor\n830´0.667ˆRPM´1ˆVIBRATION“0 (7.23)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":394,"page_label":"340","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"340 Chapter 7 Error-Based Learning\n600 700 800 900100 200 300 400 500 600\nRPMVibration\n(a)\n600 700 800 900100 200 300 400 500 600\nRPMVibration (b)\nFigure 7.10\n(a) A scatter plot of the RPM and V IBRATION descriptive features from the generators dataset shown\nin Table 7.6[339], where good generators are shown as crosses, and faulty generators are shown as\ntriangles; and (b) as decision boundary separating good generators (crosses) from faulty generators\n(triangles).\nSo, for any instance that is actually on the decision boundary, the RPM and V IBRA -\nTION values satisfy the equality in Equation (7.23)[339]. What is more interesting is that\ninstances not actually on the decision boundary behave in a very regular way. The descrip-\ntive feature values of all instances above the decision boundary will result in a negative\nvalue when plugged into the decision boundary equation, whereas the descriptive features\nof all instances below the decision boundary will result in a positive value. For example,\napplying Equation (7.23)[339]to the instance RPM “810, VIBRATION“495, which is\nabove the decision boundary in Figure 7.10(b)[340], gives the following result:\n830´0.667ˆ810´495“´205.27\nBy contrast, if we apply Equation (7.23)[339]to the instance RPM “650and V IBRATION“\n240, which is below the decision boundary in Figure 7.10(b)[340], we get\n830´0.667ˆ650´240“156.45\nFigure 7.11(a)[341]illustrates the consistent relationship between Equation (7.23)[339]and the\ndecision boundary by plotting the value of Equation (7.23)[339]for all values of RPM and\nVIBRATION .12\n12. Note that in this ﬁgure, both the RPM and V IBRATION features have been normalized to the range r´1,1s\n(using range normalization as described in Section 3.6.1[87]). It is standard practice to normalize descriptive\nfeatures whenever we are using regression models to predict a categorical target feature.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":395,"page_label":"341","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 341\n(a)\n (b)\nFigure 7.11\n(a) A surface showing the value of Equation (7.23)[339]for all values of RPM and V IBRATION , with\nthe decision boundary given in Equation (7.23)[339]highlighted; and (b) the same surface linearly\nthresholded at zero to operate as a predictor.\nBecause the values of this equation are so well behaved, we can use it to predict a cate-\ngorical target feature. Reverting to our previous notation, we have\nMwpdq“#\n1ifw¨dě0\n0otherwise(7.24)\nwhere dis a set of descriptive features for an instance; wis the set of weights in the\nmodel; and the good andfaulty generator target feature levels are represented as 0and1\nrespectively. Figure 7.11(b)[341]shows the value of Equation (7.24)[341]for every possible\nvalue of RPM and V IBRATION . This surface is known as a decision surface .\nOne problem that we need to solve in order to use the model deﬁned in Equation (7.24)[341]\nis how to determine the values for the weights, w, that will minimize the error function for\nour hypothesis Mwpdq. Unfortunately, in this case we cannot just use the gradient descent\nalgorithm. The hard decision boundary given in Equation (7.24)[341]isdiscontinuous , so\nis not differentiable, which means we cannot calculate the gradient of the error surface\nusing the derivative. Another problem with this model is that the model always makes\ncompletely conﬁdent predictions of 0or1. A model able to distinguish between instances\nthat are very close to the boundary and those that are farther away would be preferable.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":396,"page_label":"342","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"342 Chapter 7 Error-Based Learning\nWe can solve both these problems by using a more sophisticated threshold function that\nis continuous, and therefore differentiable, and that allows for the subtlety desired: the\nlogistic function .13\nThe logistic function14is given by\nlogisticpxq“1\n1`e´x(7.25)\nwhere xis a numeric value and eis Euler’s number and is approximately equal to 2.7183 .\nA plot of the logistic function for values of xin the ranger´10,10sis shown in Figure\n7.12(a)[343]. We can see that the logistic function is a threshold function that pushes values\nabove zero to 1and values below zero to 0. This is very similar to the hard threshold\nfunction given in Equation (7.24)[341], except that it has a soft boundary. The next section\nexplains how use of the logistic function allows us to build logistic regression models that\npredict categorical target features.\n7.4.4.2 Logistic regression To build a logistic regression model, we threshold the out-\nput of the basic linear regression model using the logistic function. So, instead of the\nregression function simply being the dot product of the weights and the descriptive fea-\ntures (as given in Equation (7.9)[320]), the dot product of weights and descriptive feature\nvalues is passed through the logistic function\nMwpdq“logisticpw¨dq\n“1\n1`e´w¨d(7.26)\nTo see the impact of this, we can build a multivariable logistic regression model for the\ndataset in Table 7.6[339]. After the training process (which uses a slightly modiﬁed version\nof the gradient descent algorithm, which we will explain shortly), the resulting logistic\nregression model is15\nMwp⟨RPM,VIBRATION ⟩q“1\n1`e´p´ 0.4077`4.1697ˆRPM`6.0460ˆVIBRATIONq(7.27)\n13. A hard threshold can be used fairly successfully to train prediction models for categorical targets using the\nperceptron learning rule , although we do not cover that in this book.\n14. The logistic function is a real workhorse of mathematical modeling and is used in a huge range of different\napplications. For example, the logistic function has been used to model how new words enter a language over\ntime, ﬁrst used very infrequently before moving through a tipping point to become widespread in a language.\n15. Note that in this example and in the examples that follow, a normalized version of the generators dataset is\nused (all descriptive features are normalized to the range r´1,1susing range normalization), so the weights in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":396,"page_label":"342","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13. A hard threshold can be used fairly successfully to train prediction models for categorical targets using the\nperceptron learning rule , although we do not cover that in this book.\n14. The logistic function is a real workhorse of mathematical modeling and is used in a huge range of different\napplications. For example, the logistic function has been used to model how new words enter a language over\ntime, ﬁrst used very infrequently before moving through a tipping point to become widespread in a language.\n15. Note that in this example and in the examples that follow, a normalized version of the generators dataset is\nused (all descriptive features are normalized to the range r´1,1susing range normalization), so the weights in\nEquation (7.27)[342]are different from those in Equation (7.23)[339]. If it were not for normalization, these two sets\nof weights would be the same.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":397,"page_label":"343","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 343\n−10 −5 0 5 100.00 0.25 0.50 0.75 1.00\nxlogistic(x)\n(a)\n (b)\nFigure 7.12\n(a) A plot of the logistic function (Equation (7.25)[342]) for the range of values r´10,10s; and (b) the\nlogistic decision surface that results from training a model to represent the generators dataset given\nin Table 7.6[339](note that the data has been normalized to the range r´1,1s).\nThe decision surface resulting from Equation (7.27)[342]is shown in Figure 7.12(b)[343]. The\nimportant thing to notice about this decision surface, in contrast to the decision surface in\nFigure 7.11(b)[341], is that there is a gentle transition from predictions of the faulty target\nlevel to predictions of the good generator target level. This is one of the key beneﬁts of\nusing logistic regression. Another beneﬁt of using the logistic function is that logistic\nregression model outputs can be interpreted as probabilities of the occurrence of a target\nlevel. So\nPpt“faulty|dq“Mwpdq\nand\nPpt“good|dq“1´Mwpdq\nTo ﬁnd the optimal decision boundary for a logistic regression problem, we use the gradi-\nent descent algorithm (Algorithm 4[326]) to minimize the sum of squared errors based on the\ntraining dataset. Figure 7.13[344]shows a series of the candidate models that were explored\non the way to ﬁnding this boundary. The ﬁnal panel in Figure 7.13[344]shows how the sum\nof squared errors changed during the training process.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":398,"page_label":"344","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"344 Chapter 7 Error-Based Learning\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n0 100 200 300 400 500 6005 10 15 20\nTraining Iter ationSum of Squared Errors\nFigure 7.13\nA selection of the logistic regression models developed during the gradient descent process for the\nmachinery dataset from Table 7.6[339]. The bottom-right panel shows the sums of squared errors\ngenerated during the gradient descent process.\nTo repurpose the gradient descent algorithm for training logistic regression models, the\nonly change that needs to be made is in the error delta function, which is used in the weight\nupdate rule given on Line 4[326]of Algorithm 4[326]. To derive this new weight update rule,\nimagine that there is just a single training instance, pd,tq, in our training dataset. The\npartial derivative of the error function, L2, is then\nB\nBwrjsL2pMw,Dq “B\nBwrjs1\n2pt´Mwpdqq2","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":399,"page_label":"345","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 345\nwhere wrjsis a single weight from the set of weights w. Applying the chain rule to this,\nwe get\nB\nBwrjsL2pMw,Dq “ p t´MwpdqqˆB\nBwrjspt´Mwpdqq\nButMwpdq“logisticpw¨dq, so\nB\nBwrjsL2pMw,Dq “ p t´logisticpw¨dqqˆB\nBwrjspt´logisticpw¨dqq\nApplying the chain rule again to the partial derivative part of this equation, and remember-\ning thatB\nBwrjsw¨d“drjs, we get\nB\nBwrjsL2pMw,Dq “ p t´logisticpw¨dqqˆB\nBwrjslogisticpw¨dq\nˆB\nBwrjsw¨d\n“ p t´MwpdqqˆB\nBwrjslogisticpw¨dqˆdrjs\nFortunately, the derivative of the logistic function is well known:\nd\ndxlogisticpxq“logisticpxqp1´logisticpxqq (7.28)\nSo\nB\nBwrjsL2pMw,Dq“p t´logisticpw¨dqq\nˆlogisticpw¨dqp1´logisticpw¨dqqˆdrjs (7.29)\nRewriting logisticpw¨dqasMwpdqfor readability, we get\nB\nBwrjsL2pMw,Dq “ p t´Mwpdqq (7.30)\nˆMwpdqˆp 1´Mwpdqqˆdrjs (7.31)\nThis is the partial derivative of the error surface with respect to a particular weight wrjs\nand indicates the gradient of the error surface. Using this formulation for the gradient, we\ncan write the weight update rule for logistic regression as\nwrjsÐwrjs`αˆpt´MwpdqqˆMwpdqˆp 1´Mwpdqqˆdrjs (7.32)\nwhereMwpdq“logisticpw¨dq“1\n1`e´w¨d.\nThe rule given in Equation (7.32)[345]assumes that only a single training instance exists.\nTo modify this to take into account a full training dataset, we simply need to sum across\nall the training instances as we did before in Equation (7.17)[327]. This gives us the weight","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":400,"page_label":"346","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"346 Chapter 7 Error-Based Learning\nupdate rule for multivariable logistic regression:\nwrjsÐwrjs`αˆnÿ\ni“1ppti´MwpdiqqˆMwpdiqˆp 1´Mwpdiqqˆdirjsq (7.33)\nOther than changing the weight update rule, we don’t need to make any other changes\nto the model training process presented for multivariable linear regression models. To\nfurther illustrate this process, the next section presents a worked example of training a\nmultivariable logistic regression model for an extended version of the generators dataset.\n7.4.4.3 A worked example of multivariable logistic regression One of the advantages\nof using a logistic regression model is that it works well for datasets in which the instances\nwith target features set to different levels overlap with each other in the feature space. Table\n7.7[347]shows an extended version of the generators dataset given in Table 7.6[339], including\nextra instances that make the separation between good generators and faulty generators\nless clear cut. This kind of data is very common in real-world scenarios. A scatter plot of\nthis dataset is shown in Figure 7.14[349], in which the overlap between the different types\nof generator in this dataset is clearly visible. Even though the separation between the\ninstances with the different levels of the target feature in this case is not particularly well\ndeﬁned, a logistic regression model can be trained to distinguish between the two types of\ngenerator. In the remainder of this section, we examine this in some detail.\nThere is an ongoing argument regarding whether descriptive features should be normal-\nized before being used in linear regression models. The main disadvantage of normaliza-\ntion is that the interpretative analysis discussed in Section 7.4.4[338]becomes more difﬁcult\nas the descriptive feature values used in the model do not relate to the actual feature values\nin the data. For example, if the age of a customer was used as a descriptive feature in a\nﬁnancial credit scoring model, it is more difﬁcult to talk about changes in normalized age\non a scale from 0to1than it is to discuss original age values on their natural scale, about 18\nto80. The main advantages of normalizing descriptive feature values are that all weights\nbecome directly comparable with each other (as all descriptive features are on the same\nscale), and the behavior of the gradient descent algorithm used to train the model becomes\nmuch less sensitive to the learning rate and the initial weights. Although it is less important","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":400,"page_label":"346","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"as the descriptive feature values used in the model do not relate to the actual feature values\nin the data. For example, if the age of a customer was used as a descriptive feature in a\nﬁnancial credit scoring model, it is more difﬁcult to talk about changes in normalized age\non a scale from 0to1than it is to discuss original age values on their natural scale, about 18\nto80. The main advantages of normalizing descriptive feature values are that all weights\nbecome directly comparable with each other (as all descriptive features are on the same\nscale), and the behavior of the gradient descent algorithm used to train the model becomes\nmuch less sensitive to the learning rate and the initial weights. Although it is less important\nfor simple linear regression models, for logistic regression models we recommend that de-\nscriptive feature values always be normalized. In this example, before the training process\nbegins, both descriptive features are normalized to the range r´1,1s.\nTo begin the gradient descent process, random starting values for the weights within\nthe model, wr0s,wr1s,wr2s, are selected. In this example, random values were selected\nfrom the range r´3,3sto give wr0s“´2.9465 ,wr1s“´1.0147 , and wr2s“2.1610 .\nUsing these weights, a prediction is made for every instance in the training dataset, and the\nresulting sum of squared errors is calculated. The predictions made using these weights\nand the related error are shown in Table 7.8[348]under Iteration 1 .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":401,"page_label":"347","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 347\nTable 7.7\nAn extended version of the generators dataset from Table 7.6[339].\nID RPM V IBRATION STATUS\n1 498 604 faulty\n2 517 594 faulty\n3 541 574 faulty\n4 555 587 faulty\n5 572 537 faulty\n6 600 553 faulty\n7 621 482 faulty\n8 632 539 faulty\n9 656 476 faulty\n10 653 554 faulty\n11 679 516 faulty\n12 688 524 faulty\n13 684 450 faulty\n14 699 512 faulty\n15 703 505 faulty\n16 717 377 faulty\n17 740 377 faulty\n18 749 501 faulty\n19 756 492 faulty\n20 752 381 faulty\n21 762 508 faulty\n22 781 474 faulty\n23 781 480 faulty\n24 804 460 faulty\n25 828 346 faulty\n26 830 366 faulty\n27 864 344 faulty\n28 882 403 faulty\n29 891 338 faulty\n30 921 362 faulty\n31 941 301 faulty\n32 965 336 faulty\n33 976 297 faulty\n34 994 287 faultyID RPM V IBRATION STATUS\n35 501 463 good\n36 526 443 good\n37 536 412 good\n38 564 394 good\n39 584 398 good\n40 602 398 good\n41 610 428 good\n42 638 389 good\n43 652 394 good\n44 659 336 good\n45 662 364 good\n46 672 308 good\n47 691 248 good\n48 694 401 good\n49 718 313 good\n50 720 410 good\n51 723 389 good\n52 744 227 good\n53 741 397 good\n54 770 200 good\n55 764 370 good\n56 790 248 good\n57 786 344 good\n58 792 290 good\n59 818 268 good\n60 845 232 good\n61 867 195 good\n62 878 168 good\n63 895 218 good\n64 916 221 good\n65 950 156 good\n66 956 174 good\n67 973 134 good\n68 1002 121 good","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":402,"page_label":"348","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"348 Chapter 7 Error-Based Learning\nTable 7.8\nDetails of the ﬁrst two iterations when the gradient descent algorithm is used to train a logistic\nregression model for the extended generators dataset given in Table 7.7[347].\nInitial Weights\nwr0s: -2.9465 wr1s: -1.0147 wr2s: -2.1610\nIteration 1\nTARGET Squared errorDeltapD,wrjsq\nID L EVEL Pred. Error Error wr0s wr1s wr2s\n1 1 0.5570 0.4430 0.1963 0.1093 -0.1093 0.1093\n2 1 0.5168 0.4832 0.2335 0.1207 -0.1116 0.1159\n3 1 0.4469 0.5531 0.3059 0.1367 -0.1134 0.1197\n¨¨¨\n66 0 0.0042 -0.0042 0.0000 0.0000 0.0000 0.0000\n67 0 0.0028 -0.0028 0.0000 0.0000 0.0000 0.0000\n68 0 0.0022 -0.0022 0.0000 0.0000 0.0000 0.0000\nSum 24.4738 2.7031 -0.7015 1.6493\nSum of squared errors (Sum/2) 12.2369\nNew Weights (after Iteration 1)\nwr0s: -2.8924 wr1s: -1.0287 wr2s: -2.1940\nIteration 2\nTARGET Squared errorDeltapD,wrjsq\nID L EVEL Pred. Error Error wr0s wr1s wr2s\n1 1 0.5817 0.4183 0.1749 0.1018 -0.1018 0.1018\n2 1 0.5414 0.4586 0.2103 0.1139 -0.1053 0.1094\n3 1 0.4704 0.5296 0.2805 0.1319 -0.1094 0.1155\n¨¨¨\n66 0 0.0043 -0.0043 0.0000 0.0000 0.0000 0.0000\n67 0 0.0028 -0.0028 0.0000 0.0000 0.0000 0.0000\n68 0 0.0022 -0.0022 0.0000 0.0000 0.0000 0.0000\nSum 24.0524 2.7236 -0.6646 1.6484\nSum of squared errors (Sum/2) 12.0262\nNew Weights (after Iteration 2)\nwr0s: -2.8380 wr1s: -1.0416 wr2s: -2.2271","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":403,"page_label":"349","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 349\n500 600 700 800 900 1000200 300 400 500 600\nRPMVibration\nFigure 7.14\nA scatter plot of the extended generators dataset given in Table 7.7[347], which results in instances\nwith the different target levels overlapping each other. Instances representing good generators are\nshown as crosses, and those representing faulty generators as triangles.\nThis ﬁrst candidate model is not particularly accurate with an initial sum of squared\nerrors of 12.2369 . In fact, instances 1and2are the only instances at this stage that\nare given predictions of the faulty target level, level 1(note that their prediction val-\nues are the only ones greater than 0.5). This can also be seen in the top left-hand im-\nage of Figure 7.15[350],which shows the candidate model corresponding to this initial set\nof weights. Based on the errors in these predictions, the delta contributions, labeled\naserrorDeltapD,wr0sq,errorDeltapD,wr1sqanderrorDeltapD,wr2sqin Table 7.8[348],\nfrom each training instance are calculated according to Equation (7.31)[345]. These individ-\nual delta contributions are then summed so that the weight update rule (Equation (7.33)[346])\ncan be applied, in this example using a learning rate of 0.02. So, for example, the new\nvalue of wr0sis calculated as the old value plus the learning rate times the sum of the\nerrorDeltapD,wr0sqcontributions to give ´2.9465`0.02ˆ2.7031“ ´ 2.8924 . This\ngives the new set of weights shown as New Weights (after Iteration 1) .\nThe process then starts again using these new weights as the basis for the predictions\nand errors marked as Iteration 2 in Table 7.8[348]. The new weights result in slightly more\naccurate predictions, evident from the slightly reduced sum of squared errors of 12.0262 .\nBased on the updated errors, a new set of weights is calculated, marked in Table 7.8[348]as\nNew Weights (after Iteration 2) . Table 7.8[348]shows just the ﬁrst two iterations of the\ngradient descent process for this model. The continuing process that ﬁnds the ﬁnal model","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":404,"page_label":"350","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"350 Chapter 7 Error-Based Learning\nis illustrated in Figure 7.15[350], which shows a selection of the candidate models generated\non the way to generating the ﬁnal model, and the bottom-right panel shows how the sum\nof squared errors changed during the process. The ﬁnal model trained is\nMwp⟨RPM,VIBRATION ⟩q“1\n1`e´p´ 0.4077`4.1697ˆRPM`6.0460ˆVIBRATIONq\nwhich has a sum of squared errors of 1.8804 . Obviously, because there are instances with\ndifferent levels for the target feature overlapping in the feature space, it is not possible in\nthis case to build a model that perfectly separates the good andfaulty machines. The model\ntrained, however, strikes a good balance between mistaking good machines for faulty ones\nand vice versa.\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nRPMVibration\n0 200 400 600 8005 10 15 20 25\nTraining Iter ationSum of Squared Errors\nFigure 7.15\nA selection of the logistic regression models developed during the gradient descent process for the\nextended generators dataset in Table 7.7[347]. The bottom-right panel shows the sums of squared errors\ngenerated during the gradient descent process.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":405,"page_label":"351","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 351\nTable 7.9\nA dataset describing grass growth on Irish farms in July 2012.\nID R AIN GROWTH\n1 2.153 14.016\n2 3.933 10.834\n3 1.699 13.026\n4 1.164 11.019\n5 4.793 4.162\n6 2.690 14.167\n7 3.982 10.190\n8 3.333 13.525\n9 1.942 13.899\n10 2.876 13.949\n11 4.277 8.643ID R AIN GROWTH\n12 3.754 11.420\n13 2.809 13.847\n14 1.809 13.757\n15 4.114 9.101\n16 2.834 13.923\n17 3.872 10.795\n18 2.174 14.307\n19 4.353 8.059\n20 3.684 12.041\n21 2.140 14.641\n22 2.783 14.138ID R AIN GROWTH\n23 3.960 10.307\n24 3.592 12.069\n25 3.451 12.335\n26 1.197 10.806\n27 0.723 7.822\n28 1.958 14.010\n29 2.366 14.088\n30 1.530 12.701\n31 0.847 9.012\n32 3.843 10.885\n33 0.976 9.876\n7.4.5 Modeling Non-Linear Relationships\nAll the simple linear regression and logistic regression models that we have looked at so far\nmodel a linear relationship between descriptive features and a target feature. Linear models\nwork very well when the underlying relationships in the data are linear. Sometimes, how-\never, the underlying data will exhibit non-linear relationships that we would like to capture\nin a model. For example, the dataset in Table 7.9[351]is based on an agricultural scenario\nand shows rainfall (in mm per day), R AIN, and resulting grass growth (in kilograms per\nacre per day), G ROWTH , measured on a number of Irish farms during July 2012. A scatter\nplot of these two features is shown in Figure 7.16(a)[352], from which the strong non-linear\nrelationship between rainfall and grass growth is clearly apparent—grass does not grow\nwell when there is very little rain or too much rain, but hits a sweet spot at rainfall of about\n2.5mm per day. It would be useful for farmers to be able to predict grass growth for differ-\nent amounts of forecasted rainfall so that they could plan the optimal times to harvest their\ngrass for making hay.\nA simple linear regression model cannot handle this non-linear relationship. Figure\n7.16(b)[352]shows the best simple linear regression model that can be trained for this pre-\ndiction problem. This model is\nGROWTH“13.510`´0.667ˆRAIN\nTo successfully model the relationship between grass growth and rainfall, we need to\nintroduce non-linear elements. A generalized way in which to do this is to introduce basis\nfunctions that transform the raw inputs to the model into non-linear representations but still\nkeep the model itself linear in terms of the weights. The advantage of this is that, except for","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":405,"page_label":"351","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ent amounts of forecasted rainfall so that they could plan the optimal times to harvest their\ngrass for making hay.\nA simple linear regression model cannot handle this non-linear relationship. Figure\n7.16(b)[352]shows the best simple linear regression model that can be trained for this pre-\ndiction problem. This model is\nGROWTH“13.510`´0.667ˆRAIN\nTo successfully model the relationship between grass growth and rainfall, we need to\nintroduce non-linear elements. A generalized way in which to do this is to introduce basis\nfunctions that transform the raw inputs to the model into non-linear representations but still\nkeep the model itself linear in terms of the weights. The advantage of this is that, except for\nintroducing the mechanism of basis functions, we do not need to make any other changes\nto the approach we have presented so far. Furthermore, basis functions work for both","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":406,"page_label":"352","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"352 Chapter 7 Error-Based Learning\n0 1 2 3 4 50 5 10 15\nRainGrowth\n(a)\n0 1 2 3 4 50 5 10 15\nRainGrowth (b)\nFigure 7.16\n(a) A scatter plot of the R AIN and G ROWTH feature from the grass growth dataset; and (b) the\nsame plot with a simple linear regression model trained to capture the relationship between the grass\ngrowth and rainfall.\nsimple multivariable linear regression models that predict a continuous target feature and\nmultivariable logistic regression models that predict a categorical target feature.\nTo use basis functions, we recast the simple linear regression model (see Equation (7.9)[320])\nas follows:\nMwpdq “bÿ\nk“0wrksˆφkpdq (7.34)\nwhere dis a set of mdescriptive features, wis a set of bweights, and φ0toφbare a series\nofbbasis functions that each transform the input vector din a different way. It is worth\nnoting that there is no reason that bmust equal m, and usually bis quite a bit larger than\nm—that is, there are usually more basis functions than there are descriptive features.\nOne of the most common uses of basis functions in linear regression is to train models\nto capture polynomial relationships. A linear relationship implies that the target is calcu-\nlated from the descriptive features using only the addition of the descriptive feature values\nmultiplied by weight values. Polynomial relationships allow multiplication of descriptive\nfeature values by each other and raising of descriptive features to exponents. The most\ncommon form of polynomial relationship is the second order polynomial , also known as\nthequadratic function , which takes the general form a“bx`cx2. The relationship be-\ntween rainfall and grass growth in the grass growth dataset can be accurately represented","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":407,"page_label":"353","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 353\nas a second order polynomial through the following model:\nGROWTH“wr0sˆφ0pRAINq`wr1sˆφ1pRAINq`wr2sˆφ2pRAINq\nwhere\nφ0pRAINq “ 1\nφ1pRAINq “ RAIN\nφ2pRAINq “ RAIN2\nWhat makes this approach really attractive is that, although this new model stated in\nterms of basis functions captures the non-linear relationship between rainfall and grass\ngrowth, the model is still linear in terms of the weights and so can be trained using gradient\ndescent without making any changes to the algorithm. Figure 7.17[354]shows the ﬁnal non-\nlinear model that results from this training process, along with a number of the interim\nsteps on the way to this model. The ﬁnal model is\nGROWTH“3.707ˆφ0pRAINq`8.475ˆφ1pRAINq`´ 1.717ˆφ2pRAINq\nwhereφ0,φ1, andφ2are as described before. This model captures the non-linear rela-\ntionship in the data very well but was still easy to train using a gradient descent approach.\nBasis functions can also be used for multivariable simple linear regression models in the\nsame way, the only extra requirement being the deﬁnition of more basis functions.\nBasis functions can also be used to train logistic regression models for categorical pre-\ndiction problems that involve non-linear relationships. Table 7.10[355]shows a dataset, the\nEEG dataset, based on a neurological experiment designed to capture how neural responses\nchange when experiment participants view positive images (e.g., a picture of a smiling\nbaby) and negative images (e.g., a picture of rotting food). In an experiment performed\nto capture this data, participants were shown a series of different images, and their neural\nresponses were measured using electroencephalography (EEG ). In particular, the values\nof the commonly used P20 and P45 potentials were measured while a participant viewed\neach image. These are the descriptive features in this dataset, and the target feature, T YPE,\nindicates whether the subject was viewing a positive or a negative image. If a model could\nbe trained to classify brain activity as being associated with positive images or negative\nimages, doctors could use this model to help in assessing the brain function of people who\nhave suffered severe brain injuries and are non-communicative.16Figure 7.18[355]shows a\nscatter plot of this dataset, from which it is clear that the decision boundary between the\n16. This example is very much simpliﬁed for illustration purposes, but very interesting work is done on building","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":407,"page_label":"353","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of the commonly used P20 and P45 potentials were measured while a participant viewed\neach image. These are the descriptive features in this dataset, and the target feature, T YPE,\nindicates whether the subject was viewing a positive or a negative image. If a model could\nbe trained to classify brain activity as being associated with positive images or negative\nimages, doctors could use this model to help in assessing the brain function of people who\nhave suffered severe brain injuries and are non-communicative.16Figure 7.18[355]shows a\nscatter plot of this dataset, from which it is clear that the decision boundary between the\n16. This example is very much simpliﬁed for illustration purposes, but very interesting work is done on building\nprediction models from the output of EEG and fMRI scans—for example, Mitchell et al. (2008).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":408,"page_label":"354","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"354 Chapter 7 Error-Based Learning\n1 2 3 4 50 5 10 15\nRainGrowth\n1 2 3 4 50 5 10 15\nRainGrowth\n1 2 3 4 50 5 10 15\nRainGrowth\n1 2 3 4 50 5 10 15\nRainGrowth\n1 2 3 4 50 5 10 15\nRainGrowth\n1 2 3 4 50 5 10 15\nRainGrowth\nFigure 7.17\nA selection of the models developed during the gradient descent process for the grass growth dataset\nfrom Table 7.9[351].\ntwo different types of images is not linear—that is, the two types of images are not linearly\nseparable .\nThe non-linear decision boundary that is just about perceivable in Figure 7.18[355]can\nbe represented using a third-order polynomial in the two descriptive features, P20 and\nP45. The simple regression model we trained previously cannot cope with a non-linear\ndecision boundary like the one seen in Figure 7.18[355]. We can, however, rewrite the logistic\nregression equation from Equation (7.26)[342]to use basis functions as follows:\nMwpdq “1\n1`e´¨\n˚˝bÿ\nj“0wrjsφjpdq˛\n‹‚(7.35)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":409,"page_label":"355","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 355\nTable 7.10\nA dataset showing participants’ responses to viewing positive andnegative images measured on the\nEEG P20 and P45 potentials.\nID P20 P45 T YPE\n1 0.4497 0.4499 negative\n2 0.8964 0.9006 negative\n3 0.6952 0.3760 negative\n4 0.1769 0.7050 negative\n5 0.6904 0.4505 negative\n6 0.7794 0.9190 negative\n...ID P20 P45 T YPE\n26 0.0656 0.2244 positive\n27 0.6336 0.2312 positive\n28 0.4453 0.4052 positive\n29 0.9998 0.8493 positive\n30 0.9027 0.6080 positive\n31 0.3319 0.1473 positive\n...\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nP20P45\nFigure 7.18\nA scatter plot of the P20 and P45 features from the EEG dataset. Instances representing positive\nimages are shown as crosses, and those representing negative images as triangles.\nUsing this representation with the following set of basis functions will give the learning\nprocess the ﬂexibility to ﬁnd the non-linear decision boundary required to successfully\nseparate the different types of images in the EEG dataset:17\n17. The term arising from φ7is commonly referred to as an interaction term because it allows two descriptive\nfeatures to interact in the model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":410,"page_label":"356","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"356 Chapter 7 Error-Based Learning\nφ0p⟨P20,P45⟩q“1φ4p⟨P20,P45⟩q“P452\nφ1p⟨P20,P45⟩q“P20φ5p⟨P20,P45⟩q“P203\nφ2p⟨P20,P45⟩q“P45φ6p⟨P20,P45⟩q“P453\nφ3p⟨P20,P45⟩q“P202φ7p⟨P20,P45⟩q“P20ˆP45\nThis model can be trained using gradient descent to ﬁnd the optimal decision boundary\nbetween the two different types of images. Figure 7.19[356]shows a series of the models built\nduring the gradient descent process. The ﬁnal model can accurately distinguish between\nthe two different types of image based on the measured P20 and P45 activity. Figure\n7.19(f)[356]shows a 3D plot of the ﬁnal decision surface. Note that although this decision\nsurface is more complex than the ones we have seen before (e.g., Figure 7.12[343]), the\nlogistic shape is still maintained.\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nP20P45\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nP20P45\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nP20P45\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nP20P45\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nP20P45\nFigure 7.19\nA selection of the models developed during the gradient descent process for the EEG dataset from\nTable 7.10[355]. The ﬁnal panel shows the decision surface generated.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":411,"page_label":"357","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 357\nUsing basis functions is a simple and effective way in which to capture non-linear rela-\ntionships within a linear regression model. One way to think about this process is that we\nchange the dataset from two dimensions to a higher-dimensional space. There is no limit\nto the kinds of functions that can be used as basis functions, and as we have seen in the\nprevious example, the basis functions for different descriptive features in a dataset can be\nquite different. One disadvantage of using basis functions, however, is that the analyst has\nto design the basis function set that will be used. Although there are some well-known\nsets of functions—for example, different order polynomial functions—this can be a con-\nsiderable challenge. Second, as the number of basis functions grows beyond the number\nof descriptive features, the complexity of our models increases, so the gradient descent\nprocess must search through a more complex weight space. Using basis functions is an in-\nteresting way to change the inductive bias, in particular the restriction bias, encoded in the\ngradient descent algorithm for learning regression models. By using basis functions such\nas those given in the examples in this section, we relax the restriction on the algorithm\nto consider only linear models and instead allow more complex model types such as the\nhigher-order polynomial models seen in these examples.\n7.4.6 Multinomial Logistic Regression\nThemultinomial logistic regression18model is an extension that handles categorical tar-\nget features with more than two levels. A good way to build multinomial logistic regression\nmodels is to use a set of one-versus-all models.19If we have rtarget levels, we create r\none-versus-all logistic regression models. A one-versus-all model distinguishes between\none level of the target feature and all the others. Figure 7.20[358]shows three one-versus-all\nprediction models for a prediction problem with three target levels (these models are based\non the dataset in Table 7.11[359]that is introduced subsequently in this section).\nForrtarget feature levels, we build rseparate logistic regression models Mw1toMwr:\nMw1pdq“logisticpw1¨dq\nMw2pdq“logisticpw2¨dq\n...\nMwrpdq“logisticpwr¨dq(7.36)\nwhereMw1toMwrarerdifferent one-versus-all logistic regression models, and w1to\nwrarerdifferent sets of weights. To combine the outputs of these different models, we","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":411,"page_label":"357","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"one-versus-all logistic regression models. A one-versus-all model distinguishes between\none level of the target feature and all the others. Figure 7.20[358]shows three one-versus-all\nprediction models for a prediction problem with three target levels (these models are based\non the dataset in Table 7.11[359]that is introduced subsequently in this section).\nForrtarget feature levels, we build rseparate logistic regression models Mw1toMwr:\nMw1pdq“logisticpw1¨dq\nMw2pdq“logisticpw2¨dq\n...\nMwrpdq“logisticpwr¨dq(7.36)\nwhereMw1toMwrarerdifferent one-versus-all logistic regression models, and w1to\nwrarerdifferent sets of weights. To combine the outputs of these different models, we\n18. Multinomial logistic regression models are often known as maximum entropy ,conditional maximum en-\ntropy , orMaxEnt models.\n19. This is an example of an ensemble model like those described in Section 4.4.5[158].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":412,"page_label":"358","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"358 Chapter 7 Error-Based Learning\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n(a)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq (b)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq (c)\nFigure 7.20\nAn illustration of three different one-versus-all prediction models for the customer type dataset in\nTable 7.11[359], with three target levels: (a) single (squares), (b) business (triangles), and (c) family\n(crosses).\nnormalize their results as follows:\nM1\nwkpdq“Mwkpdqÿ\nlPlevelsptqMwlpdq (7.37)\nwhereM1\nwkpdqis a revised, normalized prediction for the one-versus-all model for the target\nlevel k. The denominator in this equation sums the predictions of each of the one-versus-all\nmodels for the rlevels of the target feature and acts as a normalization term. This ensures\nthat the output of all models sums to 1. The rone-versus-all logistic regression models\nused are trained in parallel, and the revised model outputs, M1\nwkpdq, are used in calculating\nthe sum of squared errors for each model during the training process. This means that the\nsum of squared errors function is changed slightly to\nL2pMwk,Dq“1\n2nÿ\ni“1`\nti´M1\nwkpdir1sq˘2(7.38)\nThe revised predictions are also used in making predictions for query instances. The\npredicted level for a query, q, is the level associated with the one-versus-all model that\noutputs the highest result after normalization. We can write this\nMpqq“arg max\nlPlevelsptqM1\nwlpqq (7.39)\nTable 7.11[359]shows a sample from a dataset of mobile customers that includes details\nof customers’ shopping habits with a large national retail chain. Each customer’s average\nweekly spending with the chain, S PEND , and average number of visits per week to the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":413,"page_label":"359","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 359\nTable 7.11\nA dataset of customers of a large national retail chain.\nID S PEND FREQ TYPE\n1 21.6 5.4 single\n2 25.7 7.1 single\n3 18.9 5.6 single\n4 25.7 6.8 single\n...\n26 107.9 5.8 business\n27 92.9 5.5 business\n...ID S PEND FREQ TYPE\n28 122.6 6.0 business\n29 107.7 5.7 business\n...\n47 53.2 2.6 family\n48 52.4 2.0 family\n49 46.1 1.4 family\n50 65.3 2.2 family\n...\nchain, F REQ, are included along with the T YPE of customer: single ,business , orfamily .\nAn extended version of this dataset was used to build a model that can determine the type\nof a customer based on a few weeks of shopping behavior data. Figure 7.21[360]shows the\ntraining sequence for a multinomial logistic regression model trained using this data (after\nthe data had been range normalized to r´1,1s). There are three target levels, so three\none-versus-all models are built. The evolution of the decision boundary for each model is\nshown.\nThe ﬁnal one-versus-all decision boundaries shown in the bottom-middle panel of Figure\n7.21[360]do not look like the individual one-versus-all decision boundaries shown in Figure\n7.20[358]. The reason for this is that the boundaries shown in Figure 7.20[358]were trained in\nisolation, whereas the boundaries shown in Figure 7.21[360]were trained in parallel and so\nare interconnected. Although it might look like the decision boundary for the single target\nlevel shown by the solid line does not discriminate between the instances with the single\ntarget level and those with the other target levels, when used in conjunction with the other\ntwo decision boundaries, it does. We can see this in the decision boundaries shown in the\nbottom-right panel of Figure 7.21[360]. We use an example to illustrate how a prediction is\nmade using a multinomial regression model.\nThe parameters of the models learned for the three ﬁnal decision boundaries in Figure\n7.21[360]are\nMwsinglepqq“logisticp0.7993´15.9030ˆSPEND`9.5974ˆFREQq\nMwfamilypqq“logisticp3.6526`´0.5809ˆSPEND´17.5886ˆFREQq\nMwbusinesspqq“logisticp4.6419`14.9401ˆSPEND´6.9457ˆFREQq","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":414,"page_label":"360","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"360 Chapter 7 Error-Based Learning\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreq\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nSpendFreqsingl e busines s\nfamil y\nFigure 7.21\nA selection of the models developed during the gradient descent process for the customer group\ndataset from Table 7.11[359]. Squares represent instances with the single target level, triangles the\nbusiness level, and crosses the family level. The bottom-right panel illustrates the overall decision\nboundaries between the three target levels.\nFor a query instance with S PEND“25.67and F REQ“6.12, which are normalized to\nSPEND“´0.7279 and F REQ“0.4789 , the predictions of the individual models would\nbe\nMwsinglepqq“logisticp0.7993´15.9030ˆp´ 0.7279q`9.5974ˆ0.4789q\n“0.9999\nMwfamilypqq“logisticp3.6526`´0.5809ˆp´ 0.7279q´17.5886ˆ0.4789q\n“0.01278\nMwbusinesspqq“logisticp4.6419`14.9401ˆp´ 0.7279q´6.9457ˆ0.4789q\n“0.0518","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":415,"page_label":"361","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 361\nThese predictions would be normalized as follows:\nM1\nwsinglepqq“0.9999\n0.9999`0.01278`0.0518“0.9393\nM1\nwfamilypqq“0.01278\n0.9999`0.01278`0.0518“0.0120\nM1\nwbusinesspqq“0.0518\n0.9999`0.01278`0.0518“0.0487\nThis means the overall prediction for the query instance is single , as this gets the highest\nnormalized score.\n7.4.7 Support Vector Machines\nSupport vector machines (SVM ) are another approach to predictive modeling that is\nbased on error-based learning. Figure 7.22(a)[361]shows a scatter plot of a reduced version\nof the generators dataset (shown in Table 7.6[339]) with a decision boundary drawn across\nit. The instance nearest the decision boundary, based on perpendicular distance, is high-\nlighted. This distance from the decision boundary to the nearest training instance is known\nas the margin . The dashed lines on either side of the decision boundary show the extent\nof the margin, and we refer to these as the margin extents .\n−0.4 −0.2 0.0 0.2 0.4−0.4 −0.2 0.0 0.2 0.4\nRPMVibration\n(a)\n−0.4 −0.2 0.0 0.2 0.4−0.4 −0.2 0.0 0.2 0.4\nRPMVibration (b)\nFigure 7.22\nA small sample of the generators dataset with two features, RPM and V IBRATION , and two target\nlevels, good (shown as crosses) and faulty (shown as triangles): (a) a decision boundary with a very\nsmall margin; and (b) a decision boundary with a much larger margin. In both cases, the instances\nalong the margins are highlighted.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":416,"page_label":"362","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"362 Chapter 7 Error-Based Learning\nFigure 7.22(b)[361]shows a similar diagram but with a different decision boundary, which\nhas a much larger margin. The intuition behind support vector machines is that this sec-\nond decision boundary should distinguish between the two target levels much more reliably\nthan the ﬁrst. Training a support vector machine involves searching for the decision bound-\nary, or separating hyperplane ,20that leads to the maximum margin because this will best\nseparate the levels of the target feature. Although the goal of ﬁnding the best decision\nboundary is the same for algorithms that build support vector machines as it is for logistic\nregression models, the inductive bias encoded in the algorithms to select this boundary is\ndifferent, which leads to different decision boundaries being found.\nThe instances in a training dataset that fall along the margin extents, and therefore the\nmargins, are known as the support vectors . These are the most important instances in\nthe dataset because they deﬁne the decision boundary. There will always be at least one\nsupport vector for each level of the target feature, but there is no limit to how many support\nvectors there can be in total.\nWe deﬁne the separating hyperplane in the same way that we did at the beginning of the\ndiscussion of logistic regression\nw0`w¨d“0 (7.40)\nNote that this time we have separated w0from the other weights, w, as this will make later\nequations simpler.21Recall from Section 7.4.4[338]that for instances above a separating\nhyperplane\nw0`w¨dą0\nand for instances below a separating hyperplane\nw0`w¨dă0\nFor support vector machines, we ﬁrst set the negative target feature level to ´1and the\npositive target feature level to `1. We then build a support vector machine prediction\nmodel so that instances with the negative target level result in the model outputting ď´1\nand instances with the positive target level result in the model outputting ě`1. The space\nbetween the outputs of ´1and`1allows for the margin.\n20. Remember that for problems with more than two descriptive features, the decision boundary is a hyperplane\nrather than a line.\n21. This also means that we no longer use the dummy descriptive feature, dr0s, which we previously always set\nto1; see Equation (7.9)[320].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":417,"page_label":"363","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 363\nA support vector machine model is deﬁned as\nMααα,w0pqq“sÿ\ni“1ptiˆαααrisˆpdi¨qq`w0q (7.41)\nwhere qis the set of descriptive features for a query instance; pd1,t1q,...,pds,tsqares\nsupport vectors (instances composed of descriptive features and a target feature); w0is the\nﬁrst weight of the decision boundary; and αααis a set of parameters determined during the\ntraining process (there is a parameter for each support vector αααr1s,...,αααrss).22When\nthe output of this equation is greater than 1, we predict the positive target level for the\nquery, and when the output is less than ´1, we predict the negative target level. An im-\nportant feature of this equation is that the support vectors are a component of the equation.\nThis reﬂects the fact that a support vector machine uses the support vectors to deﬁne the\nseparating hyperplane and hence to make the actual model predictions.\nTo train a support vector machine, we need to ﬁnd values for each of the components\nin Equation (7.41)[363](the support vectors, w0, and theαααparameters) that deﬁne the op-\ntimal decision boundary between the target levels. This is an instance of a constrained\nquadratic optimization problem , and there are well-known approaches to solving this\ntype of problem. In this book we do not describe this step of the process in detail.23\nInstead, we focus on explaining how the process is set up and how the training process\nreﬂects the inductive bias of searching for the separating hyperplane with the maximum\nmargin. As the name constrained quadratic optimization problem suggests, this type of\nproblem is deﬁned in terms of (1) a set of constraints and (2) an optimization criterion.\nWhen training a support vector machine, we wish to ﬁnd a hyperplane that distinguishes\nbetween the two target levels, ´1and`1. So, the required constraints required by the\ntraining process are\nw0`w¨dď´1forti“´1 (7.42)\nand\nw0`w¨dě`1forti“`1 (7.43)\nFigure 7.23[364]shows two different decision boundaries that satisfy these constraints. Note\nthat the decision boundaries in these examples are equally positioned between positive and\nnegative instances, which is a consequence of the fact that decision boundaries satisfy these\nconstraints. The support vectors are highlighted in Figure 7.23[364]for each of the decision\nboundaries shown. For simplicity in later calculations, we can combine the two constraints\n22. These parameters are formally known as Lagrange multipliers .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":417,"page_label":"363","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"between the two target levels, ´1and`1. So, the required constraints required by the\ntraining process are\nw0`w¨dď´1forti“´1 (7.42)\nand\nw0`w¨dě`1forti“`1 (7.43)\nFigure 7.23[364]shows two different decision boundaries that satisfy these constraints. Note\nthat the decision boundaries in these examples are equally positioned between positive and\nnegative instances, which is a consequence of the fact that decision boundaries satisfy these\nconstraints. The support vectors are highlighted in Figure 7.23[364]for each of the decision\nboundaries shown. For simplicity in later calculations, we can combine the two constraints\n22. These parameters are formally known as Lagrange multipliers .\n23. We provide references in Section 7.6[370].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":418,"page_label":"364","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"364 Chapter 7 Error-Based Learning\n−0.4 −0.2 0.0 0.2 0.4−0.4 −0.2 0.0 0.2 0.4\nRPMVibration\n(a)\n−0.4 −0.2 0.0 0.2 0.4−0.4 −0.2 0.0 0.2 0.4\nRPMVibration\n/uni25CF/uni25CF (b)\nFigure 7.23\nDifferent margins that satisfy the constraint in Equation (7.44)[364], the instances that deﬁne the margin\nare highlighted in each case; (b) shows the maximum margin and also shows two query instances\nrepresented as black dots.\nin Equations (7.42)[363]and (7.43)[363]into a single constraint (remember that tiis always\nequal to either ´1or`1)\ntiˆpw0`w¨dqě1 (7.44)\nThe optimization criterion used when training a support vector machine allows us to\nchoose between multiple different decision boundaries that satisfy the constraint given in\nEquation (7.44)[364], such as those shown in Figure 7.23[364]. The optimization criterion used\nis deﬁned in terms of the perpendicular distance from any instance to the decision boundary\nand is given by\ndistpdq“abspw0`w¨dq\n||w||\nwhere||w||is known as the Euclidean norm ofwand is calculated\n||w||“b\nwr1s2`wr2s2`...`wrms2\nFor instances along the margin extents, abspw0`w¨dq “ 1(according to Equation\n(7.44)[364]). So, the distance from any instance along the margin extents to the decision\nboundary is1\n||w||, and because the margin is symmetrical to either side of the decision","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":419,"page_label":"365","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.4 Extensions and Variations 365\nboundary, the size of the margin is2\n||w||. The goal when training a support vector machine\nis to maximize2\n||w||subject to the constraint expressed in Equation (7.44)[364].\nOnce the constraints and optimization criterion have been deﬁned, the solution to the\nconstrained quadratic optimization process will identify and deﬁne all the components in\nEquation (7.41)[363](the support vectors, w0, and theαααparameters) for the optimal decision\nboundary.\nThe optimal decision boundary and associated support vectors for the example we have\nbeen following are shown in Figure 7.23(b)[364]. In this case good is the positive level and\nset to`1, and faulty is the negative level and set to ´1. The descriptive feature values\nand target feature values for the support vectors in these cases are p⟨´0.225,0.217⟩,`1q,\np⟨´0.066,´0.069⟩,´1q, andp⟨´0.273,´0.080⟩,´1q. The value of w0is´0.1838 , and\nthe values of the αααparameters are ⟨23.056,6.998,16.058⟩q. Figure 7.23(b)[364]shows the\nposition of two new query instances for this problem. The descriptive feature values for\nthese query instances are q1“⟨´0.314,´0.251⟩andq2“⟨´0.117,0.31⟩. For the ﬁrst\nquery instance, q1, the output of the support vector machine model is:\nMααα,w0pq1q\n“p1ˆ23.056ˆpp´0.225ˆ´0.314q`p0.217ˆ´0.251qq´0.1838q\n`p´1ˆ6.998ˆpp´0.066ˆ´0.314q`p´0.069ˆ´0.251qq´0.1838q\n`p´1ˆ16.058ˆpp´0.273ˆ´0.314q`p´0.080ˆ´0.251qq´0.1838q\n“´ 2.145\nThe model output is less than ´1, so this query is predicted to be a faulty generator. For\nthe second query instance, the model output is calculated similarly and is 1.592. This is\ngreater than`1, so this instance is predicted to be a good generator.\nIn the same way we used basis functions with logistic regression models in Section\n7.4.5[351], basis functions can be used with support vector machines to handle training data\nthat is not linearly separable. In order to use basis functions, we must update Equation\n(7.44)[364]to\ntiˆpw0`w¨φφφpdqqě1for all i (7.45)\nwhereφφφis a set of basis functions applied to the descriptive features d, and wis a set\nof weights containing one weight for each member of φφφ. Typically, the number of basis\nfunctions in φφφis larger than the number of descriptive features, so the application of the\nbasis functions moves the data into a higher-dimensional space. The expectation is that\na linear separating hyperplane will exist in this higher-dimensional space even though it","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":420,"page_label":"366","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"366 Chapter 7 Error-Based Learning\ndoes not in the original feature space. The prediction model in this case becomes\nMααα,φφφ,w0pqq“sÿ\ni“1ptiˆαααrisˆpφφφpdiq¨φφφpqqq`w0q (7.46)\nEquation (7.46)[366]requires a dot product calculation between the result of applying the\nbasis functions to the query instance and to each of the support vectors. During the training\nprocess, this is repeated multiple times. A dot product of two high-dimensional vectors is a\ncomputationally expensive operation, but a clever trick—the kernel trick —is used to avoid\nit. The same result obtained by calculating the dot product of the descriptive features of a\nsupport vector and a query instance after having applied the basis functions can be obtained\nby applying a much less costly kernel function ,kernel , to the original descriptive feature\nvalues of the support vector and the query.24The prediction equation becomes\nMααα,kernel,w0pqq“sÿ\ni“1ptiˆαααrisˆkernelpdi,qq`w0q (7.47)\nA wide range of standard kernel functions can be used with support vector machines. Some\npopular options are\nLinear kernel kernelpd,qq“d¨q`c\nwhere cis an optional constant\nPolynomial kernel kernelpd,qq“pd¨q`1qp\nwhere pis the degree of a polynomial\nfunction\nGaussian radial basis kernel kernelpd,qq“expp´γ||d´q||2q\nwhereγis a manually chosen tuning\nparameter\nThe appropriate kernel function for a particular prediction model should be selected by\nexperimenting with different options. It is best to start with a simple linear or low-degree\npolynomial kernel function and move to more complex kernel functions only if good per-\nformance cannot be achieved with this.\nThe description of the support vector machine approach given in this section assumes\nthat it is possible to separate the instances with the two different target feature levels with\na linear hyperplane. Sometimes this is not possible, even after using a kernel function to\nmove the data to a higher-dimensional feature space. In these instances, a margin cannot\nbe deﬁned, as we have done in this example. An extension of the standard support vector\nmachine approach that allows a soft margin , however, caters for this and allows overlap\n24. Question 4 in the Exercises at the end of this chapter explores the kernel trick in more detail, and worked\nexamples are provided in the solution.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":421,"page_label":"367","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.5 Summary 367\nbetween instances with target features of the two different levels. Another extension al-\nlows support vector machines to handle multinomial target features using a one-versus-all\napproach similar to that described in Section 7.4.6[357]. There are also extensions to handle\ncategorical descriptive features (similar to the approach described in Section 7.4.3[336]) and\ncontinuous target features.\nSupport vector machines have become a very popular approach to building predictive\nmodels in recent times. They can be quickly trained, are not overly susceptible to overﬁt-\nting, and work well for high-dimensional data. In contrast to logistic regression models,\nhowever, they are not very interpretable, and, especially when kernel functions are used, it\nis very difﬁcult to understand why a particular prediction has been made.\n7.5 Summary\nThesimple multivariable linear regression (Section 7.3[319]) model (for convenience, re-\npeated here as Equation (7.48)[367]) makes a prediction for a continuous target feature based\non a weighted sum of the values of a set of descriptive features. In an error-based model,\nlearning equates to ﬁnding the optimal values for these weights. Each of the inﬁnite num-\nber of possible combinations of values for the weights will result in a model that ﬁts, to\nsome extent, the relationship present in the training data between the descriptive features\nand the target feature. The optimal values for the weights are the values that deﬁne the\nmodel with the minimum prediction error.\nMwpdq“w¨d\n“mÿ\nj“0wrjsˆdrjs(7.48)\nWe use an error function to measure how well a set of weights ﬁts the relationship in\nthe training data. The most common error function used for error-based models is the sum\nof squared errors . The value of the error function for every possible weight combination\ndeﬁnes an error surface, similar to the one shown in Figure 7.24(a)[368]—for each combi-\nnation of weight values, we get a point on the surface whose coordinates are the weight\nvalues, with an elevation deﬁned by the error of the model using the weight values. To ﬁnd\nthe optimal set of weights, we begin with a set of random weight values that corresponds\nto some random point on the error surface. We then iteratively make small adjustments to\nthese weights based on the output of the error function, which leads to a journey down the\nerror surface that eventually leads to the optimal set of weights. The zig-zagging line in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":421,"page_label":"367","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of squared errors . The value of the error function for every possible weight combination\ndeﬁnes an error surface, similar to the one shown in Figure 7.24(a)[368]—for each combi-\nnation of weight values, we get a point on the surface whose coordinates are the weight\nvalues, with an elevation deﬁned by the error of the model using the weight values. To ﬁnd\nthe optimal set of weights, we begin with a set of random weight values that corresponds\nto some random point on the error surface. We then iteratively make small adjustments to\nthese weights based on the output of the error function, which leads to a journey down the\nerror surface that eventually leads to the optimal set of weights. The zig-zagging line in\nFigure 7.24(a)[368]shows an example journey across an error surface, and Figure 7.24(b)[368]\nshows the reduction in the sum of squared errors as the search for the optimal weights\nprogresses down the error surface.\nTo ensure that we arrive at the optimal set of weights at the end of this journey across\nthe error surface, we need to ensure that each step we take moves downward on the error","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":422,"page_label":"368","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"368 Chapter 7 Error-Based Learning\n(a)\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n0 5 10 15 20 25 30 350 20 40 60 80\nTraining Iter ationSum of Squared Errors (b)\nFigure 7.24\n(a) The journey across an error surface; and (b) the changing sums of squared errors during this\njourney.\nsurface. We do this by directing our steps according to the gradient of the error surface\nat each step. This is the gradient descent algorithm , which is one of the most important\nalgorithms in all of computer science, let alone machine learning.\nThe simple multivariable linear regression model that we presented at the beginning of\nthis chapter can be extended in many ways, and we presented some of the most important of\nthese. Logistic regression models (Section 7.4.4[338]) allow us to predict categorical targets\nrather than continuous ones by placing a threshold on the output of the simple multivariable\nlinear regression model using the logistic function.\nThe simple linear regression and logistic regression models that we ﬁrst looked at were\nonly capable of representing linear relationships between descriptive features and a target\nfeature. In many cases, this limits the creation of an accurate prediction model. By apply-\ning a set of basis functions (Section 7.4.5[351]) to descriptive features, models that represent\nnon-linear relationships can be created. The advantages of using basis functions is that\nthey allow models that represent non-linear relationships to be built even though these\nmodels themselves remain a linear combination of inputs (e.g., we still use something very\nsimilar to Equation (7.48)[367]to predict continuous targets). Consequently, we can still use\nthe gradient descent process to train them. The main disadvantages of using basis functions\nare, ﬁrst, that we must manually decide what set of basis functions to use; and second, that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":423,"page_label":"369","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.5 Summary 369\nthe number of weights in a model using basis functions is usually far greater than the num-\nber of descriptive features, so ﬁnding the optimal set of weights involves a search across a\nmuch larger set of possibilities—that is, a much larger weight space .\nIt is somewhat surprising how often a linear multivariable regression model can accu-\nrately represent the relationship between descriptive features and a target feature without\nthe use of basis functions. We recommend that simple linear models be evaluated ﬁrst and\nbasis functions introduced only when the performance of the simpler models is deemed\nunsatisfactory.\nThe logistic regression approach (and the SVM approach) discussed in this chapter is at\na disadvantage to those discussed in the previous chapters in that in its basic form, it can\nonly handle categorical target features with two levels. In order to handle categorical target\nfeatures with more than two levels, that is multinomial prediction problems, we need\nto use a one-versus-all approach in which multiple models are trained. This introduces\nsomething of an explosion in the number of weights required for a model, as we have\nan individual set of weights for every target feature level. This is one reason that other\napproaches are often favored over logistic regression for predicting categorical targets with\nmany levels.\nOne of the most attractive features of the regression models discussed in this chapter is\nthat they are based on a large body of research and best practice in statistics , a much older\ndiscipline than machine learning. The maturity of regression-based approaches means that\nthey are easily accepted in other disciplines (e.g., biological, physical, and social sciences)\nand that there is a range of techniques that allow a degree of analysis of regression models\nbeyond what is possible for other approaches. We saw some of these techniques in Section\n7.4.1[332]when we examined the importance of the different descriptive features in a linear\nregression model through an analysis of the model weights. A range of other approaches\nwe do not cover in this book can be used to do other in-depth analysis of regression models.\nSection 7.6[370]recommends further reading on this topic.\nNear the end of this chapter we covered support vector machines (SVM). SVM models\nare trained in a slightly different way than regression models, but the concepts underpin-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":423,"page_label":"369","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"and that there is a range of techniques that allow a degree of analysis of regression models\nbeyond what is possible for other approaches. We saw some of these techniques in Section\n7.4.1[332]when we examined the importance of the different descriptive features in a linear\nregression model through an analysis of the model weights. A range of other approaches\nwe do not cover in this book can be used to do other in-depth analysis of regression models.\nSection 7.6[370]recommends further reading on this topic.\nNear the end of this chapter we covered support vector machines (SVM). SVM models\nare trained in a slightly different way than regression models, but the concepts underpin-\nning both approaches are similar. The main advantages of SVM models are that they are\nrobust to overﬁtting and perform well for very high-dimensional problems. SVM models\nare just one of a whole range of error-based approaches that are active areas for machine\nlearning research, and new approaches are constantly being developed. In this chapter\nwe have not covered artiﬁcial neural networks , another popular error-based approach to\nlearning that is a very active research area. One type of artiﬁcial neural network can be\nbuilt by connecting layers of logistic regression models, but there are many other network\ntopologies used in practice. We cover artiﬁcial neural networks and deep learning in detail\nin Chapter 8[381].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":424,"page_label":"370","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"370 Chapter 7 Error-Based Learning\nThe next section discusses recommended readings for more information on the regres-\nsion approaches discussed in this chapter and on some of the more recent developments in\nerror-based learning.\n7.6 Further Reading\nThe key component of the gradient descent algorithm presented in this chapter is the use\nofdifferentiation to compute the slope of the error surface. Differentiation is a part of\ncalculus, which is a large and very important ﬁeld of mathematics. In Appendix C[765]\nwe provide an introduction to differentiation that covers all the techniques required to\nunderstand how the gradient descent algorithm works. If, however, you wish to get a\nbroader understanding of calculus, we recommend Stewart (2012) as an excellent textbook\non all aspects of calculus.\nFor a more in-depth treatment of regression models and their underpinnings in statistics,\nChapter 14 of Rice (2006) offers a nice treatment of the topic, and Kutner et al. (2004)\nprovides massive detail. Ayres (2008) gives a lighter discussion of the many different\nways in which regression models are applied in practice.\nBurges (1998) is still a good, freely available tutorial on support vector machines. For\nmore details, Cristianini and Shawe-Taylor (2000) is a well-respected textbook on the topic\nand covers the extensions mentioned in Section 7.4.7[361], while Vapnik (2000) gives a good\noverview of the theoretical underpinnings of support vector machines.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":425,"page_label":"371","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.7 Exercises 371\n7.7 Exercises\n1.A multivariate linear regression model has been built to predict the heating load in a\nresidential building on the basis of a set of descriptive features describing the charac-\nteristics of the building. Heating load is the amount of heat energy required to keep a\nbuilding at a speciﬁed temperature, usually 65˝Fahrenheit during the winter regard-\nless of outside temperature. The descriptive features used are the overall surface area\nof the building, the height of the building, the area of the building’s roof, and the per-\ncentage of wall area in the building that is glazed. This kind of model would be useful\nto architects or engineers when designing a new building.25The trained model is\nHEATING LOAD“´ 26.030`0.0497ˆSURFACE AREA\n`4.942ˆHEIGHT´0.090ˆROOF AREA\n`20.523ˆGLAZING AREA\nUse this model to make predictions for each of the query instances shown in the fol-\nlowing table.\nSURFACE ROOF GLAZING\nID A REA HEIGHT AREA AREA\n1 784.0 3.5 220.5 0.25\n2 710.5 3.0 210.5 0.10\n3 563.5 7.0 122.5 0.40\n4 637.0 6.0 147.0 0.60\n2.You have been hired by the European Space Agency to build a model that predicts\nthe amount of oxygen that an astronaut consumes when performing ﬁve minutes of\nintense physical work. The descriptive features for the model will be the age of the\nastronaut and their average heart rate throughout the work. The regression model is\nOXYCON“wr0s`wr1sˆAGE`wr2sˆHEART RATE\nThe table that follows shows a historical dataset that has been collected for this task.\n25. This question is inspired by Tsanas and Xifara (2012), and although the data used is artiﬁcially generated,\nit is based on the Energy Efﬁciency Dataset available from the UCI Machine Learning Repository (Bache and\nLichman, 2013) at archive.ics.uci.edu/ml/datasets/Energy+efﬁciency/.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":426,"page_label":"372","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"372 Chapter 7 Error-Based Learning\nHEART\nID O XYCON AGE RATE\n1 37.99 41 138\n2 47.34 42 153\n3 44.38 37 151\n4 28.17 46 133\n5 27.07 48 126\n6 37.85 44 145HEART\nID O XYCON AGE RATE\n7 44.72 43 158\n8 36.42 46 143\n9 31.21 37 138\n10 54.85 38 158\n11 39.84 43 143\n12 30.83 43 138\n(a)Assuming that the current weights in a multivariate linear regression model are\nwr0s“ ´ 59.50,wr1s“ ´ 0.15, and wr2s“0.60, make a prediction for each\ntraining instance using this model.\n(b)Calculate the sum of squared errors for the set of predictions generated in Part (a).\n(c)Assuming a learning rate of 0.000002 , calculate the weights at the next iteration\nof the gradient descent algorithm.\n(d)Calculate the sum of squared errors for a set of predictions generated using the\nnew set of weights calculated in Part (c).\n3.A multivariate logistic regression model has been built to predict the propensity of\nshoppers to perform a repeat purchase of a free gift that they are given. The descrip-\ntive features used by the model are the age of the customer, the socioeconomic band\nto which the customer belongs ( a,b, orc), the average amount of money the customer\nspends on each visit to the shop, and the average number of visits the customer makes\nto the shop per week. This model is being used by the marketing department to deter-\nmine who should be given the free gift. The weights in the trained model are shown\nin the following table.\nFeature Weight\nIntercept ( wr0s) -3.82398\nAGE -0.02990\nSOCIOECONOMIC BAND B -0.09089\nSOCIOECONOMIC BAND C -0.19558\nSHOP VALUE 0.02999\nSHOP FREQUENCY 0.74572\nUse this model to make predictions for each of the following query instances.\nSOCIOECONOMIC SHOP SHOP\nID A GE BAND FREQUENCY VALUE\n1 56 b 1.60 109.32\n2 21 c 4.92 11.28\n3 48 b 1.21 161.19\n4 37 c 0.72 170.65\n5 32 a 1.08 165.39","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":427,"page_label":"373","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.7 Exercises 373\n4.The use of the kernel trick is key in writing efﬁcient implementations of the support\nvector machine approach to predictive modelling. The kernel trick is based on the fact\nthat the result of a kernel function applied to a support vector and a query instance is\nequivalent to the result of calculating the dot product between the support vector and\nthe query instance after a speciﬁc set of basis functions have been applied to both—in\nother words, kernelpd,qq“φφφpdq¨φφφpqq.\n(a)Using the support vector⟨dr1s,dr2s⟩and the query instance⟨qr1s,qr2s⟩as\nexamples, show that applying a polynomial kernel with p“2,kernelpd,qq “\npd¨q`1q2, is equivalent to calculating the dot product of the support vector and\nquery instance after applying the following set of basis functions:\nφ0p⟨dr1s,dr2s⟩q“dr1s2φ1p⟨dr1s,dr2s⟩q“dr2s2\nφ2p⟨dr1s,dr2s⟩q“?\n2ˆdr1sˆdr2sφ3p⟨dr1s,dr2s⟩q“?\n2ˆdr1s\nφ4p⟨dr1s,dr2s⟩q“?\n2ˆdr2s φ5p⟨dr1s,dr2s⟩q“1\n(b)A support vector machine model has been trained to distinguish between dosages\nof two drugs that cause a dangerous interaction and those that interact safely. This\nmodel uses just two continuous features, D OSE1 and D OSE2, and two target lev-\nels,dangerous (the positive level, `1) and safe (the negative level, ´1). The\nsupport vectors in the trained model are shown in the following table.\nDOSE1 D OSE2 C LASS\n0.2351 0.4016 +1\n-0.1764 -0.1916 +1\n0.3057 -0.9394 -1\n0.5590 0.6353 -1\n-0.6600 -0.1175 -1\nIn the trained model the value of w0is0.3074 , and the values of the αααparameters\nare⟨7.1655,6.9060,2.0033,6.1144,5.9538⟩.\ni.Using the version of the support vector machine prediction model that uses\nbasis functions (see Equation 7.46) with the basis functions given in Part (a),\ncalculate the output of the model for a query instance with D OSE1“0.90and\nDOSE2“´0.90.\nii.Using the version of the support vector machine prediction model that uses a\nkernel function (see Equation 7.47) with the polynomial kernel function, cal-\nculate the output of the model for a query instance with D OSE1“0.22and\nDOSE2“0.16.\niii.Verify that the answers calculated in Parts (i) and (ii) of this question would\nhave been the same if the alternative approach (basis functions or the polyno-\nmial kernel function) had been used in each case.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":428,"page_label":"374","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"374 Chapter 7 Error-Based Learning\niv.Compare the amount of computation required to calculate the output of the\nsupport vector machine using the polynomial kernel function with the amount\nrequired to calculate the output of the support vector machine using the basis\nfunctions.\n˚5.In building multivariate logistic regression models, it is recommended that all continu-\nous descriptive features be normalized to the range r´1,1s. The following table shows\na data quality report for the dataset used to train the model described in Question 3.\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nAGE 5,200 6 40 18 22 32.7 32 32 63 12.2\nSHOP FREQUENCY 5,200 0 316 0.2 1.0 2.2 1.3 4.3 5.4 1.6\nSHOP VALUE 5,200 0 3,730 5 11.8 101.9 100.14 174.6 230.7 72.1\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Count % Mode Count %\nSOCIOECONOMIC BAND 5,200 8 3 a 2,664 51.2 b 1,315 25.3\nREPEAT PURCHASE 5,200 0 2 no 2,791 53.7 yes 2,409 46.3\nOn the basis of the information in this report, all continuous features were normalized\nusing range normalization , and any missing values were replaced using mean im-\nputation for continuous features and mode imputation for categorical features. After\napplying these data preparation operations, a multivariate logistic regression model\nwas trained to give the weights shown in the following table.\nFeature Weight\nIntercept ( wr0s) 0.6679\nAGE -0.5795\nSOCIOECONOMIC BAND B -0.1981\nSOCIOECONOMIC BAND C -0.2318\nSHOP VALUE 3.4091\nSHOP FREQUENCY 2.0499\nUse this model to make predictions for each of the query instances shown in the fol-\nlowing table (question marks refer to missing values).\nSOCIOECONOMIC SHOP SHOP\nID A GE BAND FREQUENCY VALUE\n1 38 a 1.90 165.39\n2 56 b 1.60 109.32\n3 18 c 6.00 10.09\n4 ? b 1.33 204.62\n5 62 ? 0.85 110.50","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":429,"page_label":"375","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.7 Exercises 375\n˚6.The effects that can occur when different drugs are taken together can be difﬁcult\nfor doctors to predict. Machine learning models can be built to help predict optimal\ndosages of drugs so as to achieve a medical practitioner’s goals.26In the following\nﬁgure, the image on the left shows a scatter plot of a dataset used to train a model to\ndistinguish between dosages of two drugs that cause a dangerous interaction and those\nthat cause a safe interaction. There are just two continuous features in this dataset,\nDOSE1 and D OSE2 (both normalized to the range p´1,1qusing range normalization),\nand two target levels, dangerous andsafe. In the scatter plot, D OSE1 is shown on\nthe horizontal axis, D OSE2 is shown on the vertical axis, and the shapes of the points\nrepresent the target level—crosses represent dangerous interactions and triangles rep-\nresent safeinteractions.\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nDose1Dose2\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nDose1Dose2\nIn the preceding ﬁgure, the image on the right shows a simple linear logistic regression\nmodel trained to perform this task. This model is\nPpTYPE“dangerousq“\nLogisticp0.6168`2.7320ˆDOSE1´2.4809ˆDOSE2q\nPlainly, this model is not performing well.\n(a)Would the similarity-based, information-based, or probability-based predictive\nmodeling approaches already covered in this book be likely to do a better job\nof learning this model than the simple linear regression model?\n(b)A simple approach to adapting a logistic regression model to learn this type of\ndecision boundary is to introduce a set of basis functions that will allow a non-\n26. The data used in this question has been artiﬁcially generated for this book. Mac Namee et al. (2002) is,\nhowever, a good example of prediction models used to help doctors select correct drug dosages.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":430,"page_label":"376","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"376 Chapter 7 Error-Based Learning\nlinear decision boundary to be learned. In this case, a set of basis functions that\ngenerate a cubic decision boundary will work well. An appropriate set of basis\nfunctions is as follows:\nφ0p⟨DOSE1,DOSE2⟩q“1φ1p⟨DOSE1,DOSE2⟩q“DOSE1\nφ2p⟨DOSE1,DOSE2⟩q“DOSE2φ3p⟨DOSE1,DOSE2⟩q“DOSE12\nφ4p⟨DOSE1,DOSE2⟩q“DOSE22φ5p⟨DOSE1,DOSE2⟩q“DOSE13\nφ6p⟨DOSE1,DOSE2⟩q“DOSE23φ7p⟨DOSE1,DOSE2⟩q“DOSE1ˆDOSE2\nTraining a logistic regression model using this set of basis functions leads to the\nfollowing model:\nPpTYPE“dangerousq“\nLogistic`\n´0.848ˆφ0p⟨DOSE1,DOSE2⟩q`1.545ˆφ1p⟨DOSE1,DOSE2⟩q\n´1.942ˆφ2p⟨DOSE1,DOSE2⟩q`1.973ˆφ3p⟨DOSE1,DOSE2⟩q\n`2.495ˆφ4p⟨DOSE1,DOSE2⟩q`0.104ˆφ5p⟨DOSE1,DOSE2⟩q\n`0.095ˆφ6p⟨DOSE1,DOSE2⟩q`3.009ˆφ7p⟨DOSE1,DOSE2⟩q˘\nUse this model to make predictions for the following query instances:\nID D OSE1 D OSE2\n1 0.50 0.75\n2 0.10 0.75\n3 -0.47 -0.39\n4 -0.47 0.18\n˚7.The following multinomial logistic regression model predicts the T YPE of a retail\ncustomer ( single ,family , orbusiness ) on the basis of the average amount that they\nspend per visit, S PEND , and the average frequency of their visits, F REQ:\nMwsinglepqq“logisticp0.7993´15.9030ˆSPEND`9.5974ˆFREQq\nMwfamilypqq“logisticp3.6526`´0.5809ˆSPEND´17.5886ˆFREQq\nMwbusinesspqq“logisticp4.6419`14.9401ˆSPEND´6.9457ˆFREQq\nUse this model to make predictions for the following query instances:\nID S PEND FREQ\n1 -0.62 0.10\n2 -0.43 -0.71\n3 0.00 0.00","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":431,"page_label":"377","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.7 Exercises 377\n˚8.A support vector machine has been built to predict whether a patient is at risk of\ncardiovascular disease. In the dataset used to train the model, there are two target\nlevels— high risk (the positive level, `1) orlow risk (the negative level, ´1)—and\nthree descriptive features—A GE, BMI, and B LOOD PRESSURE . The support vectors\nin the trained model are shown in the table below (all descriptive feature values have\nbeen standardized).\nBLOOD\nAGE BMI P RESSURE RISK\n-0.4549 0.0095 0.2203 low risk\n-0.2843 -0.5253 0.3668 low risk\n0.3729 0.0904 -1.0836 high risk\n0.558 0.2217 0.2115 high risk\nIn the model the value of w0is´0.0216 , and the values of the αααparameters are\n⟨1.6811,0.2384,0.2055,1.7139⟩. What predictions would this model make for the\nfollowing query instances?\nBLOOD\nID A GE BMI P RESSURE\n1 -0.8945 -0.3459 0.5520\n2 0.4571 0.4932 -0.4768\n3 -0.3825 -0.6653 0.2855\n4 0.7458 0.1253 -0.7986\n˚9.A multivariate logistic regression model has been built to diagnose breast cancer in\npatients on the basis of features extracted from tissue samples extracted by biopsy.27\nThe model uses three descriptive features—M ITOSES , a measure of how fast cells\nare growing; C LUMP THICKNESS , a measure of the amount of layering in cells; and\nBLAND CHROMATIN , a measure of the texture of cell nuclei—and predicts the status\nof a biopsy as either benign ormalignant . The weights in the trained model are shown\nin the following table.\nFeature Weight\nIntercept ( wr0s)´13.92\nMITOSES 3.09\nCLUMP THICKNESS 0.63\nBLAND CHROMATIN 1.11\n27. The data in this question has been artiﬁcially created but is inspired by the famous Wisconsin breast cancer\ndataset ﬁrst described in Mangasarian and Wolberg (1990) and is available from the UCI Machine Learning\nRepository (Bache and Lichman, 2013).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":432,"page_label":"378","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"378 Chapter 7 Error-Based Learning\n(a)Use this model to make predictions for each of the following query instances.\nCLUMP BLAND\nID M ITOSES THICKNESS CHROMATIN\n1 7 4 3\n2 3 5 1\n3 3 3 3\n4 5 3 1\n5 7 4 4\n6 10 4 1\n7 5 2 1\n(b)The following are the ground truth labels for the query instances from Part (a).\nd1 d2 d3 d4 d5 d6 d7\nbenign benign malignant benign malignant malignant benign\ni.Using the ground truth labels, calculate the squared error loss for each query\ninstance (assume that benign“0andmalignant“1).\nii.Categorical cross entropy is another loss function that is commonly used for\nclassiﬁcation models. Categorical cross entropy is deﬁned as\n´ptiˆlnpMwpdiqq`p1´tiqˆlnp1´Mwpdiqqq\nUsing the ground truth labels previously given, calculate the categorical cross\nentropy for the query set. Compare these values to the squared error loss values\nfor each instance.\n˚10.The following images are handwritten instances of the digits 0and1.28The images\nare small, 8 pixels by 8 pixels, and each pixel contains a gray level from the range\nr0,7s.\nRather than use individual pixel values, which can lead to very high-dimensional fea-\nture vectors, a simpler way to represent images for use with regression models is to\ncalculate a histogram for each image and use this as the feature vector instead. In this\ncase the histograms simply count the frequency of occurrence of each possible gray\n28. These images are based on the dataset from the UCI Machine Learning repository Dua and Graff (2017) and\noriginally described by Alimoglu and Alpaydin (1996).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":433,"page_label":"379","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"7.7 Exercises 379\nlevel in each image. The table that follows shows the histograms for a small dataset of\n16 images split between examples of digits 0and1.\nID GL-0 GL-1 GL-2 GL-3 GL-4 GL-5 GL-6 GL-7 D IGIT\n0 31 3 6 2 7 5 6 4 0\n1 37 3 1 4 1 3 2 13 1\n2 31 3 4 1 8 7 3 7 0\n3 38 2 3 0 1 1 5 14 1\n4 31 5 3 2 5 2 5 11 0\n5 32 6 3 2 1 1 5 14 1\n6 31 3 5 2 3 6 2 12 0\n7 31 4 3 4 1 5 5 11 0\n8 38 4 2 2 2 4 4 8 1\n9 38 3 2 3 4 4 1 9 1\nA logistic regression model has been trained to classify digits as either 0or1. The\nweights in this model are as follows:\nIntercept GL-0 GL-1 GL-2 GL-3 GL-4 GL-5 GL-6 GL-7\nwr0s wr1s wr2s wr3s wr4s wr5s wr6s wr7s wr8s\n0.309 0.100´0.152´0.163 0.191´0.631´0.716´0.478´0.171\nThis model has been used to make predictions for the instances in the training set\nabove. These predictions, and the related calculations required for calculating error\nanderrorDelta values are shown in the following table.\nSquared errorDeltapD,wrjsq\nIDMwpdiqtiError Error wr0swr1swr2swr3swr4swr5swr6swr7swr8s\n0 0.051 0 -0.051 ? -0.0025 -0.0765 -0.0074 -0.0148 -0.0049 -0.0173 -0.0123 -0.0148 -0.0099\n1 ? 1 0.003 0.0000 -0.0025 0.0003 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0001\n2 0.019 0 -0.019 0.0004 -0.0025 -0.0110 -0.0011 -0.0014 -0.0004 -0.0028 -0.0025 -0.0011 ?\n3 0.993 1 0.007 0.0000 ? 0.0018 0.0001 0.0001 0.0000 0.0000 0.0000 0.0002 0.0007\n4 ? 0 -0.489 0.2391 -0.0025 -3.7879 -0.6110 -0.3666 -0.2444 -0.6110 -0.2444 -0.6110 -1.3441\n5 0.945 1 ? 0.0030 -0.0025 0.0915 0.0172 0.0086 0.0057 0.0029 0.0029 0.0143 0.0400\n6 ? 0 -0.400 0.1600 -0.0025 -2.9760 -0.2880 -0.4800 -0.1920 -0.2880 -0.5760 -0.1920 -1.1520\n7 0.703 0 ? 0.4942 -0.0025 -4.5502 ? -0.4403 -0.5871 -0.1468 -0.7339 -0.7339 -1.6146\n8 0.980 1 0.020 ? -0.0025 0.0149 0.0016 0.0008 0.0008 0.0008 0.0016 0.0016 0.0031\n9 0.986 1 0.014 0.0002 -0.0025 0.0073 0.0006 0.0004 0.0006 0.0008 0.0008 0.0002 0.0017\n(a)Some of the model predictions are missing in the preceding table (marked with a\n?). Calculate these.\n(b)Some of the Error and Squared Error values are missing in the preceding table\n(marked with a ?). Calculate these.\n(c)Some of the errorDelta values are missing in the preceding table (marked with a\n?). Calculate these.\n(d)Calculate a new set of weights for this model using a learning rate of0.01.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":434,"page_label":"380","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"380 Chapter 7 Error-Based Learning\n(e)The following table shows handwritten examples of the digits 7 and 8 and their\ncorresponding histogram values.\nGL-0 GL-1 GL-2 GL-3 GL-4 GL-5 GL-6 GL-7\n35 1 5 4 5 2 4 8\n30 6 2 0 5 4 4 13\ni.Calculate the output of the model (using the updated weights calculated in the\nprevious part) for these two instances.\nii.Comment on the appropriateness of these outputs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":435,"page_label":"381","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8 Deep Learning\n“A map is not the territory it represents, but, if correct, it has a similar structure to the territory, which\naccounts for its usefulness. ”\n—Alfred Korzybski, Science and Sanity , p. 58\nIn this chapter1we introduce deep learning, an approach to machine learning that is\ninspired by how the brain is structured and operates. Deep learning is a relatively new\nterm that describes research on modern artiﬁcial neural networks. Artiﬁcial neural network\nmodels are composed of large numbers of simple processing units, called neurons, that\ntypically are arranged into layers and are highly interconnected. Artiﬁcial neural networks\nare some of the most powerful machine learning models, able to learn complex non-linear\nmappings from inputs to outputs. As such, they generally work well in domains in which\nthere are large numbers of input features (such as image, speech, or language processing),\nand for which there are very large datasets available for training. Although the history of\nartiﬁcial neural networks can be traced back to the 1940s, the term deep learning came\nto prominence only in the mid-2000s.2The term deep learning emphasizes that modern\nnetworks are deeper (in terms of number of layers) than previous networks. This extra\ndepth enables the networks to learn more complex input-output mappings.\nThe Fundamentals section of this chapter introduces the standard artiﬁcial neural net-\nwork architecture: a feedforward neural network. We then present the backpropagation\nalgorithm, the standard algorithm used to train neural networks, and illustrate how the\nalgorithm functions with a worked example.\nThe ﬁrst two extensions and variations sections explain why the backpropagation algo-\nrithm can struggle with unstable gradients when training a deep network and how a number\nof important hyper-parameter decisions, such as the choice of the network weight initial-\n1. Parts of this chapter assume a familiarity with calculus, in particular the concepts of a partial derivative and\nthe chain rule; see Appendix C[765]for an introduction to these concepts. This chapter also draws on a number of\nconcepts introduced in Chapter 7[311], including the gradient descent algorithm, and logistic regression models.\n2. See Kelleher (2019) for a history of the development of neural network models and the emergence of deep\nlearning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":436,"page_label":"382","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"382 Chapter 8 Deep Learning\nization process, and activation functions, can help with the challenge of unstable gradients.\nWe then explain how a neural network can be modiﬁed and trained to handle categorical\ntarget features, using a softmax output layer and the cross-entropy loss function. Then we\nintroduce a well-known extension to backpropagation training known as dropout, which\ncan help stop a network from overﬁtting. The ﬁnal two extensions and variations sec-\ntions introduce well-known neural network architectures, including convolutional neural\nnetworks and recurrent neural networks.\n8.1 Big Idea\nThe human brain is an incredibly powerful learning system. Thanks to neuroscience we\nnow know quite a bit about the structure of the brain. For example, we know that the brain\nworks by propagating electrical signals through a massive network of interconnected cells,\nknown as neurons. In fact, it is estimated that the human brain contains around 100 billion\ninterconnected neurons (Herculano-Houzel, 2009).\nThere are many different types of neurons in the brain; however, in general, neurons have\na simple three-part structure consisting of (1) a cell body; (2) a set of relatively short ﬁbers\nconnected to the cell body, called dendrites; and (3) a single long ﬁber connected to the cell\nbody, called an axon. The dendrites of one neuron connect to the axons of other neurons\nvia connections known as synapses. These synapses allow electrical signals to pass from\nthe axon of one neuron to a dendrite of another. Essentially, the dendrites are the neuron’s\ninput channels, and the axon is the output channel. Figure 8.1 presents a schematic of the\nstructure of a neuron that illustrates how the neuron’s cell body, dendrites, and axon are\ninterconnected and how one neuron connects to other neurons in the brain. Functionally,\nan individual neuron can be understood as a simple signal processing system. A neuron\nfunctions as an all-or-none switch: if the electrical stimuli gathered by its dendrites are\nstrong enough, the neuron transmits an electrical pulse, known as an action potential, along\nits axon; otherwise it has no output.\nAlthough neuroscience has discovered a lot about the neural structure of the brain, neu-\nroscientists are still working on understanding how learning happens in the brain and how\nhigh-level human behavior arises from the processing of neurons. In 1949 Donald O. Hebb","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":436,"page_label":"382","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"interconnected and how one neuron connects to other neurons in the brain. Functionally,\nan individual neuron can be understood as a simple signal processing system. A neuron\nfunctions as an all-or-none switch: if the electrical stimuli gathered by its dendrites are\nstrong enough, the neuron transmits an electrical pulse, known as an action potential, along\nits axon; otherwise it has no output.\nAlthough neuroscience has discovered a lot about the neural structure of the brain, neu-\nroscientists are still working on understanding how learning happens in the brain and how\nhigh-level human behavior arises from the processing of neurons. In 1949 Donald O. Hebb\nproposed a theory that attempted to explain how general human behavior emerged from the\nphysiology of the brain. The basis of this theory was that complex behavior emerges from\nthe interactions between massive numbers of highly interconnected neurons rather than\nfrom complex processing within neurons. The idea that human mental phenomena emerge\nthrough the interconnections between neurons became known as connectionism . Hebb\nalso postulated a mechanism for how lasting memories are learned in the brain on the basis\nof a process of changes to the connections between neurons:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":437,"page_label":"383","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 383\nCell\nBodyAxonDendritesSynapse\nSynapse\nFigure 8.1\nA high-level schematic of the structure of a neuron. This ﬁgure illustrates three interconnected\nneurons; the middle neuron is highlighted in black, and the major structural components of this\nneuron are labeled cell body, dendrites, and axon. Also marked are the synapses connecting the axon\nof one neuron and the dendrite of another, which allow signals to pass between the neurons.\nWhen an axon of a cell A is near enough to excite a cell B and repeatedly or persistently takes part\nin ﬁring it, some growth process or metabolic change takes place in one or both cells such that A’s\nefﬁciency, as one of the cells ﬁring B, is increased. (Hebb, 1949, p. 62)\nHebb’s theory that learning occurs through changes in the connections between neurons\nand that behavior emerges through the ﬂow of information across these connections has\nbeen very inﬂuential both in neuroscience and, as we will discuss, in deep learning.\nThe big idea in deep learning is to develop computational models that are inspired by the\nstructure and operations of the human brain. The human brain is, of course, much more\ncomplex and sophisticated than even the most advanced deep learning models. However,\nit is because deep learning models are inspired by the human brain that they are known as\nartiﬁcial neural networks: they are designed (at least at a very abstract level) to mirror the\nstructure of the brain, and the adoption of a learning mechanism based on adjusting the\nconnections between neurons can be understood as mimicking Hebb’s theory of how the\nbrain learns.\n8.2 Fundamentals\nWe begin this section by introducing the basic building block of all deep learning models,\ntheartiﬁcial neuron (see Section 8.2.1[384]); we then describe how the artiﬁcial neurons can\nbe connected together to create an artiﬁcial neural network (see Section 8.2.2[388]). After\nwe have covered the fundamentals of the models, we explain how these artiﬁcial neural","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":438,"page_label":"384","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"384 Chapter 8 Deep Learning\nnetworks can be understood as sequences of matrix multiplications (see Section 8.2.3[390]).\nUnderstanding how a neural network can be represented and implemented as a sequence\nof matrix multiplications has important implications in terms of training time speedups.\nThe matrix representation of a neural network is also useful in terms of understanding\nwhy we need to include a non-linear function as part of the information processing within\neach of the neurons in a network (see Section 8.2.4[394]) and why the depth of a network\n(i.e., the number of layers) is important (see Section 8.2.5[395]). Finally, we use the matrix\nrepresentation of a network as a compact representation of neural networks in the worked\nexamples. For example, we use the matrix representation to present elements of the worked\nexample in Section 8.3.5[421], in which we step through the training of a feedforward neural\nnetwork using backpropagation.\n8.2.1 Artiﬁcial Neurons\nThe fundamental building block of a neural network is a computational model known as an\nartiﬁcial neuron. The template structure of these computational models was ﬁrst deﬁned\nby McCulloch and Pitts (1943). McCulloch and Pitts were trying to develop a model of the\nactivity in the human brain based on propositional logic. Their inspiration for this work\nwas linking the fact that propositional logic using a Boolean representation (TRUE/FALSE\nor 1/0) and neurons in the brain are somewhat similar, insofar as they have an all-or-none\ncharacter (i.e., they act as a switch that responds to a set of inputs by outputting either a\nhigh activation or no activation). As a result, they designed a model of the neuron that\nwould take in multiple inputs and then output either a high signal, a 1, or a low signal, a 0.\nThis McCulloch and Pitts model had a two-part structure.\nIn the ﬁrst stage of the McCulloch and Pitts model, each input is multiplied by a weight,\nand the results of these multiplications are then added together. This calculation is known\nas a weighted sum because it involves summing the weighted inputs. For example, for a\nweighted sum calculation over two inputs we require two predeﬁned weights. If we assume\nthe two predeﬁned weights are w1“0.5andw2“3and the two inputs are input 1“6\nandinput 2“7, then the weighted sum calculation would proceed as follows:\np0.5ˆ6q`p3ˆ7q“24\nMore generally, we can mathematically deﬁne the weighted sum calculation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":438,"page_label":"384","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"would take in multiple inputs and then output either a high signal, a 1, or a low signal, a 0.\nThis McCulloch and Pitts model had a two-part structure.\nIn the ﬁrst stage of the McCulloch and Pitts model, each input is multiplied by a weight,\nand the results of these multiplications are then added together. This calculation is known\nas a weighted sum because it involves summing the weighted inputs. For example, for a\nweighted sum calculation over two inputs we require two predeﬁned weights. If we assume\nthe two predeﬁned weights are w1“0.5andw2“3and the two inputs are input 1“6\nandinput 2“7, then the weighted sum calculation would proceed as follows:\np0.5ˆ6q`p3ˆ7q“24\nMore generally, we can mathematically deﬁne the weighted sum calculation\nz“wr0sˆdr0s`wr1sˆdr1s`¨¨¨` wrmsˆdrmsloooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon\nweighted sum\n(8.1)\nThere are, in fact, multiple ways of mathematically deﬁning a weighted sum calculation:\nwe can use theřsymbol to reduce the length of the equation, or we can represent it as a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":439,"page_label":"385","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 385\ndot product or as a matrix product\nz“wr0sˆdr0s`wr1sˆdr1s`¨¨¨` wrmsˆdrmsloooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon\nweighted sum\n“mÿ\nj“0wrjsˆdrjs\n“w¨dloomoon\ndot product“ wTdloomoon\nmatrix product“rw0,w1,..., wms»\n————–d0\nd2\n...\ndmﬁ\nﬃﬃﬃﬃﬂ(8.2)\nwhere dis a vector of m`1descriptive features, dr0s...drms; and wr0s...wrmsare\npm`1qweights. However, no matter which way we choose to deﬁne the weighted sum,\nthe operation remains the same.\nThe weights can either be excitatory (having a positive value, which increases the prob-\nability of the neuron activating) or inhibitory (having a negative value, which decreases\nthe probability of a neuron ﬁring). We have in fact already come across the weighted sum\ncalculation in Chapter 7[311]when we deﬁned the multivariate linear regression model (see\nEquation (7.9)[320]). Recall from Chapter 7[311]thatwr0sis the equivalent of the y-intercept\nin the equation of the line from high school geometry (see (7.2.1)[313]), and dr0sis a dummy\ndescriptive feature used for notational convenience and is always equal to 1 (see (7.9)[320]).\nIncluding this dummy feature makes the dandwvectors have the same length, and this\nallows us to write the equation as the dot product of the two vectors (i.e., w¨d). The\ninclusion of the weight wr0smeans that there is one more weight term than there are real\ndescriptive features. In other words, there are more weight parameters on the model than\nthere are dimensions in the input space. This extra weight term allows the model to deﬁne\nlines that do not go through the origin of the input space: setting the wr0sto a value other\nthan 0 translates the line deﬁned by the model away from the origin of the input space, just\nas changing the value of the y-intercept in the equation of a line moves a line up and down\nthey-axis away from the origin. This wr0sterm is often referred to as the bias parameter\nbecause in the absence of any other input, the output of the weighted sum is biased to be the\nvalue of wr0s. Technically, the inclusion of the bias parameter as an extra weight in this\noperation changes the function from a linear function on the inputs to an afﬁne function .\nAn afﬁne function is composed of a linear function followed by a translation; this simply\nmeans that the mapping deﬁned by an afﬁne function from inputs to outputs is linear but\nthe plot of this mapping does not necessarily pass through the origin. For example, if we","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":439,"page_label":"385","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"as changing the value of the y-intercept in the equation of a line moves a line up and down\nthey-axis away from the origin. This wr0sterm is often referred to as the bias parameter\nbecause in the absence of any other input, the output of the weighted sum is biased to be the\nvalue of wr0s. Technically, the inclusion of the bias parameter as an extra weight in this\noperation changes the function from a linear function on the inputs to an afﬁne function .\nAn afﬁne function is composed of a linear function followed by a translation; this simply\nmeans that the mapping deﬁned by an afﬁne function from inputs to outputs is linear but\nthe plot of this mapping does not necessarily pass through the origin. For example, if we\ndropped the dr0sdummy feature and then multiplied each of the real descriptive features\nby a weight and summed the results, we would be applying a linear function to the in-\nputs. A plot of this function would pass through the origin because there is no y-intercept","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":440,"page_label":"386","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"386 Chapter 8 Deep Learning\n(or bias term) included in the calculation. However, if we then add the bias term (the y-\nintercept) to the results of the linear function, we are translating the results of the linear\nfunction away from the origin. By including the bias term in the set of weights along with\na dummy descriptive feature, the function implemented by multiplying the weights by the\nextended descriptive features is now an afﬁne function. In much of this chapter we discuss\nmatrix representations and calculations, so it is worth noting here that a linear function can\nalways be represented by a single matrix multiplication and that an afﬁne transformation\ninmdimensions can always be represented as a linear function in m`1dimensions (as we\nhave done here) and consequently as a single matrix multiplication in m`1dimensions.\nIn this chapter, afﬁne and linear functions are so closely associated that we often use the\nterm linear to refer to afﬁne operations.\nIn the second stage of the McCulloch and Pitts model, the result of the weighted sum\ncalculation, z, is then converted into a high or a low activation by comparing the value of z\nwith a manually preset threshold; if zis greater than or equal to the threshold, the artiﬁcial\nneuron outputs a 1 (high activation), and otherwise it outputs a 0 (low activation). This\nthreshold is nothing more than a number that is selected by the designer of the artiﬁcial\nneuron. Using the symbol θto denote the threshold, the second stage of processing in the\nMcCulloch and Pitts model can be deﬁned\nMwpdq“#\n1ifzěθ\n0otherwise(8.3)\nIn the McCulloch and Pitts model the weights were manually set, but subsequently in\nthis chapter we explain how these weights can be learned from data using backpropaga-\ntion. However, the two-part structure of the McCulloch and Pitts model of the neuron is the\nblueprint for the neurons used in modern neural networks. The main difference between\nthe McCulloch and Pitts model and modern artiﬁcial neurons is that the thresholding func-\ntion is replaced by other functions. Frequently, the term activation function is used as a\ngeneral term to refer to whatever function is employed in the second stage of an artiﬁcial\nneuron, because the function maps the weighted sum value, z, into the output value, or ac-\ntivation, of the neuron. Until recently, one of the most popular functions used in artiﬁcial\nneurons was the logistic function introduced in Chapter 7 (see Equation (7.25)[342]and Fig-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":440,"page_label":"386","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tion. However, the two-part structure of the McCulloch and Pitts model of the neuron is the\nblueprint for the neurons used in modern neural networks. The main difference between\nthe McCulloch and Pitts model and modern artiﬁcial neurons is that the thresholding func-\ntion is replaced by other functions. Frequently, the term activation function is used as a\ngeneral term to refer to whatever function is employed in the second stage of an artiﬁcial\nneuron, because the function maps the weighted sum value, z, into the output value, or ac-\ntivation, of the neuron. Until recently, one of the most popular functions used in artiﬁcial\nneurons was the logistic function introduced in Chapter 7 (see Equation (7.25)[342]and Fig-\nure 7.12[343]). However, today the most popular choice of function for an activation function\nis the rectiﬁed linear activation function orrectiﬁer\nrecti f ierpzq“maxp0,zq (8.4)\nNote that neurons are often referred to as units , and they are distinguished by the type\nof activation function they use. Hence a neuron that uses a logistic activation function is\nreferred to as a logistic unit , and a unit that uses the rectiﬁer function is known as a rec-\ntiﬁed linear unit orReLU . We will explain why the logistic function became so popular","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":441,"page_label":"387","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 387\nFigure 8.2\nPlots for activation functions that have been popular in the history of neural networks.\nas an activation function when we introduce the backpropagation algorithm (see Section\n8.3[403]), and why the rectiﬁed linear function replaced it when we explain the vanishing\ngradient problem (see Section 8.4.1[434]). Figure 8.2[387]shows plots of some of the ac-\ntivation functions that have been popular in neural networks over the last few decades,\nincluding the threshold, logistic, tanh , and rectiﬁer linear functions.\nFigure 8.2[387]makes it apparent that a common characteristic of all of these activation\nfunctions is that they are not linear functions. This is not a coincidence. In fact, it is the\nintroduction of a non-linearity into the input to output mapping deﬁned by a neuron that\nenables an artiﬁcial neural network to learn complex non-linear mappings; indeed, it is this\nability to learn these complex non-linear mappings that makes artiﬁcial neural networks\nsuch powerful models, in terms of their ability to be accurate on complex tasks. We will\nexplain in more detail why we need non-linear activation functions in neurons in Section\n8.2.4[394].\nIf we use the symbol ϕto represent the activation function of a neuron, we can mathe-\nmatically deﬁne an artiﬁcial neuron as follows:\nMwpdq“ϕpwr0sˆdr0s`wr1sˆdr1s`¨¨¨` wrmsˆdrmsq\n“ϕ˜mÿ\ni“0wiˆdi¸\n“ϕ¨\n˝w¨dloomoon\ndot product˛\n‚“ϕ¨\n˝ wTdloomoon\nmatrix product˛\n‚“ϕ¨\n˚˚˚˚˝rw0,w1,..., wms»\n————–d0\nd2\n...\ndmﬁ\nﬃﬃﬃﬃﬂ˛\n‹‹‹‹‚\n(8.5)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":442,"page_label":"388","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"388 Chapter 8 Deep Learning\nΣϕ= 1d0 d1\nd2\nd3\nd4\ndmw0=biasw\n1\nw2\nw3\nw4\nwmMw(d)\n...\nFigure 8.3\nA schematic of an artiﬁcial neuron.\nwhere dis a vector of m`1descriptive features (including the dummy dr0sfeature),\ndr0s,dr1s,..., drms;wis a vector of m`1weights (including the bias term) wr0s,wr1s,\n...,wrms; andϕrepresents the activation function (threshold, tanh, logistic, or rectiﬁer,\netc.) that converts the result of the weighted sum to the output activation. Figure 8.3\npresents a graphical representation of this same function. In this ﬁgure the arrows carry\nactivations in the direction the arrow is pointing, the weight label on each arrow represents\nthe weight that will be applied to the descriptive feature carried along the arrow, theřsym-\nbol represents the weighted sum of the inputs, and the ϕsymbol represents the threshold\nfunction being applied to the result of the weighted sum to convert it into an activation.\n8.2.2 Artiﬁcial Neural Networks\nAn artiﬁcial neural network consists of a network of interconnected artiﬁcial neurons. Fig-\nure 8.4[390]illustrates the structure of a basic artiﬁcial neural network. In this network the\nneurons are organized into a sequence of layers. An artiﬁcial neural network can have any\nstructure, but a layer-based organization of neurons is common. There are two types of\nneurons in this network: sensing neurons and processing neurons. First, the two squares\non the left of the ﬁgure represent the two memory locations through which inputs are pre-\nsented to this network. These locations can be thought of as sensing neurons that permit\nthe network to sense the external inputs. Although we consider these memory locations\nas (sensing) neurons within the network, the inputs presented to the network are not trans-\nformed by these sensing neurons. This is why they are represented by squares to visually\ndistinguish them from the other neurons in the network that do transform their inputs. Each\nof the circles in the network represents a processing neuron that transforms its input using\nthe previously described two-step process of a weighted sum followed by an activation\nfunction.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":443,"page_label":"389","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 389\nThe arrows connecting the neurons in the network indicate the ﬂow of information\nthrough the network. Examining the arrows in the network shows that the input to the\nprocessing neurons can be any of the following: an external input presented to the network\nvia the sensing neurons, the output activation of another processing neuron in the network,\nor a dummy input that is always set to 1 (the input from a black circle). Also, each arrow\nis labeled with the weight that the neuron receiving the activation ﬂowing along that con-\nnection applies to that activation during the weighted sum calculation. The bias terms are\nthe weights on the dummy inputs. Notice that the indices in the weight subscripts are in\nreverse order from what might be expected: the ﬁrst subscript is the index of the neuron\nto which the activation is ﬂowing, and the second subscript is the index of the neuron that\ngenerated the activation. In the case of bias terms, this second index is always equal to\nzero. The advantage of this index ordering becomes clear when we describe how matrix\nmultiplications can be used to speed up the training and inference in neural networks (see\nSection 8.2.3[390]). Training a neural network involves ﬁnding a good set of values for these\nweights.\nThe network architecture shown in Figure 8.4[390]is an example of a feedforward net-\nwork . The deﬁning characteristic of a feedforward network is that there are no loops or\ncycles in the network connections that would allow the output of a neuron to ﬂow back\ninto the neuron as an input (even indirectly). In other words, in a feedforward network\nthe activations in the network always ﬂow forward through the sequence of layers. This\nnetwork is also a fully connected network because each of the neurons in the network is\nconnected in such a way that it receives inputs from all the neurons in the preceding layer\nand passes its output activation to all the neurons in the next layer. Subsequently in this\nchapter we introduce network architectures that are not feedforward and also architectures\nthat are not fully connected.\nWhen this network is processing a set of external inputs, the inputs are presented to the\nnetwork through sensing neurons in the input layer; this causes the neurons in the next\nlayer to generate activation signals in response to these inputs; and these activations ﬂow\nforward through the network until the output layer is reached, where the response of the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":443,"page_label":"389","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"connected in such a way that it receives inputs from all the neurons in the preceding layer\nand passes its output activation to all the neurons in the next layer. Subsequently in this\nchapter we introduce network architectures that are not feedforward and also architectures\nthat are not fully connected.\nWhen this network is processing a set of external inputs, the inputs are presented to the\nnetwork through sensing neurons in the input layer; this causes the neurons in the next\nlayer to generate activation signals in response to these inputs; and these activations ﬂow\nforward through the network until the output layer is reached, where the response of the\nnetwork to the inputs is the activations of the neurons in this ﬁnal output layer. The internal\nlayers in the network, which are neither input nor output layers, are called hidden layers .\nThedepth of a neural network is equal to the number of hidden layers plus the output\nlayer. Therefore, the network in Figure 8.4[390]has three layers. The number of layers\nrequired for a network to be considered deep is an open question; however, Cybenko (1988)\nproved that a network with three layers of (processing) neurons (i.e., two hidden layers and\nan output layer) can approximate any function to arbitrary accuracy. So, here we deﬁne\nthe minimum number of hidden layers necessary for a network to be considered deep as\ntwo; under this deﬁnition the network in shown Figure 8.4[390]would be described as a deep\nnetwork. However, most deep networks have many more than two hidden layers. Today","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":444,"page_label":"390","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"390 Chapter 8 Deep Learning\n3\n1 6\n4 8\n2 7\n5\nInput\nLayer 0Hidden\nLayer 1Hidden\nLayer 2Output\nLayer 3w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0\nFigure 8.4\nA schematic of a feedforward artiﬁcial neural network.\nsome deep networks have tens or even hundreds of layers. In Section 8.2.5[395]we discuss\nhow the depth of a neural network affects the ability of the network to represent and learn\nfunctions at different levels of complexity.\n8.2.3 Neural Networks as Matrix Operations\nIn Section 8.2.1[384]we described how adding a dummy feature dr0s“1to the input vec-\ntor of a neuron and also including the y-intercept term (or bias term) from the equation\nof a line as part of the weight vector of the neuron, as wr0s, permitted us to calculate the\nweighted sum of the neuron using a single dot product between the weight vector and the\ninput vector. One advantage of this was notational convenience; we were able to simplify\nEquation (8.2)[385]. A much more important advantage, however, is that it can enable sig-\nniﬁcant computational speedups in the training and application of a neural network. There\nare two ways that this can speed up neural network training and data processing:\n1.In a fully connected network, all the neurons in one layer receive activations from all\nthe neurons in the preceding layer; and, because all the neurons in a layer receive the\nsame vector of activations as inputs, we can calculate the weighted sum calculations\nfor all of these neurons using a single vector by matrix multiplication.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":445,"page_label":"391","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 391\n2.The fact that we can implement the calculation of the weighted sums for an entire\nlayer of neurons as one matrix multiplication operation can be generalized to imple-\nmenting an entire network as a sequence of matrix multiplications (one per layer of the\nnetwork). This means that if we represent a set of input vectors as a matrix of inputs,\nwe can get the network to process all the inputs in parallel. This parallelization of the\nprocessing of a number of examples by the network is particularly useful because the\nstandard practice to train a network is to present batches of examples to the network,\nrather than to present examples one at a time.\nIn order to explain how matrix multiplications are used in a neural network, we need to\nintroduce some notation and then deﬁne the order of the matrices in the multiplication. We\nuse the convention of a bold capital letter to denote a matrix and a superscript in parentheses\nto list the relevant layer. Using this notation and taking the second layer of neurons in a\nnetwork as an example, if we name the matrix containing the weights on the edges into\nLayer 2 as Wp2q, the column vector of activations coming from the neurons in Layer 1 as\nap1q, and the column vector of weighted sums for the neurons in Layer 2 as zp2q, then the\norder of the matrices in the multiplication operation that we use in this explanation is\nzp2q“Wp2qap1q(8.6)\nIn the matrix multiplication, each element in a row in the matrix on the left is multiplied by\nthe corresponding element in each column in the matrix on the right, and then the results\nof these multiplications are summed. This means that each element in zp2qis the result of\nmultiplying each element in a row in Wp2qby the corresponding element in the column\nvector ap1qand summing the results. This is why the indices on weights in Figure 8.4[390]\nwere reversed: if we store all the weights for each neuron in a layer in a row in the weight\nmatrix for the layer, then the weights are in the correct position when the weight matrix is\nmultiplied by a column vector of activations from the previous layer.\nFigure 8.5[392]illustrates how a neural network can be deﬁned as a sequence of matrix\nmultiplication operations, with an elementwise application of an activation function to the\nresults of each multiplication. The left side of Figure 8.5[392]presents a graph-based rep-\nresentation of a neural network; this network has a single hidden layer containing three","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":445,"page_label":"391","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"vector ap1qand summing the results. This is why the indices on weights in Figure 8.4[390]\nwere reversed: if we store all the weights for each neuron in a layer in a row in the weight\nmatrix for the layer, then the weights are in the correct position when the weight matrix is\nmultiplied by a column vector of activations from the previous layer.\nFigure 8.5[392]illustrates how a neural network can be deﬁned as a sequence of matrix\nmultiplication operations, with an elementwise application of an activation function to the\nresults of each multiplication. The left side of Figure 8.5[392]presents a graph-based rep-\nresentation of a neural network; this network has a single hidden layer containing three\nneurons and an output layer with a single neuron in it. The right side of Figure 8.5[392]\nillustrates the sequence of matrix operations that this network would carry out to process\na single input vector. Figure 8.5[392]highlights how bias terms are introduced into the ac-\ntivation vectors in such a way that they are aligned with the bias term weights during the\nmatrix multiplication. The ﬁrst column of each weight matrix (ﬁlled in black) contains the\nbias term weights for each neuron in the layer. Each activation vector is augmented with\na new ﬁrst row (ﬁlled in black) containing the dummy descriptive feature dr0s“1. This\nensures that the dummy descriptive feature (row 0) will be multiplied by the bias terms\nweights (column 0) in the multiplication operation.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":446,"page_label":"392","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"392 Chapter 8 Deep Learning\nHidden Layer\nWeight Matrix1Input Layer\n=\nz(1)ϕActivations\nHidden Layer\nOutput Layer\nWeight Matrix 1\nActivations\nHidden Layer=z(2)\nϕ\nOutput+d[0]\n=\n1\nFigure 8.5\nAn illustration of the correspondence between graphical and matrix representations of a neural net-\nwork. This ﬁgure is inspired by Figure 3.9 of Kelleher (2019).\nStepping through the processing of the network in Figure 8.5[392], the sequence of calcula-\ntions necessary to generate the activations for the neurons in the hidden layer is illustrated\nin the top row of the ﬁgure. The ﬁrst operation in this sequence is the multiplication of\nthe matrix containing the weights on the connections into the neurons in the hidden layer\nby the outputs of the input layer. The weight matrix is organized so that each row con-\ntains the weights for a single neuron. The leftmost element in each row (ﬁlled in black)\nis the bias term weight. The input vector has been augmented with the dummy feature\ndr0s “ 1, which is stored in the top row of the vector. Multiplying the weight matrix\nby the augmented input vector generates the vector of weighted sums for the hidden layer\nneurons, zp1q. The activation function, ϕ, is then applied to each of the elements in zp1qto\ngenerate the activations for each of the neurons in the hidden layer. Because this vector of\nactivations from the hidden layer is passed on to another layer for processing, it has to be\naugmented with the dummy feature dr0s“1; this is represented by the label on the arrow\nlinking the top row of operations with the bottom row.\nThe bottom row illustrates the operations carried out in the output layer of the network.\nAs with the hidden layer, the ﬁrst operation is the multiplication of the layer’s weight\nmatrix by the (augmented) vector of activations from the previous layer. The weight matrix\nis again organized with one row of weights per neuron and with the bias term weight as the\nﬁrst element in the row (shown in black). There is only one neuron in the output layer, and\nso there is only one row in this weight matrix. The multiplication operation calculates the\nweighted sum for the output neuron zp2q, and this is passed through the neuron’s activation\nfunctionϕto generate the output of the network.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":447,"page_label":"393","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 393\nHidden Layer\nWeight Matrix1111Input Layer\n=\nz(1)ϕActivations\nHidden Layer\nOutput Layer\nWeight Matrix1111\nActivations\nHidden Layer=z(2)\nϕ\nOutput+d[0]=1\nFigure 8.6\nAn illustration of how a batch of examples can be processed in parallel using matrix operations.\nThe fact that we can use a sequence of matrix operations to implement how a neural\nnetwork processes a single example can be generalized to processing a number of examples\nin parallel. Figure 8.6[393]illustrates how the same sequence of operations shown in Figure\n8.5[392]to process one example can be used to enable the same network to process four\nexamples in parallel. This is achieved by replacing the input vector containing a single\nexample with an input matrix containing multiple examples. This input matrix is organized\nso that each column contains the feature vector for a single example. The gray columns in\neach of the activation and zmatrices trace the processing of the second example through\nthe sequence of operations. Notice that the weight matrices do not change shape, nor does\nthe sequence of operations, and the activation functions ϕare still applied elementwise to\nall the elements in zmatrices. The main difference in processing examples in parallel is\nthat instead of augmenting the input and activation matrices with a single dummy feature\nvalue, they are now augmented with a row of dummy features values, one per example.\nThe representation of a network as a sequence of matrix operations provides a transparent\nview on the depth of the network: a network depth is equal to the number of layers that\nhave a weight matrix associated with them. This is why the input layer is not counted as\npart of the depth of the network.\nDeep neural networks can contain millions of neurons. Also, as subsequently discussed\n(and similar to the training regime used for regression models in Chapter 7[311]), neural\nnetworks are trained by iteratively running the network on examples sampled from large\ndatasets. Consequently, unless care is taken, training a network can take an inordinately\nlong time, compared with other machine learning models. The computational speedups","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":448,"page_label":"394","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"394 Chapter 8 Deep Learning\nachieved by using matrix multiplications address the computational challenge of iterating\nthrough both a large number of neurons and a large number of examples. Implementing a\nnetwork as a sequence of matrix multiplications (1) speeds up the calculation of a weighted\nsum across each layer in the network, and (2) enables the network to parallelize the pro-\ncessing of examples. Both these speedups enable us to remove expensive forloops from\nthe implementation and training of a network: the ﬁrst removes a forloop over the neurons\nin a layer, and the second removes a forloop over the examples in a dataset. A further ben-\neﬁt of implementing a neural network using matrix operations is that it enables the use of\nspecialized hardware known as graphical processing units ( GPUs ). GPUs are designed to\ncarry out matrix operations very quickly. Implementing a neural network as a sequence of\nmatrix operations and then running the network on GPUs is now standard in deep learning.\n8.2.4 Why Are Non-Linear Activation Functions Necessary?\nA multi-layer feedforward neural network that uses only linear neurons (i.e., neurons that\ndo not include a non-linear activation function) is equivalent to a single-layer network with\nlinear neurons; in other words, it can represent only a linear mapping on the inputs. This\nequivalence is true no matter how many hidden layers we introduce into the network. We\nexplain subsequently why this is the case, but ﬁrst we note that for ease of exposition\nin this discussion, we will ignore the bias terms in the weights of a neuron. This does\nnot affect the generality of the discussion because, as we discussed earlier, a weighted\nsum that includes a bias term as a weight implements an afﬁne transformation composed\nof ﬁrst applying a linear function to the inputs and then translating the result by a bias.\nTherefore, the inclusion of the translation by the bias term allows the weighted sum to\ndeﬁne a linear function on its inputs that does not pass through the origin. However, in this\nsection, we focus on showing how a sequence of linear layers can be replaced by a single\nlinear function (as distinct from an afﬁne transformation). Furthermore, the combination\nof the translations that would be applied by including bias terms in each layer can be\nreplaced by applying a single translation (or single bias term) at the end of the processing.\nConsequently, here we assume that the weight matrices do not include bias terms.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":448,"page_label":"394","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of ﬁrst applying a linear function to the inputs and then translating the result by a bias.\nTherefore, the inclusion of the translation by the bias term allows the weighted sum to\ndeﬁne a linear function on its inputs that does not pass through the origin. However, in this\nsection, we focus on showing how a sequence of linear layers can be replaced by a single\nlinear function (as distinct from an afﬁne transformation). Furthermore, the combination\nof the translations that would be applied by including bias terms in each layer can be\nreplaced by applying a single translation (or single bias term) at the end of the processing.\nConsequently, here we assume that the weight matrices do not include bias terms.\nWith that caveat regarding bias terms stated, we return to the question of why non-linear\nactivation functions are necessary in a neural network. It turns out that understanding that\na neural network can be implemented as a sequence of matrix multiplications with the non-\nlinearity of the activation functions introduced between the matrix multiplications can help\nanswer this question.\nImagine a very simple two layer network (one hidden layer and an output layer) with lin-\near neurons (neurons that don’t include an activation function, so that the output activation\nis just the weighted sum of the inputs).3The calculation of the activations for the neurons\n3. This explanation is inspired by the discussion in Reagen et al. (2017, p. 14).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":449,"page_label":"395","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 395\nin the ﬁrst layer can be expressed as follows:\nAp1q“Wp1qAp0q(8.7)\nSimilarly, the calculations of the activations for the second layer can be expressed\nAp2q“Wp2qAp1q(8.8)\nIf we replace Ap1qin Equation (8.8)[395]with the right-hand side of Equation (8.7)[395]we get\nAp2q“Wp2q´\nWp1qAp0q¯\n(8.9)\nHowever, matrix multiplication is associative (i.e., XpYZqis equal topXYqZ); this means\nthat we can rewrite Equation (8.9)[395]\nAp2q“´\nWp2qWp1q¯\nAp0q(8.10)\nAlso, Wp2qandWp1qcan be replaced by the matrix that is generated by their product;\nletting W1“Wp2qWp1qwe get\nAp2q“W1Ap0q(8.11)\nFurthermore, the transformation implemented by the single weight matrix, generated by\nthe product of weight matrices of the linear layers, will also implement a linear transfor-\nmation on the input data. Therefore, this rewriting shows that the output of this two-layer\nnetwork with linear neurons is equivalent to a single-layer network with linear neurons.\nThis reduction of a network to a single weight matrix by using a matrix product can be\ndone no matter how many layers there are in a network, so long as none of the network\nlayers includes non-linear activation functions.\nThis analysis shows that adding layers to a network without including a non-linear acti-\nvation function between the layers appears to add complexity to the network, but in reality\nthe network remains equivalent to a single-layer linear network. The conclusion is that in\norder to create networks that can represent complex non-linear functions, it is not enough\nto add layers; we must also include non-linearities between these layers. Fortunately, we\ndo not need to add complex non-linearities between the layers; introducing simple non-\nlinearities, such as the logistic or rectiﬁer functions, between each layer is sufﬁcient to\nenable neural networks to represent arbitrarily complex functions, as long as the network\ncontains enough layers; in other words, as long as the networks are deep enough.\n8.2.5 Why Is Network Depth Important?\nGiven that the preceding analysis in Section 8.2.4[394]showed that a network with multiple\nlayers of linear neurons is equivalent to a single-layer network of linear functions, a natural\nquestion is, why is adding extra layers to a network (even with non-linearities) a useful\nthing to do? This is a particularly pertinent question, considering that the adoption of the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":450,"page_label":"396","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"396 Chapter 8 Deep Learning\nterm deep learning in the mid-2000s to describe modern neural networks was to emphasize\nthat these modern networks are deeper than most of the networks that were developed\nbetween the 1940s and the early 2000s.\nThe fundamental reason to add layers to a network is to increase the representational\ncapacity of the network. The representational capacity of a network is the set of functions\n(or mappings from inputs to outputs) that the network can implement as its weights are\nvaried (Reed and Marks, 1999). A network can represent a function if a set of weights\nexist for that network for which the network implements the function. Understanding, at\nleast in principle, whether a particular network architecture is capable of representing a\nfunction, or not, is very important, because the fundamental task in predictive modeling is\nto learn functions from data, and if the network cannot represent a function, then it cannot\nlearn it, no matter how much data we provide to the training algorithm.\nOne way to understand why depth is important to the representational capacity of a neural\nnetwork is to consider what types of functions a network that has only a single layer of\nprocessing neurons, such as the one shown in Figure 8.7[397], is capable of representing.\nNotice that each of the neurons in the output layer (Neurons 3, 4, and 5) are independent\nof each other; they receive no information from each other. In fact, this ﬁgure could be\nreconﬁgured as three separate neurons, each receiving the same input vector. For ease of\nexplanation, we will assume that each of these three neurons uses a threshold activation\nfunction. This means that each of the neurons in the output layer is equivalent to the\nMcCulloch and Pitts neuron described in Section 8.2.1[384]. These thresholded units are\nalso known as perceptron networks (Rosenblatt, 1958); we introduce this terminology\nhere because it is useful in the following discussion.\nEquation (8.3)[386]deﬁnes how the McCulloch and Pitts neuron, or perceptron network,\nmaps an input vector to an output activation. If we compare this equation with Equation\n(7.24)[341], it is apparent that a perceptron network is identical to a multivariate linear re-\ngression model with a threshold applied to it. This means that, similar to the thresholded\nmultivariate linear regression models, a perceptron is able to represent a function that dis-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":450,"page_label":"396","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"McCulloch and Pitts neuron described in Section 8.2.1[384]. These thresholded units are\nalso known as perceptron networks (Rosenblatt, 1958); we introduce this terminology\nhere because it is useful in the following discussion.\nEquation (8.3)[386]deﬁnes how the McCulloch and Pitts neuron, or perceptron network,\nmaps an input vector to an output activation. If we compare this equation with Equation\n(7.24)[341], it is apparent that a perceptron network is identical to a multivariate linear re-\ngression model with a threshold applied to it. This means that, similar to the thresholded\nmultivariate linear regression models, a perceptron is able to represent a function that dis-\ntinguishes between two classes of inputs if these two classes are linearly separable . Figure\n7.10[340]illustrates a linearly separable dataset; in that case the two classes of inputs were\ngood generators and faulty generators. These two classes are linearly separable because as\nFigure 7.10[340](b) shows, it is possible to draw a single straight line that separates one class\nfrom the other. This line is known as a decision boundary .\nThe fact that our single-layer network in Figure 8.7[397]contains three independent neu-\nrons means that the network has the potential to represent three separate linear decision\nboundaries. However, none of the neurons in the output layer are capable of representing\na non-linear decision boundary on the inputs, and therefore the network as a whole cannot\nrepresent a non-linear function.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":451,"page_label":"397","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 397\n3\n1\n4\n2\n5\nInput\nLayer 0Output\nLayer 1w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w3,0\nw4,0\nw5,0\nFigure 8.7\nA single-layer network.\nFigure 8.8[398]illustrates the distinction between linearly separable and non-linearly sep-\narable functions, using examples from Boolean logic: AND, OR, and XOR functions.\nThese three functions have the same structure; they all take two inputs that can be either\nTRUE orFALSE , and they return either TRUE orFALSE . The AND function returns TRUE\nif both inputs are TRUE andFALSE otherwise. The plot on the left of Figure 8.8[398]shows\nthe input space for the AND with the convention that FALSE has been mapped to 0and\nTRUE has been mapped to 1.4In the plot each of the four possible input combinations is\nlabeled as either triggering a TRUE response (shown in the ﬁgure by a clear dot) or FALSE\n(shown in the ﬁgure by a black dot). As the ﬁgure shows, it is possible to draw a straight\nline between these two classes of inputs. The middle plot shows a similar plot for the OR\nfunction, and again it is possible to draw a single straight line to separate the two classes\nof inputs. The XOR function returns TRUE if either but not both of its inputs are TRUE .\nThe plot on the right of Figure 8.8[398]shows the input space for the XOR and labels the in-\nput combinations as resulting in TRUE responses or FALSE responses. It is apparent from\nthis ﬁgure that it is not possible to separate the inputs that generate TRUE from those that\n4. The axes in Figure 8.8[398]are slightly offset for ease of reading.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":452,"page_label":"398","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"398 Chapter 8 Deep Learning\nx1x2\nAND\nx1x2\nORx1x2\nXOR\nFigure 8.8\nThe logical AND and OR functions are linearly separable, but the XOR is not. This ﬁgure is Figure\n4.2 of Kelleher (2019) and is used here with permission.\ngenerate FALSE with a single straight line. This is why the XOR function is not linearly\nseparable.\nIt may be somewhat surprising, but although the XOR function is very simple, a per-\nceptron cannot represent it because it is not linearly separable. This ﬁnding is notorious in\nthe history of neural networks. Through the 1950s and 1960s there was a lot of interest in\nneural networks. However, in 1969 Marvin Minsky and Seymour Papert published a book\nentitled Perceptrons that was highly critical of neural networks and in particular focused\non the fact that single-layer networks (perceptrons) were not able to represent non-linearly\nseparable functions (Minsky and Papert, 1969). This book had a major impact and is\nattributed with killing interest in neural networks for nearly a decade. In hindsight, the\ngeneral criticisms of neural networks made by Minsky and Papert have not stood the test\nof time; however, their criticism of the representational capacity of single-layer networks\nis valid.\nThe representational limitation of single-layer networks can be overcome by adding a\nsingle hidden layer to the network. Indeed, it is possible to represent the XOR function\nusing a very simple two-layer network. Figure 8.9[399]illustrates such a network: the left\nuses a directed graph representation to show the topology of the network, and the weights\non the connections; the right uses the matrix representation of the network to illustrate it\nprocessing the four possible input combinations to the XOR function in parallel. All the\nneurons in this network use the following threshold activation function:\nMwpdq“#\n1ifzě1\n0otherwise(8.12)\nVertically aligning the columns in the input matrix with the columns in the output matrix\nshows that the network correctly maps all four inputs combinations to XOR outputs: ă\n0,0ąÑ 0,ă0,1ąÑ 1,ă1,0ąÑ 1, andă1,1ąÑ 0.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":453,"page_label":"399","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1 3\n5\n2 4w3,1=0.5\nw\n4,1\n=1\nw3,2=0.5\nw4,2=1w\n5,3= −\n1\nw5,4=1w3,0=0\nw4,0=0w5,0=000.50.5\n01.01.0\nHidden Layer\nWeight Matrix1111\n0011\n01014 Possible\nInput Combinations\n=0.00.50.51.0\n0.01.01.02.0\nz(1)ϕ0001\n0111Hidden Layer\nActivations\n1 Col. Per Input\n0−11Output Layer\nWeight Matrix1111\n0001\n0111\nActivations\nHidden Layer=0110z(2)\nϕ 0110\nOutput ×4\n1 Col. Per Input+d[0]=1\nFigure 8.9\n(left) The XOR function implemented as a two-layer neural network. (right) The network processing the four possible input combinations, one\ncombination plus bias input per column: [ bias,FALSE ,FALSE ]Ñ[1,0,0]; [ bias,FALSE ,TRUE ]Ñ[1,0,1]; [ bias,TRUE ,FALSE ]Ñ[1,1,0];\n[bias,TRUE ,TRUE ]Ñ[1,1,1].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":454,"page_label":"400","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"400 Chapter 8 Deep Learning\nThe XOR example shows that a network with a single hidden layer can represent a\nnon-linearly separable function. There are, in fact, mathematical proofs that show that\nneural networks with a single hidden layer are universal approximators ; i.e., they can\nexactly represent any continuous function of multiple inputs. These proofs can be broadly\ncategorized into (1) proofs that bound the number of neurons required in the network but\nassume that the neurons in the network use complex activation functions (more complex\nthan the smooth functions, such as the logistic function, used in most neural networks) as\npart of their internal mapping of inputs to an output activation; and (2) proofs that assume\nthat the neurons use smooth functions (such as the logistic, sigmoid, or rectiﬁer) but this\nsimplicity is at the cost of having an exponential number (with respect to the dimensions\nof the inputs to the network) of neurons in the hidden layer of the network.\nThe ﬁrst category of proofs that assume the use of complex functions within the neu-\nrons is based on the foundational mathematical theorems from Kolmogorov (1963) and\nSpercher (1965). Hecht-Nielsen (1987) showed how these proofs could be applied to neu-\nral networks. However, as previously noted, these proofs assume that the neurons in the\nnetwork include functions that are much more complex (or rougher) than the smooth acti-\nvation functions used in most networks (such as the logistic functions). In fact, these proofs\nfail if the neurons are restricted to using smooth functions. This assumption is important\nbecause these internal activation functions may be as complex as, or even more complex\nthan, the target function that the network is attempting to represent. Consequently, these\ntheorems are not relevant to the practical task of training a network to learn a function be-\ncause the inclusion of these complex functions within the neurons of the network shifts the\nlearning burden from approximating the target function to ﬁnding these complex internal\nfunctions (Reed and Marks, 1999).\nThe second category of proofs assumes the use of smooth functions within neurons.\nTheuniversal approximation theorem (Hornik et al., 1989; Cybenko, 1989) proved that\nneural networks with a single hidden layer of neurons using smooth functions (such as sig-\nmoids or logistic functions) can approximate any bounded continuous function, provided","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":454,"page_label":"400","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"than, the target function that the network is attempting to represent. Consequently, these\ntheorems are not relevant to the practical task of training a network to learn a function be-\ncause the inclusion of these complex functions within the neurons of the network shifts the\nlearning burden from approximating the target function to ﬁnding these complex internal\nfunctions (Reed and Marks, 1999).\nThe second category of proofs assumes the use of smooth functions within neurons.\nTheuniversal approximation theorem (Hornik et al., 1989; Cybenko, 1989) proved that\nneural networks with a single hidden layer of neurons using smooth functions (such as sig-\nmoids or logistic functions) can approximate any bounded continuous function, provided\nthere are sufﬁcient neurons in the hidden layer of the network. For many functions the re-\nquirement of sufﬁcient neurons turns out to be exponential with respect to the dimensions\nof the network inputs. More recently, Leshno et al. (1993) extended the universal ap-\nproximation theorem to include networks with neurons using rectiﬁer activation functions;\nhowever, Mont ´ufar (2014) has shown that these networks can also require an exponential\nnumber of neurons in the hidden layer. The requirement that networks with a single hid-\nden layer and using smooth activation functions have very wide hidden layers is a serious\nshortcoming: it may result in the requirement that the network has at least one hidden unit\nfor each input conﬁguration that is being distinguished (Goodfellow et al., 2016). Such a\nnetwork conﬁguration is likely to result in the network overﬁtting the training data (i.e.,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":455,"page_label":"401","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.2 Fundamentals 401\nmemorizing all the training examples, including the noise, rather than learning the general\npatterns in the data).\nThere are two points worth highlighting:\n1.Many computational models have the ability of universal approximation of bounded\ncontinuous functions; this property is not unique to neural networks (Reed and Marks,\n1999). However, these results are still important because they show that neural net-\nworks with at least one hidden layer do have the representational capacity to approxi-\nmate most functions that we would like them to. If neural networks were not capable\nof universal approximation, then they would be much less useful for prediction.\n2.The fact that a neural network can represent a function does not guarantee that we\nwill be able to train it to learn the function: the training algorithm may not be able\nto ﬁnd the correct set of weights, or it may choose weights that overﬁt the data (i.e.,\nchoose the wrong function) (Goodfellow et al., 2016). For these reasons, the inclusion\nof more complex functions with the neurons of the network or the inclusion of more\nneurons within the hidden layer of the network makes the learning task more difﬁcult.\nThe learning challenges raised by the second point in the preceding list highlight the main\nmotivation for using networks with more than one hidden layer. It is often (but not always)\nthe case that using a deeper network can drastically reduce the number of neurons required\nto enable the network to represent a target function. For example, some functions can be\nimplemented exactly using a small neural network with two layers but require an inﬁnite\nnumber of nodes to approximate with a single hidden layer (Makhoul et al., 1989; Reed\nand Marks, 1999). Furthermore, Cybenko (1988) proved that a network with at least three\nlayers (two hidden and one output) using sigmoid activation functions can approximate\nany function (not just bounded continuous functions) with arbitrary accuracy. This does\nassume that the width of the layers is sufﬁcient to permit the network to represent the\nfunction; however, for a given function the number of neurons required in each of the\nhidden layers is not known, in general.\nFigure 8.10 shows how the representational capacity of a neural network increases as\nmore layers are added to the network. The neurons in this network use a simple threshold\nactivation function, but the intuition holds for logistic functions and the rectiﬁer function","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":455,"page_label":"401","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"and Marks, 1999). Furthermore, Cybenko (1988) proved that a network with at least three\nlayers (two hidden and one output) using sigmoid activation functions can approximate\nany function (not just bounded continuous functions) with arbitrary accuracy. This does\nassume that the width of the layers is sufﬁcient to permit the network to represent the\nfunction; however, for a given function the number of neurons required in each of the\nhidden layers is not known, in general.\nFigure 8.10 shows how the representational capacity of a neural network increases as\nmore layers are added to the network. The neurons in this network use a simple threshold\nactivation function, but the intuition holds for logistic functions and the rectiﬁer function\nbecause these functions also divide the input space of a neuron into two half-spaces: one\nin which the neuron activates and one in which it doesn’t activate. Each subsequent layer\nin the network is able to use the functions learned by the previous layer to construct a more\ncomplex function; this process of using the outputs from one or more functions and inputs\nto another function to create a new function is known as function composition . The term\nrepresentation learning is sometimes used to describe what the neurons in the hidden\nlayers of a network are doing; in a sense, each subsequent layer in the network projects the\ninputs it receives into a new representation, and the output layer of the network then learns\na mapping from a learned representation to the ﬁnal output. When this representation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":456,"page_label":"402","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"402 Chapter 8 Deep Learning\n3\n1 6\n4 8\n2 7\n5\nInput\nLayer 0Hidden\nLayer 1Hidden\nLayer 2Output\nLayer 3w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0\nFigure 8.10\nAn illustration of how the representational capacity of a network increases as more layers are added\nto the network. The squares overlaying connections coming out of a neuron illustrate the regions of\nthe input space that the neurons activate on: neurons output high activations in response to inputs\npatterns from the white region in the corresponding square, and low (or no activations) in response to\ninput patterns from the gray regions in the corresponding square. This ﬁgure was inspired by Figure\n4.2 in (Reed and Marks, 1999) and Figure 3.10 in (Marsland, 2011).\nlearning is successful, the mapping from the representation the output layer receives to\nthe target output is simpler than the mapping from the original input features to the target\nfeature, and this can result in the model’s being more accurate. The ability to automatically\nlearn useful representations (features) from data is one of the reasons why deep networks\nhave proven to be so successful on so many tasks. Figure 8.10 illustrates how using just\nthree layers and threshold activation functions, a network is able to represent a function\nthat maps to a convex region. With the addition of more neurons in each layer, the network\ncould represent a function that maps to multiple disconnected convex regions and reﬁnes\nthe shapes of these regions. Any function can be approximated by combining disconnected\nregions.\nIf a three-layer network can represent any function with arbitrary accuracy, is it helpful to\ngo deeper, or is it better to make the layers wider? There is growing empirical evidence that\ncreating deeper networks improves the generalization ability of models across a number of","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":457,"page_label":"403","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 403\ntasks.5However, adding depth to a network comes with a cost. As we see when we discuss\nthevanishing gradient problem, adding depth to a network can slow down the rate at\nwhich a network learns. Therefore, when we wish to increase the representational power\nof a network, there is often a trade-off between making a network deeper and making the\nlayers wider. Finding a good equilibrium for a given prediction task involves experimenting\nwith different architectures to see which performs best.\nIn summary, neural networks with no hidden layer (i.e., perceptrons) cannot represent\nnon-linearly separable functions. Neural networks with a single hidden layer have been\nproven to be capable of universal approximation of (bounded) continuous functions as\nlong as either (1) complex functions are integrated into the structure of the neurons, or\n(2) the hidden layer of the network is sufﬁciently (potentially exponentially) wide. Neural\nnetworks with two hidden layers and using smooth activation functions can represent any\nfunction and generally can do so using fewer neurons than networks with only a single\nhidden layer. Also, as Figure 8.10 illustrates, as layers are added to a network, neurons\nin subsequent layers are able to use the representations learned by the preceding layer\nas building blocks to construct more complex functions. Overall there is a general trend\nthat deeper networks have better performance than shallower networks, and that deeper\nnetworks are often more efﬁcient in terms of the number of neurons they require. How-\never, as networks become deeper they can become more difﬁcult to train. The next section\ndescribes how we can train networks with multiple layers of neurons using the backprop-\nagation algorithm and explains how the vanishing gradient problem can negatively affect\nthe training of deep networks.\n8.3 Standard Approach: Backpropagation and Gradient Descent\nA neuron is structurally equivalent to a logistic regression model: comparing Equation\n(8.5)[387]and Equation (7.26)[342]makes it apparent that both models calculate a weighted\nsum over an input vector and then pass the weighted sum value through a non-linear func-\ntion. Indeed, when a neuron uses the logistic function as an activation function, then these\ntwo models are identical. One consequence of this is that if a neuron uses a logistic acti-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":457,"page_label":"403","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"describes how we can train networks with multiple layers of neurons using the backprop-\nagation algorithm and explains how the vanishing gradient problem can negatively affect\nthe training of deep networks.\n8.3 Standard Approach: Backpropagation and Gradient Descent\nA neuron is structurally equivalent to a logistic regression model: comparing Equation\n(8.5)[387]and Equation (7.26)[342]makes it apparent that both models calculate a weighted\nsum over an input vector and then pass the weighted sum value through a non-linear func-\ntion. Indeed, when a neuron uses the logistic function as an activation function, then these\ntwo models are identical. One consequence of this is that if a neuron uses a logistic acti-\nvation function, then we can train the neuron in the same way that we train a logistic re-\ngression function: using the gradient descent algorithm (introduced in Chapter 7[311]) and\nwith the weight update rule deﬁned in Equation (7.33)[346]. And, if the neuron implements\na different activation function, then so long as the function is differentiable, we modify the\nweight update rule (Equation (7.33)[346]) by replacing the derivative of the logistic function\nwith the derivative of the new activation function, and apply the gradient descent algorithm\n5. See Goodfellow et al. (2016, p. 195) for relevant references.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":458,"page_label":"404","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"404 Chapter 8 Deep Learning\nas before.6Training a neuron by initializing its weights and then iteratively updating these\nweights to reduce the error of the neuron on training examples ﬁts with Hebb’s Postulate\n(introduced in Section 8.1[382]) that learning occurs in the brain through a process involving\nchanges in the connections between neurons.\nWe can, in fact, use the gradient descent algorithm to train a single-layer network (or\nperceptron), such as the one shown in Figure 8.7[397]. However, once we introduce one or\nmore hidden layers into a network, it becomes more difﬁcult to use the gradient descent\nalgorithm to train the network. The reason for this is that the gradient descent algorithm\nuses the error gradient of a model (neuron or regression model) to update the weights on\nthe inputs into the model. Although in a neural network it is relatively straightforward to\ncalculate the gradient of the error for neurons in the output layer by directly comparing the\nactivations of these neurons with the expected outputs, it is not possible to directly compare\nthe activation of a hidden neuron with the expected activation for that neuron, and so we\ncannot directly calculate an error gradient for hidden neurons.\nConsequently, before we can use the gradient descent algorithm to update the weights\nof a neuron in a hidden layer of the network, we must calculate a measure of how the\nneuron contributed to the overall error of the network at the output layer. Calculating this\nmeasure for each neuron in a network is known as the blame assignment problem. The\nbackpropagation algorithm solves the blame assignment problem. Once we have used the\nbackpropagation algorithm to solve the blame assignment problem for all the neurons in\nthe network, we can then use the weight update rule from the gradient descent algorithm to\nupdate the weights for each of the neurons in the network. The following sections explain\n(1) the general structure of the backpropagation algorithm, including the concept of a δ\n(pronounced delta ) term that describes the rate of change of the error of a network with\nrespect to changes in the weighted sum of a neuron; (2) the process used to calculate a\nδterm for each neuron in the network (this process involves backpropagating the error\ngradients of the network and is the process that gave the algorithm its name); and (3) how\ntheδterms are used in conjunction with the gradient descent weight update rule to update","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":458,"page_label":"404","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the network, we can then use the weight update rule from the gradient descent algorithm to\nupdate the weights for each of the neurons in the network. The following sections explain\n(1) the general structure of the backpropagation algorithm, including the concept of a δ\n(pronounced delta ) term that describes the rate of change of the error of a network with\nrespect to changes in the weighted sum of a neuron; (2) the process used to calculate a\nδterm for each neuron in the network (this process involves backpropagating the error\ngradients of the network and is the process that gave the algorithm its name); and (3) how\ntheδterms are used in conjunction with the gradient descent weight update rule to update\nthe weights of the network. Following these explanations we present a worked example to\nshow how the backpropagation and gradient descent algorithm can be used in partnership\nto train a neural network with hidden layers.\n8.3.1 Backpropagation: The General Structure of the Algorithm\nThe backpropagation algorithm begins by initializing the weights of the network. For the\npurposes of this explanation we assume that the weights are initialized to random values\nclose to zero (e.g., by sampling from a normal distribution with mean µ“0.0andσ“\n6. Subsequently, in this chapter we illustrate how this can be done for ReLU, neurons using rectiﬁer activation\nfunctions.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":459,"page_label":"405","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 405\n0.1). Obviously, a network with random weights is unlikely to implement a useful function.\nHowever, it will implement a function, and once the network has been initialized we can\nnow train the network to implement a useful function by iteratively presenting examples to\nthe network and using the error of the network on the examples to update the weights so\nthat the network converges on a set of weights that implement a useful function relative to\nthe patterns in the training data.\nThe key step in this iterative weight update process is solving the blame assignment\nproblem. The general structure of the backpropagation algorithm is a two-step process\nthat results in an assignment of blame (or an error gradient) to each of the neurons in the\nnetwork:\n1.Forward Pass An input pattern is presented to the network, and the activations ﬂow\nforward through the network until an output is generated.\n2.Backward Pass The error of the network is calculated by comparing the output gen-\nerated by the forward pass with the target output speciﬁed in the dataset. This error is\nshared back (backpropagated) through the network on a layer-by-layer basis until the\ninput layer is reached. During this backward pass, an error gradient for each neuron is\ncalculated. These error gradients can then be used by the gradient descent weight up-\ndate rule to update the weights for each neuron. This backward pass gives the network\nits name: backpropagation.\nFigure 8.11[406]illustrates the forward pass in a little more detail. This ﬁgure highlights\nthe calculation of the weighted sum at each neuron (e.g., z3represents the weighted sum\ncalculation at Neuron 3), and the activations for each neuron (e.g., a3represents the ac-\ntivation generated by Neuron 3). The motivation for highlighting the calculation of the\nweighted sum and activations during the forward pass of the algorithm is that these values\nare stored in memory after they are calculated and then used as part of the calculations\ninvolved in the backpropagation of the error gradients during the backward pass of the\nalgorithm.\nFigure 8.12[407]illustrates the backward pass of the algorithm. Before describing the\nbackward pass of the algorithm, we will distinguish between two types of error gradients\nthat are calculated when we are training a neural network using backpropagation:\n1.The ﬁrst type of error gradient is the rate of change of the network error with respect","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":459,"page_label":"405","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tivation generated by Neuron 3). The motivation for highlighting the calculation of the\nweighted sum and activations during the forward pass of the algorithm is that these values\nare stored in memory after they are calculated and then used as part of the calculations\ninvolved in the backpropagation of the error gradients during the backward pass of the\nalgorithm.\nFigure 8.12[407]illustrates the backward pass of the algorithm. Before describing the\nbackward pass of the algorithm, we will distinguish between two types of error gradients\nthat are calculated when we are training a neural network using backpropagation:\n1.The ﬁrst type of error gradient is the rate of change of the network error with respect\nto changes in the weighted sum calculation of a neuron . One of these error gradients\nis calculated for each neuron in the network. These error gradients are often denoted\nusing the δsymbol with a subscript indicating the relevant neuron. For example,\nδkwould denote the error gradient for neuron k. It is these δerror gradients that\nare calculated and backpropagated during the backward pass of the backpropagation\nalgorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":460,"page_label":"406","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"406 Chapter 8 Deep Learning\n3\n1 6\n4 8\n2 7\n5w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.5\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0z3a3\nz4a4\nz5a5z6a6\nz7a7z8a8\nActivations ﬂow from inputs to outputs\nFigure 8.11\nThe calculation of the zvalues and activations of each neuron during the forward pass of the back-\npropagation algorithm. This ﬁgure is based on Figure 6.5 of Kelleher (2019).\n2.The second error gradient is the rate of change of the network error with respect to\nchanges in the weights of the network . There is one of these error gradients for each\nweight in the network. These error gradients are used to update the weights of the\nnetwork. However, the calculation of these error gradients involves the δterms. So,\nwe must use the backpropagation algorithm to calculate the δterms, and once this is\ndone we can calculate the error gradients for each weight. Note that there is only a\nsingle δper neuron, but typically there are many weights associated with each neuron,\nand so the δfor a neuron will often be used in the calculation of multiple weight error\ngradients, once for the weight on each connection into the neuron.\nReturning to the backward pass of the backpropagation algorithm, Figure 8.12[407]shows\nthat the backward pass begins by calculating the δfor each of the neurons in the output\nlayer of the network. In the simplest case, this might be calculated by subtracting the\nactivation of each neuron in the output layer from the target output speciﬁed in the dataset.\nTheδs for the output neurons are then shared back to the neurons in the last hidden layer.\nThis is done by assigning a portion of the δof each output neuron to each of the hidden\nneurons connecting to it. This assignment of blame back to the neurons connecting into a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":461,"page_label":"407","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 407\n3\n1 6\n4 8\n2 7\n5w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0δ3\nδ4\nδ5δ6\nδ7δ8\nError gradients ( δs) ﬂow from outputs to inputs\nFigure 8.12\nThe backpropagation of the δvalues during the backward pass of the backpropagation algorithm.\nThis ﬁgure is based on Figure 6.6 of Kelleher (2019).\nneuron is dependent on the weight on the connection between the neurons and also on the\nactivation of the hidden neuron during the forward pass. This is why the activations of all\nneurons are recorded during the forward pass. The δfor a neuron in the last hidden layer\nis calculated by summing the portions of the δs backpropagated to it from all the neurons\nin the output layer that it connects to. Once the δs for all the neurons in the last hidden\nlayer is calculated, the process is repeated in order to calculate δs for the neurons in the\npreceding hidden layer. This process of apportioning blame for the δs in one layer back\nto the neurons in the preceding layer and then summing the blame for each neuron in the\npreceding layer to calculate its δis repeated until all the neurons in the ﬁrst hidden layer\nof the network have a δterm. At the end of this process, a δhas been calculated for every\nneuron in the network.\n8.3.2 Backpropagation: Backpropagating the Error Gradients\nTheδterm for a neuron describes the rate of change of the error (i.e., the error gradient) of\nthe network with respect to changes in the weighted sum calculated by the neuron. Using\nEto represent the error of the network at the output layer, and zkto denote the weighted\nsum calculation in neuron k, the δfor a neuron kcan be mathematically deﬁned","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":462,"page_label":"408","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"408 Chapter 8 Deep Learning\nδk“BE\nBzk(8.13)\nIn other words, the δterm for a neuron is the partial derivative of the error of the network\nwith respect to the weighted sum ( z) of the neuron. The δs for all neurons in a network\n(irrespective of whether they are in the output layer or a hidden layer) are calculated as the\nproduct of two terms:\n1.the rate of change of the error of the network with respect to changes in the activation\nof the neuron: BE{Bak; and\n2.the rate of change of the activation of the neuron with respect to changes in the\nweighted sum calculated at the neuron: Bak{Bzk.\nδk“Bak\nBzkˆBE\nBak(8.14)\nThe process used to calculate the term BE{Bakfor a neuron kis dependent on whether the\nneuron is in the output layer or in one of the hidden layers of the network. The difference\nin howBE{Bakis calculated for output and hidden neurons is how Equation (8.14)[408]gen-\neralizes over all the neurons in the network. However, the same process is used to calculate\nthe termBak{Bzkin the product irrespective of whether the neuron is an output neuron or\na hidden neuron. Because the same process is used to calculate Bak{Bzkfor all neurons\nwe describe this process ﬁrst, and then we describe how the BE{Bakterm is calculated for\noutput neurons and then for hidden neurons.\nThe termBak{Bzkrepresents the rate of change of the neuron’s activation function with\nrespect to changes in the weighted sum z(i.e., with respect to the input to the activation\nfunction). Because this Bak{Bzkterm is required in order to calculate the δfor a neuron,\nthe backpropagation algorithm assumes that the neurons in the network use differentiable\nactivation functions. The logistic function (see Equation (7.25)[342]) has a very well-known\nand relatively simple derivative7\nd\ndzlogisticpzq“logisticpzqˆp 1´logisticpzqq (8.15)\nThe derivative of a function can be understood as the slope of the graph of the function at\neach point on the graph. Figure 8.13[410]illustrates this relationship between the slope of the\ngraph of a function and its derivative. The graph of the logistic function is relatively ﬂat for\nlarge (positive or negative) values. We describe these regions as saturated: in economics\n7. We have already discussed the derivative of the logistic function in Chapter 7[311](see Equation (7.28)[345]);\nwe rewrite it here using zin place of xfor convenience.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":463,"page_label":"409","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 409\nwhen a market becomes saturated with a product, the growth in the consumption of the\nproduct ﬂattens out; by analogy, regions of curves that are ﬂat are said to be saturated. In\nthese saturated regions the derivative of the logistic function is approximately 0. The plot\nof the derivative of the logistic function in Figure 8.13[410]illustrates that as the value of z\napproaches 0 (from either side), the slope of the logistic graph increases until it reaches\nits maximum value at z“0.0; consequently, we can calculate the maximum value of the\nderivative of the logistic function using Equation (8.15)[408]by setting z“0.08\nd\ndzlogisticpz“0q“logisticp0qˆp 1´logisticp0qq\n“0.5ˆp1´0.5q\n“0.25\n(8.16)\nAs this example shows, if a neuron kuses the logistic function as its activation function,\nthen we can calculate the term Bak{Bzkby simply inputting the weighted sum for the neuron\nzkinto Equation (8.15)[408]. This is why, as Figure 8.11[406]highlights, the weighted sum for\neach neuron is stored in the forward pass of the algorithm: it is used to calculate the Bak{Bzk\nterm during the backpropagation process. The simplicity of the derivative of the logistic\nfunction is one of the reasons why the logistic function was such a popular activation\nfunction in neural networks: using logistic activation functions made it relatively easy to\nimplement the backpropagation algorithm.\nIf, however, a neuron used a different activation function, then we would use the deriva-\ntive of that function when we are calculating Bak{Bzk; however, we would do so in the same\nway, by plugging the zkvalue into the derivative.\nOnce theBak{Bzkterm has been calculated for a neuron, the other term needed to cal-\nculate the δfor a neuron kusing Equation (8.14)[408]is the rate of change of the error of\nthe network with respect to changes in the activation of the neuron: BE{Bak. As noted\npreviously, the calculation of this term is different for output neurons and hidden neurons.\nThe calculation of BE{Bakfor output neurons is dependent on the error function9that is\nused during training. We have already introduced the sum of squared errors , orL2, error\nfunction (see Equation 7.4[316]), repeated here for convenience\nL2pMw,Dq“1\n2nÿ\ni“1pti´Mwpdiqq2(8.17)\n8. As Figure 8.13[410]illustrates, logisticp0q“0.5\n9. Also known as the loss function orcost function .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":464,"page_label":"410","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"410 Chapter 8 Deep Learning\n−10 −5 0 5 100.0 0.2 0.4 0.6 0.8 1.0\nzactivation(z)logistic (z)\n∂ logistic (z)\n∂ z\nmax=0.25\nsaturated=0 saturated=0\nFigure 8.13\nPlots of the logistic function and its derivative. This ﬁgure is Figure 4.6 of Kelleher (2019) and is\nused here with permission.\nEquations (7.11)[324]to (7.14)[324]step through the derivation of the rate of change of the\nerror of a model (in that case, a linear regression model) for a single input example dwith\nrespect to changes in one of the model’s weights BL2pMw,dq{Bwrjs, resulting in\nB\nBwrjsL2pMw,dq“pt´Mwpdqqˆ´ drjs (8.18)\nThis derivation was based on the chain rule and is the product of the rate of change of\nthe error of the model with respect to its output—the term pt´Mwpdqq—and the rate of\nchange of the output of the linear regression model with respect to a change in the weight\nj, the term´drjs.\nThe term we want to deﬁne is the rate of change of the error of a neuron (a model) with\nrespect to its activation (output): BE{Bak. For this we need only the ﬁrst term from the\nproduct in Equation (8.18)[410]10\nBE\nBak“BL2pMw,dq\nBMwpdqq“t´Mwpdq“tk´ak (8.19)\n10. We introduce the subscript kon the target tkinto this equation to allow for situations in which the network\nhas multiple neurons in the output layer, and the target output then deﬁnes a separate target tkfor each of these\nneurons.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":465,"page_label":"411","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 411\nThe reason why this difference is considered a rate of change is that the larger the difference\nbetween akandtk, the faster the error of the network can be changed by changing the\nactivation. However, the direction of the calculated gradient is toward the highest value\non the error surface, and therefore to move down the error surface we should multiply it\nby´1.11Hence, using the sum of squared errors error function, the error gradient for a\nsingle output neuron kon a single example is\nBE\nBak“´ptk´akq (8.20)\nThe sum of squared errors is a particularly convenient error function to use because the\nmodel errors on different examples and also the errors on different outputs (e.g., consider\na network that has multiple neurons in the output layer) are independent and therefore the\noverall error is just the sum of the individual errors. Consequently, if we wish to calculate\nthe error gradient of a network with multiple outputs over multiple examples, we simply\nsum the error gradients for each output over the examples.\nEquation (8.21)[411]illustrates the calculation of δkfor neuron kin the output layer. This\nequation expands Equation (8.14)[408]using the difference ´ptk´akqas the value for\nBE{Bak. Working from the right of the equation, ﬁrst we calculate BE{Bakby subtracting\nakfrom tkand multiplying the result by ´1. Next,BE{Bakis multiplied by Bak{Bzk(which\nis calculated as previously described, by inputting the zkfor the neuron into the derivative\nof its activation function). For illustrative purposes, in this instance we assume that the\noutput neuron uses a logistic activation function and expand the deﬁnition of Bak{Bzkac-\ncordingly in the last two lines of the equation. The result of this product is the δkfor the\nneuron.\nδk“Bak\nBzkˆBE\nBak\n“Bak\nBzkˆ´ptk´akq\n“d\ndzlogisticpzq\nloooooomoooooon\nAssuming a logistic activation functionˆ´ptk´akq\n“plogisticpzqˆp 1´logisticpzqqqloooooooooooooooooomoooooooooooooooooon\nAssuming a logistic activation functionˆ´ptk´akq (8.21)\nSwitching the focus to the calculation of δs for hidden neurons, the calculation of BE{Bak\nfor a hidden neuron krequires that the δs for all downstream neurons be calculated ﬁrst\n11. In Chapter 7[311]we discussed dropping the minus sign from the front of ´drjs; compare Equation (7.15)[326]\nwith Equation (7.16)[327].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":466,"page_label":"412","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"412 Chapter 8 Deep Learning\n(i.e., all the neurons that the activation akis directly propagated to during the forward\npass). The reason for this is that the term BE{Bakconnects the activation of the neuron\nakto the output error of the network E. Speciﬁcally, it represents how sensitive the error\nof the network Eis to changes in ak. However, for a hidden neuron k,akonly indirectly\naffects E, via the effect it has on the activations of downstream neurons that the akis\ndirectly propagated to (and the chain reaction that these subsequent activations have on\nstill later neuron activations). Consequently, to connect the activation of a hidden neuron\nakto the network error E, we have to create a chain of connection via the activations of the\ndownstream neurons. The δs for the downstream neurons are the links in this chain, and\ntherefore we must calculate these downstream δs prior to calculating BE{Bak.\nFor example, for each neuron ithat neuron kconnects forward to\nkÑi\ntheδfor neuron iconnects the zivalue for ito the error of the network E. Furthermore,\nthe rate of change of ziwith respect to the activation of neuron k(ak) is the weight on the\nconnection from neuron kto neuron i:wi,k. As a result, we can calculate the rate of change\nof the error of the network Ewith respect to the changes in the activation akby taking the\nproduct: wi,kˆδi. However, the activation akmay be propagated to many downstream\nneurons, and therefore to calculate the total sensitivity of the network error Eto changes in\nak, we must sum this product for all nneurons that akis directly propagated to\nBE\nBak“nÿ\ni“1wi,kˆδi (8.22)\nEquation (8.23)[412]illustrates how the term BE{Bakis calculated for a hidden neuron and\nhow this term is then used to calculate the δfor a hidden neuron by multiplying it by the\ntermBak{Bzk(which, as previously described, is calculated by plugging the zkinto the\nderivative of the activation function).\nδk“Bak\nBzkˆBE\nBak\n“Bak\nBzkˆ˜nÿ\ni“1wi,kˆδi¸\n“d\ndzlogisticpzq\nloooooomoooooon\nAssuming a logistic activation functionˆ˜nÿ\ni“1wi,kˆδi¸\n“plogisticpzqˆp 1´logisticpzqqqloooooooooooooooooomoooooooooooooooooon\nAssuming a logistic activation functionˆ˜nÿ\ni“1wi,kˆδi¸\n(8.23)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":467,"page_label":"413","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 413\nTo summarize, the backward pass of the backpropagation algorithm propagates the δ\nerror gradients back through the network. These δs are the rate of change of the network\nerror with respect to the weighted sum of a neuron. The backward pass starts by using the\ncalculation illustrated by Equation (8.21)[411]to calculate a δfor each neuron in the output\nlayer. Then working backward in a layer-by-layer manner, the δfor the hidden neurons\nare calculated using Equation (8.23)[412]. This process, working backward from the output\nto the input layer, constructs for each neuron in the network a chain of connections linking\nthe weighted sum of the neuron to the error of the network. This backward process builds\nthese chains in an efﬁcient manner because the linking of a new neuron to the chain can\nbe done by extending the chains created for the neurons downstream of it. Once a δhas\nbeen calculated for every neuron in the network, the backward pass has completed (and in\na sense so has the backpropagation algorithm, at least insofar as the algorithm is a solution\nto the blame assignment problem), and we are now ready to update the weights of the\nnetwork using the δs as part of the gradient descent weight update rule.\n8.3.3 Backpropagation: Updating the Weights in a Network\nThe basic principle informing how weights in a neural network should be adjusted is that a\nweight should be updated in proportion to the sensitivity of the network error to changes in\nthe weight. The rationale is that if the network error is not sensitive to changes in a weight\n(i.e., the error does not change when the weight changes), then the error is independent of\nthe weight, or to put it another way, the weight did not contribute to the error. However,\nif the error is sensitive to changes in the weight, then the error is dependent on the weight\nand accordingly the current weight is to blame for some portion of the error.\nThe rate of change of the network error with respect to changes in a weight is written\nmathematically as BE{Bwi,kfor the weight on the connection from neuron kto neuron i.\nUsing the chain rule12we can rewrite this term as a product of three terms\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k(8.24)\nEach link in this chain of products is the rate of change of the output of a function (loss\nfunction, activation function, or weighted sum function) with respect to one of its inputs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":467,"page_label":"413","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the weight, or to put it another way, the weight did not contribute to the error. However,\nif the error is sensitive to changes in the weight, then the error is dependent on the weight\nand accordingly the current weight is to blame for some portion of the error.\nThe rate of change of the network error with respect to changes in a weight is written\nmathematically as BE{Bwi,kfor the weight on the connection from neuron kto neuron i.\nUsing the chain rule12we can rewrite this term as a product of three terms\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k(8.24)\nEach link in this chain of products is the rate of change of the output of a function (loss\nfunction, activation function, or weighted sum function) with respect to one of its inputs.\nWorking from a network weight wi,kforward toward the network error, we have\n‚the rate of change of the weighted sum function with respect to changes in one of the\nweights (Bzi{Bwi,k);\n‚the rate of change of the activation function with respect to changes in the weighted sum\n(Bai{Bzi); and\n12. See Appendix C[765].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":468,"page_label":"414","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"414 Chapter 8 Deep Learning\n‚the rate of change of the error of the network with respect to changes in the activation\nfunction (BE{Bai).\nUsing the equivalence deﬁned in Equation (8.14)[408], this product of three terms can be\nsimpliﬁed\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k\n“ δiˆBzi\nBwi,k\n(8.25)\nThis rewriting shows that (1) the backpropagation algorithm is in fact an implementation\nof the chain rule from calculus and the product used to calculate the δterms follows the\nchain rule; and (2) that we can calculate BE{Bwi,k(the term we need to update a weight wi,k)\nby multiplying the δfor a neuron iby the rate of change of the weighted sum calculation\nfor neuron iwith respect to changes in the weight wi,k. In calculus, when we take the partial\nderivative of a function with respect to a particular input, all the terms in the function that\nare not involved in the input disappear, because they are constants when the input changes.\nThis means that the partial derivative of a weighted sum function with respect to a weight\nsimpliﬁes to the derivative of the product of the weight by its input that is equal to the input\nthe weight is applied to\nBzi\nBwi,k“ak (8.26)\nThe implication of this is that once we have calculated the δfor a neuron, all we need to\ndo to calculate the sensitivity of the network error with respect to a weight on a connection\ncoming into the neuron is to multiply the neuron’s δby the activation that was propagated\nforward on that connection. Equation (8.27)[414]shows how the chain rule product is even-\ntually simpliﬁed to the simple product of a δby an activation\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k\n“ δiˆBzi\nBwi,k\n“ δiˆak (8.27)\nThis is why, as Figure 8.11[406]shows, the activation for each neuron is stored during\nthe forward pass of the algorithm; these activations are used to update the weights on\nthe connections along which they are propagated. We now know how to calculate the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":469,"page_label":"415","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 415\nsensitivity of the network error with respect to changes in a weight: BE{Bwi,k. How do we\nuse this term to update a weight?\nWe can get an insight into how we should use this term to update weights by considering\nthe case of a weight wi,kbetween an output neuron ithat uses a logistic activation function\nand a hidden neuron k. If the output of the neuron iis too high ( aiąti), then δiwill be\npositive because it will be the product of two positive terms:\n1.BE{Baiis positive when aiąti(see Equation (8.20)[411]); and\n2.Bai{Bziis alwaysě0for a logistic function.\nFurthermore, if aiąti, we know that we should decrease the output ai. Therefore,\n1.ifak, the activation from neuron kthat the weight wi,kwas applied to, is positive, we\nshould decrease the weight wi,k. In this case BE{Bwi,k“δiˆakwill be positive,\nbecause both terms in the product are positive, and so to decrease the weight wi,kwe\nshould subtract BE{Bwi,kfrom wi,k.\n2.ifak, is negative, we should increase the weight wi,k. However, in this case BE{Bwi,k“\nδiˆakwill be negative, because the product involves a positive and a negative term.\nAnd, so to increase wi,k, we should again subtract BE{Bwi,kfrom wi,k.\nIn both these cases we should subtract BE{Bwi,kfrom wi,k. The same conclusion can be\nreached via similar reasoning for the case aiăti. Hence we update the weights as follows:\nwi,kÐwi,k´αˆδiˆak (8.28)\nwhereαis the learning rate hyper-parameter and has the same function as the learning\nrate in gradient descent.13This equation states that the updated weight after processing a\ntraining example is equal to the weight used to process the training example minus αtimes\nthe sensitivity of the error of the network with respect to changes in the weight.\nEquation (8.28)[415]illustrates how a weight in a network is updated after a single train-\ning example has been processed. Updating weights after each training example is known\nason-line ,sequential , orstochastic gradient descent. One problem with updating the\nweights of a network after each example is that the error gradient calculated on a single\nexample sampled from the training set is likely to be a noisy approximation of the true\ngradient over the entire dataset; in other words, the error gradient calculated on a single\nexample may not point in the same direction as the steepest gradient when we average the\ngradients over the entire dataset. Typically, stochastic gradient descent still works because","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":469,"page_label":"415","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Equation (8.28)[415]illustrates how a weight in a network is updated after a single train-\ning example has been processed. Updating weights after each training example is known\nason-line ,sequential , orstochastic gradient descent. One problem with updating the\nweights of a network after each example is that the error gradient calculated on a single\nexample sampled from the training set is likely to be a noisy approximation of the true\ngradient over the entire dataset; in other words, the error gradient calculated on a single\nexample may not point in the same direction as the steepest gradient when we average the\ngradients over the entire dataset. Typically, stochastic gradient descent still works because\ngenerally descending the error gradient on an individual example will move the weight in a\nsimilar direction as descending the gradient over the entire dataset. However, this may not\n13. See Section 7.3.3[328]for further discussions on the role of the learning rate, and Section 7.4.2[334]for a dis-\ncussion on different approaches to setting the learning rate.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":470,"page_label":"416","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"416 Chapter 8 Deep Learning\nalways be the case, because some weight updates may move in an orthogonal direction to\nthe true gradient or even cause the error to increase; this can slow down the training of the\nnetwork. Ideally, we want to descend the true error gradient for the entire dataset. Batch\ngradient descent14involves calculating the error gradients for each weight for all the ex-\namples in a dataset and summing the gradients for each weight, and only then updating the\nweights using the summed error gradients calculated over the entire dataset. Apart from\nthe advantage of descending the true error gradient for the entire dataset, batch gradient\ndescent is also able to take advantage of the fact that a neural network, implemented as a\nsequence of matrix operations, can process multiple examples in parallel (as illustrated by\nFigure 8.6[393]and Figure 8.9[399]). Furthermore, calculating a direction of descent by taking\nan average over a set of noisy examples can make the descent of the error surface smoother,\nwhich often means that we can use a larger learning rate αwith batch gradient descent (i.e.,\nwe can take larger steps between weight updates because we are more conﬁdent of the di-\nrection we are moving). This combination of multiple examples processed in parallel and\na larger learning rate can result in much faster training times using batch gradient descent.\nHowever, in order to use batch gradient descent, we must update Equation (8.28)[415]to\naccommodate updating a weight using the sum of the error gradients. To do this, we ﬁrst\nintroduce the term ∆wi,kto denote the sum of the error gradients for the weight wi,kover one\ncomplete pass through all the examples in the training dataset. Equation (8.29)[416]deﬁnes\nhow∆wi,kis calculated for a training dataset containing mexamples. In this equation\nthe subscript jindexes over the examples in the dataset and subscripts iandkindex over\nneurons in the network. Hence δi,jis theδvalue for neuron ifor example j. We then\nrewrite Equation (8.28)[415]as Equation (8.30)[416]to use this new term\n∆wi,k“mÿ\nj“1δi,jˆak,j (8.29)\nwi,kÐwi,k´αˆ∆wi,k (8.30)\nThe distinction between stochastic gradient descent and batch gradient descent can be\nused to clarify the meanings of two related terms often used in neural network training:\nepoch anditeration . An epoch is a single pass through all the examples in the training\ndataset. Therefore, ∆wi,kis the error gradient for weight wi,kfor one epoch. The term","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":470,"page_label":"416","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"how∆wi,kis calculated for a training dataset containing mexamples. In this equation\nthe subscript jindexes over the examples in the dataset and subscripts iandkindex over\nneurons in the network. Hence δi,jis theδvalue for neuron ifor example j. We then\nrewrite Equation (8.28)[415]as Equation (8.30)[416]to use this new term\n∆wi,k“mÿ\nj“1δi,jˆak,j (8.29)\nwi,kÐwi,k´αˆ∆wi,k (8.30)\nThe distinction between stochastic gradient descent and batch gradient descent can be\nused to clarify the meanings of two related terms often used in neural network training:\nepoch anditeration . An epoch is a single pass through all the examples in the training\ndataset. Therefore, ∆wi,kis the error gradient for weight wi,kfor one epoch. The term\niteration is used to refer to a single forward and backward pass plus weight update of the\nbackpropagation algorithm. So if we have a training dataset of mexamples in stochastic\ngradient descent, it would take miterations to complete a single epoch , and this epoch\nwould involve mweight updates. Conversely, in batch gradient descent, assuming we\n14. This is the same as the batch gradient descent discussed in Chapter 7[311].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":471,"page_label":"417","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 417\nprocess all the training examples in parallel, we complete a single epoch in each iteration\nof the algorithm and so there is a single weight update per epoch.\nThe drawback of batch gradient descent is that the entire dataset must be processed be-\ntween each weight update. This can be problematic because modern datasets can be very\nlarge—it is quite possible to have datasets containing millions or even billions of examples.\nIn these scenarios we need to process millions of examples and to calculate and accumulate\nmillions of error gradients for each weight update. Even with modern computational power\nand with processing examples in parallel, this can take a long time and ultimately result in\nnetwork training taking a long time. Consequently, the standard practice in deep learning\nis to use mini-batch gradient descent . In mini-batch gradient descent, the dataset is split\ninto multiple subsets called mini-batches orbatches .15\nIdeally, each mini-batch should be created by random sampling from the dataset. How-\never, randomly sampling each mini-batch from a very large dataset may be impractical. So,\nfrequently, at the start of training the dataset is shufﬂed and then split into a sequence of\nmini-batches. The ﬁrst iteration of training is done on the ﬁrst mini-batch in the sequence,\nthe second iteration on the second mini-batch, and so on until the epoch is completed.\nAt the start of the second epoch, the sequence of mini-batches is shufﬂed and the train-\ning iterations are carried out on this new sequence of mini-batches. Equation (8.29)[416]\nand Equation (8.30)[416]work for mini-batch training in much the same way as they do for\nbatch training; the only difference is that the summation in Equation (8.29)[416]is over the\nexamples in the mini-batch rather than the entire dataset.\nTypically mini-batches are all of the same size. There are a number of factors to con-\nsider in the choice of the batch size , including16(1) larger batches provide a more accurate\nestimate of the true gradient for the entire dataset, but (2) hardware constraints may neces-\nsitate the use of smaller batches; for example, if all the examples in a mini-batch are to be\nprocessed in parallel, then the larger the batch size, the larger is the memory requirement.\nPopular batch sizes include 32, 64, 128, and even 256 examples. We noted previously that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":471,"page_label":"417","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"batch training; the only difference is that the summation in Equation (8.29)[416]is over the\nexamples in the mini-batch rather than the entire dataset.\nTypically mini-batches are all of the same size. There are a number of factors to con-\nsider in the choice of the batch size , including16(1) larger batches provide a more accurate\nestimate of the true gradient for the entire dataset, but (2) hardware constraints may neces-\nsitate the use of smaller batches; for example, if all the examples in a mini-batch are to be\nprocessed in parallel, then the larger the batch size, the larger is the memory requirement.\nPopular batch sizes include 32, 64, 128, and even 256 examples. We noted previously that\nwe can often increase the learning rate αwhen using batch gradient descent. Generally,\nﬁnding a good combination of values for the batch size and learning rate hyper-parameters\ninvolves trial-and-error experimentation.\n15. The reuse of the term batch in mini-batch learning can lead to confusion. To clarify the distinctive meanings:\nthe terms batch learning orbatch gradient descent typically indicate that the entire training set is processed\nbetween each weight update, whereas the term batch can also be used to indicate a set of examples in a mini-\nbatch training regime (as in a batch of examples ), and the term batch size describes the number of examples in\neach batch (or mini-batch).\n16. See Goodfellow et al. (2016, p. 272) and references therein for further discussion on the trade-offs in batch\nsize.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":472,"page_label":"418","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"418 Chapter 8 Deep Learning\n8.3.4 Backpropagation: The Algorithm\nAlgorithm 5[420]brings together the different topics covered in the preceding sections. The\nalgorithm assumes a dataset Dis available; it also requires that values for the learning rate\nαand the batch size Bhyper-parameters have been selected. Finally, the model assumes\nthat a convergence criterion has been speciﬁed.17Early stopping is the most popular\nstrategy used to deﬁne a convergence criterion; we explain the early stopping algorithm in\nSection 8.4.4[472]. However, the general idea of a convergence criterion is that as the training\nprogresses, the error of the network should generally decrease. However, the rate of error\nreduction after each weight update will also decrease. Consequently, there is a diminishing\nreturn on the improvement of the network relative to the time spent on training. At a certain\npoint in training, further adjustments to the weights will not result in signiﬁcant changes to\nthe network error, and the convergence criterion speciﬁes a decision process whereby we\ndecide when to stop training.\nIn Line 1[420]the data is split into mini-batches where Xpiqis a matrix that contains the\ndescriptive features for each of the examples in mini-batch i, with one column per example,\nandYpiqis a matrix (or vector) containing the corresponding labels for the examples in\nmini-batch i. The network’s weight matrices are initialized in Line 2[420]. There are Llayers\nin this network and so there are Lweight matrices, where Wpiqrepresents the weight matrix\nfor layer i. The weight matrices are arranged so that there is one row per set of weights per\nneuron in the layer.\nEach loop of the repeat loop from Line 3[420]to Line 33[420]involves an epoch of training\n(i.e., a full traversal of the training data completed via a single pass through all the mini-\nbatches). Each iteration of the forloop (Lines 4[420]to 31[420]) involves the processing of\na single mini-batch, including both a forward and backward pass of the algorithm and a\nsingle set of weight updates. In Line 5[420]the matrix of descriptive features for the examples\nin the mini-batch that is about to be processed is presented to the input layer of the network.\nThe forward pass of the algorithm occurs in the forloop Lines 6[420]to 11[420]. This forward\npass follows the set of operations illustrated in Figure 8.6[393]. Each iteration of this forloop","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":472,"page_label":"418","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Each loop of the repeat loop from Line 3[420]to Line 33[420]involves an epoch of training\n(i.e., a full traversal of the training data completed via a single pass through all the mini-\nbatches). Each iteration of the forloop (Lines 4[420]to 31[420]) involves the processing of\na single mini-batch, including both a forward and backward pass of the algorithm and a\nsingle set of weight updates. In Line 5[420]the matrix of descriptive features for the examples\nin the mini-batch that is about to be processed is presented to the input layer of the network.\nThe forward pass of the algorithm occurs in the forloop Lines 6[420]to 11[420]. This forward\npass follows the set of operations illustrated in Figure 8.6[393]. Each iteration of this forloop\npropagates the activations for the mini-batch forward through the next layer of the network.\nThe vector v, created on Line 7[420], is the vector of bias inputs, and it is as wide as the\nnumber of neurons in the layer (we use the subscript mhere as shorthand for the number of\nneurons in the layer). On Line 8[420]the bias inputs vector and the matrix of activations from\nthe previous layer Al´1are vertically concatenated so that the bias inputs are now stored\nin the ﬁrst row of Al´1; we use the notation rv;Asto represent the vertical concatenation\nof the vector/matrix vandA. This vertical concatenation operation is illustrated in Figure\n8.6[393]withdr0sadded to the activation matrix as it is propagated forward from one layer to\n17. The concept of a convergence criterion is also used in the gradient decent algorithm discussed in Chapter\n7[311]; see Algorithm 4[326].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":473,"page_label":"419","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 419\nthe next. Line 9[420]is the matrix multiplication of the layer’s weights by the activations from\nthe preceding layer. This matrix multiplication generates the weighted sums for all neurons\nin the layer for all the examples in the mini-batch and stores the results in the matrix Zplq.\nNext, on Line 10[420], the activation function ϕis applied in turn to each element of Zplqto\ngenerate the activations for each neuron in the layer for each example in the mini-batch.\nThese activations are stored in the matrix Aplq. When the algorithm exits the forloop on\nLine 11[420], the mini-batch examples will have been propagated through all Llayers of the\nnetwork, and ApLqwill store the activations for all the neurons in the output layer for all\nthe examples in the mini-batch; as per Figure 8.6[393]the activations are arranged so that the\noutput layer activations for each example will be stored as a column in ApLq.\nThis version of the algorithm is for mini-batch training, and so we know that we will\nneed to sum the error gradients for each weight across the examples in the mini-batch.\nTheforloop on Lines 12[420]to 14[420]is where the variables used to store these totals are\ninitialized. The backpropagation of the δs and the summation of the error gradients across\nthe examples in the mini-batch are done in the forloop from Line 15[420]to Line 27[420].\nThis forloop iterates through the examples in the batch, and for each example the δs for\nthe neurons are calculated and backpropagated. Recall that we use different equations to\ncalculate the δvalue for a neuron, depending on whether the neuron is an output neuron\nor a neuron in a hidden layer. We highlight this distinction by using different forloops\nfor each of these conditions. The forloop on lines 16[420]to 18[420]contains the calculations\nof theδs for neurons in the output layer, and the forloop on lines 19[420]to 23[420]is the\ncalculation of the δs for neurons in the hidden layers. Once the δs for all the neurons in\nthe network for an example have been calculated, then for each weight in the network the\nerror gradients are accumulated (Lines 24[420]to 26[420]). Consequently, at the end of the\nprocessing of the mini-batch (i.e., when the algorithm exits the forloop from Line 15[420]\nto Line 27[420]),∆wi,kwill contain the weight updates for weight wi,ksummed across all\nthe examples in the mini-batch. The ﬁnal forloop in each epoch is where the weights of","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":473,"page_label":"419","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"for each of these conditions. The forloop on lines 16[420]to 18[420]contains the calculations\nof theδs for neurons in the output layer, and the forloop on lines 19[420]to 23[420]is the\ncalculation of the δs for neurons in the hidden layers. Once the δs for all the neurons in\nthe network for an example have been calculated, then for each weight in the network the\nerror gradients are accumulated (Lines 24[420]to 26[420]). Consequently, at the end of the\nprocessing of the mini-batch (i.e., when the algorithm exits the forloop from Line 15[420]\nto Line 27[420]),∆wi,kwill contain the weight updates for weight wi,ksummed across all\nthe examples in the mini-batch. The ﬁnal forloop in each epoch is where the weights of\nthe network are updated (Lines 28[420]to 30[420]). On Line 32[420]the mini-batch sequence is\nshufﬂed between epochs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":474,"page_label":"420","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"420 Chapter 8 Deep Learning\nAlgorithm 5 The backpropagation algorithm for a feedforward network with L layers\nRequire: set of training instances D\nRequire: a learning rate αthat controls how quickly the algorithm converges\nRequire: a batch size Bspecifying the number of examples in each batch\nRequire: a convergence criterion\n1:Shufﬂe Dand create the mini-batches: rpXp1q,Yp1qq,...,pXk,Ykqs\n2:Initialize the weight matrices for each layer: Wp1q,..., WpLq\n3:repeat ŹEach repeat loop is one epoch\n4: fort=1 to number of mini-batches do ŹEach for loop is one iteration\n5: Ap0qÐXptq\nŹForward propagation\n6: forl=1 to L do\n7: vÐr10,...1ms Ź Create vthe vector of bias terms\n8: Apl´1qÐrv;Apl´1qs Ź Insert vinto the activation matrix\n9: ZplqÐWlApl´1q\n10: AplqÐϕpZplqq Ź Elementwise application of ϕtoZplq\n11: end for\n12: foreach weight wi,kin the network do\n13: ∆wi,k“0\n14: end for\n15: foreach example in the mini-batch do ŹBackpropagate the δs\n16: foreach neuron iin the output layer do\n17: δi“BE\nBaiˆBai\nBziŹSee Equation (8.21)[411]\n18: end for\n19: forl= L-1 to 1 do\n20: foreach neuron iin the layer ldo\n21: δi“BE\nBaiˆBai\nBziŹSee Equation (8.23)[412]\n22: end for\n23: end for\nŹFor each weight wi,kaccumulate ∆wi,kacross the mini-batch\n24: foreach weight wi,kin the network do\n25: ∆wi,k“∆wi,k`pδiˆakq Ź Equation (8.29)[416]\n26: end for\n27: end for\nŹUpdate the weights\n28: foreach weight wi,kin the network do\n29: wi,kÐwi,k´αˆ∆wi,k ŹEquation (8.30)[416]\n30: end for\n31: end for\n32: shufﬂe(rpXp1q,Yp1qq,...,pXk,Ykqs)\n33:until convergence occurs","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":475,"page_label":"421","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 421\n8.3.5 A Worked Example: Using Backpropagation to Train a Feedforward Network\nfor a Regression Task\nThe electrical power output from a combined cycle power plant is inﬂuenced by a number\nof ambient parameters, such as temperature and humidity; being able to accurately predict\nthe output of the power plant working a full load with respect to these parameters can\nsigniﬁcantly reduce the cost of energy production (T ¨ufekci, 2014). Table 8.1[422]lists the\nhourly averages for the A MBIENT TEMPERATURE and the R ELATIVE HUMIDITY when\na power plant is working at full load and the net hourly E LECTRICAL OUTPUT for the\nplant under these conditions. We use this dataset to illustrate training a deep feedforward\nnetwork. The E LECTRICAL OUTPUT feature is the target feature for this example. The\nfour examples in Table 8.1[422]are only a sample from a larger dataset;18however, for\nthis example, for ease of illustration we treat these four examples as if they were our full\ndataset. Furthermore, we use batch gradient descent for training and treat Table 8.1[422]as a\nbatch. Consequently, in each iteration of the training algorithm we will complete a single\nepoch (a single pass through our four examples and hence through the full dataset), and\nthere will be a single weight update per epoch.\nA standard data preprocessing practice for neural networks is to normalize the descrip-\ntive features. Consequently, we begin the worked example by explaining why normaliza-\ntion is important for neural networks and normalizing the data in Table 8.1[422]. During\nnormalization, the values of the descriptive features are mapped to a standard range, for\nexampler´1,`1sorr0,1s, using range normalization (see Equation (3.7)[87]), or stan-\ndardized in order to have a mean of 0 and a standard deviation of 1 (see Equation (3.8)[88]).\nThe reason is that to ensure that the weight updates for all the weights on the network are\non a similar scale. Recall that the update applied to a weight wi,kis always scaled by the\nakterm (both in the stochastic setting, Equation (8.28)[415], and the batch setting, Equation\n(8.29)[416]). Now consider what happens to the weights on connections from the input layer\nto the ﬁrst hidden layer if there are large differences in the values taken by different de-\nscriptive features. For example, consider an input feature recording salaries in dollars; this","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":475,"page_label":"421","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"exampler´1,`1sorr0,1s, using range normalization (see Equation (3.7)[87]), or stan-\ndardized in order to have a mean of 0 and a standard deviation of 1 (see Equation (3.8)[88]).\nThe reason is that to ensure that the weight updates for all the weights on the network are\non a similar scale. Recall that the update applied to a weight wi,kis always scaled by the\nakterm (both in the stochastic setting, Equation (8.28)[415], and the batch setting, Equation\n(8.29)[416]). Now consider what happens to the weights on connections from the input layer\nto the ﬁrst hidden layer if there are large differences in the values taken by different de-\nscriptive features. For example, consider an input feature recording salaries in dollars; this\nfeature could have a spread of values in the range of millions or more. Contrast this with\na feature that measures age in years; this feature is likely to have a spread of values with\na maximum of just over 100. In this scenario, everything else being equal, the weight up-\ndates for the salary feature will generally be larger than those for the age feature because\nthe weight updates are scaled by the feature values. A similar dynamic emerges between\nweight updates for the bias inputs, which are always equal to 1 ( a0“1), and weights\non inputs with values much larger than 1. Furthermore, large weight updates can result\n18. This example and dataset is based on the Combined Cycle Power Plant dataset available from the UCI reposi-\ntory at https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant and originally collected for the work\nreported in T ¨ufekci (2014)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":476,"page_label":"422","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"422 Chapter 8 Deep Learning\nTable 8.1\nHourly samples of ambient factors and full load electrical power output of a combined cycle power\nplant.\nID A MBIENT TEMPERATURE RELATIVE HUMIDITY ELECTRICAL OUTPUT\n˝C % MW\n1 03.21 86.34 491.35\n2 31.41 68.50 430.37\n3 19.31 30.59 463.00\n4 20.64 99.97 447.14\nin instability in model training. Indeed, controlling the size of weight updates in order to\nstabilize convergence during training is one of the reasons why the learning rate hyper-\nparameterαis included in the weight update rules.19In addition, having a large variance\non the weight updates applied to input connections can result in the network’s having rel-\natively large weights for some features. If a model has a relatively large weight on one\ninput feature, then the output of the model can be very sensitive to small changes in the\nvalue of the feature and hence the outputs of the model can be very different for similar\ninput vectors. In summary, non-normalized descriptive features can result in unstable or\nslow learning and a more unstable model in relation to generalization. If the values of a\ndescriptive feature are normally distributed, then standardizing the feature is appropriate;\nhowever, this is relatively rare, and the default is to use range normalization into either\nr´1,`1sorr0,1sfor preprocessing.\nIn a regression problem, in which the target is continuous, normalization is often applied\nto both the descriptive features and the target feature. One reason why this makes sense\nis that many activation functions have a small output range (e.g., the output of the logis-\ntic function has the range r0,1s) and it is appropriate that the range of the target feature\nmatches the output range of the activation function. Once the target feature has been nor-\nmalized, then during training the error of the network on an example can be calculated\nby directly comparing the output of the network with the normalized target feature value.\nDoing this does necessitate that in order to retrieve the predicted value of the target in the\noriginal range of the target feature, the output from the network must be mapped back to\nthe original scale for the target feature, but generally, this is not a difﬁcult thing to do. For\nexample, if the target feature originally had the range rmin,maxsand range normalization\nhas been applied to map this to the range of r0,1s, then the activation of a logistic unit in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":476,"page_label":"422","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"matches the output range of the activation function. Once the target feature has been nor-\nmalized, then during training the error of the network on an example can be calculated\nby directly comparing the output of the network with the normalized target feature value.\nDoing this does necessitate that in order to retrieve the predicted value of the target in the\noriginal range of the target feature, the output from the network must be mapped back to\nthe original scale for the target feature, but generally, this is not a difﬁcult thing to do. For\nexample, if the target feature originally had the range rmin,maxsand range normalization\nhas been applied to map this to the range of r0,1s, then the activation of a logistic unit in\nthe output layer of the network aican be mapped to the corresponding value in the original\nrange of the target feature by maxˆai. An alternative strategy for using neural networks\nfor regression is to use linear units in the output layer (i.e., units that do not use an ac-\n19. See Section 7.4.2[334].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":477,"page_label":"423","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 423\nTable 8.2\nThe minimum and maximum values for the A MBIENT TEMPERATURE , RELATIVE HUMIDITY , and\nELECTRICAL OUTPUT features in the power plant dataset.\nAMBIENT TEMPERATURE RELATIVE HUMIDITY ELECTRICAL OUTPUT\nMin 1.81˝C 25.56% 420.26MW\nMax 37.11˝C 100.16% 495.76MW\nTable 8.3\nTherange-normalized hourly samples of ambient factors and full load electrical power output of a\ncombined cycle power plant, rounded to two decimal places.\nID A MBIENT TEMPERATURE RELATIVE HUMIDITY ELECTRICAL OUTPUT\n˝C % MW\n1 0.04 0.81 0.94\n2 0.84 0.58 0.13\n3 0.50 0.07 0.57\n4 0.53 1.00 0.36\ntivation function and simply output the weighted sum zas their activation). Using linear\nunits in the output layer means that the output of the network can have the same range as\nthe non-normalized target feature. This has the advantage that the outputs of the network\ndo not have to be transformed back into the original target feature range. However, the\ndownside to not normalizing the target feature in a regression problem is that if the target\nfeature has a large range, then during training the error of the network on an example can\nbe very large, resulting in very large error gradients, which in turn can result in large weight\nupdates and an unstable learning process (similar to the instability that can arise with large\ninputs, but in this case the instability arises from large errors).\nFor our example we apply range normalization to both the descriptive features and the\ntarget feature. Note that for range normalization of the features we need the minimum and\nmaximum values for each feature. Table 8.2[423]lists these values for the original complete\ndataset20(as distinct from our sample of four examples). Table 8.3[423]lists the examples\nafter the features have been range-normalized into the range r0,1s.\nWe use the network architecture illustrated in Figure 8.4[390]as the structure for the model\nwe train. We also assume that all the neurons use a logistic activation function. Figure\n8.14[425]illustrates the forward pass for the examples in Table 8.3[423]through the network in\nFigure 8.4[390]. This network has a depth of three, and so it has three weight matrices. These\nweight matrices are organized so that each row contains the weights for the connections\ncoming into one neuron. To help align the elements of the weight matrices shown in Figure","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":477,"page_label":"423","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"dataset20(as distinct from our sample of four examples). Table 8.3[423]lists the examples\nafter the features have been range-normalized into the range r0,1s.\nWe use the network architecture illustrated in Figure 8.4[390]as the structure for the model\nwe train. We also assume that all the neurons use a logistic activation function. Figure\n8.14[425]illustrates the forward pass for the examples in Table 8.3[423]through the network in\nFigure 8.4[390]. This network has a depth of three, and so it has three weight matrices. These\nweight matrices are organized so that each row contains the weights for the connections\ncoming into one neuron. To help align the elements of the weight matrices shown in Figure\n20. Available from the UCI repository at https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":478,"page_label":"424","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"424 Chapter 8 Deep Learning\n8.14[425]with the connections in the graph representation of the network in Figure 8.4[390],\nthe rows and columns of the weight matrices are labeled using the neuron identiﬁers from\nFigure 8.4[390](the labels are the numbers inside circles). For example, the top row of the\nmatrix shown in Figure 8.4[390]labeled Hidden layer 1 Weight Matrix contains the weights\non the connections coming into Neuron 3 in Figure 8.4[390]; hence this row has the label 3.\nThe middle column in this weight matrix represents the weights on the connections from\nNeuron 1 to each of the neurons in hidden layer 1: Neurons 3, 4 and 5, respectively. The\nﬁrst column in each weight matrix contains the weights for the bias terms, and so these\ncolumns have no label. For this example, the weight matrices were randomly initialized\nfrom the range r´0.5,`0.5s\nThe matrix labeled Input Layer contains the inputs for the examples, augmented with the\nbias inputs for each neuron. Each column in this matrix contains the descriptive features\nfor one of the examples in Table 8.3[423]. We have used the convention of a gray background\nto track the ﬂow of the second example ( d2) through the network: the input vector for d2\nisăbias“1,AMBIENT TEMPERATURE “0.84,RELATIVE HUMIDITY“0.58ą. The\ngray column in the matrix Zp1qcontains the weighted sums for the neurons 3, 4, and 5 for\nthis input vector; e.g., for Neuron 3 the weighted sum for d2isz3“ ´ 0.0042 . Tracing\nd2through the logistic activation function, the activation for Neuron 3 for d2isa3“\n0.4990 . Note that in order to keep the number of decimal points required to represent the\ncalculations through this forward pass manageable for presentation purposes, the outputs\nof the logistic functions for each layer have been rounded to four decimal places, and these\nrounded activations were the activations used in the subsequent calculations. Note also\nthat the weight matrix labels on the left of the ﬁgure are also the correct neuron labels for\nthe weighted sum Zand activation matrix rows on that line. For example, the ﬁrst row in\nZp1qcontains the weighted sum for Neuron 3 (i.e., z3) ford1,d2,d3, and d4, respectively.\nSimilarly, the ﬁrst row in the Activations Hidden Layer 1 matrix contains the activations for\nNeuron 3 ( a3) for the d1,d2,d3, and d4, respectively. Following the ﬂow of d2through all\nthree layers of the network, the prediction output by the model for this example is 0.4718","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":478,"page_label":"424","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of the logistic functions for each layer have been rounded to four decimal places, and these\nrounded activations were the activations used in the subsequent calculations. Note also\nthat the weight matrix labels on the left of the ﬁgure are also the correct neuron labels for\nthe weighted sum Zand activation matrix rows on that line. For example, the ﬁrst row in\nZp1qcontains the weighted sum for Neuron 3 (i.e., z3) ford1,d2,d3, and d4, respectively.\nSimilarly, the ﬁrst row in the Activations Hidden Layer 1 matrix contains the activations for\nNeuron 3 ( a3) for the d1,d2,d3, and d4, respectively. Following the ﬂow of d2through all\nthree layers of the network, the prediction output by the model for this example is 0.4718\n(see Cell 2, the gray cell, in the Activations Output Layer matrix). Table 8.4[426]shows the\ncalculation of the per example error for the four examples and the sum of squared errors\nfor the model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":479,"page_label":"425","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"+0.10−0.20+0.11\n−0.19+0.09−0.13\n+0.04+0.11−0.02\nHidden Layer 1\nWeight Matrix1.00 1.00 1.00 1.00\n0.04 0.84 0.50 0.53\n0.81 0.58 0.07 1.00\nInput Layer=+0.1811 −0.0042 +0.0077 +0.1040\n−0.2917 −0.1898 −0.1541 −0.2723\n+0.0282 +0.1208 +0.0936 +0.0783Z(1)\nϕ0.5452 0.4990 0.5019 0.5260\n0.4276 0.4527 0.4616 0.4323\n0.5070 0.5302 0.5234 0.5196Activations\nHidden Layer 1\n+0.15−0.04+0.10+0.06\n+0.12+0.20+0.14−0.09\nHidden Layer 2\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.5452 0.4990 0.5019 0.5260\n0.4276 0.4527 0.4616 0.4323\n0.5070 0.5302 0.5234 0.5196\nActivations\nHidden Layer 1=+0.201372 +0.207122 +0.207488 +0.203366\n+0.243274 +0.235460 +0.237898 +0.238958Z(2)\nϕ0.5502 0.5516 0.5517 0.5507\n0.5605 0.5586 0.5592 0.5595Activations\nHidden Layer 2\n+0.10+0.12−0.50\nOutput Layer\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.5502 0.5516 0.5517 0.5507\n0.5605 0.5586 0.5592 0.5595\nActivations\nHidden Layer 2=−0.114226 −0.113108 −0.113396 −0.113666Z(3)\nϕ0.4715 0.4718 0.4717 0.4716Activations\nOutput Layer3\n4\n51 2 d1d2d3d4\n6\n73 4 5\n86 7+d[0]=1\n+d[0]=1\nFigure 8.14\nThe forward pass of the examples listed in Table 8.3[423]through the network in Figure 8.4[390].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":480,"page_label":"426","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"426 Chapter 8 Deep Learning\nTable 8.4\nThe per example error after the forward pass illustrated in Figure 8.14[425], the per example BE{Ba8,\nand the sum of squared errors for the model over the dataset of four examples.\nd1 d2 d3 d4\nTarget 0.9400 0.1300 0.5700 0.3600\nPrediction 0.4715 0.4718 0.4717 0.4716\nError 0.4685 -0.3418 0.0983 -0.1116\nBE{Ba8: Errorˆ´1 -0.4685 0.3418 -0.0983 0.1116\nError20.21949225 0.11682724 0.00966289 0.01245456\nSSE: 0.17921847\nThe next stage is to backpropagate the δs for the neurons in the network. When we\nare processing a batch of examples, we need to do the backpropagation of the δs for each\nexample. However, for the sake of space, here we illustrate the backpropagation process for\na single example. We use d2because the processing of this example has been highlighted\nin Figure 8.14[425], and so it will be easier to identify the relevant zkandakvalues for each\nneuron for this example. Figure 8.15[427]illustrates the forward pass of d2through the\nnetwork. This ﬁgure shows the weights on each connection and for each neuron shows\nthe weighted sum zcalculated by that neuron (the number on the left of the neuron) and\nthe activation zfor the neuron (the number on the right of the neuron). Cross-referencing\nFigure 8.14[425], these zandavalues are found in the corresponding gray boxes in that\nﬁgure.\nThe ﬁrstδwe calculate is for Neuron 8: δ8. Neuron 8 is an output neuron and so we\nuse the process illustrated in Equation (8.21)[411]. As shown by Equation (8.21)[411]and\nfollowing Equation (8.20)[411], the termBE{Bais the error of the neuron multiplied by ´1,\nand so from Table 8.4[426]we see that for d2\nBE\nBa“0.3418 (8.31)\nThe second term we need in order to calculate δ8is the rate of change of the activation\nfunction with respect to the changes in the weighted sum z:Ba{Bz. This is calculated by\nplugging the zfor the neuron into the derivative of the activation function. The neurons in\nthis network all use the logistic function as their activation function, and so the derivative of\nthis function is given by Equation (8.15)[408]. As we proceed through the steps of calculating\ntheδs for each of the neurons, we will need to calculate the Ba{Bzterm for each neuron,\nand so for ease of reference, Table 8.5[428]lists this value for each neuron; for space and\nconvenience considerations we have rounded these values to four decimal places and used\nthe rounded values in our calculations. From Table 8.5[428], for Neuron 8, Ba{Bz“0.2492 .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":480,"page_label":"426","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"function with respect to the changes in the weighted sum z:Ba{Bz. This is calculated by\nplugging the zfor the neuron into the derivative of the activation function. The neurons in\nthis network all use the logistic function as their activation function, and so the derivative of\nthis function is given by Equation (8.15)[408]. As we proceed through the steps of calculating\ntheδs for each of the neurons, we will need to calculate the Ba{Bzterm for each neuron,\nand so for ease of reference, Table 8.5[428]lists this value for each neuron; for space and\nconvenience considerations we have rounded these values to four decimal places and used\nthe rounded values in our calculations. From Table 8.5[428], for Neuron 8, Ba{Bz“0.2492 .\nWe now have the two terms we need to calculate δ8, and Equation 8.32[427]steps through\nthe calculation of δ8ford2. Note that for space considerations we have rounded δ8to four","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":481,"page_label":"427","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 427\n3\n1 6\n4 8\n2 7\n5−0.20\n0.09\n0.110.11\n−0.13\n−0.02−0.04\n0.200.10\n0.140.06\n−0.090.12\n−0.500.10\n−0.19\n0.040.15\n0.120.10−0.0042 0.4990\n−0.1898 0.4527\n0.1208 0.53020.207122 0.5516\n0.235460 0.5586−0.113108 0.4718\nActivations ﬂow from inputs to outputs\nFigure 8.15\nAn illustration of the forward propagation of d2through the network showing the weights on each\nconnection, and the weighted sum zand activation avalue for each neuron in the network.\ndecimal places, and we will round the δs for other neurons similarly and use the rounded\nvalues in our calculations so that the results of the calculations match the numbers that are\npresented on the page.\nδ8“BE\nBa8ˆBa8\nBz8\n“0.3418ˆ0.2492\n“0.0852 (8.32)\nNeuron 8 is the only output neuron in the network, and so once δ8has been calculated,\nwe can then proceed to backpropagate the error gradient of the network for d2and calculate\ntheδs for Neurons 6 and 7. These are hidden neurons, so we use the Equation (8.23)[412]\nto calculate δ6andδ7; Equation (8.33)[428]steps through the calculation of δ6and Equation\n(8.34)[428]steps through the calculation for δ7","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":482,"page_label":"428","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"428 Chapter 8 Deep Learning\nTable 8.5\nTheBa{Bzfor each neuron for Example 2 rounded to four decimal places.\nNEURON zBa{Bz\n3 -0.004200 0.2500\n4 -0.189800 0.2478\n5 0.120800 0.2491\n6 0.207122 0.2473\n7 0.235460 0.2466\n8 -0.113108 0.2492\nδ6“BE\nBa6ˆBa6\nBz6\n“´ÿ\nδiˆwi,6¯\nˆBa6\nBz6\n“pδ8ˆw8,6qˆBa6\nBz6\n“p0.0852ˆ0.12qˆ0.2473\n“ 0.0025 (8.33)\nδ7“BE\nBa7ˆBa7\nBz7\n“´ÿ\nδiˆwi,7¯\nˆBa7\nBz7\n“pδ8ˆw8,7qˆBa6\nBz6\n“p0.0852ˆ´0.50qˆ0.2466\n“ ´ 0.0105 (8.34)\nOnce theδs for Neurons 6 and 7 have been calculated, we are ready to propagate the error\ngradients back to the ﬁrst hidden layer. The process used to calculate the δs for Neurons 3,\n4, and 5 is the same as that used to calculate δ6andδ7. Equation (8.35)[429]steps through this\ncalculation for δ3, Equation (8.36)[429]shows the calculation of δ4, and Equation (8.37)[429]\nlists the calculations of δ5","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":483,"page_label":"429","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 429\nδ3“BE\nBa3ˆBa3\nBz3\n“´ÿ\nδiˆwi,3¯\nˆBa3\nBz3\n“ ppδ6ˆw6,3q`pδ7ˆw7,3qqˆBa3\nBz3\n“pp0.0025ˆ´0.04q`p´0.0105ˆ0.20qqˆ0.2500\n“ ´ 0.0006 (8.35)\nδ4“BE\nBa4ˆBa4\nBz4\n“´ÿ\nδiˆwi,4¯\nˆBa4\nBz4\n“ppδ6ˆw6,4q`pδ7ˆw7,4qqˆBa4\nBz4\n“pp0.0025ˆ0.10q`p´0.0105ˆ0.14qqˆ0.2478\n“ ´ 0.0003 (8.36)\nδ5“BE\nBa5ˆBa5\nBz5\n“´ÿ\nδiˆwi,5¯\nˆBa5\nBz5\n“ ppδ6ˆw6,5q`pδ7ˆw7,5qqˆBa5\nBz5\n“pp0.0025ˆ0.06q`p´0.0105ˆ´0.09qqˆ0.2491\n“ 0.0003 (8.37)\nWe have now calculated the δs for all the neurons in the network for d2. Figure 8.16[430]\nshows the network with each neuron labeled with its corresponding δ. Eachδexpresses\nthe rate of change (or sensitivity) of the error of the network with respect to changes in\nthe weighted sum calculation of a speciﬁc neuron. In order to update the weights on the\nconnections coming into a neuron, we must connect the rate of change of the error of the\nnetwork to changes in each weight. Equation (8.27)[414]deﬁned how the rate of change of\nthe error of a network with respect to a weight in the network BE{Bwi,kcan be calculated\nusing the chain rule. This calculation reduces to multiplying the δfor the neuron that\nuses the weight in its weighted sum by the activation that the weight was applied to in\nthe weighted sum. Table 8.6[431]lists the calculations of BE{Bwi,kfor each weight in the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":484,"page_label":"430","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"430 Chapter 8 Deep Learning\n3\n1 6\n4 8\n2 7\n5w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0−0.0006\n−0.0003\n0.00030.0025\n−0.01050.0851\nError gradients ( δs) ﬂow from outputs to inputs\nFigure 8.16\nTheδs for each of the neurons in the network for Example 2.\nnetwork for d2. This table is ordered so that the weights for inputs to Neuron 8 are at the\ntop, and then as we move down the table, we move back through the network.\nIf we are using stochastic gradient descent, in which we update the weights after each\nexample has been presented to the network, then to update a weight in the network we\nﬁrst calculate a BE{Bwi,kterm for the weight similarly to the calculations shown in Table\n8.6[431]ford2and then plug the BE{Bwi,kvalue for the weight into Equation (8.28)[415]. For\nexample, if we assume that d2was the ﬁrst example processed21and that we have set the\nlearning rate hyper-parameter to α“0.2, then we would update weight w7,5as follows:\n21. This assumption of treating d2as the ﬁrst example permits us to use the original w7,5“´0.09in our example\ncalculation rather than the updated value for the weight that would be used if d1had already been processed and\nthe weights updated accordingly.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":485,"page_label":"431","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 431\nTable 8.6\nTheBE{Bwi,kcalculations for d2for every weight in the network. We use the neuron index 0 to\ndenote the bias input for each neuron.\nNEURON i NEURON k wi,k δi ak BE{Bwi,k\n8 0 w8,0 0.0852 1 0 .0852ˆ1“0.0852\n8 6 w8,6 0.0852 0.5516 0 .0852ˆ0.5516“0.04699632\n8 7 w8,7 0.0852 0.5586 0 .0852ˆ0.5586“0.04759272\n7 0 w7,0´0.0105 1 ´0.0105ˆ1“´0.0105\n7 3 w7,3´0.0105 0.4990´0.0105ˆ0.4527“´0.0052395\n7 4 w7,4´0.0105 0.4527´0.0105ˆ0.4527“´0.00475335\n7 5 w7,5´0.0105 0.5302´0.0105ˆ0.5302“´0.0055671\n6 0 w6,0 0.0025 1 0 .0025ˆ1“0.0025\n6 3 w6,3 0.0025 0.4990 0 .0025ˆ0.4527“0.0012475\n6 4 w6,4 0.0025 0.4527 0 .0025ˆ0.4527“0.00113175\n6 5 w6,5 0.0025 0.5302 0 .0025ˆ0.5302“0.0013255\n5 0 w5,0 0.0003 1 0 .0003ˆ1“0.0003\n5 1 w5,1 0.0003 0 .84 0 .0003ˆ0.84“0.000252\n5 2 w5,2 0.0003 0 .58 0 .0003ˆ0.58“0.000174\n4 0 w4,0´0.0003 1 ´0.0003ˆ1“´0.0003\n4 1 w4,1´0.0003 0 .84´0.0003ˆ0.84“´0.000252\n4 2 w4,2´0.0003 0 .58´0.0003ˆ0.58“´0.000174\n3 0 w3,0´0.0006 1 ´0.0006ˆ1“´0.0006\n3 1 w3,1´0.0006 0 .84´0.0006ˆ0.84“´0.000504\n3 2 w3,2´0.0006 0 .58´0.0006ˆ0.58“´0.000348\nw7,5“w7,5´αˆ δ7ˆa5\n“w7,5´αˆBE\nBwi,k\n“´0.09´0.2ˆ ´ 0.0055671\n“´0.08888658 (8.38)\nHowever, in this example we use batch gradient descent training, and so for each weight\nupdate we must ﬁrst calculate a table equivalent to Table 8.6[431]for each example. Once\nthese tables have been created, for each weight we sum the BE{Bwi,kfor that weight across\nthe tables. This summation gives us the ∆wi,kterm in Equation (8.30)[416]. Table 8.7[432]\nillustrates this calculation for weight w7,5, and Equation 8.39[432]shows how the ∆w7,5is\nused as part of Equation 8.30[416]to update the weight after the batch of four examples has\nbeen processed; again, in this weight update we assume that α“0.2.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":486,"page_label":"432","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"432 Chapter 8 Deep Learning\nTable 8.7\nThe calculation of ∆w7,5across our four examples.\nMINI-BATCH BE\nBw7,5 EXAMPLE\nd1 0.00730080\nd2´0.00556710\nd3 0.00157020\nd4´0.00176664\n∆w7,5= 0.00153726\nw7,5“w7,5´αˆ∆wi,k\n“´0.09´0.2ˆ0.00153726\n“´0.0903074520 (8.39)\nIf we update all the weights in the network using the process illustrated in Equation\n8.39[432]for weight w7,5and then run the same examples through a forward pass of the\nnetwork, we reduce the error of the model by a small amount; Table 8.8[433]lists the per ex-\nample error and sum of squared errors for the four examples after all the weights have been\nupdated. Comparing Table 8.8[433]with Table 8.4[426], we see that the model made a slightly\ndifferent prediction for each example; the prediction for d2is now 0.4741 whereas the\noriginal prediction was 0.4718 . Also, the total error of the model on the dataset, measured\nhere by the sum of squared errors, has dropped slightly from 0.17921847 to0.17896823 ,\nan error reduction of 0.00025024 . Admittedly, this drop in error is tiny, but it is a reduction\nin error, and this reduction was achieved by updating randomly initialized weights once.\nIn a full network training scenario there would be many such weight updates as the train-\ning progressed through multiple epochs and, for each epoch, multiple mini-batches; the\nreduction of the network error would accumulate over these weight updates. For exam-\nple, assuming that we continue to train the network on just our small sample dataset with\nα“0.2and updating the weights after each pass through our data, then the training of\nthe network would converge to an S S Eă0.0001 after 7,656 epochs.22Table 8.9[433]lists\nthe model predictions for each of the examples and the calculation of the sum of squared\n22. The most popular strategy to deﬁne a convergence criterion to stop training a neural network is early stopping .\nEarly stopping is designed to avoid a model overﬁtting the training data. We explain the early stopping algorithm\nin Section 8.4.4[472]. However, for ease of explanation in this section, we have simply used a threshold of S S Eă\n0.0001 as our convergence criterion. Using a threshold on the error of the model error on the training set as a\nconvergence criterion—as we are doing here—is very likely to result in the model overﬁtting the training data.\nHence it is worth emphasizing that we are using this convergence criterion only for ease of explanation, and in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":486,"page_label":"432","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"22. The most popular strategy to deﬁne a convergence criterion to stop training a neural network is early stopping .\nEarly stopping is designed to avoid a model overﬁtting the training data. We explain the early stopping algorithm\nin Section 8.4.4[472]. However, for ease of explanation in this section, we have simply used a threshold of S S Eă\n0.0001 as our convergence criterion. Using a threshold on the error of the model error on the training set as a\nconvergence criterion—as we are doing here—is very likely to result in the model overﬁtting the training data.\nHence it is worth emphasizing that we are using this convergence criterion only for ease of explanation, and in\ngeneral we recommend that you use early stopping when you are training a neural network.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":487,"page_label":"433","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.3 Standard Approach: Backpropagation and Gradient Descent 433\nTable 8.8\nThe per example error after each weight has been updated once, the per example BE{Ba8, and the\nsum of squared errors for the model.\nd1 d2 d3 d4\nTarget 0.9400 0.1300 0.5700 0.3600\nPrediction 0.4738 0.4741 0.4740 0.4739\nError 0.4662 -0.3441 0.0960 -0.1139\nBE{Ba8: Errorˆ´1 -0.4662 0.3441 -0.0960 0.1139\nError20.21734244 0.11840481 0.009216 0.01297321\nSSE: 0.17896823\nTable 8.9\nThe per example prediction, error, and the sum of squared errors after training has converged to an\nS S Eă0.0001 .\nd1 d2 d2 d2\nTarget 0.9400 0.1300 0.5700 0.3600\nPrediction 0.9266 0.1342 0.5700 0.3608\nError 0.0134 -0.0042 0.0000 -0.0008\nError20.00017956 0.00001764 0.00000000 0.00000064\nSSE: 0.00009892\nerrors once training has converged. Figure 8.17[434]illustrates how the sum of squared er-\nrors of the network changes during the training. This plot shows that the reduction in the\nerror achieved by each weight update (one per epoch) is initially quite small. However,\nafter approximately 5,000 epochs, the rate of decrease of the error increases dramatically\nuntil it ﬂattens out again around 6,000 epochs. From then on, the sum of squared errors\nof the network continues to slowly reduce until it reaches the convergence criterion of\nS S Eă0.0001 . As with the regression examples in Chapter 7[311], the proﬁle of this error\nreduction over the course of the training is sensitive to a number of factors, for example,\nthe learning rate αand how it is updated throughout the training (see Section 7.3.3[328]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"434 Chapter 8 Deep Learning","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":488,"page_label":"434","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"0 2000 4000 60000.00 0.05 0.10 0.15\nEpochSum of Squared Errors\nFigure 8.17\nA plot showing how the sum of squared errors of the network changed during training.\n8.4 Extensions and Variations\nOne of the biggest challenges in training a deep neural network is to ensure that the ﬂow\nof error gradients back through the layers of a network is stable during training. Con-\nsequently, in Sections 8.4.1[434]and 8.4.2[447]we explain and motivate the current standard\napproaches to deciding on two of the most important hyper-parameter decisions for keep-\ning error gradients stable: the selection of the activation function, and the initialization\nof the weights. Then in Section 8.4.3[463]we explain how to adapt the design of a neural\nnetwork to classiﬁcation problems by using a softmax output layer and the cross-entropy\nloss function. Whether deep learning is applied to regression or classiﬁcation, the size and\ncomplexity of deep learning models make them susceptible to overﬁtting , and so in Sec-\ntion 8.4.4[472]we cover the two most popular extensions to backpropagation that are used to\ntry to avoid overﬁtting: early stopping anddropout . We then conclude the extensions and\nvariations of deep learning by introducing two of the most popular network architectures\nused in deep learning: convolutional neural networks (Section 8.4.5[477]) and recurrent\nneural networks (Section 8.4.6[499]).\n8.4.1 Vanishing Gradients and ReLUs\nFigure 8.16[430]shows the network from the worked example in a graph form with each of\nthe neurons labeled with the δfor the neuron. These δs were calculated by backpropagating\nthe error on Example 2. If we compare the δfor Neuron 8 (the output neuron) with the\nδs for the neurons in the ﬁrst hidden layer (Neurons 3, 4, and 5), it becomes clear that\ntheδvalues become smaller as they are propagated back from the output layer to the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":489,"page_label":"435","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 435\nearlier layers in the network: δ8“0.0851 , whereasδ3“ ´ 0.0006 ,δ4“ ´ 0.0003 , and\nδ5“0.0003 are all smaller in magnitude. This phenomenon is known as the vanishing\ngradient problem, because in deep networks the δterms tend toward zero and vanish as\nwe propagate the δs back to the early layers.\nVanishing gradients are a serious challenge for training deep networks because the δs\nare the learning signal that tells each neuron how it should update its weights in order to\nimprove the network’s performance on an example. In other words, the updates applied to\na weight are a function of the δfor the neuron that uses the weight in its weighted sum.\nHowever, as the δs vanish, the learning signal attenuates, and this can slow down the rate\nat which the early layers of the network learn. Concretely, this means that in each weight\nupdate, the weights in the earlier layers in the network are updated by smaller amounts\nthan the neurons in the later layers. This differential in the weight updates is apparent in\nTable 8.6[431], which is arranged so that BE{Bwi,kterms for the weights on inputs to output\nlayer (Neuron 8) are at the top of the table, and then as we move down the table, we move\nback through the network. Inspecting the right-hand column of this table and comparing\nthe magnitude of the BE{Bwi,kterms, the most signiﬁcant digit in the BE{Bwi,kterms for\nweights on inputs to Neuron 8 are at 10´2; by comparison the most signiﬁcant digit for the\nBE{Bwi,kterms for the weights on inputs to Neurons 3, 4, and 5 (the ﬁrst hidden layer) are\nat10´4. This differential means that the neurons in the ﬁrst hidden layer will learn more\nslowly than the neurons output layer. Furthermore, the deeper the network becomes, the\nslower the earlier layers learn (because the learning signal, the error gradient, attenuates as\nit is backpropagated through the layers).\nThis is a serious problem because the ability of a deep neural network to learn a useful\nrepresentation of the inputs works by the earlier layers of the network extracting low-level\nfeatures from the raw data and then the later layers learning to combine these features\nin useful ways (see Figure 8.10[402]for an illustration of this for a three-layer network).\nHowever, if the neurons in the early layers of the network take a long time to train, then the\nneurons in the later layers cannot efﬁciently build upon the outputs of these early neurons.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":489,"page_label":"435","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"slower the earlier layers learn (because the learning signal, the error gradient, attenuates as\nit is backpropagated through the layers).\nThis is a serious problem because the ability of a deep neural network to learn a useful\nrepresentation of the inputs works by the earlier layers of the network extracting low-level\nfeatures from the raw data and then the later layers learning to combine these features\nin useful ways (see Figure 8.10[402]for an illustration of this for a three-layer network).\nHowever, if the neurons in the early layers of the network take a long time to train, then the\nneurons in the later layers cannot efﬁciently build upon the outputs of these early neurons.\nThe net result is that the vanishing gradient problem can cause deep networks to take a\nvery long time to train. The vanishing gradient problem is a direct consequence of the fact\nthat the backpropagation algorithm is based on the chain rule . Using backpropagation, the\nerror gradient at any point in the network is a product of the gradients up to that point; for\nconvenience, we repeat Equation (8.25)[414]here\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k\n“ δiˆBzi\nBwi,k\n(8.40)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":490,"page_label":"436","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"436 Chapter 8 Deep Learning\nReading from left to right, the ﬁrst term in this product is the rate of change of the error\ngradient of the network with respect to changes in the activation (or output) of a neuron i;\nthe second term is the rate of change of the activation of neuron iwith respect to changes\ninzi(the weighted sum for neuron i); and the third term is the rate of change of ziwith\nrespect to changes in the weight wi,k.\nThe earlier in the network a weight occurs, the more terms there are in the product. This\nexpansion in the number of terms arises from the fact that for all hidden neurons the δterm\nis calculated by a product of (1) BE{Bai, which itself is a product of the weighted sum\nof theδs backpropagated to the neuron; and (2) the derivative of the neuron’s activation\nfunction, that is, a term of the form Bak{Bzk(see Equation (8.23)[412]). However, if these\nbackpropagated δs came from hidden neurons, then they were also calculated as a product\nof the weighted sum of the δs backpropagated to those hidden neurons and the Bak{Bzk\nterm for those neurons. To illustrate this growth in the number of terms in the chain of\nproducts as we move back through a network, imagine a simple feedforward network with\njust three neurons, i,j, and k, arranged so that ifeeds forward into jandjintok\niÑjÑk\nEquation 8.41[436]illustrates the expansion of the chain of products as the error gradient is\npropagated back through the network. By the time the original error gradient term BE{Bak\nis propagated back to δiit is multiplied by two weights, wj,iandwk,j, and three times by\nthe derivative of the activation function of a neuron with respect to the weighted sum of\nthe neuron:Bak{Bzk,Baj{Bzj,Bai{Bzi. We return to the discussion of weights in the next\nsection, Section 8.4.2[447]; our focus in this section is on the effect of repeatedly multiplying\nthe error gradient by the derivative of an activation function\nδi“BE\nBaiˆBai\nBzi\n“hkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkj\nwj,iˆ δjˆBai\nBzi\n“wj,iˆhkkkkkkkkkkkkkikkkkkkkkkkkkkj\nwk,jˆδkˆBaj\nBzjˆBai\nBzi\n“wj,iˆwk,jˆhkkkkkikkkkkj\nBE\nBakˆBak\nBzkˆBaj\nBzjˆBai\nBzi\n(8.41)\nMultiplying a number by a number less than 1 makes the number smaller. With this in\nmind, it is instructive to refer to Figure 8.13[410], which plots the logistic function and the\nderivative of the logistic function. Figure 8.13[410]shows that the maximum value of the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":491,"page_label":"437","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 437\nderivative of the logistic function is 0.25. The ﬁgure also shows that the derivative is 0 (or\nnear 0) for the saturated regions of the function: the logistic function saturates for large\nnegative («ză ´ 2) or positive values ( zą ` 2); this means that the rate of change of\nthe output of the logistic function with respect to small changes in z, when zhas a large\nnegative or positive value, is 0 (or near 0).\nEquation 8.41[436]shows that during backpropagation the error gradient is repeatedly mul-\ntiplied by a derivative of activation functions, one multiplication for each neuron that the\nerror gradient is backpropagated through. When a neuron uses a logistic function as its\nactivation function, then the maximum value the derivative can take is 0.25. Consequently,\nbackpropagating the error gradient through the neuron that uses a logistic activation func-\ntion involves multiplying the error term by a value ď0.25and that is«0forzvalues in\nthe saturated regions of the logistic function. If all the neurons in a network use a logistic\nactivation function or another activation function whose derivative has a small range less\nthan 1, then the error gradient will get smaller and smaller as it is backpropagated through\nthe networks layers, and the scaling down of the gradient is particularly severe for neurons\nwhose zvalue is in the saturated region of the activation function.\nGiven that the vanishing gradient problem arises from repeatedly multiplying the error\ngradients by the derivative of the activation functions, one way to address the vanishing\ngradient problem is to use a different activation function. At the start of the chapter we\nintroduced the rectiﬁed linear function:23\nrecti f ierpzq“maxp0,zq“#\nzifzą0\n0otherwise(8.42)\nThe derivative of the rectiﬁer function is:\nd\ndzrecti f ierpzq“#\n1ifzą0\n0otherwise(8.43)\nThe fact that the derivative of the rectiﬁer function takes either a 0or a1value means that\nduring backpropagation for some neurons in a network, the δvalue will be pushed to zero.\nHowever, this is not necessarily a problem: if we consider the backpropagation process\nas error gradients, δs, ﬂowing through a layer of neurons then, although for some neurons\nin the layer the δs will go to zero, for others the gradients will pass through unscaled.\nIn particular, in any neuron where the input to the rectiﬁer function is greater than zero\n(zą0) then the neuron will activate during the forward pass and the partial derivative","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":491,"page_label":"437","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"zifzą0\n0otherwise(8.42)\nThe derivative of the rectiﬁer function is:\nd\ndzrecti f ierpzq“#\n1ifzą0\n0otherwise(8.43)\nThe fact that the derivative of the rectiﬁer function takes either a 0or a1value means that\nduring backpropagation for some neurons in a network, the δvalue will be pushed to zero.\nHowever, this is not necessarily a problem: if we consider the backpropagation process\nas error gradients, δs, ﬂowing through a layer of neurons then, although for some neurons\nin the layer the δs will go to zero, for others the gradients will pass through unscaled.\nIn particular, in any neuron where the input to the rectiﬁer function is greater than zero\n(zą0) then the neuron will activate during the forward pass and the partial derivative\nof the activation function will be 1 during the backward pass. In these cases the error\ngradients are able to be backpropagated through the neuron without scaling. Consequently,\nthe error gradients in the backward pass can propagate along the paths of active neurons\n23. See Equation (8.4)[386]and Figure 8.2[387].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":492,"page_label":"438","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"438 Chapter 8 Deep Learning\nin the network, and because Bak{Bzk“1along these paths, the gradients can ﬂow back\nthrough a deeper network (Glorot et al., 2011). Note that neurons that use a rectiﬁed linear\nactivation function are known as ReLUs (short for rectiﬁed linear units ).\nTechnically, the derivative of the rectiﬁer function is undeﬁned when zk“0, so strictly,\nthe rectiﬁer function is not an appropriate choice for gradient descent and backpropagation,\nwhich assume differentiable functions. However, a common practice in neural networks\nis to choose a derivative value for zk“0of zero, and we have integrated this convention\ninto the deﬁnition of Equation (8.43)[437]. This convention has been found generally to work\nwell.\nFigure 8.18[440]shows the forward propagation of the examples in Table 8.3[423]through\nthe same network that we used in the worked example, with the single difference that all\nthe neurons in the network are now ReLUs.24Table 8.10[441]lists the per example error of\nthis ReLU network resulting from the forward pass in Figure 8.18[440]. This table also lists\nthe sum of squared errors for the ReLU model.25\nTo illustrate the effect that switching to a rectiﬁed linear activation function has on a\nnetwork, we step through the forward and backward pass of the backpropagation algorithm\nford2. Figure 8.19[441]illustrates the forward pass for d2through the ReLU network. This\nﬁgure shows the weights on each connection and for each neuron shows the weighted sum\nzcalculated by that neuron (the number on the left of the neuron) and the activation afor\nthe neuron (the number on the right of the neuron). These zandavalues will be found\nin the corresponding gray boxes in Figure 8.18[440]. Notice that both Neurons 3 and 4 have\nan activation of zero for d2. The reason is that both these neurons happen to have large\nnegative weights on at least one of their inputs. However, the forward propagation through\nthe network still occurs along an active path through Neuron 5.\nIn order to calculate and backpropagate the δs for d2through the network, we need the\nBa{Bzfor each neuron in the network for this example, Table 8.11[442]lists these values;\nbecause all the neurons use a rectiﬁed linear activation function, the Ba{Bzare either 0 or\n1 (as per derivative of the rectiﬁed linear function given in Equation 8.43[437]). From these\nvalues we can see that the error gradient will be able to ﬂow backward through Neurons 8,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":492,"page_label":"438","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"an activation of zero for d2. The reason is that both these neurons happen to have large\nnegative weights on at least one of their inputs. However, the forward propagation through\nthe network still occurs along an active path through Neuron 5.\nIn order to calculate and backpropagate the δs for d2through the network, we need the\nBa{Bzfor each neuron in the network for this example, Table 8.11[442]lists these values;\nbecause all the neurons use a rectiﬁed linear activation function, the Ba{Bzare either 0 or\n1 (as per derivative of the rectiﬁed linear function given in Equation 8.43[437]). From these\nvalues we can see that the error gradient will be able to ﬂow backward through Neurons 8,\n7, and 6 without being scaled down by the multiplication by Ba{Bzin theδcalculations.\nEquation 8.44[442]lists the calculations of the δs for d2for all the neurons in the ReLU\nnetwork. Comparing the δs for the ReLU network with those calculated for the logistic\nnetwork for the same example (see Figure 8.16[430]) shows that, where the derivative of the\n24. Note that as with the worked example, to keep the number of decimal points required to represent the calcu-\nlations through this forward pass manageable, the outputs of the activation function (in this instance the rectiﬁed\nlinear function) for each layer have been rounded to four decimal places, and these rounded activations were the\nactivations used in the subsequent calculations.\n25. Note that comparing the sum of squared errors for the logistic network in Table 8.4[426]with the sum of squared\nerrors for the ReLU model in Table 8.10[441]is not useful, because both of these are errors for random networks.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":493,"page_label":"439","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 439\nReLU has not pushed the δto0, the drop in the magnitude of the δs as we move back\nthrough the network is not as severe; for example, in Figure 8.16[430]there is a consistent\nreduction in the magnitude of the δvalues from 10´2in the output layer to 10´4in the ﬁrst\nhidden layer, and in the ReLU network the drop is from 10´2in the output layer to 10´3in\nthe ﬁrst hidden layer. The fact that both Neurons 3 and 4 had an activation of 0 in response\ntod2resulted in both of these neurons having a δ“0. This means that weights for these\nneurons will not be updated for this example.\nWe can illustrate the speed up in training that can be achieved by switching a network\nfrom using the logistic activation function to a rectiﬁed linear activation function if we\ncompare the number of epochs required for the network from the worked example to con-\nverge to an S S Eă0.0001 when it uses ReLUs instead of logistic units. If we initialize\nthe network with the same initial set of weights that we used in Section 8.3.5[421]and use\nthe same data, training regime, and learning rate ( α“0.2), then the ReLU version of the\nnetwork converges to an S S Eă0.0001 in just 424 epochs, as compared with the 7,656\nepochs required to train the logistic network. Table 8.12[443]lists the predictions of the\ntrained ReLU network for each of the examples and the calculation of the sum of squared\nerrors once training has converged. Figure 8.20[443]illustrates how the sum of squared errors\nof the ReLU network changes during the training of the network. The plot shows that the\ninitial sum of squared errors of the network is just under 0.6, and this drops to ă0.2after\nthe ﬁrst epoch. After approximately 150 epochs the rate of decrease of the sum of squared\nerrors increases, but training also becomes a bit unstable with the sum of squared errors of\nthe network sometimes increasing and decreasing dramatically. This type of instability can\nbe indicative that the learning rate αis too high. In fact, because the rectiﬁer linear function\nis unbounded,26it is often the case that smaller learning rates are necessary in training a\nReLU network than in training a network with logistic units. Figure 8.21[444]plots the SSE\nof the ReLU network across the training epochs when we use a smaller learning rate, in\nthis caseα“0.1. Not only is the learning more stable (i.e., smoother) but the training of\nthe network actually converges on the stop criterion of S S Eă0.0001 in fewer epochs,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":493,"page_label":"439","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"errors increases, but training also becomes a bit unstable with the sum of squared errors of\nthe network sometimes increasing and decreasing dramatically. This type of instability can\nbe indicative that the learning rate αis too high. In fact, because the rectiﬁer linear function\nis unbounded,26it is often the case that smaller learning rates are necessary in training a\nReLU network than in training a network with logistic units. Figure 8.21[444]plots the SSE\nof the ReLU network across the training epochs when we use a smaller learning rate, in\nthis caseα“0.1. Not only is the learning more stable (i.e., smoother) but the training of\nthe network actually converges on the stop criterion of S S Eă0.0001 in fewer epochs,\n412 instead of 424. This illustrates how sensitive the training of a deep neural network can\nbe to a range of hyper-parameters, such as the learning rate and activation function.\n26. By unbounded , we mean that, unlike the logistic function where the maximum value it will return is 1, the\nrectiﬁer linear function may return any value up to `8.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":494,"page_label":"440","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"+0.10−0.20+0.11\n−0.19+0.09−0.13\n+0.04+0.11−0.02\nHidden Layer 1\nWeight Matrix1.00 1.00 1.00 1.00\n0.04 0.84 0.50 0.53\n0.81 0.58 0.07 1.00\nInput Layer=+0.1811 −0.0042 +0.0077 +0.1040\n−0.2917 −0.1898 −0.1541 −0.2723\n+0.0282 +0.1208 +0.0936 +0.0783Z(1)\nϕ0.1811 0.0000 0.0077 0.1040\n0.0000 0.0000 0.0000 0.0000\n0.0282 0.1208 0.0936 0.0783Activations\nHidden Layer 1\n+0.15−0.04+0.10+0.06\n+0.12+0.20+0.14−0.09\nHidden Layer 2\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.1811 0.0000 0.0077 0.1040\n0.0000 0.0000 0.0000 0.0000\n0.0282 0.1208 0.0936 0.0783\nActivations\nHidden Layer 1=+0.144448 +0.157248 +0.155308 +0.150538\n+0.153682 +0.109128 +0.113116 +0.133753Z(2)\nϕ0.1444 0.1572 0.1553 0.1505\n0.1537 0.1091 0.1131 0.1338Activations\nHidden Layer 2\n+0.10+0.12−0.50\nOutput Layer\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.1444 0.1572 0.1553 0.1505\n0.1537 0.1091 0.1131 0.1338\nActivations\nHidden Layer 2=+0.040478 +0.064314 +0.062086 +0.05116Z(3)\nϕ0.0405 0.0643 0.0621 0.0512Activations\nOutput Layer3\n4\n51 2 d1d2d3d4\n6\n73 4 5\n86 7+d[0]=1\n+d[0]=1\nFigure 8.18\nThe forward pass of the examples listed in Table 8.3[423]through the network in Figure 8.4[390]when\nall the neurons are ReLUs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":495,"page_label":"441","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 441\n3\n1 6\n4 8\n2 7\n5−0.20\n0.09\n0.110.11\n−0.13\n−0.02−0.04\n0.200.10\n0.140.06\n−0.090.12\n−0.500.10\n−0.19\n0.040.15\n0.120.10−0.0042 0.0000\n−0.1898 0.0000\n0.1208 0.12080.157248 0.1572\n0.109128 0.10910.064314 0.0643\nActivations ﬂow from inputs to outputs\nFigure 8.19\nAn illustration of the forward propagation of d2through the ReLU network showing the weights on\neach connection, and the weighted sum zand activation avalue for each neuron in the network.\nTable 8.10\nThe per example error of the ReLU network after the forward pass illustrated in Figure 8.18[440], the\nper exampleBE{Ba8, and the sum of squared errors for the ReLU model.\nd1 d2 d3 d4\nTarget 0.9400 0.1300 0.5700 0.3600\nPrediction 0.0405 0.0643 0.0621 0.0512\nError 0.8995 0.0657 0.5079 0.3088\nBE{Ba8: Errorˆ´1´0.8995´0.0657´0.5079´0.3088\nError20.80910025 0.00431649 0.25796241 0.09535744\nSSE: 0.58336829","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":496,"page_label":"442","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"442 Chapter 8 Deep Learning\nTable 8.11\nTheBa{Bzfor each neuron for d2rounded to four decimal places.\nNEURON zBa{Bz\n3 -0.004200 0\n4 -0.189800 0\n5 0.120800 1\n6 0.157248 1\n7 0.109128 1\n8 0.064314 1\nδk“BE\nBakˆBak\nBzi\nδ8“ ´ 0.0657ˆ1.0\n“ ´ 0.0657\nδ7“ pδ8ˆw8,7qˆBa7\nBz7\n“ p´0.0657ˆ´0.50qˆ1\n“ 0.0329\nδ6“ pδ8ˆw8,6qˆBa6\nBz6\n“ p´0.0657ˆ0.12qˆ1\n“ ´ 0.0079\nδ5“ ppδ6ˆw6,5q`pδ7ˆw7,5qqˆBa5\nBz5\n“pp´0.0079ˆ0.06q`p0.0329ˆ´0.09qqˆ1\n“ ´ 0.0034\nδ4“ ppδ6ˆw6,4q`pδ7ˆw7,4qqˆBa4\nBz4\n“pp´0.0079ˆ0.10q`p0.0329ˆ0.14qqˆ0\n“ 0\nδ3“ ppδ6ˆw6,3q`pδ7ˆw7,3qqˆBa3\nBz3\n“pp´0.0079ˆ´0.04q`p0.0329ˆ0.20qqˆ0\n“ 0\n(8.44)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":497,"page_label":"443","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 443\nTable 8.12\nThe ReLU network’s per example prediction, error, and the sum of squared errors after training has\nconverged to an S S Eă0.0001 .\nd1 d2 d3 d4\nTarget 0.9400 0.1300 0.5700 0.3600\nPrediction 0.9487 0.1328 0.5772 0.3679\nError -0.0087 -0.0028 -0.0072 -0.0079\nError20.00007569 0.00000784 0.00005184 0.00006241\nSSE: 0.00009889\n●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●●●●●●●\n●●●●\n●●●●\n●●●●●●\n●●●●●●\n●●●●●●●●●●\n●●●●●●●●●●●●●●\n●●●●●●●●●●●●●●\n●●\n●●\n●●\n●●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●\n●●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5 0.6\nEpochSum of Squared Errors\nFigure 8.20\nA plot showing how the sum of squared errors of the ReLU network changed during training when\nα“0.2.\nWe have seen that switching a network from a logistic activation function to a rectiﬁed\nlinear activation function can speed up training; however, a less obvious effect of this\nswitch is that it also tends to make the representations learned by a network sparse . What\nthis means is that for any given input vector, only a subset of the neurons in the network will\nactivate (i.e., aią0). This is because the rectiﬁed linear function has a zero output value\nfor half its domain (i.e., for all zď0). For example, consider a feedforward network that\nhas been initialized with weights randomly sampled with uniform probability from a range\nsuch asr´0.5,`0.5s. In such a network, immediately after initialization, approximately\nhalf the hidden neurons in the network will have activations equal to zero; in a sense, in\na network with a sparse representation, each input vector ﬂows through a subset of active\npathways in the network (Glorot et al., 2011). By contrast in a network using logistic","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":498,"page_label":"444","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"444 Chapter 8 Deep Learning\n●\n●\n●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5 0.6\nEpochSum of Squared Errors\nFigure 8.21\nA plot showing how the sum of squared errors of the ReLU network changed during training when\nα“0.1.\nactivation functions, the vast majority of neurons will be active for all inputs. Using a\nsparse representation can reduce the energy consumed by a network (Reagen et al., 2017)\nand is also more biologically plausible (Glorot et al., 2011). However, if the representations\nbecome too sparse, then the performance of the network may deteriorate.\nFor example, consider Neuron 4 in our example network. This neuron has two large\nnegative weights ( w4,0“´0.19andw4,2“´0.13). The effect of these negative weights\nis apparent in the Zp1qmatrix in forward pass of the mini-batch through the ReLU network\nin Figure 8.18[440]: for all four examples of the mini-batch z4ă0, and hence a4“0.\nThe fact that the ReLU activation function for Neuron 4 is saturated for all four examples\nmeans that Neuron 4 is essentially dead during the forward pass of the algorithm (it does\nnot activate for any example) and this reduces the representational capacity of the network.\nFurthermore, during the backward pass of the algorithm the derivative of the rectiﬁed linear\nfunction for Neuron 4 is zero for all four examples, causing the error gradient to be pushed\nto zero for all four examples, and so the weights on connections into Neuron 4 receive no\nupdates. Consequently, no matter how long we train the network, Neuron 4 will remain in\nthis dead state. This dynamic of a ReLU being in a state where it is inactive for all (or nearly\nall) inputs and consequently it never updated and so never becomes active is known as the\ndying ReLU problem. If too many neurons in a network are dead, then the network will\nnot converge during training. For example, in our example network, if Neuron 8 is dead,\nthe network will never converge on the training stop criterion, because no error gradients\nwill be backpropagated to the earlier layers and so no training will occur. Similarly, if","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":499,"page_label":"445","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 445\nNeurons 6 and 7 are dead, then no error gradients will be backpropagated past this layer\nof neurons and the network is essentially reduced to a perceptron network composed of\njust Neuron 8, as none of the other neurons will have their input weights updated during\ntraining.\nA simple heuristic that is sometimes used to try to avoid dead ReLUs is to initialize all\nthe bias weights of a network to small positive values, such as 0.1, because this increases\nthe likelihood that most of the neurons will initially be active for most of the training\nexamples and so these neurons can learn from these examples (Goodfellow et al., 2016,\np. 187). However, this heuristic is not guaranteed to avoid dead ReLUs: returning to the\nforward propagation of d2through the ReLU network (see Figure 8.19[441]), in this network\nNeuron 3 already has a bias of w3,0“0.1, but this positive bias is dominated by the\nlarge negative weight w3,1“ ´ 0.20. Therefore, Neuron 3 illustrates that even with the\nheuristic of setting the bias weights to 0.1, a ReLU may still not activate to every (or in\nextreme cases any) input pattern. We return to the topic of weight initialization strategies\nin Section 8.4.2[447]. Another approach to avoiding dead ReLUs is to modify the rectiﬁed\nlinear function so that it does not saturate for ză0. Two popular variants of ReLU that\nadopt this strategy are the Leaky ReLU (Maas et al., 2013) and the Parametric ReLU\n(He et al., 2015).\nThe leaky rectiﬁed linear function has a small (predeﬁned) non-zero gradient when ză\n0. Maas et al. (2013) set the non-zero gradient for ză0to0.01, giving the following\ndeﬁnition of this function:\nrecti f ier leakypzq“#\nz ifzą0\n0.01ˆz otherwise(8.45)\nThe derivative we use to backpropagate a δthrough the recti f ier leakyfunction is\nd\ndzrecti f ier leakypzq“#\n1 ifzą0\n0.01 otherwise(8.46)\nNot surprisingly, neurons that use the leaky rectiﬁed linear function as their activation\nfunction are known as Leaky ReLUs. When we switch a network from ReLUs to Leaky\nReLUs, we sacriﬁce the potential beneﬁts in terms of energy efﬁciency of sparse represen-\ntations for a gradient that may potentially be more robust during training. Leaky ReLUs\nalways activate to some extent for every input; however, given that for zď0the derivative\nof the recti f ier leakyfunction is very small ( 0.01), a leaky ReLU with large negative weight\nwill still learn very slowly.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":499,"page_label":"445","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"z ifzą0\n0.01ˆz otherwise(8.45)\nThe derivative we use to backpropagate a δthrough the recti f ier leakyfunction is\nd\ndzrecti f ier leakypzq“#\n1 ifzą0\n0.01 otherwise(8.46)\nNot surprisingly, neurons that use the leaky rectiﬁed linear function as their activation\nfunction are known as Leaky ReLUs. When we switch a network from ReLUs to Leaky\nReLUs, we sacriﬁce the potential beneﬁts in terms of energy efﬁciency of sparse represen-\ntations for a gradient that may potentially be more robust during training. Leaky ReLUs\nalways activate to some extent for every input; however, given that for zď0the derivative\nof the recti f ier leakyfunction is very small ( 0.01), a leaky ReLU with large negative weight\nwill still learn very slowly.\nHe et al. (2015) introduced the Parametric ReLU (PReLU) for which the main distinc-\ntion from the Leaky ReLU was that rather than using a ﬁxed predeﬁned gradient for zď0,\nthis gradient can be learned as a parameter for each neuron in the network. In other words,\neach neuron in the network learns a separate gradient for its activation function for the re-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":500,"page_label":"446","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"446 Chapter 8 Deep Learning\ngion zď0. As a result, when we deﬁne the activation function for a PReLU, we introduce\na subscript on the terms to identify the neuron and the corresponding gradient being used.\nHence for a PReLU i, the activation function is deﬁned as\nrecti f ier parametricpziq“#\nzi ifzią0\nλiˆziotherwise(8.47)\nThe derivative used to backpropagate δs through the recti f ier parametric function is then\ndeﬁned as\nd\ndzrecti f ier parametricpziq“#\n1ifzią0\nλiotherwise(8.48)\nThe parameter λis learned in tandem with the weights of the network. Similar to the\nweights of the network, the λparameter is initialized to a value and is then iteratively\nupdated as training progresses. In their experiments He et al. (2015) initialized the λpa-\nrameters in their networks to 0.25. Also similar to the weights in a network, the updating\nof aλis proportional to the error gradient of the network with respect to changes in that\nparameter\nBE\nBλi“BE\nBaiˆBai\nBλi(8.49)\nThe ﬁrst term in this product, BE{Bai, is the rate of change of the error of the network\nwith respect to changes in the activation function and is calculated as it would be for the\nweights in the network; for neurons in the output layer, it is calculated using Equation\n8.20[411], and for neurons in the hidden layer, it is calculated as the weighted sum of the\nδs backpropagated to the neuron, per Equation 8.22[412]. The second term in the product,\nBai{Bλi, is the gradient of the activation function with respect to changes in λi. This gradi-\nent is given by\nBai\nBλi“#\n0ifzią0\nziotherwise(8.50)\nOnce we have calculated the BE{Bλiterm, we update λiusing the following update rule:27\nλiÐλi´αˆBE\nBλi(8.51)\nwhereαis a learning rate.\n27. The similarity of this update rule to the standard weight update rule is apparent if we compare this equation\nwith Equation 8.28[415].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":501,"page_label":"447","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 447\nTo summarize, although the inclusion of a non-linear activation function within neurons\nin a network enables the network to represent non-linear mappings from inputs to out-\nputs, the selection of which non-linear function we use can have a signiﬁcant effect on the\ntraining speed of a deep network. For many years the logistic function was the default\nactivation function used in neural networks. The primary reason was that it had such a\nsimple derivative. However, although its derivative is simple, that it has a max value of\n0.25contributes to the vanishing gradient problem in neural networks. This is why in re-\ncent years most researchers have switched to using the rectiﬁer function (or variants) as the\ndefault activation function. Similar to the logistic function, the rectiﬁer function saturates\nin part of its domain, and this can lead to a dying ReLU dynamic in which a unit does\nnot activate for any of the training instances, and consequently the neuron is stuck in a\ndead non-active state. The leaky ReLU and parametric ReLU were developed to address\nthis potential problem. However, the particular choice of which variant of ReLU to use is\nnetwork- and task-dependent, and, similarly to most hyper-parameters, needs to be decided\nupon through experimentation. We noted in this section that a heuristic that is used to help\navoid saturating the rectiﬁer function is to initialize the bias weights in the network to 0.1.\nThis heuristic highlights the interaction between how we choose to initialize the weights\nin the network and how the error gradients ﬂow during backpropagation. The next section\ndiscusses this interaction in more detail and introduces some popular weight initialization\nschemes for deep networks.\n8.4.2 Weight Initialization and Unstable Gradients\nSo far in this chapter we have been initializing the bias terms and weights in our worked\nexamples by sampling from a uniform distribution with a range of r´0.5,`0.5s. The main\nadvantage of this initialization regime is its simplicity. However, using this regime also\nallowed us to highlight some of the problems that arise when weights are set naively, such\nas the problem of vanishing gradients, and dead neurons. If we wish to train a deep net-\nwork, we want the behavior of the network, in terms of the variance of the layer’s z values,\nactivations, and error gradients, to be similar across all the layers of the network. The","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":501,"page_label":"447","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"schemes for deep networks.\n8.4.2 Weight Initialization and Unstable Gradients\nSo far in this chapter we have been initializing the bias terms and weights in our worked\nexamples by sampling from a uniform distribution with a range of r´0.5,`0.5s. The main\nadvantage of this initialization regime is its simplicity. However, using this regime also\nallowed us to highlight some of the problems that arise when weights are set naively, such\nas the problem of vanishing gradients, and dead neurons. If we wish to train a deep net-\nwork, we want the behavior of the network, in terms of the variance of the layer’s z values,\nactivations, and error gradients, to be similar across all the layers of the network. The\nreason why keeping the behavior of a network across its layers similar is useful in creating\ndeep networks is that it allows us to add more layers to the network. However, naively ini-\ntializing weights can result in unstable behavior within the dynamics of a network during\ntraining, resulting in saturated activation functions (as a consequence of zvalues becoming\ntoo large or small) or unstable error gradients.\nThe ﬁrst way that a naive weight initialization can result in instability during training is\nthat the weights on the connections into a neuron are too large. Then the zvalue for the\nneuron will be large, which can result in the activation function for the neuron becoming\nsaturated . For example, a large negative zvalue («ză´2) or a large positive zvalue («\nzą`2) will cause the logistic function to saturate, and a large negative ză0will cause\nthe rectiﬁed linear function to saturate. The derivative of a saturated activation function is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":502,"page_label":"448","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"448 Chapter 8 Deep Learning\n0 (or near 0). Consequently, saturated activation functions are problematic for gradient-\ndescent–based algorithms (such as backpropagation) because these algorithms work by\niteratively adjusting the weights using small increments that are scaled by the derivative\nof the activation function. This means that neurons with saturated activation functions can\nget stuck: their weights never change substantially during training because the incremental\nupdates are either 0 or are tiny. For example, recall Neuron 4 in the ReLU network in\nSection 8.4.1[434]; this neuron was inactive for all the input examples and so the weights of\nthe neuron would never be updated, and it was therefore stuck in this dead state. A general\nprinciple that we can derive from this is that neurons with saturated activation functions\nlearn slowly (or not at all), and so we should take care when initializing weights to avoid\nsaturating neurons. For most activation functions, avoiding saturation at initialization is\nachieved by avoiding large (positive or negative) zvalues, which in turn are avoided by\ninitializing the weights to be close to 0.\nThe second way that a naive weight initialization can lead to instability during training is\nthat very small or large weights can result in unstable gradients. Equation 8.41[436]showed\nhow the chain of products used to backpropagate an error gradient through a network of\nthree neurons ( i,j, and k) expands. Equation 8.52[449]shows the last line from Equation\n8.41[436]and is annotated to explain how extreme weights can make gradients unstable. No-\ntice that the error term BE{Bakis multiplied by two weights: wj,iandwk,j. If both these\nweights areą1, then the error gradient term will get larger each time it is multiplied;\nconversely, if both these weights are ă1, then the error gradient term will get smaller\neach time it is multiplied. The problem of gradients becoming too large is known as ex-\nploding gradients . Extreme cases of exploding gradients can result in numerical overﬂow\ngenerating NaN (not-a-number) gradients. However, even when the gradients are deﬁned,\nvery large gradients are a problem. The calculation of an error derivative with respect to a\nweight is valid only for small changes in that weight. If the error gradient (the derivative)\nbecomes too large, then the weights will be updated by a large amount, and the resulting","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":502,"page_label":"448","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"weights areą1, then the error gradient term will get larger each time it is multiplied;\nconversely, if both these weights are ă1, then the error gradient term will get smaller\neach time it is multiplied. The problem of gradients becoming too large is known as ex-\nploding gradients . Extreme cases of exploding gradients can result in numerical overﬂow\ngenerating NaN (not-a-number) gradients. However, even when the gradients are deﬁned,\nvery large gradients are a problem. The calculation of an error derivative with respect to a\nweight is valid only for small changes in that weight. If the error gradient (the derivative)\nbecomes too large, then the weights will be updated by a large amount, and the resulting\nchanges in the output of a neuron, from one iteration to the next, will be so large that the\ntraining will become unstable.28Conversely, if weights are very small (too close to 0),\nthen the error gradient will tend to vanish, and the weight updates will be so small that\ntraining the network will take an inordinate amount of time. The complementary problems\n28. This is why we use a learning rate αto scale the weight updates. Scaling a weight update using a learning rate\nworks because the error derivative deﬁnes only the direction the weight update should take and not the update\nsize; and, scaling by the learning rate changes only the step size and not the direction of the update. However,\neven when we use a learning rate to scale updates, large weights can still result in exploding gradients, which in\nturn result in inappropriately large weight updates. We discussed the problem of large weight updates in Chapter\n7[311]Section 7.4.2[334], in which we compared the effect of different learning rates and introduced the idea of\nusing learning rate decay. A relevant point from that discussion was that large weight updates could result in the\nerror actually increasing (see Figure 7.9[336]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":503,"page_label":"449","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 449\nof exploding and vanishing gradients can be understood as examples of the more general\nchallenge of unstable gradients\nδi“wj,iˆwk,jˆlooooomooooon\nextreme weightsÑ\nunstable gradientsBE\nBakˆBak\nBzkˆBaj\nBzjˆBai\nBzi loooooooooomoooooooooon\nextreme weightsÑ\nsaturated activationsÑ\nvanishing gradients(8.52)\nThe third way in which naive weights initialization can cause unstable gradients arises\nfrom the fact that the variance of the output of a weighted sum is a function of three factors:\nthe number of inputs to the weighted sum, the variance of the inputs, and the variance of\nthe weights. Consequently, if the relationship between the number of inputs to a weighted\nsum and the variance of the weights is incorrect, then the result of a weighted sum can\nhave either a larger variance than the variance of its inputs or a smaller variance than the\nvariance of its inputs. This property of weighted sum calculations can result in unstable\ndynamics in both the forward and backward pass of the backpropagation algorithm because\nweighted sum calculations are used in both these phases; in the forward pass the calcula-\ntion of the zvalue for each neuron is done using a weighted sum, and in the backward pass\nthe calculation of a δfor a neuron includes the calculation of the term BE{Bakwhich is a\nweighted sum of the δs backpropagated to that neuron (see Equation 8.22[412]). Later in this\nsection we return to the question of how to counteract this property of weighted sum cal-\nculations. First, we will empirically show how different variations of weight initialization\ncan interact with this property of weighted sum calculations in order to introduce different\ntypes instability into the internal training dynamics within a network.\nTo illustrate how adjusting the variance of the sample distribution used to initialize the\nweights of a network affects the dynamics of the network during training, we will scale up\nour example network so that the effects become apparent. Our new network is designed to\nwork with our scenario of predicting the electrical output of a combined cycle power plant:\nthe network has two neurons in the input layer for the inputs A MBIENT TEMPERATURE\nand the R ELATIVE HUMIDITY , and one neuron in the output layer for the target E LEC-\nTRICAL OUTPUT . However, the network now has ﬁve fully connected hidden layers with\n100 neurons in each layer. Figure 8.22[450]illustrates this new network architecture. Fur-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":503,"page_label":"449","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"To illustrate how adjusting the variance of the sample distribution used to initialize the\nweights of a network affects the dynamics of the network during training, we will scale up\nour example network so that the effects become apparent. Our new network is designed to\nwork with our scenario of predicting the electrical output of a combined cycle power plant:\nthe network has two neurons in the input layer for the inputs A MBIENT TEMPERATURE\nand the R ELATIVE HUMIDITY , and one neuron in the output layer for the target E LEC-\nTRICAL OUTPUT . However, the network now has ﬁve fully connected hidden layers with\n100 neurons in each layer. Figure 8.22[450]illustrates this new network architecture. Fur-\nthermore, in order to isolate the effect of weight initialization on training dynamics from\nthe problem of saturated activation functions, the neurons in this network use a linear acti-\nvation function that outputs the same value that it receives as input: ai“zi. The derivative\nof this activation function with respect to zis always 1: each unit change in the value of\nziresults in a unit change in the value of ai. Consequently, the gradients in this network","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":504,"page_label":"450","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"450 Chapter 8 Deep Learning\n...............1\n1001\n1001\n1001\n1001\n100\nInput\nLayer 0Hidden\nLayer 1Hidden\nLayer 2Hidden\nLayer 3Hidden\nLayer 4Hidden\nLayer 5Output\nLayer 6\nFigure 8.22\nThe architecture of the neural network used in the weight initialization experiments. Note that the\nneurons in this network use a linear activation function: ai“zi.\nwill not be affected by saturated activation functions.29Apart from adjusting the network\narchitecture, we also increase the size of the training data to a sample of 100 examples, and\nwestandardize30all the features to have a mean of 0 and a standard deviation of 1.31\nWith this extended network architecture, the question arises of how we will initialize the\nweights. Although weight initialization is clearly important, at present relatively little is\nknown in principle about how to select a good set of initial weights. Most weight initial-\nization processes are based on heuristics that try to ensure that the weights are neither too\nbig nor too small. Different heuristics are often used for the bias terms and the weights.\nTypically, bias terms are initialized to 0. However, in some instances we may wish to set\nbias terms to non-zero values; for example, ReLUs saturate when ză0, and so to avoid\ndead ReLUs, the heuristic of initializing the bias terms for ReLU units to a small positive\n29. Note that using linear activation functions in this way means that the network as a whole implements a\nlinear function. However, in this instance, this simpliﬁcation is appropriate because the purpose of this network\narchitecture design is to illustrate the effect of different weight initialization regimes rather than to be accurate on\nthe task.\n30. For more on the standardization of inputs, see the discussion on data preprocessing at the start of the worked\nexample in Section 8.3.5[421].\n31. In this case, we chose to standardize the features in the data rather than range-normalize them, in order to align\nthe data conﬁgurations used to generate the plots in this section with the assumptions made in the accompanying\nmathematical analysis.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":505,"page_label":"451","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 451\nnumber (such as 0.1) is sometimes used (Goodfellow et al., 2016, pp. 296–297). For the\ndiscussion in this section, however, we assume that biases are initialized to 0.\nTraditionally, the most frequent heuristic used to initialize the weights of a neural net-\nwork was randomly sampling values from a normal or uniform distribution with a mean\nof 0. For example, we might sample the weights from a normal distribution with mean\nµ“0.0andσ“0.01. When weights are initialized in this way, we can control the\ninitial scale of the weights by controlling the variance of the normal distribution. Figure\n8.23[453]illustrates the internal dynamics of the network shown in Figure 8.22[450]during the\nﬁrst iteration of training if we initialize the weights of the network by sampling from a\nnormal distribution with µ“0.0andσ“0.01and pass our 100 examples through the\nnetwork as a single mini-batch. Each of the four ﬁgures in Figure 8.23[453]uses side-by-\nside violin plots32to illustrate the distribution of a network property (weights, weighted\nsums, activations, or δs) across the ﬁve hidden layers in the architecture during the ﬁrst\ntraining iteration. Figure 8.23(a)[453]illustrates the distribution of weight values across each\nof the ﬁve hidden laters immediately after initialization. The difference in the distribution\nbetween hidden layer 1 ( HL1) and the other hidden layers is caused by the fact that there\nare only two inputs into each of the neurons in HL1whereas there are 100 inputs into\neach of the neurons in the other hidden layers. Consequently, the plot of values for HL1is\ngenerated from a much smaller sample of weight values than the plots for the other hidden\nlayers. Figure 8.23(b)[453]illustrates how the zvalues vary across the layers of the network\nduring the forward pass of the algorithm. It is clear that the variance of the distribution of\nzvalues dramatically reduces as we move forward through the network, and this reduction\nin the variance together with the fact that the median of the zvalues is 0 across all the\nlayers indicates that the zvalues are consistently getting smaller. This vanishing zvalues\ndynamic is a result of the fact that the zvalues are calculated using a weighted sum, and in\nthis network conﬁguration the relationship between the number of inputs to each weighted\nsum and the variances of the weights is such that the variance of the zvalues is scaled down","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":505,"page_label":"451","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"layers. Figure 8.23(b)[453]illustrates how the zvalues vary across the layers of the network\nduring the forward pass of the algorithm. It is clear that the variance of the distribution of\nzvalues dramatically reduces as we move forward through the network, and this reduction\nin the variance together with the fact that the median of the zvalues is 0 across all the\nlayers indicates that the zvalues are consistently getting smaller. This vanishing zvalues\ndynamic is a result of the fact that the zvalues are calculated using a weighted sum, and in\nthis network conﬁguration the relationship between the number of inputs to each weighted\nsum and the variances of the weights is such that the variance of the zvalues is scaled down\nat each layer in the network. We return subsequently to this vanishing zdynamic to explain\nin more detail how it arises. The plot of activation values directly mirrors the plot of the z\nvalues because the neurons are using a linear activation function. Figure 8.23(d)[453]clearly\nillustrates vanishing gradients across layers in the network; that the δvalues are tending\ntoward 0 is indicated by the fact that the median of the δvalues across the layers is 0 but\nthe variance of δvalues rapidly shrinks as we move backward from the last hidden layer\n(HL5) to the ﬁrst layer ( HL1). Remember that the neurons in this network all use a linear\nactivation function that has a derivative value of 1. This removes the scaling of the δs by\n32. A violin plot is abox plot that has been augmented to show the probability density of a data at different\nvalues in the range. The use of violin plots to illustrate the variance across layers within a network was inspired\nby a blog post by Daniel Godoy (see https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-\ninitializers-35aee1a28404).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":506,"page_label":"452","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"452 Chapter 8 Deep Learning\nthe derivative of the activation function, and so in this instance these vanishing gradients\nare caused by both the repeated multiplication by small weights during backpropagation\nand the fact that the calculation of a δfor a neuron involves a weighted sum calculation33\nthat in this network conﬁguration results in the variance of the δs shrinking as they are\nbackpropagated through the layers.\nGiven that the vanishing gradients exhibited in Figure 8.23(d)[453]are partly caused by the\nrepeated multiplication by small weights, we can try to avoid this problem by making the\nnetwork weights larger. Figure 8.24[454]illustrates what happens in the internal dynamics of\nthe network if we increase the standard deviation of the distribution from which we sample,\nin this instance a normal distribution with µ“0.0andσ“0.2. The impact of this larger\nstandard deviation on the weights in the network is evident if we compare Figure 8.23(a)[453]\nwith Figure 8.24(a)[454]: although the distribution of weights within each layer looks similar\nin the two ﬁgures, the scales on the weights axes show that the weights plotted in Figure\n8.24(a)[454]have a larger variance from the median 0, indicating that the weights tend to\nbe larger. It turns out that these slightly larger weight values can dramatically affect the\ninternal dynamics of the network during training. First, Figure 8.24(b)[454]shows that now\nthezvalues consistently become larger as we move forward through the network; this is\na complete reversal of the vanishing zvalue dynamic shown in Figure 8.23(b)[453]. This\nexploding zvalue dynamic is a problem because if we wish to avoid saturated activation\nfunctions, we need to stop the zvalues in neurons taking an extreme value (where the con-\ncept of extreme is dependent on the activation function). We subsequently explain what is\ncausing both these vanishing and exploding zvalues. Second, Figure 8.24(d)[454]shows that\nthe network now has an exploding gradient dynamic during backpropagation, with the\nvariance of δvalues rapidly increasing as they are backpropagated through the network.\nAs in the previous analysis, in this network the instability in gradient propagation (in this\ninstance exploding gradients) is not due to a scaling of the gradients by the derivative of the\nactivation function; the linear activation function used by the neurons in this network has a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":506,"page_label":"452","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"functions, we need to stop the zvalues in neurons taking an extreme value (where the con-\ncept of extreme is dependent on the activation function). We subsequently explain what is\ncausing both these vanishing and exploding zvalues. Second, Figure 8.24(d)[454]shows that\nthe network now has an exploding gradient dynamic during backpropagation, with the\nvariance of δvalues rapidly increasing as they are backpropagated through the network.\nAs in the previous analysis, in this network the instability in gradient propagation (in this\ninstance exploding gradients) is not due to a scaling of the gradients by the derivative of the\nactivation function; the linear activation function used by the neurons in this network has a\nderivative of 1, and so the gradients are not changed by this derivative during backpropaga-\ntion. Instead, the exploding exploding gradients exhibited in Figure 8.24(d)[454]is caused by\na similar process to the vanishing zvalues plotted in Figure 8.23(b)[453]and the exploding\nzvalues plotted in Figure 8.24(b)[454]; the connection between these three processes is that\nthey all involve a weighted sum.\nTo explain the relationship between the weighted sum calculation and the phenomena\nof vanishing and exploding zandδvalues, we will analyze the relationship between the\nvariance of zfor a single neuron in the ﬁrst hidden layer of a network and the variance\nof the weights used in calculating that z. This neuron receives nininputs d1,..., dnin(note\nthat for this discussion we ignore the bias input and weight, and we drop the layer pkq\n33. Speciﬁcally, the calculation of the term BE{Bak(see Equation 8.22[412]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":507,"page_label":"453","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"HL1 HL2 HL3 HL4 HL5−0.04−0.0200.020.04\nLayerWeights(a) Weights by Layer\nHL1 HL2 HL3 HL4 HL5−0.0500.05\nLayerWeighted Sums (Zs)\n(b) Weighted Sum ( z) by Layer\nHL1 HL2 HL3 HL4 HL5−0.0500.05\nLayerActivations\n(c) Activations by Layer\nHL1 HL2 HL3 HL4 HL5−0.0500.05\nLayerDeltas (error gradients)\n(d)δs by Layer\nFigure 8.23\nThe internal dynamics of the network in Figure 8.22[450]during the ﬁrst training iteration when the\nweights were initialized using a normal distribution with µ“0.0,σ“0.01.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":508,"page_label":"454","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"HL1 HL2 HL3 HL4 HL5−0.500.5\nLayerWeights(a) Weights by Layer\nHL1 HL2 HL3 HL4 HL5−20−100102030\nLayerWeighted Sums (Zs)\n(b) Weighted Sum ( z) by Layer\nHL1 HL2 HL3 HL4 HL5−20−100102030\nLayerActivations\n(c) Activations by Layer\nHL1 HL2 HL3 HL4 HL5−200−1000100200\nLayerDeltas (error gradients)\n(d)δs by Layer\nFigure 8.24\nThe internal dynamics of the network in Figure 8.22[450]during the ﬁrst training iteration when the\nweights were initialized using a normal distribution with µ“0.0andσ“0.2.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":509,"page_label":"455","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 455\nsuperscript when we are discussing a single neuron). Consequently, this neuron will use\nninweights in its weighted sum w“ăw1,...wniną\nz“pw1ˆd1q`pw2ˆd2q`¨¨¨`pwninˆdninq (8.53)\nThe starting point for our explanation of the relationship between weighted sum calcula-\ntions and vanishing and exploding zandδvalues is the Bienaym ´e formula from statistics,\nwhich states that the variance of the sum of uncorrelated random variables is the sum of\ntheir variance\nvar˜nÿ\ni“1Xi¸\n“nÿ\ni“1varpXiq (8.54)\nAssuming that all the weights and the inputs are independent and identically distributed,34\nthen the products in the weighted sum can be considered uncorrelated and we can state the\nvariance of zas follows:\nvarpzq“varppw1ˆd1q`pw2ˆd2q`...pwninˆdninqq\n“ninÿ\ni“1varpwiˆdiq (8.55)\nAlso assuming that each weight wiis independent of the corresponding input di, then\nvariance of each of these products is given:35\nvarpwˆdq“r EpWqs2varpdq`r Epdqs2varpWq`varpWqvarpdq (8.56)\nNote that we have dropped the subscript on wbecause every wiis sampled from the\nsame distribution and hence has the same expected value and variance: EpWqis the ex-\npected value of a weight (i.e., the probabilistic average, or mean value, of the weights),\nandvarpWqis the shared scaler variance of all the weights. Similarly, we have dropped\nthe subscript from dbecause we are assuming that the inputs have been standardized to\nhave a mean of 0 and a standard deviation of 1, and so all the inputs have the same mean\nand variance: Epdqis the mean value of an input, and varpdqis the shared scaler variance\nof all the inputs. If we have sampled our weights from a distribution with mean 0, then\nEpWq“0, and if the inputs have been standardized, then Epdq“0, and so the Equation\n(8.56)[455]simpliﬁes to\n34. This is a naive assumption because we are making it to simplify the discussion irrespective of whether or not\nit is true.\n35. The variance of the product of two independent random variables X and Y is given by varpXˆYq “\nrEpXqs2varpYq`r EpYqs2varpXq`varpXqvarpYq.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":510,"page_label":"456","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"456 Chapter 8 Deep Learning\nvarpwˆdq“varpWqvarpdq (8.57)\nThis identity permits us to rewrite the variance of zin our case study neuron\nvarpzq“ninÿ\ni“1varpwiˆdiq“ninvarpWqvarpdq (8.58)\nEquation 8.58[456]states that the variance of zis equal to the variance of the inputs ( varpdq)\nscaled by ninvarpWq. On one hand, this analysis reveals that the variance of zis dependent\non the number of inputs the neuron receives, nin; in general, the larger the number of\ninputs, the larger the variance. However, it also tells us that the scaling of the variance of\nzis dependent on the product ninvarpWq. Consequently, we can counteract this scaling\nby the number of inputs by setting varpWq“1{nin; that is, by setting the variance of the\ndistribution that the weights for a neuron are sampled from to 1{nin. When varpWq“1{nin\nthenninvarpWq“1and the variance of zfor the neuron is solely dependent on the variance\nof the inputs, which if standardized will have a variance of 1. In a fully connected network\nninis the same for all the neurons in a layer, and so for these networks we can set the\nvariance of the distribution from which the weights are sampled on a layer-by-layer basis.\nThis analysis explains the vanishing zvalues plotted in Figure 8.23(b)[453]. Recall that\nthis plot was generated using input data that had been standardized, and so varpdq “ 1.\nIn this network, for the neurons in the ﬁrst hidden layer npHL1q\nin“2and for neurons in all\nthe other hidden layers nin“100. Furthermore, the weights in the network were sampled\nfrom a normal distribution with µ“0.0andσ“0.01, and therefore the weights in each\nlayer have a variance of varpWq“σ2“0.012“0.0001 . This conﬁguration means that\nvariance of z values across the neurons in layer HL1is\nvarpZpHL1qq“npHL1q\ninˆvarpWpHL1qqˆvarpdpHL1qq (8.59)\n“2ˆ0.0001ˆ1\n“0.0002\nwhere varpZHL1qdenotes the shared scalar variance of all the z values across the neu-\nrons in layer HL1;nHL1\ninis the number of inputs coming into each neuron in layer HL1;\nvarpWpHL1qqdenotes the shared scalar variance of all the weights across the neurons in\nlayer HL1; and varpdpHL1qdenotes the shared scalar variance of all the inputs to layer\nHL1. When checking this against Figure 8.23(b)[453], note that a variance of σ2“0.0002\nis equivalent to a standard deviation of σ«0.014. That the neurons in this network use\na linear activation function means that varpZpHL1qqis also the variance of the activations","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":510,"page_label":"456","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"variance of z values across the neurons in layer HL1is\nvarpZpHL1qq“npHL1q\ninˆvarpWpHL1qqˆvarpdpHL1qq (8.59)\n“2ˆ0.0001ˆ1\n“0.0002\nwhere varpZHL1qdenotes the shared scalar variance of all the z values across the neu-\nrons in layer HL1;nHL1\ninis the number of inputs coming into each neuron in layer HL1;\nvarpWpHL1qqdenotes the shared scalar variance of all the weights across the neurons in\nlayer HL1; and varpdpHL1qdenotes the shared scalar variance of all the inputs to layer\nHL1. When checking this against Figure 8.23(b)[453], note that a variance of σ2“0.0002\nis equivalent to a standard deviation of σ«0.014. That the neurons in this network use\na linear activation function means that varpZpHL1qqis also the variance of the activations\npropagated forward to the next hidden layer, and so it is also the variance of the inputs\nto the next layer: varpZpHL1qq “ varpApHL1qq “ varpdpHL2qq. For HL2we know that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":511,"page_label":"457","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 457\nnpHL2q\nin“100andvarpWpHL2qq“0.0001 , and so we can now calculate the variance of z\nacross the neurons in layer HL2as follows:\nvarpZpHL2qq“npHL2q\ninˆvarpWpHL2qqˆvarpdpHL2qq (8.60)\n“100ˆ0.0001ˆ0.0002\n“0.000002\nEssentially, in this network, because of the use of linear activations, the variance of zfor\nlayer kin the network is the variance of zin the preceding layer, varpzpk´1qq, scaled by\nnpkq\ninˆvarpWpkqq. For all the remaining hidden layers in this network nin“100, and so\nthis process of scaling the variance of zby100ˆ0.0001“0.01will continue through\neach of the subsequent hidden layers in this network. This is why the variance of zrapidly\ndecreases for each layer as we move forward through the network as shown in Figure\n8.23(b)[453].\nThis analysis also explains the exploding zvalues plotted in Figure 8.24[454]. In this\ninstance the weights in the network were sampled from a normal distribution with µ“0.0\nandσ“0.2, which means that for all k:varpWpkqq“σ2“0.22“0.04. As a result, the\nvariance of the zvalues across the neurons in HL1isvarpZpHL1qq“2ˆ0.04“0.08(i.e.,\nσ«0.283), and this is then scaled by 100ˆ0.04“4at each of the other hidden layers in\nthe network. This is why the variance of the zvalues rapidly increases as we move forward\nthrough the network as shown in Figure 8.24(b)[454].\nA similar analysis also explains the exploding δvalues plotted in Figure 8.24(d)[454]. In\nthis case, however, the number of inputs to the weighted sum calculation within a neuron\nduring the backpropagation process is the number of neurons that the neuron propagated\nits activation to during the forward pass (see Equation 8.22[412]). If we use noutto denote\nthis number, then for the neurons in the last hidden layer npHL5q\nout“1because there is only a\nsingle neuron in the output layer, and for the neurons in all the other layers in the network\nnout“100. As with the analysis of the scaling of the variances for zin the forward pass,\nthe scaling of the variance of the δvalues as they are backpropagated through each layer\nkis a function of number of inputs to each neuron in the layer (here npkq\nout) multiplied by\nthe variance of the weights for that layer varpWpkqq. The same weights are used during\nthe forward and backward passes of a single iteration of backpropagation. Consequently,\nthe weights used during the backpropagation of the δs plotted in Figure 8.24(d)[454]are\nsampled from a normal distribution with µ“0.0andσ“0.2, which means that for all","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":511,"page_label":"457","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"out“1because there is only a\nsingle neuron in the output layer, and for the neurons in all the other layers in the network\nnout“100. As with the analysis of the scaling of the variances for zin the forward pass,\nthe scaling of the variance of the δvalues as they are backpropagated through each layer\nkis a function of number of inputs to each neuron in the layer (here npkq\nout) multiplied by\nthe variance of the weights for that layer varpWpkqq. The same weights are used during\nthe forward and backward passes of a single iteration of backpropagation. Consequently,\nthe weights used during the backpropagation of the δs plotted in Figure 8.24(d)[454]are\nsampled from a normal distribution with µ“0.0andσ“0.2, which means that for all\nthe layers varpWpkqq “σ2“0.22“0.04. If for the purposes of this discussion we\nnaively assume that the variance of the δvalues backpropagated to the neurons in HL5is\nequal to 1, then we can expect the variance of the δs backpropagated to HL4to be equal to\n100ˆ0.04ˆ1“4. Furthermore, for each subsequent layer that the δs are backpropagated","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":512,"page_label":"458","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"458 Chapter 8 Deep Learning\nthrough, the variance will increase by a factor of 4, and it is this scaling that causes the\nexploding gradients evident in Figure 8.24(d)[454].\nWe noted at the start of this section that in order to train a deep network, it is important to\nkeep the internal behavior of the network (i.e., the variance of the z values, activations, and\nδs) similar across all the layers during training, because doing so allows us to add more\nlayers to the network while avoiding saturated units (by avoiding extreme zvalues) and\nexploding or vanishing δs. The preceding analysis highlighted that the variance of output of\na weighted sum is dependent on the number of inputs to the weighted sum (be it ninduring\nforward propagation or noutduring backward propagation). This observation is the basis for\na number of weight initialization regimes that adjust the variance of the distribution used to\nsample the weights for a neuron based on its connections to other neurons in the network.\nFor example, we noted previously that in a fully connected feedforward network, if we set\nvarpWpkqq“1{npkq\nin, then the variance of the zvalues in layer kis dependent solely on the\nvariance of the inputs to that layer; and if the inputs are standardized, then the variance\nof the zvalues will not be scaled for that layer. If we do this when sampling the weights\nfor each of the layers in the network, then the variance of the zvalues across all the layers\nwill be stable. The different weight initialization regimes that have been developed vary,\ndepending on whether they take both ninandnoutinto account and the activation functions\nwith which they work best.\nOne of the best known of these weight initialization regimes is called Xavier initializa-\ntion.36There are a number of variants of Xavier initialization used in practice, but the\noriginal version of Xavier initialization was designed for fully connected feedforward net-\nworks and worked on a layer-by-layer basis. The original version of Xavier initialization\nconsidered both the forward activation through the network and the backward propaga-\ntion of gradients, and so the calculation of the variance of the distribution from which the\nweights for a layer are sampled takes both the inputs to a layer npkq\ninand the number of\noutputs from a layer npkq\noutinto account; it is calculated as follows:\nvarpWpkqq“2\nnpkq\nin`npkq\nout(8.61)\nwhere varpWpkqqis the variance of all the weights in layer k;npkq\ninis the number of neurons","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":512,"page_label":"458","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tion.36There are a number of variants of Xavier initialization used in practice, but the\noriginal version of Xavier initialization was designed for fully connected feedforward net-\nworks and worked on a layer-by-layer basis. The original version of Xavier initialization\nconsidered both the forward activation through the network and the backward propaga-\ntion of gradients, and so the calculation of the variance of the distribution from which the\nweights for a layer are sampled takes both the inputs to a layer npkq\ninand the number of\noutputs from a layer npkq\noutinto account; it is calculated as follows:\nvarpWpkqq“2\nnpkq\nin`npkq\nout(8.61)\nwhere varpWpkqqis the variance of all the weights in layer k;npkq\ninis the number of neurons\nthat feed inputs to layer k; and npkq\noutis the number of neurons that neurons in layer kconnect\nforward to (in a fully connected feedforward network npkq\ninis equal to the number of neurons\n36.Xavier initialization is named after the ﬁrst author (Xavier Glorot) of the paper that introduced this layer-\nwise approach to weight initialization, Glorot and Bengio (2010). However, in some places the author’s second\nname is used instead of his ﬁrst name to describe the same initialization algorithm, so that it is sometimes called\nGlorot initialization .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":513,"page_label":"459","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 459\nin layer k´1andnpkq\noutis the number of neurons in layer k`1). Often in practice, however,\na simpler variant of Xavier initialization is used that just considers npkq\nin\nvarpWpkqq“1\nnpkq\nin(8.62)\nFigure 8.25[460]illustrates the internal dynamics of the network in Figure 8.22[450]during\nthe ﬁrst training iteration when the weights of each layer in the network are sampled from\na normal distribution with a mean of 0 and a variance calculated using Equation (8.62)[459].\nNotice that the zvalues, activations, and δs have a relatively similar distribution across\neach of the layers of the network; that is, the training of the network is not suffering from\neither exploding zvalues (which would saturate activation functions if they were used) or\nexploding or vanishing δs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":514,"page_label":"460","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"HL1 HL2 HL3 HL4 HL5−0.3−0.2−0.100.10.20.3\nLayerWeights(a) Weights by Layer\nHL1 HL2 HL3 HL4 HL5−1−0.500.51\nLayerWeighted Sums (Zs)\n(b) Weighted Sum ( z) by Layer\nHL1 HL2 HL3 HL4 HL5−1−0.500.51\nLayerActivations\n(c) Activations by Layer\nHL1 HL2 HL3 HL4 HL5−0.500.51\nLayerDeltas (error gradients)\n(d)δs by Layer\nFigure 8.25\nThe internal dynamics of the network in Figure 8.22[450]during the ﬁrst training iteration when the\nweights were initialized using Xavier initialization.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":515,"page_label":"461","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 461\nXavier initialization has empirically been shown to often lead to faster training and is one\nof the most popular weight initialization approaches in deep learning. It is generally used\nwhen networks use logistic ortanh activation functions. However, a modiﬁed version of\nthis weight initialization heuristic is recommended when the network uses rectiﬁed linear\nunits (He et al., 2015). This weight initialization heuristic is known as He initialization\n(or sometimes it is called Kaiming initialization ) and is deﬁned as follows:\nvarpWpkqq“2\nnpkq\nin(8.63)\nSometimes a blend of Xavier andHeinitialization is used. For example, in a network\nusing ReLUs, Xavier initialization (Equation (8.62)[459]) could be used to deﬁne the variance\nfor the weights in the ﬁrst layer, because the rectiﬁed function has not been applied to the\ninputs, and then He initialization (Equation (8.63)[461]) could then be used for the later layers\nin the network (He et al., 2015). Following this strategy, for a fully connected three-layer\nReLU network with 100 inputs, 80 neurons in the ﬁrst hidden layer, 50 neurons in the\nsecond hidden layer, and 5 neurons in the output layer, the weight matrix for each layer\nwould be initialized as shown in Equation (8.64)[461], where the notation Wpkq∼Npµ,σq\nindicates that the values in the weight matrix for layer kshould be sampled from a normal\ndistribution with mean µand standard deviation σ.\nWp1q∼N˜\n0,c\n1\n100¸\nWp2q∼N˜\n0,c\n2\n80¸\nWp3q∼N˜\n0,c\n2\n50¸\n(8.64)\nFigure 8.26[462]illustrates the internal dynamics of the network in Figure 8.22[450]during\nthe ﬁrst training iteration when ReLUs are used and the weights for neurons in HL1are\nsampled using Xavier initialization and the weights for the neurons in the later layers are\nsampled using He initialization. The effect of the rectiﬁed linear activation function is\nevident in Figure 8.26(c)[462], where the activations are all positive. Notice as well that the\ndistribution of the zandδvalues is relatively stable across the layers (see Figure 8.26(b)[462]\nand Figure 8.26(d)[462]). This stability in the internal dynamics of the network, particularly\nwith respect to the gradients, is likely to result in the network learning much faster than\nnetworks affected by vanishing or exploding gradients, and it is a result of careful weight\ninitialization.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":516,"page_label":"462","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"HL1 HL2 HL3 HL4 HL5−0.6−0.4−0.200.20.40.6\nLayerWeights(a) Weights by Layer\nHL1 HL2 HL3 HL4 HL5−1.5−1−0.500.511.5\nLayerWeighted Sums (Zs)\n(b) Weighted Sum ( z) by Layer\nHL1 HL2 HL3 HL4 HL500.51\nLayerActivations\n(c) Activations by Layer\nHL1 HL2 HL3 HL4 HL5−0.500.51\nLayerDeltas (error gradients)\n(d)δs by Layer\nFigure 8.26\nThe internal dynamics of the network in Figure 8.22[450], using ReLUs, during the ﬁrst training itera-\ntion when the weights were initialized using He initialization.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":517,"page_label":"463","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 463\n8.4.3 Handling Categorical Target Features: Softmax Output Layers and Cross-\nEntropy Loss Functions\nAll the examples that we have looked at so far have been regression problems. To create\na neural network that can predict a multi-level categorical feature, we make three adjust-\nments:\n1.we represent the target feature using one-hot encoding ;\n2.we change the output layer of the network to be a softmax layer ; and\n3.we change the error (or loss) function we use for training to be the cross-entropy\nfunction.\nAone-hot encoding is a vector-based representation of a categorial feature value.37A\none-hot vector has one element per level of the categorical feature. For example, if a feature\ncan take three levels (e.g., low,medium ,high), then the vector would have three elements.\nIt is known as a one-hot representation because at most one element in the vector will have\nthe value 1 and all the other elements will be 0, with the value of the feature indicated\nby whichever element is 1. For our three-level categorical feature we might decide that\nx1,0,0yindicates low,x0,1,0yindicates medium , andx0,0,1yindicates high. When we\nare representing the target feature in a dataset using a one-hot encoding, then for each\ninstance in the dataset there is a one-hot vector encoding the level of the target for that\ninstance. Table 8.13[464]lists the power plant dataset after the target feature E LECTRICAL\nOUTPUT has been converted to a three-level categorical feature, by applying binning to\nthe range-normalized values ( lowď0.33,mediumď0.66,highą0.66), and then encoded\nusing one-hot encoding.\nIn a softmax output layer there is a single neuron for each level of the target feature.\nFor example, if the prediction task is to predict the level of a categorical feature that can\ntake three levels (e.g., low,medium ,high), then the output layer of the network would\nhave three neurons. The activation function used by the neurons in a softmax layer is the\nsoftmax function ; for an output layer with mneurons, the softmax activation function is\ndeﬁned as follows:\nϕsmpziq“ezi\nřm\nj“1ezm(8.65)\nThe softmax activation function normalizes the zscores for a layer of neurons so that the\nsum of the activations of the neurons is 1. Hence using the softmax function, the activation\nof each neuron is dependent on the size of its zvalue relative to the zvalues of the other\nneurons in the output layer. The softmax function will always return a positive value for","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":517,"page_label":"463","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"take three levels (e.g., low,medium ,high), then the output layer of the network would\nhave three neurons. The activation function used by the neurons in a softmax layer is the\nsoftmax function ; for an output layer with mneurons, the softmax activation function is\ndeﬁned as follows:\nϕsmpziq“ezi\nřm\nj“1ezm(8.65)\nThe softmax activation function normalizes the zscores for a layer of neurons so that the\nsum of the activations of the neurons is 1. Hence using the softmax function, the activation\nof each neuron is dependent on the size of its zvalue relative to the zvalues of the other\nneurons in the output layer. The softmax function will always return a positive value for\nevery neuron because ezis always positive, even if zis negative. Another useful property\n37. We can also use one-hot encodings to represent categorical descriptive features (see Section 7.4.3[336]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":518,"page_label":"464","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"464 Chapter 8 Deep Learning\nTable 8.13\nTherange-normalized hourly samples of ambient factors and full load electrical power output of a\ncombined cycle power plant, rounded to two decimal places, and with the (binned) target feature\nrepresented using one-hot encoding.\nID A MBIENT TEMPERATURE RELATIVE HUMIDITY Electrical Output\n˝C % low medium high\n1 0.04 0.81 0 0 1\n2 0.84 0.58 1 0 0\n3 0.50 0.07 0 1 0\n4 0.53 1.00 0 1 0\nTable 8.14\nThe calculation of the softmax activation function ϕsmover a vector of three logits l.\nl0 l1 l2\nl 1.5 -0.9 0.6\neli 4.48168907 0.40656966 1.8221188ř\nieli 6.71037753\nϕsmpliq0.667874356 0.060588195 0.27153745\nofezis that e0is 1; as a result, even in the unlikely event that all the zs are 0, we avoid the\nproblem of a division by 0, and each neuron will have an activation of 1{m.\nIn the context of a softmax layer, the non-normalized zvalues are often referred to as\nlogits . Therefore, to align with this general terminology, for this discussion we switch\nfrom discussing zvalues for a neuron to discussing the logit of the neuron. We use the\nnotation lto denote a vector of logits for a layer of neurons, and lito indicate the logit\nfor the ithneuron in the layer. Using the notation ϕsmto denote the softmax activation\nfunction,ϕsmplqdenotes applying the softmax function to the vector of logits l, andϕsmpliq\nrefers to the calculation of the softmax value for the ithlogit, that is, the output activation\nof the ineuron in the layer. Note that ϕsmpliqcan be understood as equivalent to ϕsmpziq\nin Equation (8.65)[463]; the notational differences arise because we are now specifying the\nparameter as an index of a logit rather than specifying the zscore directly. Table 8.14[464]\nillustrates the calculation of the softmax function for a vector of three logits (i.e., three z\nvalues),\nThe fact that the softmax function returns a normalized set of positive values across\nthe layer allows us to interpret the activation of each neuron in the layer as a probability.\nThere is one neuron in an output softmax layer per target feature level, and so the softmax\nfunction returns one probability per level. The natural consequence of this is that using\na softmax layer, each neuron is trained to predict the probability of one of the levels of\nthe categorical target feature. The ﬁnal prediction is then made by taking the feature level","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":519,"page_label":"465","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 465\n3 8\n1 6\n4 9\n2 7\n5 10\nInput\nLayer 0Hidden\nLayer 1Hidden\nLayer 2Softmax\nLayerϕsmP(ti=low |di)\nP(ti=medium |di)\nP(ti=high |di)w3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7\nw9,6\nw9,7\nw\n10\n,6w10,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,0\nw9,0\nw10,0\nFigure 8.27\nA schematic of a feedforward artiﬁcial neural network with a three-neuron softmax output layer.\nwhose neuron predicts the highest probability. Figure 8.27[465]illustrates a fully connected\nfeedforward neural network with a softmax output layer with three neurons for our three-\nlevel ( low,medium andhigh) target feature; the dotted rectangle around the softmax layer\nhighlights that the activation function ϕnormalizes the logit values across the neurons in\nthe layer.\nThe cross-entropy error (or loss) function is generally used in contexts in which the\noutput of a network can be interpreted as a probability distribution over a set of exclu-\nsive categories. In the general scenario of a model predicting a distribution over a set of\ncategories, the cross-entropy loss function is deﬁned as\nLCE`\nt,ˆP˘\n“´ÿ\njtjln`ˆPj˘\n(8.66)\nwhere LCEis the cross-entropy loss; tis the target feature represented using one-hot encod-\ning (the target distribution over the categories); ˆPis the distribution over the categories that\nthe model has predicted; lnis the natural logarithm function; and jis an index over both\nthe target distribution tand the predicted distribution ˆP. This loss function is called cross-\nentropy because in information theory cross-entropy is used to describe a measure of the\ndifference in nats between two probability distributions over the same set of events. The","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":520,"page_label":"466","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"466 Chapter 8 Deep Learning\ninformation theory basis for this function can be seen in the similarity between Equation\n(8.66)[465]and the equation for Shannon’s entropy, Equation (4.1)[125].\nThe cross-entropy loss measures the dissimilarity between the true distribution tand the\npredicted distribution ˆP. In situations where the true distribution tis encoded as a one-hot\nvector, the cross-entropy loss function can be simpliﬁed to:\nLCE`\nt,ˆP˘\n“´ ln`ˆP‹˘\n(8.67)\nˆP‹indicates the predicted probability for the true category (i.e., the category encoded as a\n1 in the one-hot encoded vector t). To illustrate this simpliﬁcation with a case study, we\nassume that our distribution is over three categories, and the target distribution is encoded\nas a one-hot vector. Now if the target distribution over a particular instance is t“r0,1,0s\n(i.e., the second category is the correct category), then the cross-entropy summation in\nEquation (8.66)[465]expands as follows:\nLCE`\nt,ˆP˘\n“´ÿ\njtjln`ˆPj˘\n“´``\nt0ln`ˆP0˘˘\n``\nt1ln`ˆP1˘˘\n``\nt2ln`ˆP2˘˘˘\n“´``\n0 ln`ˆP0˘˘\n``\n1 ln`ˆP1˘˘\n``\n0 ln`ˆP2˘˘˘\n“´1 ln`ˆP1˘\n(8.68)\nEquation (8.68)[466]shows that all the terms that involve a 0element from tdisappear, and\nthe loss simpliﬁes to the negative log of the predicted probability for the true class.\nTo understand why Equation (8.67)[466]is an appropriate measure to use as the loss func-\ntion for categorical training, we should ﬁrst remember that the loss function is the function\nthat we wish to minimize during training, and so we wish the loss function to return a large\nvalue when there is a large difference between the true and predicted probability distribu-\ntions, and a small value when tandˆPare similar or identical. For this discussion it may\nbe useful to quickly refer to Figure (4.6)[125]to see a plot of how the negative log of a prob-\nability changes as the probability changes (Figure (4.6)[125]shows this plot for binary logs,\nbut the general shape of the plot is similar for natural logs). For probabilities near 0, the\nnegative log returns a large number, and for probabilities near 1 the negative log returns a\nvalue near 0. Now, imagine a scenario in which the model makes a correct prediction and\nthe maximum probability in ˆPis assigned to the correct category. In this scenario, then ˆP‹\nwill be relatively close to 1 (and the better the model’s prediction the closer to one ˆP‹will\nbe). As ˆP‹approaches 1, then the negative log of this probability approaches 0. In other","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":520,"page_label":"466","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"be useful to quickly refer to Figure (4.6)[125]to see a plot of how the negative log of a prob-\nability changes as the probability changes (Figure (4.6)[125]shows this plot for binary logs,\nbut the general shape of the plot is similar for natural logs). For probabilities near 0, the\nnegative log returns a large number, and for probabilities near 1 the negative log returns a\nvalue near 0. Now, imagine a scenario in which the model makes a correct prediction and\nthe maximum probability in ˆPis assigned to the correct category. In this scenario, then ˆP‹\nwill be relatively close to 1 (and the better the model’s prediction the closer to one ˆP‹will\nbe). As ˆP‹approaches 1, then the negative log of this probability approaches 0. In other\nwords, the loss of the model reduces as the model’s predictions improve. In the comple-\nmentary scenario, the model makes an incorrect prediction and the maximum probability\ninˆPis assigned to the incorrect category. Because a probability distribution must sum to 1,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":521,"page_label":"467","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 467\nan increase in one probability results in a decrease in one or more of the other probabilities\nin the distribution. Hence if the model assigns the maximum probability mass to an incor-\nrect category, this will reduce ˆP‹and, as Figure (4.6)[125]illustrates, this will result in the\nnegative log of the ˆP‹rapidly increasing. This is exactly the behavior we desire from a loss\nfunction: small values for correct predictions and large values for incorrect predictions.\nUsing Equation (8.67)[466]we can now calculate a loss for the network’s predictions over\na set of exclusive categories. However, to train the network we must backpropagate this\nloss through the network. To do this we must adjust the derivatives used in the calculation\nof theδs for the output neurons because we have changed the activation function used by\nthese neurons. However, we do not need to change anything with regard to the calculation\nof theδs for the hidden neurons; once we have updated the calculation of the δs for the\noutput neurons, then the error gradients can ﬂow back through the network as previously.\nEquations (8.69)[467]to (8.72)[467]step through the deﬁnition of the δkfor a neuron in a\nsoftmax output layer when a cross-entropy loss function is used. Equation (8.69)[467]is\na restatement of Equation (8.13)[408], which provides the general deﬁnition of the δfor a\nneuron kas the partial derivative for the error (or loss) of the network with respect to the\nweighted sum of neuron k:BE{Bzk. Equation (8.70)[467]restates this deﬁnition in terms\nof the error of the network as calculated using the cross-entropy loss function and also\ntaking the partial derivative with respect to a change in the logit for neuron kin the output\nlayer. Equation (8.71)[467]speciﬁes that the cross-entropy loss for the network is dependent\nsolely on the negative natural log of the probability of the correct prediction, per Equation\n(8.67)[466]. The logit for an output neuron k,lk, can only indirectly affect the loss in terms of\nhow it changes the predicted probability for the true category: ˆP‹. Equation (8.72)[467]uses\nthe chain rule to make this explicit by deﬁning δkas the product of the rate of change of the\nnegative natural log of the predicted probability of the true category with respect to changes\nin that probability and the rate of change of the predicted probability of the true category","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":521,"page_label":"467","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"layer. Equation (8.71)[467]speciﬁes that the cross-entropy loss for the network is dependent\nsolely on the negative natural log of the probability of the correct prediction, per Equation\n(8.67)[466]. The logit for an output neuron k,lk, can only indirectly affect the loss in terms of\nhow it changes the predicted probability for the true category: ˆP‹. Equation (8.72)[467]uses\nthe chain rule to make this explicit by deﬁning δkas the product of the rate of change of the\nnegative natural log of the predicted probability of the true category with respect to changes\nin that probability and the rate of change of the predicted probability of the true category\nwith respect to changes in the logit (we encountered this expansion step previously in a\ndifferent guise; recall that BE{Bzk“BE{BakˆBak{Bzk).\nδk“BE\nBzk(8.69)\n“BLCE`\nt,ˆP˘\nBlk(8.70)\n“B´ln`ˆP‹˘\nBlk(8.71)\n“B´ln`ˆP‹˘\nB`ˆP‹˘ˆB`ˆP‹˘\nBlk(8.72)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":522,"page_label":"468","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"468 Chapter 8 Deep Learning\nFrom calculus, the derivative of the natural log is\ndlnx\ndx“1\nx(8.73)\nUsing this identity, we can deﬁne the ﬁrst term in the product in Equation (8.72)[467]\nB´ln`ˆP‹˘\nB`ˆP‹˘“´1\nˆP‹(8.74)\nThis is simply 1 divided by the model’s predicted probability for the correct category.\nThe second term in the product in Equation (8.72)[467]is the rate of change of the predicted\nprobability for the true category with respect to the logit for one of the neurons in the\nsoftmax layer. In other words, this is the rate of change of the softmax calculation for the\nactivation of the output neuron for the true class with respect to the logits of a neuron in\nthe output layer. There are two cases that we need to handle with this derivative (1) when\nlkis the logit for the neuron whose activation is the probability of the correct category (i.e.,\nk“‹), and (2) when lkis the logit for a neuron whose activation is the probability for one\nof the incorrect categories (i.e., k‰‹).\nThe reason why we need two different derivatives for the softmax function to handle\nthese two cases is that when we are taking the derivative with respect to changes in the\nlogit for the neuron whose activation is ˆP‹(i.e. k“ ‹), then adjusting logit kchanges\nboth the numerator and the denominator in the softmax for the calculation of ˆP‹, whereas\nin the case that the logit with respect to which we are taking the derivative is for a neuron\ncorresponding to another category, changing the logit changes only the denominator of the\nsoftmax. We do not derive the derivatives of the softmax for each of these cases, as that\nis relatively convoluted, involving in quotient rule from calculus; instead, we simply state\nthem\nB`ˆP‹˘\nBlk“#ˆP‹`\n1´ˆPk˘\nifk“‹\n´ˆP‹ˆPk otherwise(8.75)\nWe can now deﬁne the calculation of the δfor a neuron in a softmax output layer using\na cross-entropy loss function as follows:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":523,"page_label":"469","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 469\nδk“B´ln`ˆP‹˘\nB`ˆP‹˘ˆB`ˆP‹˘\nBlk(8.76)\n“ ´1\nˆP‹ˆB`ˆP‹˘\nBlk(8.77)\n“ ´1\nˆP‹ˆ#ˆP‹`\n1´ˆPk˘\nifk“‹\n´ˆP‹ˆPk otherwise(8.78)\n“#\n´`\n1´ˆPk˘\nifk“‹\nˆPk otherwise(8.79)\nEquation (8.76)[469]is taken from Equation (8.72)[467]; the step to Equation (8.77)[469]uses\nEquation (8.74)[468]; the rewrite in Equation (8.78)[469]uses Equation (8.75)[468]; and ﬁnally\nEquation (8.79)[469]is the simpliﬁcation we get when the terms cancel out after the product.\nEquation (8.79)[469]states thatδfor the neuron in the softmax output layer whose activa-\ntion is the predicted probability for the correct category as speciﬁed by a 1 in the one-hot\nencoded target vector using the cross-entropy loss function is\nδk“‹“´`\n1´ˆPk˘\n(8.80)\nFor each of the other neurons in the softmax output layer, their δis simply their activation\nδk‰‹“ˆPk (8.81)\nWe illustrate the calculation of δs for neurons in a softmax output layer using the mini-\nbatch of examples listed in Table 8.13[464]and the network architecture shown in Figure\n8.27[465]. Figure 8.28[470]illustrates the forward pass for this mini-batch through this net-\nwork. Note that we are assuming the neurons in the hidden layers are ReLUs and that the\nﬁnal layer is a softmax layer. To highlight this change in activation functions between the\nlayers, we have labeled the ϕsymbol in the ﬁgure with the name of the activation function\nit represents. The weights in this network have been initialized so that all the bias terms\nare equal to`1.0and the other weights are listed in the weight matrices shown in Figure\n8.28[470]. Further, to aid in presentation we have rounded the activations for each layer to\nfour decimal places and used these rounded activations as inputs to the later calculations.\nFocusing on the softmax activations for the output layer for all four examples, all three\nneurons output similar values; this is not surprising given that this is a randomly initialized\nnetwork. However, for the purpose of illustration it is worth noting that the model would\nreturn the class label with the highest probability for each example; hence the model would\nreturn a prediction of lowfor all four examples in the mini-batch.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":524,"page_label":"470","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"+0.10−0.02+0.07\n+0.10+0.06+0.18\n+0.10+0.05−0.04\nHidden Layer 1\nWeight Matrix1.00 1.00 1.00 1.00\n0.04 0.84 0.50 0.53\n0.81 0.58 0.07 1.00\nInput Layer=0.1559 0.1238 0.0949 0.1594\n0.1672 0.1968 0.1356 0.2118\n0.0696 0.1188 0.1222 0.0865Z(1)\nϕReLU0.1559 0.1238 0.0949 0.1594\n0.1672 0.1968 0.1356 0.2118\n0.0696 0.1188 0.1222 0.0865Activations\nHidden Layer 1\n+0.10−0.03+0.09+0.06\n+0.10+0.20−0.01−0.09\nHidden Layer 2\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.1559 0.1238 0.0949 0.1594\n0.1672 0.1968 0.1356 0.2118\n0.0696 0.1188 0.1222 0.0865\nActivations\nHidden Layer 1=+0.114547 +0.121126 0.116689 0.1194700\n+0.123244 +0.112100 0.106626 0.1219770Z(2)\nϕReLU0.1145 0.1211 0.1167 0.1195\n0.1232 0.1121 0.1066 0.1220Activations\nHidden Layer 2\n+0.10−0.03+0.09\n+0.10−0.07+0.06\n+0.10+0.08−0.05\nOutput Layer\nWeight Matrix1.0000 1.0000 1.0000 1.0000\n0.1145 0.1211 0.1167 0.1195\n0.1232 0.1121 0.1066 0.1220\nActivations\nHidden Layer 2=+0.107653 +0.106456 +0.106093 +0.107395\n+0.099377 +0.098249 +0.098227 +0.098955\n+0.103000 +0.104083 +0.104006 +0.103460Logits\nϕSM0.3348 0.3345 0.3344 0.3347\n0.3320 0.3318 0.3318 0.3319\n0.3332 0.3337 0.3337 0.3334Softmax Activations\nOutput Layer3\n4\n51 2 d1d2d3d4\n6\n73 4 5\n8\n9\n106 7+d[0]=1\n+d[0]=1\nFigure 8.28\nThe forward pass of the mini-batch of examples listed in Table 8.13[464]through the network in Figure\n8.27[465].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":525,"page_label":"471","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 471\nTable 8.15\nThe calculation of the softmax activations for each of the neurons in the output layer for each example\nin the mini-batch, and the calculation of the δfor each neuron in the output layer for each example\nin the mini-batch.\nd1 d2 d3 d4\nPer Neuron Per Example logits\nNeuron 8 0.107653 0.106456 0.106093 0.107395\nNeuron 9 0.099377 0.098249 0.098227 0.098955\nNeuron 10 0.103000 0.104083 0.1040060 0.103460\nPer Neuron Per Example eli\nNeuron 8 1.113661238 1.112328983 1.111925281 1.11337395\nNeuron 9 1.104482611 1.103237457 1.103213186 1.104016618\nNeuron 10 1.108491409 1.109692556 1.109607113 1.109001432ř\nieli 3.326635258 3.325258996 3.324745579 3.326392\nPer Neuron Per Example Softmax Activations\nNeuron 8 0.3348 0.3345 0.3344 0.3347\nNeuron 9 0.3320 0.3318 0.3318 0.3319\nNeuron 10 0.3332 0.3337 0.3337 0.3334\nPer Neuron Target One-Hot Encodings\nNeuron 8 0 1 0 0\nNeuron 9 0 0 1 1\nNeuron 10 1 0 0 0\nPer Neuron Per Example δs\nNeuron 8 0.3348 -0.6655 0.3344 0.3347\nNeuron 9 0.3320 0.3318 -0.6682 -0.6681\nNeuron 10 -0.6668 0.3337 0.3337 0.3334\nTable 8.15[471]steps through the calculations that bring us from the logits for each of the\nneurons for each example to the corresponding softmax activations and then to the δs for\neach neuron for each example. Table 8.15[471]is split into ﬁve segments with each segment\ncontaining information on Neurons 8, 9, and 10, and the calculations ﬂow from the top of\nthe table to the bottom. The top segment of Table 8.15[471]lists the logit values for Neurons\n8, 9, and 10 for each of the examples in the mini-batch (these logits are taken directly from\nFigure 8.28[470]). The second segment in Table 8.15[471]listseraised to the power of the\ncorresponding logit ( eli) and also the per example sum of these values (ř\nieli). The third\nsegment lists the per neuron and per example softmax activations; these values are calcu-\nlated by dividing the elivalue in the corresponding cell in the second segment by the sum\nfor that columnř\nieli. For example, the calculation of the softmax activation for Neuron\n8 and example d1is1.113661238{3.326635258«0.3348 . These softmax activations are\nalso shown in Figure 8.28[470]. The bottom two segments of the table illustrate the calcula-\ntion of theδs for each neuron for each example. The fourth segment of the table lists the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":526,"page_label":"472","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"472 Chapter 8 Deep Learning\nper example one-hot encoding of the target for each example in the mini-batch. These 0\nand 1 values tell us which equation to use in order to calculate the corresponding δ(be-\nlow) based on the corresponding softmax activation (above). Whenever the target one-hot\nencoding is a 0, we use Equation (8.81)[469]to calculate the δ; and whenever it is a 1, we\nuse Equation (8.80)[469]to calculate the δ. For example, for d1the target one-hot encoding\nfor Neuron 8 is 0, and as a result we use Equation (8.81)[469]to calculate the δ; this entails\nsimply copying the softmax activation. However, for the same example, the target one-hot\nencoding for Neuron 10 is 1, and so we use Equation (8.80)[469]to calculate the δfor neuron\n10 in this example. The result of this calculation is ´0.6668 .\nOnce theδs for the output neurons have been calculated, the backpropagation of the δs\nthrough the network and the updating of the weights progresses as in the previous exam-\nples. For example, to update weight w9,6we would ﬁrst calculate ∆w9,6using Equation\n(8.29)[416]. Equation (8.82)[472]shows this calculation with the per example δs for neuron 9\ntaken from Table 8.15[471], and the activations for Neuron 6 are from Figure 8.28[470]\n∆w9,6“4ÿ\nj“1δ9,jˆa6,j\n“p0.3320ˆ0.1145q`p0.3318ˆ0.1211q\n`p´0.6682ˆ0.1167q`p´0.6681ˆ0.1195q\n“0.038014`0.04018098`´0.07797894`´0.07983795\n“´0.07962191 (8.82)\nThe weight can then be updated using the batch weight update rule (see Equation (8.30)[416]),\nwhere we assume a learning rate of α“0.01, as shown in Equation (8.83)[472]\nw9,6“w9,6´αˆ∆w9,6\n“´0.07´0.01ˆ´0.07962191\n“´0.07´p´0.000796219q\n“´0.069203781 (8.83)\n8.4.4 Early Stopping and Dropout: Preventing Overﬁtting\nDeep learning models can have millions of parameters, and this complexity makes them\nprone to overﬁtting . Two of the most commonly used methods to avoid overﬁtting in\nneural networks are early stopping anddropout (Srivastava et al., 2014).\nThe fundamental idea underpinning early stopping is that we can identify the point\nduring an iterative training algorithm (such as backpropagation) when a model begins to\noverﬁt the training data as being the point when the error of the model on a validation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":527,"page_label":"473","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 473\ndataset begins to increase. This idea of using a validation set to identify when overﬁtting\noccurs is illustrated in Figure 9.3[542]in Chapter 9[533]in which we discuss the use of a\nvalidation set in the general setting of designing a model evaluation experiment. Building\non this idea, the early stopping algorithm uses the performance of the model on a validation\ndataset to determine when to stop training the model. To apply early stopping, we ﬁrst set\naside a portion of the training data as a validation set. Then during training after each\niteration (or number of iterations), the current model is run on the validation set and its\nerror is recorded. If the error on the validation starts to increase, then we should stop\ntraining. One slight complication of this approach is that during training, the error of a\nmodel can ﬂuctuate even without the occurrence of overﬁtting, for example if the learning\nrate is too high; therefore, applying a strict rule of stopping training immediately after the\nﬁrst time the validation error increases can be too conservative a criterion for stopping.\nConsequently, it is standard to use a patience parameter to control early stopping. The\npatience parameter is a predeﬁned threshold (i.e., it is a hyper-parameter) that speciﬁes the\nnumber of times in a row we will permit the error on the validation set to be higher than the\nlowest recorded so far before we stop training. For example, if the patience parameter is set\nto 10 and we test the model on the validation set after every iteration, then we would allow\ntraining to continue until we observe 10 successive validation errors higher than the lowest\nrecorded so far at which point our patience would run out and we stop training. Note that\neach time we observe a decrease in the best validation error, we reset the patience count\nto zero. Using a patience parameter allows the validation error to ﬂuctuate a bit during\ntraining while still allow training to progress; it is only when we have observed a clear\ntrend over multiple iterations of a relatively high validation error that we stop training.\nNaturally, when our patience runs out, we roll back training to the version of the model\nthat produced the lowest validation set error. Doing this requires us to store the parameters\nof the model each time we observe a drop in the validation error. Algorithm 6[474]lists the\nearly stopping algorithm. We recommend using early stopping as the default strategy to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":527,"page_label":"473","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"each time we observe a decrease in the best validation error, we reset the patience count\nto zero. Using a patience parameter allows the validation error to ﬂuctuate a bit during\ntraining while still allow training to progress; it is only when we have observed a clear\ntrend over multiple iterations of a relatively high validation error that we stop training.\nNaturally, when our patience runs out, we roll back training to the version of the model\nthat produced the lowest validation set error. Doing this requires us to store the parameters\nof the model each time we observe a drop in the validation error. Algorithm 6[474]lists the\nearly stopping algorithm. We recommend using early stopping as the default strategy to\ncontrol when to stop training a neural network.\nAnother simple and very effective technique to stop overﬁtting is called dropout . When\nwe use dropout, each time we load a training example we choose a random set of neurons\nfrom the input and hidden layers and drop (or delete) them from the network for that\ntraining instance. We then do the forward and backward pass of the backpropagation and\nthe weight update as usual for that example; the distinction is just that these processes\nwill be run on the smaller network that remains after the selected neurons were dropped.\nFor example, the weights connected to neurons that are dropped for an example do not\nreceive updates on that example. Then for the next example we randomly choose a new\nset of nodes to drop and then do backpropagation as usual for this new example, but this\ntime on the new reduced version of the network. In fact, not only is a different randomly\nselected set of neurons dropped for each training example, but a different set of neurons is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":528,"page_label":"474","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"474 Chapter 8 Deep Learning\nAlgorithm 6 The early stopping algorithm\nRequire: pthe patience parameter\nRequire: Dνa validation set\n1:bestValidationError “8\n2:tmpValidationError “0\n3:θ“initial model parameters\n4:θbest“0\n5:patienceCount = 0\n6:while patienceCount ăpdo\n7:θ“new model parameters after most recent weight update\n8: tmpValidationError = calculateValidationError( θ,Dν)\n9: ifbestValidationError ětmpValidationError then\n10: bestValidationError = tmpValidationError\n11:θbest“θ\n12: patienceCount = 0\n13: else\n14: patienceCount = patienceCount + 1\n15: end if\n16:end while\n17:return Best Model Parameters θbest\nrandomly selected for each presentation of a training example. Consequently, for a given\nexample, a different set of neurons is dropped each time it is presented to the network (i.e.,\nfor each epoch). Figure 8.29[475]illustrates how different small networks are generated for\neach training example by randomly dropping neurons from the original large network.\nThe most popular method for implementing dropout is known as inverted dropout .38\nWhen we use inverted dropout we drop a neuron by multiplying the activation of the neuron\nduring the forward pass by zero. This means that its activation does not ﬂow forward\nthrough the network, and hence it has no effect on the output of the model. Also, during\nbackpropagation no error gradients ﬂow back through the dropped neurons; their δs are\nset to 0. This makes sense because their activation will not have been used to generate\nthe output of the model and so will not have contributed to the error of the model. As a\nconsequence, the weights on a dropped neuron won’t receive any weight updates for that\nexample.\nAlgorithm 7[476]provides a pseudocode deﬁnition of how the forward and backward\npasses of the backpropagation algorithm are modiﬁed to include inverted dropout. This\n38. This explanation of inverted dropout is inspired by a description given in Andrew Ng’s Coursera course; the\nvideo is available at https://www.youtube.com/watch?v=D8PJAL-MZv8&feature=youtu.be.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":529,"page_label":"475","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 475\n3\n1 6\n4 8\n2 7\n5\nNo Dropoutw3,1\nw4,1\nw\n5,1w3,2\nw4,2\nw5,2w6,3\nw\n7,3w6,4\nw7,4w6.6\nw7,5w8,6\nw8,7w3,0\nw4,0\nw5,0w6,0\nw7,0w8,03\n1 6\n4 8\n2 7\n5\nDropout for Example 1w4,1\nw\n5,1w6,4\nw6.6w8,6w4,0\nw5,0w6,0\nw8,03\n1 6\n4 8\n2 7\n5\nDropout for Example 2w3,1\nw\n5,1w3,2\nw5,2w6,3\nw6.6w8,6w3,0\nw5,0w6,0\nw8,0\nFigure 8.29\nAn illustration of how different small networks are generated for different training examples by\napplying dropout to the original large network. The gray nodes mark the neurons that have been\ndropped from the network for the training example.\nimplementation works on a layer-by-layer basis. During the forward pass for each input\nand hidden layer in the network a vector DropMask of 0 or 1 values is sampled from a\nBernoulli distribution with probability ρthat a sampled value will be 1. The length of\nDropMask should be equal to the number of neurons in the layer (see Line 2[476]). In Line\n3[476]the elementwise multiplication of the vector containing the activations of the neurons\nin the layer and the DropMask vector is performed (we use the notation dto denote an\nelementwise product).39In the updated activation vector aplq1generated by this multiplica-\ntion, the activations for all the neurons whose position in the activation vector correspond\nwith a 0value in DropMask will be 0. Next, in Line 4[476]each element in new activation\nvector aplq1is divided by the parameter ρ. The name inverted dropout comes from this\ndivision of the non-zeroed activations by the ρparameter. The reason we perform this\ndivision by ρis to scale up the non-zero activations in the new activation vector so that\nthe weighted sum calculations in the next layer are of a similar magnitude to what they\nwould have been if none of the activations had been set to 0. The beneﬁt of this is that the\nzvalues for the neurons in the next layer will be of a similar magnitude during training,\nwhen we are using dropout, as they will be during testing/inference (when we do not use\ndropout). The activation vector created by this division aplq2is the activation vector prop-\nagated forward to the next layer. During the backward pass, once we have calculated the\nδs for the neurons in a layer, we multiply each neuron’s δby the corresponding element\ntheDropMask vector that was created for that layer during the forward pass (Line 7[476]).\nAs a result of this multiplication for any neuron whose activation in the forward pass was","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":529,"page_label":"475","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"would have been if none of the activations had been set to 0. The beneﬁt of this is that the\nzvalues for the neurons in the next layer will be of a similar magnitude during training,\nwhen we are using dropout, as they will be during testing/inference (when we do not use\ndropout). The activation vector created by this division aplq2is the activation vector prop-\nagated forward to the next layer. During the backward pass, once we have calculated the\nδs for the neurons in a layer, we multiply each neuron’s δby the corresponding element\ntheDropMask vector that was created for that layer during the forward pass (Line 7[476]).\nAs a result of this multiplication for any neuron whose activation in the forward pass was\nset to 0 by the multiplication with DropMask , theδvalue of the neuron will also be set to\n39. This operation is sometimes called the Hadamard product (see Appendix D[771]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":530,"page_label":"476","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"476 Chapter 8 Deep Learning\n0. This ensures that no error gradients ﬂow back through the neurons that were dropped\nfor this example. We then use the updated δs to backpropagate the error gradients to the\npreceding layer and also for the weight update calculations.\nAlgorithm 7 Extensions to Backpropagation to Use Inverted Dropout\nRequire:ρprobability that a neuron in a layer will not be dropped\nŹForward Pass\n1:foreach input or hidden layer ldo\n2: DropMaskplq“pm1,..., msizeplqq„Bernoullipρq\n3: aplq1“aplqdDropMaxplq\n4: aplq2“1\nρaplq1\n5:end for\nŹBackward Pass\n6:foreach layer lin backward pass do\n7:δplq“δplqdDropMaxplq\n8:end for\nOnce the model has been trained we do not use dropout. Using dropout during inference\nwould introduce random noise to the inference process. Dropout is both simple and very\neffective, and applying dropout is standard practice in most deep learning research today.\nTheρparameter is a hyper-parameter that is preset before training. Typical values for ρ\nare0.8for the input layer, and 0.5for hidden layers (Goodfellow et al., 2016, p. 253).\nRandomly dropping neurons from a network during training may seem like a surprising\nway to improve the performance of the model. There are a number of perspectives on un-\nderstanding how dropout helps with overﬁtting. One way to understand how dropout helps\nis to recognize that because we use a different network on each training example, we are in\neffect training an ensemble of a very large number of smaller networks rather than training\na single large model, and these smaller networks are less complex and so are less likely\nto overﬁt. Another way to understand how dropout helps is to notice that the training data\nlooks different at every epoch because each time an example is presented to a network a\ndifferent set of input neurons is set to 0. This variation in the data stops the model from\nmemorizing the training data and forces it to learn patterns that generalize over sets of fea-\ntures rather than relying on a particular feature (or small subset of features). The fact that\nmore features contribute to the predictions made by the model has the effect that the weight\nupdates get spread out across more weights because more weights will have been involved\nin the prediction and hence will have contributed to the error. Spreading out the weight\nupdates means that the weight will in general remain smaller. Keeping all the weights in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":530,"page_label":"476","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"looks different at every epoch because each time an example is presented to a network a\ndifferent set of input neurons is set to 0. This variation in the data stops the model from\nmemorizing the training data and forces it to learn patterns that generalize over sets of fea-\ntures rather than relying on a particular feature (or small subset of features). The fact that\nmore features contribute to the predictions made by the model has the effect that the weight\nupdates get spread out across more weights because more weights will have been involved\nin the prediction and hence will have contributed to the error. Spreading out the weight\nupdates means that the weight will in general remain smaller. Keeping all the weights in\na network small helps to keep a model’s predictions relatively stable with respect to small\nchanges in the input: if a model has some relatively large weights, then the model can be","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":531,"page_label":"477","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 477\nvery sensitive to small changes in features to which these weights are applied. Models\nwhose output changes drastically in response to small changes in the input are likely over-\nﬁtting the data, because a small amount of noise in the input data can have a large effect\non the outputs generated by the model. The approach of avoiding overﬁtting by modifying\nthe learning algorithm in order to generate models that are stable with respect to changes\nin the input is generally known as regularization . Hence dropout can be understood as a\nregularization technique that improves the stability of the resulting model. Indeed, early\nstopping can also be understood as a regularization technique because it limits the number\nof updates to the weights in a model and by so doing keeps individual weights from getting\ntoo large.\n8.4.5 Convolutional Neural Networks\nAn artiﬁcial neural network is built up by connecting lots of simple processing units, and\ntherefore neural networks have a very ﬂexible structure. This ﬂexibility in the network\ndesign space can be exploited to tailor a network to process different types of data. For\nexample, rather than using full connectivity between layers (as we have done so far), we\nmight decide to constrain the connectivity between layers so that each neuron in one layer\nconnects only to a subset of the neurons in the next layer. Done correctly, tailoring the\narchitecture of a network can help the network to learn a particular task by guiding the\nnetwork to learn useful functions for the target task.\nIn this section we motivate and explain the key architectural characteristics of convolu-\ntional neural networks (or CNNs) which are primarily tailored to process grid like data,\nsuch as image data. The CNN architecture was originally applied to handwritten digit\nrecognition, and much of the early work on CNNs was based on the MNIST (pronounced\nem-nist ) dataset40(Le Cun et al., 1998). The dataset contains 60,000 training and 10,000\ntest images of handwritten digits from approximately 250 writers. Each image contains a\nsingle digit that has been size-normalized and centered. Figure 8.30[478]shows some exam-\nples of the images from the dataset. Each image is labeled with the digit it contains, and\nthe prediction task is to return the correct label for each image. Each image is grayscale\nand can be represented as a grid of 28 by 28 integers in the range r0,255swhere a 0 value","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":531,"page_label":"477","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"such as image data. The CNN architecture was originally applied to handwritten digit\nrecognition, and much of the early work on CNNs was based on the MNIST (pronounced\nem-nist ) dataset40(Le Cun et al., 1998). The dataset contains 60,000 training and 10,000\ntest images of handwritten digits from approximately 250 writers. Each image contains a\nsingle digit that has been size-normalized and centered. Figure 8.30[478]shows some exam-\nples of the images from the dataset. Each image is labeled with the digit it contains, and\nthe prediction task is to return the correct label for each image. Each image is grayscale\nand can be represented as a grid of 28 by 28 integers in the range r0,255swhere a 0 value\nindicates a white pixel, a value of 255 indicates a black pixel, and numbers between 0 and\n255 indicate shades of gray. We use this handwritten digit recognition task as the basis for\nour examples in this section. In particular, we use the 6-by-6 matrix grayscale encoding of\na 4, shown in Equation (8.84)[478]as the input pattern for our examples\n40.NIST is the acronym for the institute that collected the data, the National Institute for Standards and Technol-\nogy, and Mindicates that the original data has been modiﬁed to make it easier to use for machine learning. The\nMNIST dataset is available at http://yann.lecun.com/exdb/mnist/.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":532,"page_label":"478","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"478 Chapter 8 Deep Learning\nFigure 8.30\nSamples of the handwritten digit images from the MNIST dataset. Image attribution: Josef Step-\npan, used here under the Creative Commons Attribution-Share Alike 4.0 International license\nhttps://creativecommons.org/licenses/by-sa/4.0) and was sourced via Wikimedia Commons https:\n//commons.wikimedia.org/wiki/File:MnistExamples.png.\n»\n————————–000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬂ(8.84)\nConvolutional neural networks have three distinctive characteristics:\n1.local receptive ﬁelds;\n2.weight sharing; and\n3.sub-sampling (pooling).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":533,"page_label":"479","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 479\n8.4.5.1 Local receptive ﬁelds and ﬁlters The concept of local receptive ﬁeld comes\nfrom research on visual perception in cats. In the early 1960s Hubel and Wiesel carried\nout a series of experiments in which they used probes to track the neural activity in the\nbrains of sedated cats while simple visual features such as horizontal or vertical lines of\nlight were projected onto different locations on a dark screen (Hubel and Wiesel, 1962).\nThey discovered neurons in the brains of cats that activated only when a visual feature\nappeared at speciﬁc locations in the visual ﬁeld. For a given neuron to react, a speciﬁc\nvisual feature had to occur at a particular location in the visual ﬁeld; if the feature was\nmoved to a different location in the visual ﬁeld, then the neuron did not activate, nor did\nit activate if a different feature occurred at its target location. Furthermore, some groups\nof neurons reacted to the same visual feature, but each neuron in the group reacted when\nthe feature occurred at different locations; for example, one neuron would react to the\nfeature if it occurred in the bottom-right of the screen whereas a different neuron would\nreact if the feature occurred in the top-left of the screen. Together this set of neurons\ncould determine whether the visual feature occurred anywhere in the screen. By having\nmultiple such groups of neurons, in which each group contained neurons that specialized\nin identifying a particular visual feature and that as a whole inspected the entire visual ﬁeld\nfor the target feature, the cat was able to perceive multiple different features occurring at\ndifferent locations at the same time. However, each of the neurons in each group only\ninspected a local region of the visual ﬁeld for a single feature, and these local regions\nbecame known as local receptive ﬁelds . The advantage of a local receptive ﬁeld is that\nfor a given neuron, the learning task is simpliﬁed to learning whether a particular feature\noccurs in a speciﬁc local region rather than learning to activate when one or more features\noccur anywhere in the visual ﬁeld.\nInspired by these results, neural network research started to design networks in which\nneurons in one layer received input only from a localized subset of neurons in the preceding\nlayer, that is, each neuron had a local receptive ﬁeld in the preceding layer. Using local\nreceptive ﬁelds, neurons can learn to extract low-level features in the input (such as a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":533,"page_label":"479","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"became known as local receptive ﬁelds . The advantage of a local receptive ﬁeld is that\nfor a given neuron, the learning task is simpliﬁed to learning whether a particular feature\noccurs in a speciﬁc local region rather than learning to activate when one or more features\noccur anywhere in the visual ﬁeld.\nInspired by these results, neural network research started to design networks in which\nneurons in one layer received input only from a localized subset of neurons in the preceding\nlayer, that is, each neuron had a local receptive ﬁeld in the preceding layer. Using local\nreceptive ﬁelds, neurons can learn to extract low-level features in the input (such as a\nsegment or an oriented edge in an image), and these features can be passed on to neurons\nin later layers that combine these low-level features into more complex features. Figure\n8.31[480]illustrates the concept of a local receptive ﬁeld in a neural network. In this ﬁgure,\nthe input to the network is a 6-by-6 matrix of grayscale values to represent a 6-by-6 image41\nof a 4; the 4 is shown in the matrix by 255 values. The neuron in the ﬁgure takes 9 inputs,\narranged in a two-dimensional 3-by-3 grid, mirroring the two-dimensional nature of the\nimage. Apart from the grid nature of the inputs, the rest of the processing within the\nneuron is the same as previously described in this chapter: the result of a weighted sum of\n41. We are using this smaller 6-by-6 image rather than the full 28-by-28 MNIST digit image dimensions to\nsimplify the illustration.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":534,"page_label":"480","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"480 Chapter 8 Deep Learning\n000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6-by-6 Pixel Image InputΣϕ000 000 000\n000 255 000\n000 255 000w1w2w3\nw4w5w6\nw7w8w9Output\nFigure 8.31\nA 6-by-6 matrix representation of a grayscale image of a 4, and a neuron with a receptive ﬁeld that\ncovers the top-left corner of the image. This ﬁgure was inspired by Figure 2 of Kelleher and Dobnik\n(2017).\ninputs is passed through a non-linear activation function. It is relatively standard in image\nprocessing to use ReLUs for the network, and so we assume throughout this section that the\nneurons use the rectiﬁed linear function as their activation function. Furthermore, for the\npurpose of simplifying the discussion and examples in this section, we drop the bias term\nin this ﬁgure and throughout most of this section. However, it is important to remember\nthat neurons in a convolutional network do have bias terms and that they are learned in\nthe same way, as they are feedforward networks. As previously, the bias term is simply an\nextra weight that is multiplied by the dummy input value 1, and the result of this product is\nincluded in the weighted sum of the neuron. The bias term is also updated in the same way\nthat any other weight would be updated. In Figure 8.31[480], the key thing to note about this\nﬁgure is that the neuron receives inputs only from a small predeﬁned region of the input;\nin other words, the neuron receives inputs only from the pixels in its local receptive ﬁeld.\nFor the purpose of illustration, let us assume that the neuron shown in Figure 8.31[480]\nuses the set of weights listed in Equation (8.85)[480]\n»\n—–0 0 0\n1 1 1\n0 0 0ﬁ\nﬃﬂ (8.85)\nEquation (8.86)[481]lists the calculation of the activation for this neuron for this set of\ninputs.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":535,"page_label":"481","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 481\n000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6-by-6 Pixel Image InputΣϕ255 000 255\n255 255 255\n000 000 255w1w2w3\nw4w5w6\nw7w8w9Output\nFigure 8.32\nA 6-by-6 matrix representation of a grayscale image of a 4, and a neuron with a different receptive\nﬁeld from the neuron in Figure 8.31[480]. This ﬁgure was inspired by Figure 2 of Kelleher and Dobnik\n(2017).\nai“recti f ierppw1ˆ000q`pw2ˆ000q`pw3ˆ000q\n`pw4ˆ000q`pw5ˆ255q`pw6ˆ000q\n`pw7ˆ000q`pw8ˆ255q`pw9ˆ000qq\n“recti f ierpp0ˆ000q`p0ˆ000q`p0ˆ000q\n`p1ˆ000q`p1ˆ255q`p1ˆ000q\n`p0ˆ000q`p0ˆ255q`p0ˆ000qq\n“255 (8.86)\nThis calculation shows that if a neuron uses the set of weights in Equation (8.85)[480],\nthen the activation of the neuron is solely dependent on the values along the middle row\nof inputs. All the other input values are ignored because they are given a weight of 0.\nConsequently, neurons using this set of weights can be thought of as rudimentary detectors\nfor horizontal edges because they will have maximum activation if there is a horizontal line\nacross the middle row of their inputs. For example, Figure 8.32[481]shows a second neuron\nwith a different local receptive ﬁeld, and Equation (8.87)[482]shows the calculation of the\nactivation of this neuron if it uses the same set of weights. In this case, there are maximum\ngrayscale values ( 255) across the middle row of the neurons receptive ﬁeld, and as a result\nof the interaction between this input pattern and the weights in Equation (8.85)[480], this\nneuron has a very large activation for this input.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":536,"page_label":"482","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"482 Chapter 8 Deep Learning\nai“recti f ierppw1ˆ255q`pw2ˆ000q`pw3ˆ255q\n`pw4ˆ255q`pw5ˆ255q`pw6ˆ255q\n`pw7ˆ000q`pw8ˆ000q`pw9ˆ255qq\n“recti f ierpp0ˆ255q`p0ˆ000q`p0ˆ255q\n`p1ˆ255q`p1ˆ255q`p1ˆ255q\n`p0ˆ000q`p0ˆ000q`p0ˆ255qq\n“765 (8.87)\nEquation (8.88)[482]shows some other sets of weights that our example neurons could use.\nIt is relatively straightforward to see that neurons using the weight matrix on the left will\nhave a high activation if their local receptive ﬁeld contains high values down the central\ncolumn of the ﬁeld, neurons using the weight matrix in the middle will respond if there\nare high values down the left-right diagonal of the receptive ﬁeld, neurons using the third\nweight matrix will have a high activation if there are more high values down the central\ncolumn of the receptive ﬁeld than there are in the other pixels in the input, and similarly the\ndistribution of positive and negative values in the matrix on the right means that neurons\nusing this set of weights will have a high activation if there are more high values across\nthe middle row of the receptive ﬁeld than in other places in the input. There are, of course,\nmany different weight matrices that could be deﬁned, each of which would cause a neuron\nto activate in response to a different visual pattern in the neuron’s local receptive ﬁeld. In\nfact, given that the weights are real numbers, there are inﬁnitely many combinations of\nweights, and we are using only integer values in the examples for the sake of clarity in\npresentation.\nThe set of weights used by a neuron determine the type of visual feature to which the\nneuron activates in response; consequently, these weight matrices are called ﬁlters because\nthey ﬁlter the input by returning high activations for certain patterns of inputs and low acti-\nvations for others. In this discussion on local receptive ﬁelds, the ﬁlters we have presented\nare hand designed for the purpose of illustration. In reality, these ﬁlters are learned by the\nconvolutional network in the same way that weights are learned in a fully connected feed-\nforward network. Allowing the network to learn the ﬁlter weights means that the network\nis able to learn which visual patterns are useful to extract from the visual input in order to\nbe successful at the prediction task on which it is being trained.\n»\n—–0 1 0\n0 1 0\n0 1 0ﬁ\nﬃﬂ»\n—–1 0 0\n0 1 0\n0 0 1ﬁ\nﬃﬂ»\n—–´1`1´1\n´1`1´1\n´1`1´1ﬁ\nﬃﬂ»\n—–´1´1´1\n`1`1`1\n´1´1´1ﬁ\nﬃﬂ (8.88)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":537,"page_label":"483","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 483\n8.4.5.2 Weight sharing and translation equivariant feature detection When a neuron\napplies a ﬁlter to its local receptive ﬁeld, it is a local visual feature detector for which\nthe visual feature is a pattern of input values. If the feature occurs in the neuron’s local\nreceptive ﬁeld, then the neuron will have a high activation. However, an image processing\nsystem should be able to detect whether a visual feature occurs in an image irrespective of\nwhere in the image it occurs. Technically, this property is described as the model being\nequivariant to the translation of features.\nConvolutional neural networks achieve translation equivariant feature detection through\nweight sharing . This is done by organizing neurons into groups in which all the neurons\nin the group apply the same ﬁlter to their inputs. In other words, in a convolutional neural\nnetwork, when two neurons share weights, they share all their weights, because they use\nthe same ﬁlter. When two or more neurons share a ﬁlter, then each weight in the ﬁlter is\nused multiple times during the forward pass of the training algorithm to process a given\ninput (once by each neuron that uses the ﬁlter). During the backward pass of the algorithm,\nwe calculate a separate weight update for each neuron that uses the weight, and then the\nﬁnal weight update that is applied is the sum of these separate weight updates. By summing\nthe weight updates across the neurons that use it, we retain a single consistent weight for\nall the neurons. This is similar to the way we summed the weight updates for a weight\nduring batch training (see Equation (8.30)[416]); the difference here is that for each training\nexample we sum over the weight updates for each neuron that uses the weight (as opposed\nto weight updates for different training examples). So, if a weight is shared by mdifferent\nneurons, then the weight update after processing a single example is deﬁned as follows:\n∆wi,˚“mÿ\ni“1δiˆa˚\nwi,˚Ðwi,˚´αˆ∆wi,˚ (8.89)\nwhere iiterates over the neurons that share the weight; and ˚is a wildcard placeholder for\nthe index of the appropriate input neuron to each neuron that uses the weight. When we\nare using batch gradient descent, this summation can be extended to include a summation\nover the examples in the batch.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":538,"page_label":"484","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths\n.000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths\n000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":538,"page_label":"484","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths\n000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths\n000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":538,"page_label":"484","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000\n6 by 6 Pixel Image Input000 000 255 255\n255 000 255 255\n255 255 255 255\n000 000 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weigths\nFigure 8.33\nIllustration of the organization of a set of neurons that share weights (use the same ﬁlter) and their\nlocal receptive ﬁelds such that together the receptive ﬁelds cover the entirety of the input image.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":539,"page_label":"485","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 485\nThe way that a convolutional neural network uses weight sharing to achieve translation\nequivariant feature detection is by organizing the local receptive ﬁelds of a set of neurons\nthat share a ﬁlter (and hence share their weights) so that (1) each neuron’s receptive ﬁeld\ncovers a slightly different region of the visual ﬁeld compared with the other neurons in the\nset; and (2) together the receptive ﬁelds of the neurons in the set cover the entire visual\nﬁeld. Figure 8.33[484]illustrates how the local receptive ﬁelds of a set of neurons can be\norganized so that together they cover the entirety of the input. This ﬁgure contains 16\nsubﬁgures, with each subﬁgure containing a matrix of input data (on the left) representing\nthe input image and a grid of 16 circles (on the right) in which each circle represents a\nneuron. Each subﬁgure highlights the local receptive ﬁeld in the input of the highlighted\nneuron in the set of neurons.\nThe name convolutional neural network comes from the fact that it is possible to im-\nplement the processing of an image by a set of neurons that share a ﬁlter with a single\nneuron that applies the ﬁlter to each region of the image in sequence and stores the result\nfor each region. In mathematics, the process of passing a function over a sequence of val-\nues is known as convolving a function , and by analogy a set of neurons that share a ﬁlter\n(and thereby each implements the same function) and that are organized such that together\ntheir receptive ﬁelds cover the input are convolving a function over the input.42\nThe fact that the joint receptive ﬁelds of the neurons in each set cover the entire input\nmeans that if the relevant visual feature (where relevance is deﬁned by the ﬁlter used by\na set of neurons) occurs anywhere in the input, then at least one of the neurons in the\nset will have a high activation. Indeed, the activations of the neurons in a set provide a\nmap of where in the input the relevant visual feature occurred, and for this reason the set\nof activations for a set of neurons that share a ﬁlter is called a feature map . Equation\n(8.90)[486]and Equation (8.91)[486]illustrate how the feature map generated by the neurons\nin Figure 8.33[484]changes if the ﬁlter used by the neurons to process the example input\nis changed. Note that in generating the feature map for each of these equations, we have\napplied the rectiﬁed linear activation function to the results of the weighted sum of each","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":539,"page_label":"485","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"a set of neurons) occurs anywhere in the input, then at least one of the neurons in the\nset will have a high activation. Indeed, the activations of the neurons in a set provide a\nmap of where in the input the relevant visual feature occurred, and for this reason the set\nof activations for a set of neurons that share a ﬁlter is called a feature map . Equation\n(8.90)[486]and Equation (8.91)[486]illustrate how the feature map generated by the neurons\nin Figure 8.33[484]changes if the ﬁlter used by the neurons to process the example input\nis changed. Note that in generating the feature map for each of these equations, we have\napplied the rectiﬁed linear activation function to the results of the weighted sum of each\nreceptive ﬁeld and the ﬁlter.43\n42. Technically, what a convolution network is actually calculating should be called a cross-correlation (Char-\nniak, 2019, p. 52), but we ignore this technicality for the purposes of this discussion.\n43. In some texts, such as Goodfellow et al. (2016), the application of the activation function is treated as a\nseparate step after the feature map has been generated by the application of the ﬁlter. Here we include the\napplication function as part of the generation of the feature map.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":540,"page_label":"486","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"486 Chapter 8 Deep Learning\n»\n————–000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬂ\nloooooooooooooooooooomoooooooooooooooooooon\nInput ImageÑ«´1`1´1\n´1`1´1\n´1`1´1ﬀ\nloooooooomoooooooon\nConvolved FilterÑ»\n—–510 0 255 0\n510 0 0 0\n255 0 255 0\n0 0 0 0ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nFeature Map(8.90)\n»\n————–000 000 000 000 000 000\n000 255 000 000 000 000\n000 255 000 255 000 000\n000 255 255 255 255 000\n000 000 000 255 000 000\n000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬂ\nloooooooooooooooooooomoooooooooooooooooooon\nInput ImageÑ«´1´1´1\n`1`1`1\n´1´1´1ﬀ\nloooooooomoooooooon\nConvolved FilterÑ»\n—–0 0 0 0\n0 0 0 0\n255 0 255 0\n0 0 0 0ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nFeature Map(8.91)\n8.4.5.3 Filter hyper-parameters: Dimension, stride, and padding The dimensional-\nity of a feature map generated by applying a ﬁlter to an input is determined by the number\nof neurons used to process the input (each element in a feature map corresponds to the out-\nput of one neuron). We can see this if we compare the dimensionality of the features maps\nin Equation (8.90)[486]and Equation (8.91)[486]with the number of neurons shown in Figure\n8.33[484]. There are three hyper-parameters that affect the number of neurons required to\ncover the entirety of an input and hence the dimensionality of the resulting feature map;\nthese are ﬁlter dimensions, the stride, and the padding.\nTheﬁlter dimension hyper-parameter speciﬁes the size of the ﬁlter in each dimension.\nThe illustration in Figure 8.33[484]assumes that the neurons are using a two-dimensional 3-\nby-3 (height by width) ﬁlter. However, larger and smaller ﬁlters are possible, and as ﬁlters\nbecome larger the number of neurons required to cover the input naturally gets smaller and\nvice versa. For example, if we decreased the dimension of our ﬁlter to a 2-by-2 dimension\nthen we would need to increase the set of neurons to a 5-by-5 layer in order to cover\nthe input, and this would result in a 5-by-5 feature map. Conversely, if we increased the\ndimension of our ﬁlter to a 4-by-4 ﬁlter, then we could cover the input with a 3-by-3 layer\nof neurons generating a 3-by-3 feature map. Choosing a good ﬁlter size for a given dataset\noften involves a trial-and-error process of experimenting with different options. Also, all\nour ﬁlter examples so far have been two-dimensional ﬁlters. The reason is that we have","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":540,"page_label":"486","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"become larger the number of neurons required to cover the input naturally gets smaller and\nvice versa. For example, if we decreased the dimension of our ﬁlter to a 2-by-2 dimension\nthen we would need to increase the set of neurons to a 5-by-5 layer in order to cover\nthe input, and this would result in a 5-by-5 feature map. Conversely, if we increased the\ndimension of our ﬁlter to a 4-by-4 ﬁlter, then we could cover the input with a 3-by-3 layer\nof neurons generating a 3-by-3 feature map. Choosing a good ﬁlter size for a given dataset\noften involves a trial-and-error process of experimenting with different options. Also, all\nour ﬁlter examples so far have been two-dimensional ﬁlters. The reason is that we have\nbeen focusing on processing a grayscale image that is a two-dimensional input. However, it\nis quite possible to use one-dimensional ﬁlters or ﬁlters with three or more dimensions. For\nnow we continue with using two-dimensional ﬁlters, but we return to this topic in Section\n8.4.5.5[492].\nThe stride parameter speciﬁes the distance between the center of the local receptive\nﬁeld of one neuron and the center of the local receptive ﬁelds of its horizontal or vertical","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":541,"page_label":"487","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 487\nneighbor in the set of neurons sharing the ﬁlter. The conﬁguration of receptive ﬁelds in\nFigure 8.33[484]uses a horizontal and vertical stride of 1; this means that as we move from\none neuron to the next horizontally, the corresponding receptive ﬁelds also move by one\ncolumn in the input space. Similarly, as we move from one neuron to the next vertically,\nthe receptive ﬁelds also move by one row in the input space. Using a horizontal and vertical\nstride of 1 means that there is a relatively large overlap in the receptive ﬁelds between a\nneuron and its neighbors. Other strides are possible, and it is possible to use different\nhorizontal and vertical strides. For example, if we used a horizontal and vertical stride of 3\nin Figure 8.33[484], then there would be no overlap between the receptive ﬁelds of different\nneurons, and this would also reduce the number of neurons required to cover the input.\nSimilar to the ﬁlter dimensions, ﬁnding the appropriate stride for a given dataset involves\ntrial-and-error experimentation.\nThere are two related phenomena in Figure 8.33[484]that, in some instances, maybe un-\ndesirable consequences of how the receptive ﬁelds for the neurons have been deﬁned. The\nﬁrst is that if we use a 4-by-4 layer of neurons, to cover a 6-by-6 input matrix, the dimen-\nsionality of the resulting feature map is also 4-by-4. In some cases we may wish to avoid\nthis reduction in dimensionality between the input and the feature map. The second phe-\nnomenon is that there is a difference in the number of times that each pixel in the image is\nused as an input to a neuron in the grid. This differential is largest between the pixels at the\ncorners of the image versus the pixels in the middle of the image; only one of the receptive\nﬁelds covers the top-left pixel in the image, whereas nine receptive ﬁelds cover the pixel\nat coordinatep3,3q. Both of these phenomena are a consequence of the fact that we are\napplying the ﬁlter only to valid pixels in the image. We are using the term valid here to\ndistinguish the pixels that occur in the image from imaginary (or padding ) pixels that we\nmight invent around the border of an image. Figure 8.34[488]illustrates what happens if we\npad the boundary of an image with imaginary pixels. In this ﬁgure, the imaginary pixels\nare shown in gray, and the valid (real) pixels are in the center of the matrix, shown in black.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":541,"page_label":"487","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"corners of the image versus the pixels in the middle of the image; only one of the receptive\nﬁelds covers the top-left pixel in the image, whereas nine receptive ﬁelds cover the pixel\nat coordinatep3,3q. Both of these phenomena are a consequence of the fact that we are\napplying the ﬁlter only to valid pixels in the image. We are using the term valid here to\ndistinguish the pixels that occur in the image from imaginary (or padding ) pixels that we\nmight invent around the border of an image. Figure 8.34[488]illustrates what happens if we\npad the boundary of an image with imaginary pixels. In this ﬁgure, the imaginary pixels\nare shown in gray, and the valid (real) pixels are in the center of the matrix, shown in black.\nThe boundary between imaginary and valid pixels is highlighted by a thickly outlined rect-\nangle enclosing the valid pixels. All the imaginary pixels have been given a value of 000.\nThe ﬁgure also illustrates the local receptive ﬁeld of the ﬁrst neuron in the grid; note that\nthis receptive ﬁeld includes imaginary pixels. Adding this padding to the image increases\nthe number of neurons required to cover the image, assuming that the ﬁlter size and hor-\nizontal and vertical strides are maintained. In fact, there are now as many neurons in the\ngrid as there are valid pixels in the image. Furthermore, although there is still a differen-\ntial between some of the valid pixels in terms of the number of neurons that take them as\ninput, this differential has been decreased; each of the valid corner pixels is now present in\nfour receptive ﬁelds, as opposed to one as previously, whereas the count of receptive ﬁelds\ncovering a pixel in the center of the image is not affected by the padding. When padding\nis applied it is generally added to all edges as equally as is possible. As with ﬁlter size and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":542,"page_label":"488","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"488 Chapter 8 Deep Learning\n000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000\n000 000 255 000 000 000 000 000\n000 000 255 000 255 000 000 000\n000 000 255 255 255 255 000 000\n000 000 000 000 255 000 000 000\n000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000\n6-by-6 Pixel Image Input\nwith 2 Rows and 2 Columns of Padding000 000 255 255 255 255\n000 000 255 255 255 255\n255 000 255 255 255 255\n255 255 255 255 255 255\n000 000 255 255 255 255\n000 000 255 255 255 255\nConvolution of a Filter via\nSet of Neurons Sharing Weights\nFigure 8.34\nA grayscale image of a 4 after padding has been applied to the original 6-by-6 matrix representation,\nand the local receptive ﬁeld of a neuron that includes both valid and padded pixels.\nstride length, the selection of whether to use padding or not is task dependent and is often\nbased on trial and error.\nThe particular choices of ﬁlter size, stride length, and padding is task dependent. How-\never, a popular combination is to use a stride length of 1 and to pad the image with imagi-\nnary pixels (Charniak, 2019, p. 56). This combination ensures that the output from a layer\nof neurons applying a ﬁlter across an image has the same dimensions as the input image.\nMaintaining the dimensionality between input and output becomes important in convolu-\ntional neural networks when we use multiple layers of neurons, the output for one layer\nbeing interpreted as the image input to the next layer. In these cases, unless the dimension-\nality is maintained by using imaginary pixels, then the dimensionality of the input to each\nlayer reduces for each subsequent layer. Equation (8.92)[489]and Equation (8.93)[489]each\nlist a ﬁlter weight matrix and the feature map generated by using a set of neurons to process\nour example input in Equation (8.84)[478]after the input has had padding applied and using\na stride length of 1. The generated feature maps have the same 6-by-6 dimensionality as\nthe input image (before padding was applied). As a comparator, the ﬁlters used in these\ntwo equations are the same as those used in Equation (8.90)[486]and Equation (8.91)[486]; the\ndifference now is that the generated feature maps are larger, and indeed some of the new\ncells have positive values.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":543,"page_label":"489","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 489\n»\n—————————————–000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000\n000 000 255 000 000 000 000 000\n000 000 255 000 255 000 000 000\n000 000 255 255 255 255 000 000\n000 000 000 000 255 000 000 000\n000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nlooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooon\nInput ImageÑ»\n—–´1`1´1\n´1`1´1\n´1`1´1ﬁ\nﬃﬂ\nlooooooooomooooooooon\nConvolved FilterÑ»\n————————–0 255 0 0 0 0\n0 510 0 255 0 0\n0 510 0 0 0 0\n0 255 0 255 0 0\n0 0 0 0 0 0\n0 0 0 255 0 0ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nloooooooooooooooomoooooooooooooooon\nFeature Map\n(8.92)\n»\n—————————————–000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000\n000 000 255 000 000 000 000 000\n000 000 255 000 255 000 000 000\n000 000 255 255 255 255 000 000\n000 000 000 000 255 000 000 000\n000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nlooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooon\nInput ImageÑ»\n—–´1´1´1\n`1`1`1\n´1´1´1ﬁ\nﬃﬂ\nlooooooooomooooooooon\nConvolved FilterÑ»\n————————–0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 255 0 255 0 255\n0 0 0 0 0 0\n0 0 0 0 0 0ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nloooooooooooooooooomoooooooooooooooooon\nFeature Map\n(8.93)\n8.4.5.4 Pooling The precise location of a visual feature in an image may not be rele-\nvant for an image-processing task. For example, knowing that there is an eye in the top-left\nregion of an image is useful for face recognition, but the extra precision of knowing that it\nis centered at pixel (998,742) may not be useful. Indeed, in training an image processing\nmodel we typically want the model to generalize over the precise locations of features in\ntraining images so that it can still use these features when they occur in offset conﬁgura-\ntions in new images. The most straightforward way to make a model abstract away from\nthe precise location of visual features is to sub-sample the feature maps. In convolutional\nneural networks, sub-sampling is done using sub-sampling layers . Each neuron in a sub-\nsampling layer has a local receptive ﬁeld in a feature map generated by the previous layer,\nwhich will have convolved a ﬁlter over the input to that layer. However, typically the local\nreceptive ﬁelds of neurons in a sub-sampling layer do not overlap (in contrast with the over-\nlapping receptive ﬁelds used when we arrange neurons to convolve a ﬁlter). Consequently,\nthere are fewer output activations from a sub-sampling layer than there are inputs: one","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":543,"page_label":"489","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tions in new images. The most straightforward way to make a model abstract away from\nthe precise location of visual features is to sub-sample the feature maps. In convolutional\nneural networks, sub-sampling is done using sub-sampling layers . Each neuron in a sub-\nsampling layer has a local receptive ﬁeld in a feature map generated by the previous layer,\nwhich will have convolved a ﬁlter over the input to that layer. However, typically the local\nreceptive ﬁelds of neurons in a sub-sampling layer do not overlap (in contrast with the over-\nlapping receptive ﬁelds used when we arrange neurons to convolve a ﬁlter). Consequently,\nthere are fewer output activations from a sub-sampling layer than there are inputs: one\noutput per local receptive ﬁeld and multiple inputs per ﬁeld. The amount of sub-sampling\napplied is dependent on the dimensions of the receptive ﬁelds of the neurons; for example,\nusing non-overlapping 2-by-2 receptive ﬁelds, the output from a sub-sampling layer will\nhave half the number of rows and columns as the feature map input to the layer. In early\nconvolution networks, the activation of sub-sampling neurons was often the average of the\nvalues in the feature map covered by the local receptive ﬁeld of the neuron. Many modern\nconvolutional networks use a max function that simply returns the maximum value in the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":544,"page_label":"490","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"490 Chapter 8 Deep Learning\nregion of the feature map covered by the local receptive ﬁeld. Applying a max function\nto a local receptive ﬁeld is often referred to as max pooling . Equation (8.94)[490]illus-\ntrates the result of applying max pooling to the feature map from Equation (8.93)[489]using\nnon-overlapping local receptive ﬁelds with a dimensionality of 2-by-2.\n»\n—————————————–000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000\n000 000 255 000 000 000 000 000\n000 000 255 000 255 000 000 000\n000 000 255 255 255 255 000 000\n000 000 000 000 255 000 000 000\n000 000 000 000 000 000 000 000\n000 000 000 000 000 000 000 000ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nlooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooon\nInput ImageÑ»\n—–´1´1´1\n`1`1`1\n´1´1´1ﬁ\nﬃﬂ\nlooooooooomooooooooon\nConvolved FilterÑ\nÑ»\n————————–0 0 0 0 0 0\n0 0 0 0 0 0\n0 0 0 0 0 0\n0 255 0 255 0 255\n0 0 0 0 0 0\n0 0 0 0 0 0ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nloooooooooooooooooomoooooooooooooooooon\nFeature MapÑmax poolingÑ»\n—–0 0 0\n255 255 255\n0 0 0ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nOutput from Sub-sampling\n(8.94)\nUsing a max function requires that the backward pass in the backpropagation algorithm\nbe updated slightly. The reason is that a max function allows through only the maximum\nvalue from its inputs, and so the non-max values did not affect the output (and hence the\nerror) of the network. As a result, there is no error gradient with respect to the non-max\nvalues that the max function received. Furthermore, the gradient for the max function\n(Ba{Bz) for the max value is 1, because the output of the activation function will be linear\nfor small changes in the input value that achieved the max (i.e., it will change by the same\namount as that input value is changed). Consequently, in backpropagating through a max\nfunction, the entire error gradient is backpropagated to the neuron that propagated forward\nthe max value, and the other neurons receive an error gradient of zero.\nTo illustrate the backpropagation process through a convolutional layer, we need to shift\nour focus from the ﬂow of data through the layer to the neural architecture of the layer.\nEquation (8.95)[491]illustrates an extended version of the convolutional network that would\nimplement the data processing illustrated in Equation (8.94)[490]. Note that in Equation\n(8.95)[491]each symbol in a matrix represents a neuron in a layer, rather than a data point.\nThe matrix on the left of Equation (8.95)[491]represents the 6-by-6 layer of neurons that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":544,"page_label":"490","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"function, the entire error gradient is backpropagated to the neuron that propagated forward\nthe max value, and the other neurons receive an error gradient of zero.\nTo illustrate the backpropagation process through a convolutional layer, we need to shift\nour focus from the ﬂow of data through the layer to the neural architecture of the layer.\nEquation (8.95)[491]illustrates an extended version of the convolutional network that would\nimplement the data processing illustrated in Equation (8.94)[490]. Note that in Equation\n(8.95)[491]each symbol in a matrix represents a neuron in a layer, rather than a data point.\nThe matrix on the left of Equation (8.95)[491]represents the 6-by-6 layer of neurons that\nshare the ﬁlter listed in Equation (8.94)[490]. It is the output of this layer of neurons that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":545,"page_label":"491","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 491\ngenerates the feature map in Equation (8.94)[490]. The matrix in the middle of Equation\n(8.95)[491]represents the 3-by-3 sub-sampling layer. Each neuron in this layer has a local\nreceptive ﬁeld of dimensions 2-by-2, and there is no overlap between the receptive ﬁelds\nof the neurons in this layer. The matrix on the right of Equation (8.95)[491]represents an\nextension to the architecture that we have discussed. This matrix contains two neurons that\nare fully connected to the sub-sampling layer; that is, each neuron in this layer receives\ninputs from all the neurons in the sub-sampling layer. For ease of reference, we have\nexplicitly labeled a number of neurons in this architecture A, B, C, D, and E\n»\n————————–O O O O O O\nO O O O O O\nA O O O O O\nO B O O O O\nO O O O O O\nO O O O O Oﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nlooooooooooooooomooooooooooooooon\nNeurons that convolve the FilterÑ»\n—–O O O\nC O O\nO O Oﬁ\nﬃﬂ\nloooooomoooooon\nSub-sampling LayerÑ«\nD\nEﬀ\nloomoon\nFully Connected Layer(8.95)\nTo begin the backpropagation process through the convolutional layer, we assume that\ntheδs for the neurons D and E have already been calculated. We can now calculate the δ\nfor Neuron C\nδC“BE\nBaCˆBaC\nBzC\n“ppδDˆwD,Cq`pδEˆwE,Cqqˆ1\n(8.96)\nThis is the standard calculation we would use to calculate the δfor any hidden neuron,\nsee Equation (8.23)[412], with the slight simpliﬁcation that, as mentioned previously, the\ngradient of the activation function Bac{BzC“1. We calculate the δs for the other neurons\nin the sub-sampling layer in a similar fashion.\nOnce we have calculated a δfor each of the neurons in the sub-sampling layer, we can\nthen backpropagate these δs to the layer of neurons that convolve the ﬁlter. Recall that\nthe receptive ﬁelds of the neurons in the sub-sampling layer do not overlap. Consequently,\neach of the neurons in the ﬁrst layer connects only to a single neuron in the sub-sampling\nlayer. For example, neurons A and B are both in the receptive ﬁeld of Neuron C, but neither\nA nor B feeds forward into any of the other neurons in the sub-sampling layer. From the\nfeature map in Equation (8.94)[490]we can ascertain that aA“0andaB“255. In fact,\nNeuron B has the highest activation for any of the neurons in the local receptive ﬁeld of\nNeuron C. This means that δA“0because Neuron A did not have the maximum value in\nthe local receptive ﬁeld of the sub-sampling neuron to which it is connected. The activation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":546,"page_label":"492","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"492 Chapter 8 Deep Learning\nfor Neuron B, however, was the maximum value for the local receptive ﬁeld the neuron is\nin, and so the δvalue for Neuron B is calculated as shown in Equation (8.97)[492]. Notice\nthatwC,B“1. The reason is that the max function does not apply weights to its inputs or,\nto put it another way, all the inputs have a weight of 1. Also, Neuron B is a ReLU and so\nBaB{BzB“1because aBą0\nδB“BE\nBaBˆBaB\nBzB\n“pδDˆwC,BqˆBaB\nBzB\n“pδDˆ1qˆ1\n(8.97)\nTheδs for the other neurons in the ﬁrst layer will either be 0 (if they, like Neuron A,\ndid not produce the maximum value in the local receptive ﬁeld of the sub-sampling neuron\nto which they connect) or can be calculated in a similar way to Neuron B. Once the δfor\neach of the neurons in this layer has been calculated, the weight updates for each weight in\nthe ﬁlter can be calculated by summing weight updates across the neuron in the layer, per\nEquation (8.89)[483].\n8.4.5.5 Handling color images and multiple ﬁlters All the example ﬁlters that we\npreviously presented were two-dimensional. The reason is that the MNIST handwriting\nrecognition case study we are using in this section involves grayscale images. Conse-\nquently we require only a two-dimensional ﬁlter because all pixel information for an im-\nage can be represented in a two-dimensional matrix indexing over the height and width of\nthe grayscale image. However, color images typically encode three types of information\nfor each pixel—the red, green and blue information, with other colors generated via the\ncombination of these three primary colors. Encoding the red, green, and blue (RGB) infor-\nmation is normally done using a separate two-dimensional matrix for each color, with the\ndimensions of each of these two-dimensional matrices equal to the pixel resolution of the\nimage. The term channel is used to describe the number of matrices used to encode the in-\nformation in an image. An RGB image has three channels, and a grayscale image will have\none channel. If we were processing RGB images, then we would use three-dimensional\nﬁlters: height by width by channel.\nTypically, the third dimension of a ﬁlter is referred to as the depth of the ﬁlter, with the\nterm channel speciﬁcally used to describe the depth of the data representation of a color\nimage. The depth of a ﬁlter must match the depth of the input. Consequently, if we are\ndesigning a ﬁlter to process a color image, with three color channels (red, green, and blue),","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":546,"page_label":"492","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"image. The term channel is used to describe the number of matrices used to encode the in-\nformation in an image. An RGB image has three channels, and a grayscale image will have\none channel. If we were processing RGB images, then we would use three-dimensional\nﬁlters: height by width by channel.\nTypically, the third dimension of a ﬁlter is referred to as the depth of the ﬁlter, with the\nterm channel speciﬁcally used to describe the depth of the data representation of a color\nimage. The depth of a ﬁlter must match the depth of the input. Consequently, if we are\ndesigning a ﬁlter to process a color image, with three color channels (red, green, and blue),\nwe can vary the height and width dimensions of the ﬁlter, but the depth dimension must be\n3. For example, we could experiment with using a 2-by-2-by-3 ﬁlter (height by width by","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":547,"page_label":"493","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 493\ndepth). Equation (8.98)[493]illustrates the structure of such a three-dimensional ﬁlter. We\nhave included the bias term w0in this ﬁlter in order to highlight that although the ﬁlter\nnow has three dimensions, there is still only one bias term.\n»\n–w0loomoon\nbias«\nw1w2\nw3w4ﬀ«\nw5w6\nw7w8ﬀ«\nw9w10\nw11w12ﬀﬁ\nﬂ (8.98)\nAdding depth to a ﬁlter does not involve a major change in the way a neuron applies a\nﬁlter to its local receptive ﬁeld. For example, if a neuron is applying a 2-by-2-by-3 ﬁlter,\nthen its local receptive ﬁeld will have the same dimensions (this is why the depth of the\nﬁlter must match the depth of the input). If the input happens to be a color image, then\nwe can distinguish the different layers of depth by the color channels. In this context, the\nneuron will apply a different 2-by-2 ﬁlter to each color channel: one 2-by-2 ﬁlter is applied\nto the red values of the pixels in the receptive ﬁeld; another 2-by-2 ﬁlter is applied to the\ngreen values of the pixels in the receptive ﬁeld; and the third 2-by-2 ﬁlter is applied to the\nblue values of the pixels in the receptive ﬁeld. Then the results of these three calculations\nare summed together along with the bias, to generate a single scalar value that is pushed\nthrough the activation function and then stored in the feature map.44Equation (8.99)[493]\nlists a 2-by-2-by-3 ﬁlter that has been annotated to indicate which parts of the ﬁlter are\napplied to which channel\n»\n———–w0“0.5looomooon\nbias«\nw1“1w2“1\nw3“0w4“0ﬀ\nloooooooooooomoooooooooooon\nRed Channel«\nw5“0w6“1\nw7“0w8“1ﬀ\nloooooooooooomoooooooooooon\nGreen Channel«\nw9“1w10“0\nw11“0w12“1ﬀ\nlooooooooooooomooooooooooooon\nBlue Channelﬁ\nﬃﬃﬃﬂ\n(8.99)\nFor the purposes of illustration, imagine that we are using this ﬁlter to process the fol-\nlowing 3-by-3 RGB image (the pixel values here are not real; they have been selected to\nease the calculations in the example):\n»\n——————–»\n—–1 1 1\n0 0 0\n0 0 0ﬁ\nﬃﬂ\nlooooooomooooooon\nRed Channel»\n—–0 0 2\n0 0 2\n0 0 2ﬁ\nﬃﬂ\nlooooooomooooooon\nGreen Channel»\n—–3 0 0\n0 3 0\n0 0 3ﬁ\nﬃﬂ\nlooooooomooooooon\nBlue Channelﬁ\nﬃﬃﬃﬃﬃﬃﬂ(8.100)\n44. As noted previously, in some texts, such as Goodfellow et al. (2016), the application of the activation function\nis treated as a separate step after the feature map has been generated by the application of the ﬁlter. In these\nscenarios, the raw scalar value would be stored in the feature map.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":548,"page_label":"494","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"494 Chapter 8 Deep Learning\nAssuming a stride length of 1 and no padding on the input, we would require a 2-by-2\nlayer of neurons to convolve the ﬁlter over this image. The top-left neuron in this layer\nwould have a local receptive ﬁeld covering the 2-by-2 square in the top-left of each of the\nchannels. Equation 8.101[494]lists the values from the image that are inside this neuron’s\nlocal receptive ﬁeld\n»\n———–«\n1 1\n0 0ﬀ\nloooomoooon\nRed Channel«\n0 0\n0 0ﬀ\nloooomoooon\nGreen Channel«\n3 0\n0 3ﬀ\nloooomoooon\nBlue Channelﬁ\nﬃﬃﬃﬂ(8.101)\nThe output activation for this neuron would be calculated as follows:\nz“ppw0ˆ1q\n`pw1ˆ1q`p w2ˆ1q`p w3ˆ0q`p w4ˆ0q\n`pw5ˆ0q`p w6ˆ0q`p w7ˆ0q`p w8ˆ0q\n`pw9ˆ3q`p w10ˆ0q`p w11ˆ0q`p w12ˆ3qq\n“0.5`1`1`0`0`0`0`0`0`3`0`0`3\n“8.5\na“recti f ierpzq\n“recti f ierp8.5q\n“8.5 (8.102)\nThe activations for the other three neurons using this ﬁlter would be calculated in a similar\nway, resulting in the following feature map being generated by this 2-by-2 layer of neurons:\n«\n8.5 6.5\n0.5 10.5ﬀ\n(8.103)\nAdding depth to a ﬁlter not only enables a convolutional neural network to process color\nimages that contain multiple channels; it also enables a convolutional network to have a\nsequence of multi-ﬁlter convolutional layers. The reason is that although a convolutional\nlayer that runs multiple ﬁlters in parallel over its input will generate multiple feature maps,\nthe next convolutional layer can treat these multiple feature maps as if they were a single\nmulti-channel input. In this case, the number of channels in the input to this layer would\nbe equal to the number of ﬁlters in the preceding convolutional layer. This is done by\nstacking the feature maps together. In these stacked feature map inputs, each channel then\nencodes the information from a particular ﬁlter spectrum (for example, the information\nfrom the horizontal edge detector ﬁlter spectrum) instead of encoding the information in a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":549,"page_label":"495","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 495\nparticular color spectrum (Charniak, 2019). Consequently, adding depth to ﬁlters not only\nenables convolutional neural networks to process multi-dimensional input; it also enables\nthe networks to apply multiple ﬁlters in parallel to the same input and for later layers in\nthe network to integrate information from across these layers. As a result, convolutional\nnetworks can learn to identify, extract, and use multiple different features in the input.\nThe sequence of convolving a ﬁlter over an input, then applying a non-linear activation\nfunction, and ﬁnally sub-sampling the resulting feature maps is relatively standard in most\nmodern convolutional networks, and often this sequence of operations is taken as deﬁning\na convolutional layer. As we discussed previously, a network may have multiple convo-\nlutional layers in sequence because the outputs from a sub-sampling layer may be passed\nas an input to another ﬁlter convolution, and so this sequence of operations may be re-\npeated multiple times. Padding may be applied to retain dimensionality, and in some cases\nthe non-linearity activation or sub-sampling may be dropped in some convolutional layers.\nGenerally, the later layers of a convolutional network will include one or more fully con-\nnected layers (such as those shown in previous examples) with a softmax output layer if\nthe model is being used for classiﬁcation. Figure 8.35[497]presents a schematic of how two\nconvolutional layers might be sequenced in a simple convolutional neural network. Figure\n8.35[497]also shows that there may be multiple ﬁlters applied in parallel in a convolutional\nlayer. In this network, Convolutional layer 1 includes Filters 1 and 2 and so generates two\nfeature maps. These two feature maps are then stacked together and fed forward as input to\nthe second convolutional layer. Convolutional layer 2 includes Filters 3 and 4, and it does\nnot include a sub-sampling layer.45Filters 3 and 4 will both have a depth of 2 because\nthe input to the second convolutional layer is the two stacked features maps generated by\nConvolutional layer 1. The last two layers of the network are typical of the types of layers\nthat are used near the output of a convolutional network when it is used for image classi-\nﬁcation. The second-to-last layer is a dense fully connected layer (i.e., each neuron in this\nlayer receives the complete feature maps generated by Filters 3 and 4 as input) that feeds","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":549,"page_label":"495","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"feature maps. These two feature maps are then stacked together and fed forward as input to\nthe second convolutional layer. Convolutional layer 2 includes Filters 3 and 4, and it does\nnot include a sub-sampling layer.45Filters 3 and 4 will both have a depth of 2 because\nthe input to the second convolutional layer is the two stacked features maps generated by\nConvolutional layer 1. The last two layers of the network are typical of the types of layers\nthat are used near the output of a convolutional network when it is used for image classi-\nﬁcation. The second-to-last layer is a dense fully connected layer (i.e., each neuron in this\nlayer receives the complete feature maps generated by Filters 3 and 4 as input) that feeds\nforward to the softmax output layer.\nFigure 8.36[498]provides a worked example of data processing and ﬂow through a convo-\nlutional network similar in structure to the architecture blueprint shown in Figure 8.35[497].\nThe main structural difference is that the network in Figure 8.36[498]does not include a soft-\nmax output layer. For ease of presentation we have reduced the input image to be a single\ncolumn of 7 color pixels. We have also simpliﬁed the RGB values to be only 1s or 0s,\nand similarly we have selected values for the ﬁlter weight that, hopefully, make it easier to\nfollow the ﬂow of the data processing (rather than ﬁlter values that encode meaningful fea-\nture detectors). These simpliﬁcations aside, the processing and ﬂow of data through Figure\n8.36[498]is representative of the ﬂow through a multi-layer, multi-ﬁlter convolutional neural\n45. This omission of the sub-sampling layer is done simply to illustrate that it is optional.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":550,"page_label":"496","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"496 Chapter 8 Deep Learning\nnetwork. Starting on the left of the ﬁgure, the column of 7 three-channel (RGB) pixels is\nfed into the ﬁrst convolutional layer. This ﬁrst convolutional layer contains two layers of 6\nneurons. The 6 neurons in the top layer share the weights in Filter 1, and the 6 neurons in\nthe bottom layer share the weights in Filter 2. Both Filters 1 and 2 have a dimensionality\nof 2-by-1-by-3. There are 6 neurons in each of these layers because we are assuming a step\nsize of 1, and so it requires six neurons to convolve one of these ﬁlters over the 7-by-1-by-3\ninput. Feature map 1 contains the 6 activations for the 6 neurons that applied Filter 1 to the\ninput, and Feature map 2 contains the 6 activations for the 6 neurons that applied Filter 2\nto the input. Note that all the neurons in this network are ReLU, so each of these activa-\ntions was calculated in each neuron by passing the result of the weighted sum calculation\nthrough a rectiﬁed linear activation function (similar to the calculation listed in Equation\n(8.102)[494]). The ﬁrst convolutional layer uses a max pooling layer to sub-sample each of\nthe feature maps. The neurons in these max pooling layers have non-overlapping local re-\nceptive ﬁelds, and each local receptive ﬁeld covers two cells in a feature map. The results\nof each of the max pooling layers are then stacked together to create a multi-channel input\nfor the second layer. The dimensions of this input are 3-by-2-by-2, and so all the ﬁlters in\nthe second layer have a depth of 2. The second convolutional layer uses two ﬁlters (Filters\n3 and 4) and so contains two layers of neurons that share weights. The top layer of neurons\nshare the weights in Filter 3, and the bottom layer share the weights in Filter 4. There are\ntwo neurons in each of these layers because we are assuming a step size of 1, and so it\nrequires two neurons to convolve a 2-by-1-by-2 ﬁlter over the 3-by-2-by-2 input. Feature\nmap 3 and Feature map 4 contain the activations of the neurons in each of these two layers.\nThe ﬁnal output of the network is generated by a single ReLU that is fully connected to\nboth Feature map 3 and Feature map 4. The weights used by this ReLU are shown on the\nedges feeding into the unit.\nThe data, ﬁlter weights, and scale of the network shown in Figure 8.36[498]have been sim-\npliﬁed for the purposes of illustration. Consequently, the overall network output of 1.7has","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":550,"page_label":"496","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"share the weights in Filter 3, and the bottom layer share the weights in Filter 4. There are\ntwo neurons in each of these layers because we are assuming a step size of 1, and so it\nrequires two neurons to convolve a 2-by-1-by-2 ﬁlter over the 3-by-2-by-2 input. Feature\nmap 3 and Feature map 4 contain the activations of the neurons in each of these two layers.\nThe ﬁnal output of the network is generated by a single ReLU that is fully connected to\nboth Feature map 3 and Feature map 4. The weights used by this ReLU are shown on the\nedges feeding into the unit.\nThe data, ﬁlter weights, and scale of the network shown in Figure 8.36[498]have been sim-\npliﬁed for the purposes of illustration. Consequently, the overall network output of 1.7has\nno particular meaning. A more realistic example of the complexity and scale of a modern\nconvolutional network is the AlexNet network (Krizhevsky et al., 2012). AlexNet is one\nof the most famous convolutional networks in the history of deep learning. The reason\nfor its fame is that its victory in the ImageNet LargeScale Visual Recognition Challenges\n(ILSVRC) in 2012 was a watershed moment for deep learning that reinvigorated a lot of\ninterest in the ﬁeld of neural networks. The AlexNet architecture included ﬁve convo-\nlutional layers, followed by three fully connected (dense) layers. The ﬁrst convolutional\nlayer had 96 different ﬁlters and used a ReLU non-linearity and max pooling. The second\nconvolutional layer had 256 ﬁlters, and also used a ReLU non-linearity and max pooling.\nThe third, fourth, and ﬁfth convolutional layers had 384, 384, and 256 ﬁlters, respectively,\nand none of these layers included a non-linearity or a max pooling operation. The ﬁnal\nthree dense layers had 4096 neurons each. In total, AlexNet had 60 million weights and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":551,"page_label":"497","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 497\n650,000 neurons. Since 2012, however, several larger convolutional networks have been\ndeveloped, and new and larger models continue to be announced.\nRaw Image Input\nConvolve Filter 1\nNon-Linear Activation\nSub-SamplingConvolve Filter 2\nNon-Linear Activation\nSub-Sampling\nStacked\nFeature Maps\nConvolve Filter 3\nNon-Linear ActivationConvolve Filter 4\nNon-Linear Activation\nDense Layer\n(Fully Connected)\nOutput Layer\n(Softmax)\nFigure 8.35\nSchematic of the typical sequences of layers found in a convolutional neural network.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":552,"page_label":"498","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1\n0\n0\n0\n1\n1\n1R1\n0\n1\n0\n1\n0\n1G0\n1\n1\n1\n0\n0\n1BNeurons Sharing\nFilter 1w0=0.51\n0\n\n1\n0\n\n0\n1\n\n\nFilter 1\nNeurons Sharing\nFilter 2\nw0=0.31\n1\n\n0\n1\n\n1\n0\n\nFilter 23.5\n1.5\n2.5\n0.5\n2.5\n2.5Feature Map 1\n1.3\n2.3\n1.3\n3.3\n2.3\n3.3\nFeature Map 2Max\nPooling\nMax\nPooling3.5\n2.5\n2.5\n2.3\n3.3\n3.3Neurons Sharing\nFilter 3\nNeurons Sharing\nFilter 4w0=0.41\n0\n\n1\n0\n\n\nFilter 3\nw0=0.60\n1\n\n1\n0\n\nFilter 46.2\n6.2Feature Map 3\n5.4\n6.4\nFeature Map 41 w\n0=0.1\nw\n1=0.2w\n2= −\n0.1\nw3=0.3\nw4= −0.1a=1.7\nFigure 8.36\nWorked example illustrating the dataﬂow through a multilayer, multiﬁlter CNN.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":553,"page_label":"499","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 499\n8.4.6 Sequential Models: Recurrent Neural Networks and Long Short-Term Memory\nNetworks\nThe convolutional neural networks discussed in Section 8.4.5[477]are ideally suited to pro-\ncessing data that have a ﬁxed-size grid-like structure and where the basic features have a\nlocal extent, such as images. There are, however, many domains in which the data has\na sequential varying-length structure and in which interactions between data points may\nspan long distances in the sequence. Natural language is an example of this type of data:\nit is naturally sequential, one word follows the other, each sentence may have a different\nnumber of words (varying length), and it contains long-distance dependencies between el-\nements. For example, in English, the subject and verb of a sentence should agree. Compare\nthe sentences “The dog in that house is aggressive” with “The dogs in that house are ag-\ngressive.” In the ﬁrst sentence, the subject of the sentence is singular, dog, and so we use\nthe singular form of the verb is; in the second sentence, the subject is plural, dogs , and so\nwe use the plural form of the verb are.46Processing data of this type requires a model that\nhas the capacity to remember relevant information from earlier in the sequence. Recurrent\nneural networks (RNN) are designed to process this type of data.\nA recurrent neural network works in discrete time. In processing a sequence, the network\ntakes one input from the sequence at each time point. The deﬁning characteristic of a\nrecurrent neural network is that it contains feedback connections, and so, unlike a feed-\nforward network, which is a directed acyclic graph, a recurrent neural network is a directed\ncyclic graph. These cycles, or recurrent links, are the reason these networks are called\nrecurrent networks. That a recurrent network contains cycles means that the output from\na neuron at one time point may be fed back into the same neuron at another time point.\nA consequence of this is that the network has a memory over past activations (and hence\npast inputs that contributed to these activations). This is why these networks are useful\nfor processing sequential data that exhibit long-distance dependencies. There are a variety\nof different recurrent neural network architectures; in this section we introduce two of the\nmost popular: simple recurrent networks (also known as Elman networks (Elman, 1990)),","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":553,"page_label":"499","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"cyclic graph. These cycles, or recurrent links, are the reason these networks are called\nrecurrent networks. That a recurrent network contains cycles means that the output from\na neuron at one time point may be fed back into the same neuron at another time point.\nA consequence of this is that the network has a memory over past activations (and hence\npast inputs that contributed to these activations). This is why these networks are useful\nfor processing sequential data that exhibit long-distance dependencies. There are a variety\nof different recurrent neural network architectures; in this section we introduce two of the\nmost popular: simple recurrent networks (also known as Elman networks (Elman, 1990)),\nand long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997).\n8.4.6.1 Simple recurrent neural networks A simple recurrent neural network archi-\ntecture is a feedforward architecture with one hidden layer that has been extended with a\nmemory buffer that is used to store the activations from the hidden layer for one time-step.\nOn each time-step, the information stored in the memory buffer is concatenated with the\nnext input to each neuron. This process of storing activations in the memory buffer at one\n46. This example is taken from (Mahalunkar and Kelleher, 2018), which reports on experiments that use for-\nmal grammars to understand the representational capacity of recurrent neural networks to model long-distance\ndependencies.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":554,"page_label":"500","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"500 Chapter 8 Deep Learning\ntime-step and reading from the buffer at the next time-step is how the recurrent connections\nare implemented.\nFigure 8.37[502]illustrates the architecture of a simple recurrent network. Note that for\nease of presentation the network schematics in this ﬁgure have been rotated so that the\nforward ﬂow of information through each network is from the input layer at the bottom to\nthe output layer at the top. Furthermore, to align with most explanations of recurrent neural\nnetworks, in the subsequent discussion we adopt the following conventions: xdenotes an\ninput; hdenotes a hidden layer; and ydenotes the output from the network. The goal of the\nnetwork schematic on the left of Figure 8.37[502]is to provide a high-level overview of the\ntemplate structure of a simple recurrent neural network. Consequently, in this schematic\nwe have abstracted away from some of the details of a network architecture: for example,\nthe layers of neurons are represented by rectangles with rounded corners; and the (multiple)\nconnections between neurons in different layers are represented by single arrows labeled\nwith the name of the weight matrix for the weights on those connections. The labels on\nthe rectangles indicate whether the rectangle represents the input layer xt, the hidden layer\nht, the output layer yt, or the activation buffer that stores the activations of the hidden layer\nfrom the previous time-step ht´1.\nAlthough there are four sets of connections in this network (input to hidden, hidden to\noutput, hidden to buffer, and buffer to hidden), there are only three weight matrices in the\nnetwork. There are no weights on the connections between the output of the hidden layer\nand the memory buffer. The reason is that the transfer of hidden neuron activations to the\nmemory buffer is a simple copy operation. That there are no weights on these connections\nis indicated in Figure 8.37[502]by a dashed arrow that represents these connections. There\nare, however, weights on the connections from the memory buffer to each of the neurons.\nThese weights are necessary because the information read from the memory buffer is pro-\ncessed by the hidden neurons in the same way that each of the inputs is. The three weight\nmatrices are\n1.Whxcontaining the weights for the connections between the input layer ( x) and the\nhidden layer ( h);\n2.Wyhcontaining the weights for the connections between the hidden layer ( h) and the\noutput layer ( y); and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":554,"page_label":"500","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"memory buffer is a simple copy operation. That there are no weights on these connections\nis indicated in Figure 8.37[502]by a dashed arrow that represents these connections. There\nare, however, weights on the connections from the memory buffer to each of the neurons.\nThese weights are necessary because the information read from the memory buffer is pro-\ncessed by the hidden neurons in the same way that each of the inputs is. The three weight\nmatrices are\n1.Whxcontaining the weights for the connections between the input layer ( x) and the\nhidden layer ( h);\n2.Wyhcontaining the weights for the connections between the hidden layer ( h) and the\noutput layer ( y); and\n3.Whhcontaining the weights for the connections between the memory buffer and the\nhidden layer. This matrix has the subscript hhbecause in actuality these weights are\napplied to recurrent connections from the hidden layer back to the hidden layer.\nThe network schematic on the right of Figure 8.37[502]illustrates the details of how neu-\nrons within the different layers of a speciﬁc example simple recurrent network are con-\nnected. This example network has two neurons in the input layer (Neurons 1 and 2), three\nneurons in the hidden layer (Neurons 3, 4, and 5), and two neurons in the output layer\n(Neurons 6 and 7). In this example we assume that the hidden neurons and the output","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":555,"page_label":"501","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 501\nlayer are ReLUs. At each time-step a new input vector is presented to the network; this\nﬂows forward to the hidden layer. Each of the hidden layer neurons receives both the\ninput vector and a vector containing the information stored in the memory buffer at the\nsame time. Processing these inputs, the hidden neurons generate activations that are then\npropagated forward to the output layer and also written to the activation memory buffer\n(overwriting whatever information was in the memory buffer). At the next time-step, these\nnew activations are then fed back to the hidden neurons in parallel with the new input.\nThe forward propagation of the activations through a simple recurrent network is deﬁned\nas follows (where the subscript tdenotes the time-step of the system; and there is one input\nper time-step—although this input may be a vector of values—and so the subscript talso\ndeﬁnes the index in the input sequence of the current input):\nht“ϕppWhh¨ht´1q`pWhx¨xtq`w0q (8.104)\nyt“ϕpWyh¨htq (8.105)\nEquation (8.104)[501]deﬁnes how the activations for the hidden layer for input tare gen-\nerated. This is done in the same way as the previous examples (a weighted summation of\ninputs followed by the application of a non-linear activation function). The reason why\nEquation 8.104[501]has a more complicated form then previously is that the neuron has two\nsets of inputs (from the input layer and the activation buffer), and so it has two separate\nweight matrices; also, to be as transparent as possible, we have explicitly represented the\nbias terms for the weights in a separate vector w0. Stepping through Equation 8.104[501]we\nhave the following four operations:\n1.a dot product between the information stored in the memory buffer (the hidden layer\nactivations from the previous time-step encoded as vector ht´1) and the weight matrix\nfor the weights on the connections from the memory buffer to the hidden neurons;\n2.a dot product between the input vector for this time-step xtand the weight matrix for\nthe weights on the connections between the input layer and the hidden layer Whx;\n3.the summation of the results of these two dot products with the bias terms for the\nhidden layer neurons; and ﬁnally,\n4.the weighted sum calculated in Step (3) is passed through a non-linear activation func-\ntionϕ.\nAs noted, the activation vector htof the hidden layer for input tis propagated forward","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":555,"page_label":"501","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.a dot product between the information stored in the memory buffer (the hidden layer\nactivations from the previous time-step encoded as vector ht´1) and the weight matrix\nfor the weights on the connections from the memory buffer to the hidden neurons;\n2.a dot product between the input vector for this time-step xtand the weight matrix for\nthe weights on the connections between the input layer and the hidden layer Whx;\n3.the summation of the results of these two dot products with the bias terms for the\nhidden layer neurons; and ﬁnally,\n4.the weighted sum calculated in Step (3) is passed through a non-linear activation func-\ntionϕ.\nAs noted, the activation vector htof the hidden layer for input tis propagated forward\nto the output layer, and also to the memory buffer where it is stored for one time-step.\nEquation (8.105)[501]speciﬁes how the output activations ytfor input tare then generated:\na weighted sum is calculated via a dot product operation between the weight matrix Wyh\nand the activations vector from the hidden layer ht, and this is passed through a non-linear\nactivation function ϕ.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":556,"page_label":"502","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"502 Chapter 8 Deep Learning\nyt\nht\nxt ht−1Wyh\nWhx Whh6 7\n3 4 5 8 9 10\n1 2Input\nLayerHidden\nLayerOutput\nLayer\nActivation\nBuﬀerWyh\nWhh\nWhxt−1 t−1 t−1\nFigure 8.37\nSchematic of the simple recurrent neural architecture.\n8.4.6.2 Backpropagation through time The fact that a recurrent neural network is\nfundamentally an augmented feedforward network means that training a network using\nbackpropagation is quite similar to training a normal feedforward network. For example, if\nthe network shown on the right of Figure 8.37[502]was applied only to a single input, then we\nwould calculate the δs for the neurons in the output layer.47and then backpropagate these\nδs to the hidden layer neurons. The main novelty in this scenario is that the neurons in the\nhidden layer of a simple recurrent neural network have two weight matrices associated with\nthem: WhxandWhh. However, for the purposes of updating the weights on the connections\ninto a hidden layer neuron, this distinction is irrelevant and the same δis used to update all\nthe weights on the connections into a neuron.\nThings become a little more complex when a recurrent network is applied to a sequence\nof inputs. The variant of backpropagation used to train a recurrent neural network is called\nbackpropagation through time . A recurrent neural network is deliberately designed so\nthat when the network processes an input at time tin a sequence, the output generated\nis dependent not only on input tbut also on the previous inputs in the sequence. This\nmeans that the error for the output at time tis also dependent on the states of the network\nfor all the previous inputs in the sequence ( t´1,t´2, and so on) back to the start of the\nsequence. Backpropagation works by using the chain rule to assign blame to each of\n47. The details of how this is done depend on how the output layer is organized. For networks trained using the\nsum of squared errors loss function, the δfor output neurons is calculated using Equation (8.21)[411]; for neurons\nin a softmax output layer the δis calculated using Equation 8.72[467].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":557,"page_label":"503","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 503\nthe model’s parameters (weights) in proportion to the sensitivity of the network’s error to\nchanges in those weights. This means that we must backpropagate the error at time tto\nall the parameters that contributed to the error. In other words we must backpropagate the\nerror back through the previous states of the network. Hence the name backpropagation\nthrough time : as we backpropagate through the previous states of the network, we are in\neffect going back through time.\nFigure 8.38[504]provides a graphical representation of a recurrent neural network unrolled\nthrough three time-steps.48The subscripts on the x,h, and ylayer labels indicate that these\nlayers have a different state at each time-step. The box labeled h0represents that state\nof the activation memory buffer when the model is initialized. The activation memory\nbuffer is not shown in this ﬁgure because the feedback loop of storing the hidden state\nactivations from one time-step in the buffer and reading from the buffer at the next time-\nstep is represented by the horizontal arrows between each htlayer. Notice that unlike\nthe layers, the weight matrices do not have a time subscript on them. The reason is that\nthe network uses the same weights to process Input 1 as it does to process Input 3. This\nmeans that even though the network is unrolled through three time-steps, it still has only\n3 weight matrices (not 9). In a sense, the unrolled recurrent neural network is similar to\na convolutional neural network in that weights are shared between different neurons: the\nneurons in the hidden layer at time t“1use exactly the same weights as the neurons in the\nhidden layer at time t“2andt“3, and so on. Understanding that we are dealing with\nshared weights is important because the standard process for updating a shared weight is\n(a) to calculate the weight update for the weight at each location in the network where it is\nused; (b) to sum these weight updates together; and (c) ﬁnally to update the weight once\nusing this summed weight update. This is exactly what we do to update the weights in our\nrecurrent neural network.\nTo train a recurrent neural network using backpropagation through time, we ﬁrst do a\nforward pass by presenting each input in the sequence in turn and unrolling the network\nthrough time (as shown in Figure 8.38[504]). The unrolling of the network through time dur-\ning the forward pass means that some neurons49will occur multiple times in the unrolled","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":557,"page_label":"503","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"shared weights is important because the standard process for updating a shared weight is\n(a) to calculate the weight update for the weight at each location in the network where it is\nused; (b) to sum these weight updates together; and (c) ﬁnally to update the weight once\nusing this summed weight update. This is exactly what we do to update the weights in our\nrecurrent neural network.\nTo train a recurrent neural network using backpropagation through time, we ﬁrst do a\nforward pass by presenting each input in the sequence in turn and unrolling the network\nthrough time (as shown in Figure 8.38[504]). The unrolling of the network through time dur-\ning the forward pass means that some neurons49will occur multiple times in the unrolled\nnetwork, and we store the weighted sum zvalue and activation value aof each neuron at\neach time-step. This is the same process that we use for the forward pass of a standard\n48. Figure 8.38[504]is based on a ﬁgure from Kelleher (2016). As with the schematic on the left of Figure 8.37[502],\nthis ﬁgure abstracts over some of the details of the network. For example, the layers of neurons are here repre-\nsented by rectangles with rounded corners with the labels on the rectangles indicating whether the rectangle\nrepresents the input layer xt, the hidden layer ht, the output layer yt, or the hidden layer from the previous time-\nstepht´1; and the (multiple) connections between neurons in different layers are here represented by a single\narrow labeled with the name of the weight matrix for the weights on those connections.\n49. For this discussion we treat all neurons in an unrolled network that use the same set of weights as instances\nof the same neuron; for example, for a neuron in the hidden layer, a different instance of that neuron will occur\nin each time-step.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":558,"page_label":"504","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"504 Chapter 8 Deep Learning\ny1 y2 y3\nh0 h1 h2 h3\nx1 x2 x3Whh Whh Whh\nWhx Whx WhxWyh Wyh Wyh\nFigure 8.38\nA simple RNN model unrolled through time (in this instance, three time-steps).\nfeedforward network (see Figure 8.11[406]); the slightly complicating factor here is that in\nan unrolled recurrent network a neuron may occur multiple times (for example, neurons\nin the hidden layer will occur once for each time-step), and so for each neuron we have a\ntime-stamped sequence of zandavalues.\nOnce the forward pass is complete, we calculate an error term for each of the outputs\nof the network. The total error of the network is then the sum of these individual errors.\nFor the network in Figure 8.38[504]we would calculate three errors: one for y1, one for y2,\nand one for y3. Each of these errors is then backpropagated through the unrolled network.\nFigure 8.39[506]illustrates the path taken by the error gradients as each error for each time-\nstep is backpropagated through the network. Figure 8.39[506]also shows how often each\nweight matrix was used in the generation of each output. For example, Wyhwas involved\nonce in the generation of y3whereas WhhandWhxwere involved three times. The fact\nthat a weight matrix may be involved multiple times in the generation of an output means\nthat backpropagating the error for an output can result in multiple error gradients being\ncalculated for a weight: one error gradient for each time the weight was involved in gen-\nerating the output. For example, if we examine Figure 8.39[506]we see that when Et“3is\nbackpropagated through the network, a single error gradient is calculated for each weight\ninWyhbecause this matrix occurs only once in the unrolled network, but three separate\nerror gradients are calculated for each weight in WhhandWhx. This is why during the\nforward pass we unrolled the network and stored a separate weighted sum zand activation\nafor each occurrence of a neuron in the unrolled network. As we backpropagate through","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":559,"page_label":"505","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 505\neach time-step in the unrolled network, we use the corresponding weighted sum zand acti-\nvation afor a neuron at that time-step to backpropagate the error gradient for that neuron.\nIn backpropagating the error Et“2we will calculate a single error gradient for each weight\ninWyhand two error gradients for each weight in WhhandWhx; when we backpropagated\nthe error for y1we calculated one error gradient for each weight in each of the three weight\nmatrices. Once we have calculated all these error gradients for a sequence, we then update\neach weight by summing all the error gradients for that weight and then using the summed\nerror gradient to update the weight. This means that for the example in Figure 8.39[506]we\nwould sum three error gradients for each weight in Wyhand six error gradients for each\nweight in Whhand each weight in Wyx. As we mentioned previously, this is similar to the\nway the error gradients for shared weights in a convolutional neural network are summed,\nand then the weight is updated once using this summed gradient.\nNote that in Figure 8.38[504]we have shown the network generating an output for each\ninput it receives. However, recurrent networks are quite ﬂexible and can be deployed in\ndifferent scenarios. For example, it may be that the network outputs only a single value\nonce it has processed the whole sequence; this scenario might hold if we were training a\nnetwork to process a sentence and then return a label describing the sentiment— positive\nornegative —expressed in the sentence. In this case we would calculate an error (or loss)\nonly at the output at the end of the sequence. This error is then backpropagated through the\nunrolled network, in the same way that Et“3is backpropagated in Figure 8.39[506]resulting\nin a single error gradient for each weight in Wyhand three separate error gradients for each\nweight in WhhandWhx. Again, the error gradients for a weight are summed and then the\nweight is updated once.\nFor long sequences of inputs it can become cumbersome and also computationally ex-\npensive to keep track of all the error gradients for all the different weights in the unrolled\nnetwork. Consequently, a common practice when training a recurrent neural network to\nprocess a long sequence is to break the sequence up into subsequences. A typical size for\na subsequence might be 20 inputs. The forward and backward pass is then carried out on","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":559,"page_label":"505","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"unrolled network, in the same way that Et“3is backpropagated in Figure 8.39[506]resulting\nin a single error gradient for each weight in Wyhand three separate error gradients for each\nweight in WhhandWhx. Again, the error gradients for a weight are summed and then the\nweight is updated once.\nFor long sequences of inputs it can become cumbersome and also computationally ex-\npensive to keep track of all the error gradients for all the different weights in the unrolled\nnetwork. Consequently, a common practice when training a recurrent neural network to\nprocess a long sequence is to break the sequence up into subsequences. A typical size for\na subsequence might be 20 inputs. The forward and backward pass is then carried out on\neach subsequence in turn: in the forward pass the network is unrolled over a subsequence,\nand in the backward pass the error gradients are backpropagated only through this trun-\ncated unrolled network. Information can ﬂow forward from one subsequence to the next\nby using the hidden state of the network at the end of processing one subsequence to ini-\ntialize the activation buffer at the start of the next subsequence. Algorithm 8[507]provides a\npseudocode deﬁnition of the backpropagation through time algorithm for a single sequence\nof input-output pairs of length n.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":560,"page_label":"506","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"506 Chapter 8 Deep Learning\ny1\nh0 h1\nx1Et=1\nWhh\nWhxWyh\ny2\nh0 h1 h2\nx1 x2Et=2\nWhh Whh\nWhx WhxWyh\ny3\nh0 h1 h2 h3\nx1 x2 x3Et=3\nWhh Whh Whh\nWhx Whx WhxWyh\nFigure 8.39\nAn illustration of the different iterations of backpropagation during backpropagation through time.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":561,"page_label":"507","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 507\nAlgorithm 8 The Backpropagation Through Time Algorithm\nRequire: h0initialized hidden state\nRequire: xa sequence of inputs\nRequire: ya sequence of target outputs\nRequire: nlength of the input sequence\nRequire: Initialized weight matrices (with associated biases)\nRequire: ∆wa data structure to accumulate the summed weight updates for each weight\nacross time-steps\n1:fort“1tondo\n2: Inputs“rx0,..., xts\n3: htmp“h0\n4: fori“0totdo ŹUnroll the network through tsteps\n5: htmp“ForwardPropagate pInputsris,htmpq\n6: end for\n7: ˆyt“OutputLayerphtmpq Ź Generate the output for time-step t\n8: Et“yrts´ˆyt ŹCalculate the error at time-step t\n9: Backpropagate pEtq Ź Backpropagate Etthrough tsteps\n10: For each weight, sum the weight updates across the unrolled network and update\n∆w\n11:end for\n12:Update the network weights using ∆w\nIn Section 8.4.4[472]we introduced dropout as one of the standard methods used to stop\na deep learning network from overﬁtting. Using dropout during the training of a recurrent\nneural network can be problematic because dropping different neurons from the network\nat different time-steps across a sequence can stop the network from propagating important\ninformation forward through the sequence. The standard technique for applying dropout\nto a recurrent neural network during training is known as variational RNN (Goldberg,\n2017). In variational RNN, the dropout mask is selected once per sequence (rather than at\neach input), and so the same neurons are dropped across all time-steps in the sequence; for\nmore details see Gal and Ghahramani (2016).\n8.4.6.3 Long short-term memory networks Recurrent neural networks are particu-\nlarly susceptible to the exploding gradients andvanishing gradients problems we dis-\ncussed in Section 8.4.1[434]and Section 8.4.2[447]. The reason is that during the backward\npass, error gradients will be multiplied by the Whhmatrix multiple times—once for each\ntime-step through which we backpropagate. This repeated multiplication of the error gra-\ndient can rapidly scale up the size of the gradient if a weight in Whhisą1(causing\nour weight updates to become too large and our training to become unstable) or cause the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":562,"page_label":"508","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"508 Chapter 8 Deep Learning\ngradient to vanish if the weight is very small. As a result, although in principle a recurrent\nneural network has the ability to propagate information across the spans of long-distance\ndependencies in input sequences, the vanishing and exploding gradient problems limit the\nability of these networks to learn these dependencies.\nLong short-term memory (LSTM) networks are speciﬁcally designed to improve the\nability of a recurrent network to model dependencies over long distances in a sequence\n(Hochreiter and Schmidhuber, 1997). They achieve this goal by removing the repeated\nmultiplication by the Whhmatrix during backpropagation. Figure 8.40[509]illustrates the\ninternal structure of an LSTM unit. Note that each line in Figure 8.40[509]represents a\nvector of activations, and the `symbol represents an elementwise vector addition and d\nrepresents an elementwise vector product. The fundamental element in an LSTM network\nis the cell. In Figure 8.40[509]the cell is depicted by the line extending from ct´1toctacross\nthe top of the diagram. The cell provides a path that carries the activations of the network\nforward through the time-steps as the network processes a sequence. The activations in the\ncell can take values in the range r´1,`1s. The propagation of activations along the cell is\ncontrolled by three gates : the forget gate , the input gate , and the output gate . The forget\ngate removes information from the cell, the input gate adds information to the cell, and\nthe output gate decides which information should be output by the network at the current\ntime-step.\nThe input to all three of these gates is the vector of hidden state activations propagated\nforward from the previous time-step ht´1concatenated with the current input vector xt. In\nFigure 8.40[509]this concatenation is depicted by the intersection of the ht´1andxtlines\nin the bottom-left corner of the ﬁgure. We use the term hxtto write the vector that is the\nresult of concatenating ht´1with xt. For example, if ht´1“ ra,bsandxt“ rc,ds, then\nhxt“ra,b,c,ds.\nThe ﬂow of information in Figure 8.40[509]is from left to right. The forget gate is the\nleftmost gate in Figure 8.40[509], and it is the ﬁrst gate to process the inputs to the time-\nstep. The forget gate works by passing hxtthrough a layer of neurons that use sigmoid\nactivation functions. This layer of neurons is the same width as the LSTM cell, and so","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":562,"page_label":"508","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"forward from the previous time-step ht´1concatenated with the current input vector xt. In\nFigure 8.40[509]this concatenation is depicted by the intersection of the ht´1andxtlines\nin the bottom-left corner of the ﬁgure. We use the term hxtto write the vector that is the\nresult of concatenating ht´1with xt. For example, if ht´1“ ra,bsandxt“ rc,ds, then\nhxt“ra,b,c,ds.\nThe ﬂow of information in Figure 8.40[509]is from left to right. The forget gate is the\nleftmost gate in Figure 8.40[509], and it is the ﬁrst gate to process the inputs to the time-\nstep. The forget gate works by passing hxtthrough a layer of neurons that use sigmoid\nactivation functions. This layer of neurons is the same width as the LSTM cell, and so\nthere is one activation in the output of the sigmoid layer for each activation in the cell\nstate. Next, an elementwise product of the vector of activations in the cell state ct´1with\nthe vector of activations from the sigmoid layer is performed. This elementwise product\nis depicted in Figure 8.40[509]by thedsymbol at the intersection of the cell state and the\noutput of the forget gate sigmoid layer in the top-left of the ﬁgure. The sigmoid activation\nfunction outputs values in the range 0 to 1 so that the multiplication of the cell state by\nthe sigmoid layer activations has the effect of pushing all the cell state activations that\nhave a corresponding sigmoid activation near 0 to 0 (i.e., these activations are forgotten)\nand all the cell state activations that have a corresponding sigmoid activation near 1 to\nbe maintained (i.e., they are remembered) and propagated forward. Equation (8.106)[509]","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":563,"page_label":"509","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 509\nForget Input Output\n⊙ ⊙T+\nTσ σσ⊙ ct−1 ct\nht−1 ht\nxt output\nFigure 8.40\nA schematic of the internal structure of a long short-term memory unit. In this ﬁgure a σrepresents\na layer of neurons that use a sigmoid activation function, a T represents a layer of neurons that\nuse a tanh activation function, the dsymbols represent elementwise vector multiplication (i.e., the\nHadamard product), and `represents an elementwise vector addition operation. This ﬁgure is based\non Figure 5.4 of Kelleher (2019), which in turn was inspired by an image by Christopher Olah\n(available at: http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\ndeﬁnes the calculation of the forget mask for time-step t, and Equation (8.107)[509]deﬁnes\nthe ﬁltering of the cell state by the forget mask. Note that we use the notation c;to represent\nthe vector of activations in the cell state in the interval between the update by the forget\ngate and the subsequent update by the input gate\nft“ϕsigmoidpWpfq¨hxtq (8.106)\nc;“ct´1dft (8.107)\nwhere hxtis the concatenation of ht´1andxt; and Wpfqis the forget gate matrix of weights.\nFor example, imagine an LSTM unit with the inputs and Wpfqmatrix (the zeros are the bias\nterms) as listed in Equation (8.108)[510]","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":564,"page_label":"510","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"510 Chapter 8 Deep Learning\nct´1“»\n—–1\n1\n1ﬁ\nﬃﬂht´1“«\n1\n1ﬀ\nxt“”\n4ı\nWpfq“»\n—–0 1 1 1\n0´1´1´1\n0 0 0 0ﬁ\nﬃﬂ\n(8.108)\nGiven this context, the processing of the forget gate would be as follows (note that in this\ncalculation hxtis augmented with a bias input 1):\n»\n—–0 1 1 1\n0´1´1´1\n0 0 0 0ﬁ\nﬃﬂ\nlooooooooooooomooooooooooooon\nWpfqˆ»\n———–1\n1\n1\n4ﬁ\nﬃﬃﬃﬂ\nloomoon\nhxt“»\n—–6\n´6\n0ﬁ\nﬃﬂ\nlooomooon\nZpfq\ntÑϕsigmoidÑ»\n—–0.997527377\n0.002472623\n0.500000000ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nft\n»\n—–0.997527377\n0.002472623\n0.500000000ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nftd»\n—–1\n1\n1ﬁ\nﬃﬂ\nloomoon\nct´1“»\n—–0.997527377\n0.002472623\n0.500000000ﬁ\nﬃﬂ\nloooooooooomoooooooooon\nc;\n(8.109)\nNotice that because of the way that the sigmoid activation function operates, large posi-\ntive values in the Zpfq\ntvector (i.e.,`6) are mapped to values close to 1 in ft:0.002472623 ,\nwhereas large negative values in Zpfq\nt(such as´6) are mapped to values near 0; and Zpfq\nt\nvalues near 0 are mapped to values around 0.5. Comparing each term in ct´1with the cor-\nresponding term in c;illustrates how the elementwise product of ftupdates the cell state.\nFor example, the ﬁrst value in ct´1is1, and this is multiplied by an ftvalue near 1 resulting\nin ac;value of 0.997527377 . This c;value is very close to the original ct´1, which shows\nthat the forget gate has largely retained the value in the cell state (or in other words, the cell\nstate has largely remembered this value). However, the second value in ct´1is also 1but\nthis is multiplied by an ftvalue near zero resulting in a c;value of 0.002472623 . In this\ncase, the forget gate has largely erased the original cell value from the cell state (i.e., the cell\nstate has now forgotten the original value). Finally, the third term in ct´1is multiplied by\nanftof0.5, which results in the value of 0.5inc;. This last term illustrates an intermediate\nbehavior in the forget gate in which the original cell state is half-forgotten/half-retained.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":565,"page_label":"511","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 511\nThe fundamental information processing pattern within the forget gate can be character-\nized as passing the concatenated hxtvector through a layer of sigmoid neurons in order\nto generate a vector mask, and then using an elementwise product of this mask with the\ncell state to update (or ﬁlter) the cell’s activations. This information processing pattern\nof creating a vector mask and then using it in an elementwise product to update another\nactivation vector is present in both of the other gates in an LSTM.\nThe task of the input gate is to decide, on the basis of the current input and the propa-\ngated hidden state ( hxt),which elements of the cell state, c;, should be updated and how\nthese elements should be updated. The input gate uses separate paths of information pro-\ncessing to make each of these decisions and then merges the results of these decisions\nusing an elementwise product. The ﬁrst path of information processing in the input gate\ndecides which elements of the cell state should be updated. In this ﬁrst path of processing\nhxtis passed through a layer of sigmoid units that is the same width as the cell state. The\noutput of this layer of neurons is a vector mask, in which each element in the vector is in\nthe ranger0,1s. A value near 1 indicates that the corresponding element of the cell state\nshould be updated, and a value near 0 indicates that the corresponding element of the cell\nstate should be preserved as is. This ﬁrst path of information processing in the input gate\nis similar to the process used in the forget gate; however, in this case the generate vector\nmask is not applied directly to the cell state but rather is used to ﬁlter the vector of output\nactivations generated by the second path of processing in the input gate. In this second path\nof processing hxtis passed through a layer of tanh units. The layer of tanh units is as wide\nas the cell state and so there is one tanh activation per activation in the cell state. The tanh\nactivation function outputs values in the range r´1,`1s. The fact that the tanh function\nhas a range of r´1,`1smeans that activations in the cell state can be both increased and\ndecreased at each time-step. If the LSTM used only sigmoid units, then the activations in\nthe cell would monotonically increase across time and this would have the unwanted con-\nsequence of the cell state tending to saturate with maximum activations as the sequence","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":565,"page_label":"511","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"activations generated by the second path of processing in the input gate. In this second path\nof processing hxtis passed through a layer of tanh units. The layer of tanh units is as wide\nas the cell state and so there is one tanh activation per activation in the cell state. The tanh\nactivation function outputs values in the range r´1,`1s. The fact that the tanh function\nhas a range of r´1,`1smeans that activations in the cell state can be both increased and\ndecreased at each time-step. If the LSTM used only sigmoid units, then the activations in\nthe cell would monotonically increase across time and this would have the unwanted con-\nsequence of the cell state tending to saturate with maximum activations as the sequence\nof time-steps increases, irrespective of the inputs. These two paths of processing are then\nmerged using an elementwise product operation between the tanh activations and the sig-\nmoid activations (i.e., the vector mask). The cell state is then updated by adding the vector\ngenerated by the elementwise produce of the tanh and sigmoid activations. The state of the\ncell after the update gate is the cell state that is propagated forward to the next time-step,\nand so after this update the cell state is now ct(rather than c;). The calculations in the\ninput gate are deﬁned by the following equations:\ni:t“ϕsigmoidpWpi:q¨hxtq (8.110)\ni;t“ϕtanhpWpi;q¨hxtq (8.111)\nit“i:tdi;t (8.112)\nct“c;`it (8.113)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":566,"page_label":"512","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"512 Chapter 8 Deep Learning\nThe fact that the input gate has two paths of processing means that it uses two separate\nweight matrices to process the input, one on each processing path. In these equations\nwe distinguish between these two paths of data processing and the weight matrices using\nthe:and;. The:symbol marks the path of processing that generates the vector mask\nthat controls which activations in the cell state are updated (see Equation (8.110)[511]). The\n;symbol marks the path of processing that generates the candidate cell update, prior to\nthe ﬁltering by the vector mask (see Equation (8.111)[511]). This distinction between the\nprocessing paths is why the weight matrix in Equation (8.110)[511]has a:in the superscript\n(i.e.,Wpi:q) whereas the weight matrix in Equation (8.111)[511]has a;. Equation (8.112)[511]\ndeﬁnes how these the two paths of processing are merged using an elementwise product,\ndenoted by the dterm. Finally, Equation (8.113)[511]speciﬁes how the cell state, post the\nforget gate, is updated by the input gate to generate the cell state for this time-step ct.\nThe task of the output gate is to decide which parts of the ctshould be passed to the\noutput layer of the network and on to the next time-step as the propagated hidden state.\nSimilar to the input gate, the output gate has two paths of processing, one involving a\nsigmoid layer and the other involving a tanh layer. The functioning of the output gate has\na similar interpretation as the input gate: the tanh layer decides what information might be\nrelevant to output from the current cell state, and the sigmoid layer uses the hxtvector to\ndecide which activations are most relevant to output at this time-step. The output gate uses\na three-step process:\n1.the vector of cell activations ctis passed through a layer of tanh units to create a vector\nof candidate output activations;\n2.thehxtvector is passed through a layer of sigmoid units to generate a vector mask that\nin this instance will control which of the activations within the candidate output vector\nwill actually be propagated to the output layer; and\n3.the two vectors of activations generated by Steps 1 and 2 are merged using an ele-\nmentwise product, and the result of this operation is the vector of activations that is\npropagated to the output layer.\nThese processing steps are described mathematically in the following equations:\no:t“ϕsigmoidpWpo:q¨hxtq (8.114)\no;t“ϕtanhpWpo;q¨ctq (8.115)\not“o:tdo;t (8.116)\nht`1“ot (8.117)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":566,"page_label":"512","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"1.the vector of cell activations ctis passed through a layer of tanh units to create a vector\nof candidate output activations;\n2.thehxtvector is passed through a layer of sigmoid units to generate a vector mask that\nin this instance will control which of the activations within the candidate output vector\nwill actually be propagated to the output layer; and\n3.the two vectors of activations generated by Steps 1 and 2 are merged using an ele-\nmentwise product, and the result of this operation is the vector of activations that is\npropagated to the output layer.\nThese processing steps are described mathematically in the following equations:\no:t“ϕsigmoidpWpo:q¨hxtq (8.114)\no;t“ϕtanhpWpo;q¨ctq (8.115)\not“o:tdo;t (8.116)\nht`1“ot (8.117)\nSimilar to the equations for the input gate, we use the symbols :and;to distinguish the\ndifferent paths of information processing in the output gate. Equation (8.114)[512]describes\nhow the output gate uses the hxtvector to generate the vector mask o:tthat ﬁlters what","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":567,"page_label":"513","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 513\nactivations are sent to the output layer and also propagated forward as the next hidden\nstate; Equation (8.115)[512]speciﬁes how the current cell state ctis passed through a layer\nof tanh units to generate a candidate output vector o;t; and Equation (8.116)[512]describes\nhow the actual output vector that is propagated to the output layer otis generated using an\nelementwise product of the vector mask o:tand the candidate output vector o;t. Finally,\nEquation (8.117)[512]ensures that the vector of activations that is propagated to the output\nlayer is the same vector that is propagated to the next time-step as the LSTM hidden state.\nBringing all the LSTM equations together speciﬁes the sequence of calculations that\noccur in the forward pass of an LSTM\nft“ϕsigmoidpWpfq¨hxtq\nc;“ct´1dft\ni:t“ϕsigmoidpWpi:q¨hxtq\ni;t“ϕtanhpWpi;q¨hxtq\nit“i:tdi;t\nct“c;`it\no:t“ϕsigmoidpWpo:q¨hxtq\no;t“ϕtanhpWpo;q¨ctq\not“o:tdo;t\nht`1“ot\nLSTMs have a complex internal structure containing multiple layers of neurons, and\nthey can be considered networks in their own right. However, they can also be used as the\nbuilding block of a recurrent neural network. This is achieved by replacing the hidden layer\nin a recurrent neural network with an LSTM unit. LSTMs have proven very successful at\nprocessing language; for example, they are currently the standard network used for speech\nrecognition on mobile phones.\nFigure 8.41[514]presents a worked example of the forward propagation of activations\nthrough an LSTM unit. In this example, the cell state propagated forward from the previ-\nous time-step is ct´1“r0.3,0.6s; the hidden state propagated forward from the previous\ntime-step is ht“r0.1,0.8s; and the input at the xt“r0.9s. In this instance, the weights for\nthe different matrices in the LSTM unit are randomly sampled from a normal distribution\nµ“0,σ“0.1, and the bias terms have been initialized to 0. In this ﬁgure, the activation\nﬂow through the gates is ordered vertically: the top row of the ﬁgure illustrates the ﬂow\nthrough the forget gate; the next two rows illustrate the ﬂow through the two paths of the\ninput gate; and the bottom two rows illustrate the ﬂow through the two paths of the output\ngate. The vector of activations that the unit would propagate forward as the cell state for","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":568,"page_label":"514","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"514 Chapter 8 Deep Learning\n0.00−0.26+0.12+0.08\n0.00−0.03+0.08+0.01Forget Gate: W(f)\n1.00\n0.10\n0.80\n0.90hxt\n=+0.142\n+0.070Z(f)\nt\nϕσ0.535440468\n0.517492858ft\n⊙+0.30\n+0.60ct−1\n=+0.160632140\n+0.310495715c‡\n0.00−0.09−0.06+0.06\n0.00−0.01−0.02 0.00Input Gate: W(i†)\n=1.00\n0.10\n0.80\n0.90hxt\n=−0.003\n−0.017Z(i†)\nt\nϕσ0.499250001\n0.495750102i†t\n0.00+0.10+0.02−0.08\n0.00−0.14 0.00+0.13Input Gate: W(i‡)\n1.00\n0.10\n0.80\n0.90hxt\n=−0.046\n+0.103Z(i‡)\nt\nϕT−0.045967582\n+0.102637297i‡t⊙−0.022949315\n+0.050882450it++0.137682825\n+0.361378165ct\n0.00−0.16+0.14\n0.00+0.08+0.12Output Gate: W(o‡)\n1.00\n+0.137682825\n+0.361378165+d[0]=1\n=+0.028563691\n+0.054380006Z(o‡)\nt\nϕT+0.028555925\n+0.054326465o‡t\n0.00+0.01+0.12+0.23\n0.00−0.16+0.08+0.04Output Gate: W(o†)\n1.00\n0.10\n0.80\n0.90hxt\n=+0.304\n+0.084Z(o†)\nt\nϕσ+0.575420058\n+0.520987661o†t\n⊙+0.016431652\n+0.028303418ot\nFigure 8.41\nThe ﬂow of activations through a long short-term memory unit during forward propagation when\nct´1“r0.3,0.6s,ht“r0.1,0.8s, and xt“r0.9s.\nthe next time ( ct) step is shown on the top right of the ﬁgure, and the vector of activations\notthat the unit would propagate both to the output layer for this time-step and on to the\nnext time-step as the hidden state is shown in the bottom-right of the ﬁgure.\nIt is standard that hidden state hand the cell state chave the same size. The larger\nthe size of the hidden state, the larger the representational capacity of the LSTM. In this\nexample, the hidden state has a size of 2; however, typically the size of the hidden state is\nmuch larger. For example, it is not uncommon to see sizes of 128, 256, 512, and 1,024.\nThe dimensions of the weight matrices are determined by the size of the hidden state. If\nHis the size of the hidden state, and the input xhas a dimension of n(in this example\nn“1), then the dimensions of the weight matrices in the forget gate and input gate ( Wpfq,\nWpi:q, and Wpi;q) and the sigmoid layer in the output gate ( Wpo:q), including bias terms is:\nHˆp1`n`Hq. This ensures that there are Hneurons in the layers in these gates. As\na result, irrespective of the width of the input x, the activation vectors for these layers will","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":569,"page_label":"515","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 515\nbe the same size as the cell state c, and so the dimensions of the vectors in the elementwise\noperations that update the cell state will match. Figure 8.41[514]shows the dimensions of\nthe different weight matrices in the unit where H“2andn“1. In this unit each of\nthese weight matrices has dimensions 2ˆ4. The weight matrix for the tanh layer in the\noutput gate ( Wpo;q, including bias terms, has dimensions Hˆp1`Hq. This ensures that\nthe output vector from this layer o;tis the same size as Hand so is the correct dimensions\nfor the elementwise product with the o:tvector, and also that the vector resulting from this\noperation will have a size of H. This is important because it ensures that ht´1andhtare\nthe same size.\n8.4.6.4 Backpropagating through an LSTM cell Figure 8.42[516]illustrates the ﬂow of\nerror gradients through an LSTM during backpropagation. In this ﬁgure the error gradients\nﬂow from right to left. The backpropagation process within an LSTM begins with three\nvectors of error gradients\n1.BEt{Bot: the rate of change of the error of the network at time-step twith respect to\nchanges in the activation vector otthat was propagated to the output layer during the\nforward pass;\n2.BEt`1{Bht: the rate of change of the error of the network at time-step t`1with respect\nto changes in the activation vector htthat was propagated forward to the next time-step\nduring the forward pass; and\n3.BEt`1{Bct: the rate of change of the error of the network at time-step t`1with respect\nto changes in the cell state ctthat was propagated forward to the next time-step during\nthe forward pass.\nDuring the forward pass of an LSTM unit, there are three operations on activation vectors\nthat are novel with respect to the other network architectures we have examined: forks in\ncomputational ﬂow, elementwise products of vectors, and elementwise addition of vectors.\nTo be able to backpropagate error gradients through an LSTM, we need to understand how\nthe novel operations in the forward pass are handled in the backward pass.\nA fork in computational ﬂow during forward propagation results in the same activation\nvector ﬂowing in two directions, with each path generating error gradients that must be\nmerged in the backpropagation stage. This occurs twice in an LSTM: (1) the cell state\nctvector ﬂows forward into the next time-step and is also passed through a tanh layer\nas part of the output layer; and (2) the vector of output activations otﬂows forward to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":569,"page_label":"515","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"computational ﬂow, elementwise products of vectors, and elementwise addition of vectors.\nTo be able to backpropagate error gradients through an LSTM, we need to understand how\nthe novel operations in the forward pass are handled in the backward pass.\nA fork in computational ﬂow during forward propagation results in the same activation\nvector ﬂowing in two directions, with each path generating error gradients that must be\nmerged in the backpropagation stage. This occurs twice in an LSTM: (1) the cell state\nctvector ﬂows forward into the next time-step and is also passed through a tanh layer\nas part of the output layer; and (2) the vector of output activations otﬂows forward to\nboth the output layer and the next time-step (as the propagated hidden state ht). Forks in\nthe forward computation ﬂow are handled in backpropagation by summing the derivatives\nthat are ﬂowing back along each of the fork branches. For example, in Figure 8.42[516]the\nvector of error gradients BE{Botis calculated using an elementwise addition of BEt{Bot\nandBEt`1{Bht","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":570,"page_label":"516","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"516 Chapter 8 Deep Learning\nForget Input Output\n⊙ ⊙T+\nTσ σσ⊙∂Et\n∂ct−1∂Et+1\n∂ct\n∂Et\n∂ht−1∂Et+1\n∂ht\n∂Et\n∂xt∂Et\n∂ot∂E\n∂ot∂E\n∂o‡\n∂E\n∂o†∂E\n∂ct∂E\n∂ct\n∂E\n∂ct\n∂E\n∂i‡∂E\n∂i†∂E\n∂fδo‡\nδo† δi‡ δi† δf\n∂Et\n∂hx\nFigure 8.42\nThe ﬂow of error gradients through a long short-term memory unit during backpropagation.\nBE\nBot“BEt\nBot`BEt`1\nBht(8.118)\nAll three gates in an LSTM involve an elementwise product of two activation vectors\n(see Equations (8.107)[509], (8.112)[511], and (8.116)[512]). For each of these cases, what we\nwish to calculate during backpropagation is the rate of change of the error with respect to\nchanges in each of the inputs to the product. This means that we will generate two sets\nof error gradients when we backpropagate through an elementwise vector product, one for\neach branch of data that ﬂows into the product. In backpropagation, the error gradient with\nrespect to an input to a product of two terms is the error gradient with respect to the result\nof the product multiplied by the other input to the product. In dealing with an elementwise\nvector product, this is applied to each of the separate products in turn. So, for example,\nbackpropagating the error gradient BE{Botthrough the elementwise vector product in the\noutput gate (see Equation (8.116)[512]) produces the following two error gradient vectors:\nBE\nBo;“BE\nBotdo: (8.119)\nBE\nBo:“BE\nBotdo; (8.120)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":571,"page_label":"517","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 517\nwheredis an elementwise vector product, o:was calculated in the forward pass using\nEquation (8.114)[512]; and o;was calculated in the forward pass using Equation (8.115)[512].\nFigure 8.42[516]indicates the ﬂow of error gradients back through the elementwise product\nin the output gate by labeling each path emerging from the operation with its respective\nerror gradient vector: BE{Bo;andBE{Bo;, respectively. We backpropagate through the\nother elementwise products that occur in the forward pass within the LSTM using a similar\nstrategy: in order to calculate the backpropagated error gradient for one input to a product,\nwe multiply the error gradient for the result of the product by the other input to the product.\nThe forward computational ﬂow through the LSTM input gates include an elementwise\naddition of two activation vectors. This operation is used in the input gate to update the\ncell state (see Equation (8.113)[511]). The termBE{Bctdescribes the vector of error gradients\nthat are backpropagated through this operation. The same error gradient vector ﬂows back\nalong both paths that feed into the elementwise summation in the forward path. The is why\nin Figure 8.42[516]both of the paths emerging from the elementwise summation are labeled\nwith the same term as the input arrow: BE{Bct.\nHaving covered how we backpropagate through branches, elementwise products, and\nelementwise additions in the forward pass, we are now able to backpropagate the error\ngradients through the LSTM. Starting on the right, we calculate BE{Botper Equation\n(8.118)[516]and then backpropagate this through the output gate elementwise product to\ncalculateBE{Bo;(Equation (8.119)[516]) andBE{Bo;(Equation (8.120)[516]). Next we cal-\nculateBE{Bct. To calculate these error gradients we must backpropagate BE{Bo;through\na tanh layer and then merge the resulting gradients with the error gradients from the next\ntime-step with respect to the current cell state. This is done as follows:\nδo;“BE\nBo;dBo;t\nBct;“BE\nBo;d`\n1´tanh2pc;tq˘\nloooooooomoooooooon\nDerivate of tanh, i.e.:Ba\nBz(8.121)\nBE\nBct“δo;`BEt`1\nBct(8.122)\nIn Equation (8.121)[517]we calculate a vector containing δvalues for the neurons in the\noutput gate tanh layer by backpropagating the error gradients BE{Bo;back through that\ntanh activation function. Then in Equation (8.122)[517]we merge the two sets of gradients\nthat arise from the fork in the forward propagation of ctto both the next time-step (as the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":571,"page_label":"517","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"culateBE{Bct. To calculate these error gradients we must backpropagate BE{Bo;through\na tanh layer and then merge the resulting gradients with the error gradients from the next\ntime-step with respect to the current cell state. This is done as follows:\nδo;“BE\nBo;dBo;t\nBct;“BE\nBo;d`\n1´tanh2pc;tq˘\nloooooooomoooooooon\nDerivate of tanh, i.e.:Ba\nBz(8.121)\nBE\nBct“δo;`BEt`1\nBct(8.122)\nIn Equation (8.121)[517]we calculate a vector containing δvalues for the neurons in the\noutput gate tanh layer by backpropagating the error gradients BE{Bo;back through that\ntanh activation function. Then in Equation (8.122)[517]we merge the two sets of gradients\nthat arise from the fork in the forward propagation of ctto both the next time-step (as the\ncell state) and the output layer for this time-step.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":572,"page_label":"518","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"518 Chapter 8 Deep Learning\nFocusing on the input gate, the gradients for each of the inputs to the elementwise product\ncan now be calculated as follows:\nBE\nBi;“BE\nBctdi: (8.123)\nBE\nBi:“BE\nBctdi; (8.124)\nSimilarly, the gradients for the inputs to the elementwise product in the forget gate are\ncalculated\nBE\nBct´1“BE\nBctdft (8.125)\nBE\nBf“BE\nBctdct´1 (8.126)\nEach of the terms BE{Bf(Equation (8.126)[518]),BE{Bi:(Equation (8.124)[518]),BE{Bi;\n(Equation (8.123)[518]), andBE{Bo:(Equation (8.120)[516]) describes a vector of error gradi-\nents with respect to the output activations of the neurons in one of the sigmoid and tanh\nlayers in the LSTM. In order to calculate the δs for the neurons in each of these layers, we\nmust multiply these error gradients by the derivative of the activation function for the layer\nwith respect to the inputs to the activation function (i.e., Ba{Bz). Recall that we are dealing\nwith vectors of gradients, and so for each layer we will do an elementwise product with\na vector of activation function derivatives; also, the derivatives of the activation functions\nare dependent on whether the neurons in the layer are using a sigmoid or tanh function.\nδf“BE\nBfdBft\nBzf“BE\nBfdpftdp1´ftqq (8.127)\nδi:“BE\nBi:dBi:t\nBzi:“BE\nBi:dpi:tdp1´i:tqq (8.128)\nδi;“BE\nBi;dBi;t\nBzi;“BE\nBi;d`\n1´tanh2pi;tq˘\n(8.129)\nδo:“BE\nBo:dBo:t\nBzo:“BE\nBo:dpo:tdp1´o:tqq (8.130)\nOnce we have calculated the δs for the neurons in these layers, we have two further sets of\ncalculations to perform in order to complete the backpropagation through the LSTM. First,\nwe need to calculate the updates for each weight in each of these layers; and second, we\nneed to calculate the vector of gradients BEt{Bht´1that are backpropagated to the previous\ntime-step.\nAssuming that we are training the LSTM network using backpropagation through\ntime , then for each time-step, we calculate the update for each weight by multiplying","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":573,"page_label":"519","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.4 Extensions and Variations 519\ntheδfor the neuron that uses the weight by the input value that weight was applied to,\nand then we sum these weight updates across the time-steps. The weight is then updated\nusing the summed weight update. For example, the update for the weights in Wpfqwould\nbe calculated as follows:\n∆Wpfq“δf¨hx⊺(8.131)\nNote that here we use the notation hx⊺to write the transpose of the vector hx; we use\ntranspose of the vector to ensure that this matrix and vector product is deﬁned. We can\nillustrate the process of calculating a weight update for the weights in Wpfqby using the\ncontext provided by the forward pass example shown in Figure 8.41[514]. For the purposes\nof this example, we assume that the following error gradients are already calculated:\nBEt`1\nBct“«\n0.35\n0.50ﬀ\nBEt`1\nBht“«\n0.75\n0.25ﬀ\nBEt\nBot“«\n0.15\n0.60ﬀ\nGiven these terms we calculate the update for the weights in Wpfqusing the sequence of\ncalculations listed in Equations (8.132)[519]to (8.138)[520]\nBE\nBot“«\n0.75\n0.25ﬀ\nloomoon\nBEt`1{Bht`«\n0.15\n0.60ﬀ\nloomoon\nBEt{Bot“«\n0.9\n0.85ﬀ\n(8.132)\nBE\nBo;“«\n0.9\n0.85ﬀ\nloomoon\nBE{Botd«\n0.575420058\n0.52098661ﬀ\nloooooooomoooooooon\no:“«\n0.517878052\n0.442839512ﬀ\n(8.133)\nδo;“«\n0.517878052\n0.442839512ﬀ\nloooooooomoooooooon\nBE{Bo;d«\n0.999184559\n0.997048635ﬀ\nloooooooomoooooooon\n1´tanhpctq“«\n0.517455753\n0.441532531ﬀ\n(8.134)\nBE\nBct“«\n0.517455753\n0.441532531ﬀ\nloooooooomoooooooon\nδo;`«\n0.35\n0.50ﬀ\nloomoon\nBEt`1{Bct“«\n0.867455753\n0.941532531ﬀ\n(8.135)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":574,"page_label":"520","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"520 Chapter 8 Deep Learning\nBE\nBf“«\n0.867455753\n0.941532531ﬀ\nloooooooomoooooooon\nBE{Bctd«\n0.3\n0.6ﬀ\nloomoon\nct´1“«\n0.260236726\n0.564919518ﬀ\n(8.136)\nδf“«\n0.260236726\n0.564919518ﬀ\nloooooooomoooooooon\nBE{Bfd«\n0.248743973\n0.249694ﬀ\nloooooooomoooooooon\nftdp1´ftq“«\n0.064732317\n0.141057014ﬀ\n(8.137)\n∆Wpfq“«\n0.064732317\n0.141057014ﬀ\nloooooooomoooooooon\nδf¨”\n1.00 0.10 0.80 0.90ı\nlooooooooooooooomooooooooooooooon\nhx⊺\n“«\n0.064732317 0 .006473232 0 .051785854 0 .058259085\n0.141057014 0 .014105701 0 .112845611 0 .126951313ﬀ\n(8.138)\nTo calculate the vector of gradients BEt{Bht´1, we ﬁrst need to calculate the error gra-\ndient with respect to changes in hxtfor each of the four layers of neurons that receive hxt\nas input: the forget gate sigmoid layer, the input gate sigmoid layer, the input gate tanh\nlayer, and the output gate sigmoid layer. For each of these layers we calculate the error\ngradient with respect to the layer’s input ( hxt) by multiplying the δvalues for the neurons\nin the layer by the weights the neuron uses. This is similar to the way we calculate the\nerror gradients with respect to the weights of a neuron, the difference here being that we\nwant the error gradient with respect to the input hxt, and so in this case we multiply the δ\nby the weights rather than the inputs. Once the vector of error gradients with respect to hxt\nhas been calculated for each of the layers, BE{Bhxis calculated using an elementwise sum\nof these vectors of gradients\nBE\nBhx“´\nWpfq⊺¨δf¯\n`´\nWpi:q⊺¨δi:¯\n`´\nWpi;q⊺¨δi;¯\n`´\nWpo:q⊺¨δo:¯\n(8.139)\nThe vector of gradients BE{Bhxincludes error gradients for each element of ht´1and\nxt. This is because ht´1andxtwere concatenated together in the forward pass. Hence the\nvector of gradients BEt{Bht´1is extracted from BE{Bhxby splitting the vector at the index\nthat joined ht´1andxtwhen they were concatenated.\nLSTMs were speciﬁcally designed to be able to model long-distance dependencies in\na sequence. The fundamental element in the design of these networks was to remove\nthe repeated multiplication of error gradients by weight matrices during backpropagation\nthrough time. Equation (8.139)[520]may appear to undermine this design goal, because\nthe calculation of the backpropagated error gradients in the vector BEt{Bht´1involves a\nmultiplication by weight matrices. However, this is not the only vector of error gradients\nthat are backpropagated; recall, the BEt{Bct´1is also backpropagated to the previous time-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":575,"page_label":"521","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.5 Summary 521\nstep. Equation (8.140)[521]expands out the full set of operations and terms that are used\nin the calculation of BEt{Bct´1. The point to note in this equation is that the calculation\ndoes not involve a weight term. Consequently, error gradients with respect to the cell state,\nc, are not repeatedly multiplied by a (weight) term that is shared across time-steps, and\nso these gradients are relatively stable as they ﬂow backward along the cell state path. It\nis primarily this path of error gradients ﬂow (rather than the backpropagated hidden state\nBEt{Bht´1gradients) that enable LSTMs to learn long-distance dependencies\nBE\nBct´1“ftdBE\nBct\n“ftdˆBEt`1\nBct`δo;˙\n“ftdˆBEt`1\nBct`ˆ`\n1´tanh2pc;tq˘\ndBE\nBo;˙˙\n“ftdˆBEt`1\nBct`ˆ`\n1´tanh2pc;tq˘\ndˆ\no:dBE\nBot˙˙˙\n“ftdˆBEt`1\nBct`ˆ`\n1´tanh2pc;tq˘\ndˆ\no:dˆBEt`1\nBht`BEt\nBot˙˙˙˙\n(8.140)\n8.5 Summary\nThe distinctive characteristic of deep learning models is that they are deeper and larger\nthan older neural networks. The depth of a network is measured by the number of layers\nof neurons within the network. The reason why depth is important is that by introducing\nmultiple layers into a network, the network is able to learn a hierarchy of features with later\nlayers in the network building on the features that previous layers have learned to extract\nfrom the raw input. A consequence of this layer-wise transformation of the input data into\nnew representations is that as the network becomes deeper, the ability of the network to\nrepresent more complex relationships between descriptive and target features is increased.\nThe standard algorithm for training a deep neural network combines the backpropagation\nalgorithm (which solves the blame assignment problem) with stochastic gradient descent\n(which we use to update the weights in the network after blame has been assigned to each\nneuron). A difﬁculty with training a deep network with backpropagation and gradient de-\nscent is unstable gradients (either vanishing or exploding gradients). Two key innovations\nthat helped with the unstable gradient problem were the adoption of the rectiﬁed linear\nactivation function and the development of new techniques for weight initialization.\nThe ability of deep neural networks to learn and represent complex mappings from inputs\nto outputs is why these networks have been so successful at so many complex tasks. How-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":576,"page_label":"522","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"522 Chapter 8 Deep Learning\never, this representational capacity also means that deep networks are likely to suffer from\noverﬁtting. Dropout is a very simple and effective method that helps to stop overﬁtting.\nFinally, the fact that deep learning models are combined by connecting artiﬁcial neurons\ntogether means that we can tailor the structure of a network toward the characteristics of\nthe data on which we are planning to run the network. Convolutional neural networks are\na network architecture that have been very successful at image processing tasks. Convo-\nlutional neural networks combine local receptive ﬁelds, weight sharing, and sub-sampling.\nThey generally work very well on data that has a grid-like structure and in which the\nlow-level features in the data have a local extent. By contrast, recurrent neural networks\nare designed to process sequential data that may have long distances between dependent\nfeatures in the input sequence. These recurrent models use a feedback loop to maintain,\nevolve, and propagate a representation of the pertinent information within the sequence\nhistory. Recurrent networks also use weight sharing with the same weight matrices being\nreused at each time-step in a sequence. The fact that the same weights are applied on the\nfeedback loop within a recurrent network means that these networks are very susceptible\nto unstable gradients, as the repeated multiplication by the same weight of the gradient as\nit is propagated back through the unrolled network can cause the gradient to explode or\nvanish. Long short-term memory (LSTM) networks have addressed instability in the prop-\nagation of weights through a sequential model by removing this repeated multiplication by\nthe shared weights.\nIn terms of using deep learning for a machine learning task, the ﬁrst question to ask\nis whether deep learning is really necessary or appropriate. Deep learning works best in\ncomplex domains with lots of features and large datasets. Further, deep learning generally\nrequires relatively powerful computer hardware. If the domain is not complex, or if the\navailable data is small, or if the available hardware is not powerful enough for deep learn-\ning, then it is likely that you will have more success by exploring whether an alternative\nmachine learning model can be used to develop a viable solution for the task. Once the\ndecision to use deep learning has been made, the next decision is to choose the network","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":576,"page_label":"522","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the shared weights.\nIn terms of using deep learning for a machine learning task, the ﬁrst question to ask\nis whether deep learning is really necessary or appropriate. Deep learning works best in\ncomplex domains with lots of features and large datasets. Further, deep learning generally\nrequires relatively powerful computer hardware. If the domain is not complex, or if the\navailable data is small, or if the available hardware is not powerful enough for deep learn-\ning, then it is likely that you will have more success by exploring whether an alternative\nmachine learning model can be used to develop a viable solution for the task. Once the\ndecision to use deep learning has been made, the next decision is to choose the network\narchitecture to use. We have covered three architectures in this chapter: feedforward net-\nworks are appropriate if the descriptive features are in a ﬁxed-sized vector; if the input\ndata has a grid-like structure, then consider a convolutional neural network; and if the in-\nput may be a variable length sequence, then a recurrent neural network or LSTM network\nwill be most appropriate. As mentioned in the chapter, ReLUs (or variants) are now the\ndefault activation function to use, although if a network architecture speciﬁes a particular\nactivation function, then follow the speciﬁcation (e.g., LSTMs specify the use of sigmoid\nand tanh activations). With regard to the network training, take care with the weight ini-\ntialization process, and it is generally a good idea to include dropout. Finally, as discussed\nin the Information Based Learning chapter, in which we discussed tree pruning to avoid","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":577,"page_label":"523","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.6 Further Reading 523\noverﬁtting, using the performance of the model on the validation set to decide when to stop\nthe training and weight updates is nearly always the case in training a neural network.\n8.6 Further Reading\nDeep learning has grown out of research on neural networks. Consequently, many books\non neural networks cover the fundamentals of deep learning. Neural network texts that we\nwould recommend include Bishop (1996) and Reed and Marks (1999). Kelleher (2019)\nprovides an overview of the history of deep learning that highlights the major developments\nin the ﬁeld and the trends that are driving its adoption across a broad range of domains, and\nit discusses likely future developments in the ﬁeld. The explanation of the backpropagation\nalgorithm presented in this chapter is based on Chapter 6 of Kelleher (2019). Goodfellow\net al. (2016) provides a comprehensive overview of the ﬁeld. From a programming per-\nspective, we would recommend Charniak (2019) as an introduction to programming deep\nlearning models using TensorFlow, and Trask (2019) provides a great introduction to im-\nplementing neural networks using Numpy. Finally, we also recommend Goldberg (2017)\nfor an accessible introduction to the use of deep learning for natural language processing\nand Reagen et al. (2017) for a computer architecture perspective on deep learning.\nDeep learning is currently attracting a huge amount of attention within the machine learn-\ning community. Consequently, there is rapid progress within the ﬁeld, with new techniques\nand network architectures being published every month. As a result, it is not possible in\na single chapter to cover all possible deep learning topics. Some of the topics that, space\npermitting, we would have included are batch normalization (Ioffe and Szegedy, 2015),\nwhich can speed up the training of very deep networks; algorithms that adaptively adjust\nthe learning rate parameter such as Adam (Kingma and Ba, 2014); and more recent neu-\nral network architectures such as Generative Adversarial Networks (Goodfellow et al.,\n2014), and attention-based architectures, such as the Transformer (Vaswani et al., 2017).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":578,"page_label":"524","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"524 Chapter 8 Deep Learning\n8.7 Exercises\n1.The following image shows an artiﬁcial neuron that takes three inputs\nΣϕ= 1d0\nd1\nd2\nd3w0= 0.20w1=−0.10\nw2= 0.15\nw3= 0.05Mw(d)\n(a)Calculate the weighted sum for this neuron for the input vector\nd“r0.2,0.5,0.7s\n(b)What would be the output from this neuron if the activation function ϕis a thresh-\nold activation with θ“1?\n(c)What would be the output from this neuron if the activation function ϕis the\nlogistic function?\n(d)What would be the output from this neuron if the activation function ϕis the\nrectiﬁed linear function?\n2.The following image shows an artiﬁcial neural network with two sensing neurons\n(Neurons 1 and 2) and 3 processing neurons (Neurons 3, 4, and 5)\n1 3\n5\n2 4w3,1=0.1\nw\n4,1=0\n.2\nw3,2=0.5\nw4,2=0.4w5,3=0.3\nw5,4=0.7w3,0=0.6\nw4,0=0.8w5,0=0.9","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":579,"page_label":"525","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.7 Exercises 525\n(a)Assuming that the processing neurons in this network use a logistic activation\nfunction, what would be the output of Neuron 5 if the network received the input\nvector: Neuron 1 = 0.7 and Neuron 2 = 0.3?\n(b)Assuming that the processing neurons in this network use a ReLU activation func-\ntion, what would be the output of Neuron 5 if the network received the input vec-\ntor: Neuron 1 = 0.7 and Neuron 2 = 0.3?\n(c)The following image provides a template diagram for the sequence of matrix op-\nerations that our neural network would use to process the input vector Neuron 1 =\n0.7 and Neuron 2 = 0.3. Assuming that the processing neurons in the network use\na ReLU activation function, ﬁll in the diagram with the appropriate weights, bias\nterms, weighted sum values, and activations.\n0.60.10.5\n0.80.20.4\nHidden Layer\nWeight Matrix1.0\n0.7\n0.3Activations\nInput Layer\n=0.82\n1.06\nz(1)ϕ0.82\n1.06Activations\nHidden Layer\n0.90.30.7Output Layer\nWeight Matrix1.00\n0.82\n1.06\nActivations\nHidden Layer= 1.888z(2)\nϕ 1.888\nOutput+d[0]=1\n3.The following image shows an artiﬁcial network with two layers of linear neurons\n(i.e., neurons that have no activation function and whose output is simply the result of\nthe weighted sum of their inputs). Furthermore, these neurons have no bias terms.\n1 3\n5\n2 4w3,1=0.3\nw\n4,1=0\n.1\nw3,2=0.2\nw4,2=0.6w5,3=0.4\nw5,4=0.8\n(a)Calculate the output for Neuron 5 for the input vector: Neuron 1 = 0.9, Neuron 2\n= 0.5.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":580,"page_label":"526","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"526 Chapter 8 Deep Learning\n(b)Calculate the weight matrix for the single layer network that would implement the\nequivalent mapping that this two-layer network implements.\n(c)Show that the single-layer network using the weight matrix you calculated in Part\n2 generates the same output as the network for the input vector: Neuron 1 = 0.9,\nNeuron 2 = 0.5.\n4.The following image illustrates the topology of a simple feedforward neural network\nthat has a single sensing neuron (Neuron 1), a single hidden processing neuron (Neu-\nron 2), and a single processing output neuron (Neuron 3).\n1 2 3w2,1=0.2 w3,2=0.3w2,0=0.1 w3,0=0.1\n(a)Assuming that the processing neurons use logistic activation functions, that the\ninput to the network is Neuron 1 = 0.2 and that the desired output for this input is\n0.7:\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the δvalues for each of the neurons in the network (i.e., δ3,δ2).\niii.Using the δvalues you calculated above, calculate the sensitivity of the error\nof the network to changes in each of the weights of the network (i.e., BE{Bw3,2,\nBE{Bw3,0,BE{Bw2,1,BE{Bw2,0).\niv.Assuming a learning rate of α“0.1, calculate the updated values for each of\nthe weights in the network ( w3,2,w3,0,,w2,1,w2,0,) after the processing of this\nsingle training example.\nv.Calculate the reduction in the error of the network for this example using the\nnew weights, compared with using the original weights.\n(b)Assuming that the processing neurons are ReLU s, that the input to the network is\nNeuron 1 = 0.2, and that the desired output for this input is 0.7:\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the δvalues for each of the neurons in the network (i.e., δ3,δ2).\niii.Using theδvalues you have calculated in the preceding, calculate the sensitivity\nof the error of the network to changes in each of the weights of the network (i.e.,\nBE{Bw3,2,BE{Bw3,0,BE{Bw2,1,BE{Bw2,0).\niv.Assuming a learning rate of α“0.1, calculate the updated values for each\nof the weights in the network ( w3,2,w3,0,,w2,1,w2,0) after the processing of this\nsingle training example.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":581,"page_label":"527","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.7 Exercises 527\nv.Calculate the reduction in the error for this example using the new weights for\nthe network, compared with using the original weights.\n5.The following image illustrates the topology of a simple feedforward neural network\nthat has a single sensing neuron (Neuron 1), three hidden processing neuron (Neurons\n2, 3, and 4), and a single processing output neuron (Neuron 5).\n3\n1 2 5\n4w2,1=0.3w3,2=0.2\nw4,2=0.5w5,3=0.4\nw5,4=0.6w2,0=0.1w3,0=0.1\nw4,0=0.1w5,0=0.1\n(a)Assuming that the processing neurons use logistic activation functions, that the\ninput to the network is Neuron 1 = 0.5 and that the desired output for this input is\n0.9:\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the δvalues for each of the processing neurons in the network (i.e.,\nδ5,δ4,δ3,δ2).\niii.Using the δvalues you have calculated, calculate the sensitivity of the error of\nthe network to changes in each of the weights of the network (i.e., BE{Bw5,4,\nBE{Bw5,3,BE{Bw5,0,BE{Bw4,2,BE{Bw4,0,BE{Bw3,2,BE{Bw3,0,BE{Bw2,1,\nBE{Bw2,0).\niv.Assuming a learning rate of α“0.1, calculate the updated values for each of\nthe weights in the network ( w5,4,w5,3,w5,0,w4,2,w4,0,w3,2,w3,0,,w2,1,w2,0,)\nafter the processing of this single training example.\nv.Calculate the reduction in the error of the network for this example using the\nnew weights, compared with using the original weights.\n(b)Assuming that the processing neurons are ReLU s, that the input to the network is\nNeuron 1 = 0.5 and that the desired output for this input is 0.9\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the δvalues for each of the processing neurons in the network (i.e.,\nδ5,δ4,δ3,δ2).\niii.Using the δvalues you have calculated, calculate the sensitivity of the error of\nthe network to changes in each of the weights of the network (i.e., BE{Bw5,4,\nBE{Bw5,3,BE{Bw5,0,BE{Bw4,2,BE{Bw4,0,BE{Bw3,2,BE{Bw3,0,BE{Bw2,1,BE{Bw2,0).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":582,"page_label":"528","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"528 Chapter 8 Deep Learning\niv.Assuming a learning rate of α“0.1, calculate the updated values for each of\nthe weights in the network ( w5,4,w5,3,w5,0,w4,2,w4,0,w3,2,w3,0,,w2,1,w2,0,)\nafter the processing of this single training example.\nv.Calculate the reduction in the error of the network for this example using the\nnew weights, compared with using the original weights.\n6.The following image illustrates the topology of a feedforward neural network that has\ntwo sensing neurons (Neurons 1 and 2), two hidden processing neuron (Neurons 3,\nand 4), and two processing output neurons (Neurons 5 and 6).\n1 3 5\n2 4 6w3,1=−0.1\nw4,1=0.2\nw3,2=−0.2\nw4,2=0.3w5,3=−0.1\nw6,3=0.1\nw5,4=0.2\nw6,4=−0.2w3,0=0.1\nw4,0=0.1w5,0=0.1\nw6,0=0.1\n(a)Assuming that the processing neurons use logistic activation functions, that the\ninput to the network is Neuron 1 = 0.3 and Neuron 2 = 0.6, and that the desired\noutput for this input is Neuron 5 = 0.7 and Neuron 6 = 0.4:\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the sum of squared errors for this network for this example.\niii.Calculate the δvalues for each of the processing neurons in the network (i.e.,\nδ6,δ5,δ4,δ3).\niv.Using the δvalues you calculated above, calculate the sensitivity of the error\nof the network to changes in each of the weights of the network (i.e., BE{Bw6,4,\nBE{Bw6,3,BE{Bw6,0,BE{Bw5,4,BE{Bw5,3,BE{Bw5,0,BE{Bw4,2,BE{Bw4,1,\nBE{Bw4,0,BE{Bw3,2,BE{Bw3,1,BE{Bw3,0).\nv.Assuming a learning rate of α“0.1, calculate the updated values for each of\nthe weights in the network ( w6,4,w6,3,w6,0,w5,4,w5,3,w5,0,w4,2,w4,1,w4,0,w3,2,\nw3,1,w3,0,) after the processing of this single training example.\nvi.Calculate the reduction in the sum of squared error of the network for this\nexample using the new weights, compared with using the original weights.\n(b)Assuming that the processing neurons use a rectiﬁer activation functions, that the\ninput to the network is Neuron 1 = 0.3 and Neuron 2 = 0.6 and that the desired\noutput for this input is Neuron 5 = 0.7 and Neuron 6 = 0.4:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":583,"page_label":"529","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.7 Exercises 529\ni.Calculate the output generated by the network in response to this input.\nii.Calculate the sum of squared errors for this network on this example.\niii.Calculate the δvalues for each of the processing neurons in the network (i.e.,\nδ6,δ5,δ4,δ3).\niv.Using the δvalues you calculated above, calculate the sensitivity of the error\nof the network to changes in each of the weights of the network (i.e., BE{Bw6,4,\nBE{Bw6,3,BE{Bw6,0,BE{Bw5,4,BE{Bw5,3,BE{Bw5,0,BE{Bw4,2,BE{Bw4,1,\nBE{Bw4,0,BE{Bw3,2,BE{Bw3,1,BE{Bw3,0).\nv.Assuming a learning rate of α“0.1, calculate the updated values for each of\nthe weights in the network ( w6,4,w6,3,w6,0,w5,4,w5,3,w5,0,w4,2,w4,1,w4,0,w3,2,\nw3,1,w3,0,) after the processing of this single training example.\nvi.Calculate the reduction in the sum of squared error of the network for this\nexample using the new weights, compared with using the original weights.\n7.Assuming a fully connected feedforward network where all the neurons uses a linear\nactivation function (i.e., ai“zi) and with the following topology:\n(a)100 neurons in the input layer\n(b)5 hidden layers with 2,000 neurons in each layer\n(c)10 neurons in the output layer\nIf all the inputs to the network have been standardized to have a mean value of 0 and\na standard deviation of 1, and the initial weights for the network are sampled from a\nnormal distribution with mean 0.0 and standard deviation of σ“0.01,then:\n(a)Calculate the variance of the zvalues across for the neurons in the ﬁrst hidden\nlayer in the ﬁrst iteration of training.\n(b)Calculate the variance of the zvalues across for the neurons in the last hidden layer\nin the ﬁrst iteration of training.\nAssuming that the variance of the δs for the output layer is equal to 1:\n(a)Calculate the variance of the δs across for the neurons in the last hidden layer in\nthe ﬁrst iteration of training.\n(b)Calculate the variance of the δs values across for the neurons in the ﬁrst hidden\nlayer in the ﬁrst iteration of training.\n(c)Is the training dynamic of this network stable, or is it suffering from vanishing or\nexploding gradients?\n˚8.Assuming a network architecture that has four neurons in a softmax output layer. If\nthe one-hot encoding of the target for the current training example is t“ r0,0,1,0s\nand the logits for the four neurons in the softmax output layer for this example are\nr0,0.5,0.25,0.75s, then what is the δvalue for each of the four neurons?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":584,"page_label":"530","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"530 Chapter 8 Deep Learning\n˚9.Assuming a feedforward neural network that has 4 neurons in hidden layer kand that\nwe are training this network using inverted dropout withρ“0.5. If the activations\nfor the neurons in layer kare as follows: r0.2,0,4,0,3,0.1sand the DropMask for\nlayer kisr1,0,1,0s, calculate the activation vector that is actually propagated to layer\nk`1after inverted dropout has been applied.\n˚10.The ﬁgure below illustrates a layer of a convolutional neural network that is process-\ning a one-dimensional input. For ease of reference each of the neurons in the network\nhas been labeled: 1,2,3,4,5,6,7. The architecture of the network consists of ReLUs\nthat share a ﬁlter (Neurons 1,2,3,4), followed by a sub-sampling layer containing two\nmax pooling units (Neurons 5,6), and then a fully connected layer containing a single\nReLU (Neuron 7). The ReLU in the ﬁrst layer has a 3-by-1 receptive ﬁeld, and there is\na stride of 1 used in this layer. The max pooling units have a receptive ﬁeld of 2-by-1,\nand there is no overlap between the receptive ﬁelds of the max pooling units.\n1\n1\n1\n0\n0\n0Input\nNeurons Sharing\nFilter\n1\n2\n3\n4w0=0.75w1= +1\nw2= +1\nw3= −1\n\n\nFilter\n?\n?\n?\n?Feature MapMax\nPooling\n5\n6?\n?71 w70=0.1\nw75=0.2\nw76=0.5?\n(a)What value will this network output?\n(b)Assuming the target output for this input is 1, calculate the δfor each neuron in\nthe network.\n(c)Calculate the weight update for each weight in the ﬁlter: w0,w1,w2,w3.\n˚11.Assume a simple recurrent neural network architecture matching the one shown in the\ndetailed schematic on the left of Figure 8.37[502]. This network has two input neurons,\nthree ReLUs in the hidden layer, and two ReLUs in the output layer. Also, all the\nbias terms in the network are equal to 0.1, and the weight matrices of the network\n(excluding bias terms) are listed in Equation (8.141)[530].\n»\n—–´0.07 0.05\n´0.04 0.1\n´0.05´0.05ﬁ\nﬃﬂ\nlooooooooooomooooooooooon\nWhx»\n—–´0.22´0.1 0.05\n´0.04´0.09´0.06\n´0.09 0´0.08ﬁ\nﬃﬂ\nlooooooooooooooooomooooooooooooooooon\nWhh«\n0.06´0.01´0.18\n´0.06 0.13 0.14ﬀ\nlooooooooooooooooomooooooooooooooooon\nWyh\n(8.141)\n(a)Ifxt“r1,0.5sandht´1“r0.05,0.2,0.15s, calculate the value of yt.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":585,"page_label":"531","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8.7 Exercises 531\n(b)Assuming that the target output for time tt“r0,1s, calculate the δvalue for each\nneuron in the network.\n˚12.Assuming that the LSTM cell state propagated forward from the last time-step is\nct´1“«\n`0.5\n´0.5ﬀ\n(a)What will be the value of ctif\nft“«\n`1.00\n`1.00ﬀ\ni:t“«\n`1.00\n`1.00ﬀ\ni;t“«\n´0.25\n`0.25ﬀ\n(b)What will be the value of ctif\nft“«\n`1.00\n`1.00ﬀ\ni:t“«\n0.00\n0.00ﬀ\ni;t“«\n´0.25\n`0.25ﬀ\n(c)What will be the value of ctif\nft“«\n0.00\n0.00ﬀ\ni:t“«\n`1.00\n`1.00ﬀ\ni;t“«\n´0.25\n`0.25ﬀ\n(d)What will be the value of ctif\nft“«\n0.00\n0.00ﬀ\ni:t“«\n0.00\n0.00ﬀ\ni;t“«\n´0.25\n`0.25ﬀ\n˚13.Equations (8.132)[519]to (8.138)[520]step through the calculation of the weight update\nforWpfqin the context of the forward pass presented in Figure 8.41[514]and under the\nassumption that the following error gradients are already calculated:\nBEt`1\nBct“«\n0.35\n0.50ﬀ\nBEt`1\nBht“«\n0.75\n0.25ﬀ\nBEt\nBot“«\n0.15\n0.60ﬀ\n(a)Given this context, calculateEt\nBct´1.\n(b)Given this context, calculate the vector of error gradients with respect to the input\nhxtfor the forget gate sigmoid layer.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":587,"page_label":"533","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9 Evaluation\n“Essentially, all models are wrong, but some are useful. ”\n—George E. P. Box\nIn this chapter we describe how to evaluate machine learning models built for predictive\ndata analytics tasks. We start by outlining the fundamental goals of evaluation before de-\nscribing the standard approach of measuring the misclassiﬁcation rate for a model on a\nhold-out test set . We then present extensions and variations of this approach that describe\ndifferent performance measures for models predicting categorical, continuous, and multi-\nnomial targets; how to design effective evaluation experiments; and how to continually\nmeasure the performance of models after deployment.\n9.1 Big Idea\nThe year is 1904, and you are a research assistant working in the lab of physicist Professor\nRen´e Blondlot, at the University of Nancy, in France. Until recently, spirits have been very\nhigh in the lab due to the discovery earlier the previous year of a new form of electromag-\nnetic radiation called N rays (Blondlot, 1903). The existence of N rays was ﬁrst hinted\nat in an experiment performed at the lab that was designed to answer questions about the\nexact nature of the recently discovered X-ray radiation. This experiment showed behavior\nuncharacteristic of X-rays, which Professor Blondlot interpreted to mean that another, dif-\nferent type of electromagnetic radiation must exist. This new type of radiation was named\nthe N ray (after the University of Nancy), and experiments were designed to demonstrate\nits existence. These experiments were performed in Nancy and conﬁrmed, to the satis-\nfaction of everyone involved, that N rays did indeed exist. This new discovery caused\nripples of great excitement in the international physics community and greatly enhanced\nthe reputations of the lab at Nancy and Professor Blondlot.\nDoubt has begun to surround the phenomenon of N rays, however, as a number of in-\nternational physicists have not been able to reproduce the results of the experiments that\ndemonstrate their existence. You are currently preparing for a visit by the American physi-\ncist Professor Robert W. Wood to whom Professor Blondlot has agreed to demonstrate the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":588,"page_label":"534","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"534 Chapter 9 Evaluation\nexperiments that show the effects of N rays. In one of these experiments, the brightening\nof a small spark that occurs when an object that supposedly emits N rays is brought close\nto it is measured. In a second experiment, the refractive effect of passing N rays through a\nprism (something that does not happen to X-rays) is demonstrated. You carefully prepare\nthe apparatus for these experiments, and on the 21stof September 1904, you spend three\nhours assisting Professor Blondlot in demonstrating them to Professor Wood.\nJust over a week later, you are very disappointed to read an article published by Wood\nin the journal Nature (Wood, 1904) that completely refutes the existence of N rays. He\ndismisses the experimental setup for the experiments you demonstrated as entirely inap-\npropriate. Even more dramatically, he reports that he actually interfered with the second\nexperiment by removing the prism from the apparatus during the demonstration (because\nthe experiment was completed in darkness, Wood was able to do this without anybody\nnoticing), which made no difference to the results that you measured and reported, so it\ncompletely undermines them. Within a few years of the publication of this article, the\nconsensus within the physics research community is that N rays do not exist.\nThe story of Professor Blondlot and N rays is true,1and it is one of the most famous\nexamples in all of science of how badly designed experiments can lead to completely in-\nappropriate conclusions. There was no fraud involved in the work at the Nancy lab. The\nexperiments designed to show the existence of N rays simply relied too much on subjective\nmeasurements (the changes in the brightness of the spark was measured by simple human\nobservation) and did not account for all the reasons other than the presence of N rays that\ncould have created the phenomena observed.\nThebig idea to take from this example to predictive data analytics projects is that when\nwe evaluate predictive models, we must ensure that the evaluation experiments are de-\nsigned so that they give an accurate estimate of how the models will perform when de-\nployed. The most important part of the design of an evaluation experiment for a predictive\nmodel is ensuring that the data used to evaluate the model is not the same as the data used\nto train the model.\n9.2 Fundamentals\nOver the last four chapters, we have discussed a range of approaches to building machine","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":588,"page_label":"534","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"observation) and did not account for all the reasons other than the presence of N rays that\ncould have created the phenomena observed.\nThebig idea to take from this example to predictive data analytics projects is that when\nwe evaluate predictive models, we must ensure that the evaluation experiments are de-\nsigned so that they give an accurate estimate of how the models will perform when de-\nployed. The most important part of the design of an evaluation experiment for a predictive\nmodel is ensuring that the data used to evaluate the model is not the same as the data used\nto train the model.\n9.2 Fundamentals\nOver the last four chapters, we have discussed a range of approaches to building machine\nlearning models that make various kinds of predictions. The question that we must answer\nin the Evaluation phase of the CRISP-DM process (recall Section 1.6[15]) isCan the model\ngenerated do the job that it has been built for? The purpose of evaluation is threefold:\n‚to determine which of the models that we have built for a particular task is most suited\nto that task\n1. Detailed descriptions of the story of Professor Blondlot and N rays are available in Klotz (1980) and Ashmore\n(1993).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":589,"page_label":"535","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set 535\n‚to estimate how the model will perform when deployed\n‚to convince the business for whom a model is being developed that the model will meet\ntheir needs\nThe ﬁrst two items in this list focus on measuring and comparing the performance of\na group of models to determine which model best performs the prediction task that the\nmodels have been built to address. The deﬁnition of best is important here. No model will\never be perfect, so some fraction of the predictions made by every model will be incorrect.\nThere are, though, a range of ways in which models can be incorrect, and different analytics\nprojects will emphasize some over others. For example, in a medical diagnosis scenario, we\nwould require that a prediction model be very accurate in its diagnoses and, in particular,\nnever incorrectly predict that a sick patient is healthy, as that patient will then leave the\nhealth-care system and could subsequently develop serious complications. On the other\nhand, a model built to predict which customers would be most likely to respond to an\nonline ad only needs to do a slightly better than random job of selecting those customers\nthat will actually respond in order to make a proﬁt for the company. To address these\ndifferent project requirements, there is a spectrum of different approaches to measuring\nthe performance of a model, and it is important to align the correct approach with a given\nmodeling task. The bulk of this chapter discusses these different approaches and the kinds\nof modeling tasks that they best suit.\nAs indicated by the third item in the list above, there is more to evaluation than measuring\nmodel performance. For a model to be successfully deployed, we must consider issues\nlike how quickly the model makes predictions, how easy it easy for human analysts to\nunderstand the predictions made by a model, and how easy it is to retrain a model should\nit go stale over time. We return to these issues in the ﬁnal section of this chapter.\n9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set\nThe basic process for evaluating the effectiveness of predictive models is simple. We take a\ndataset for which we know the predictions that we expect the model to make, referred to as\natest set , present the instances in this dataset to a trained model, and record the predictions\nthat the model makes. These predictions can then be compared to the predictions we","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":589,"page_label":"535","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"like how quickly the model makes predictions, how easy it easy for human analysts to\nunderstand the predictions made by a model, and how easy it is to retrain a model should\nit go stale over time. We return to these issues in the ﬁnal section of this chapter.\n9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set\nThe basic process for evaluating the effectiveness of predictive models is simple. We take a\ndataset for which we know the predictions that we expect the model to make, referred to as\natest set , present the instances in this dataset to a trained model, and record the predictions\nthat the model makes. These predictions can then be compared to the predictions we\nexpected the model to make. Based on this comparison, a performance measure can be\nused to capture, numerically, how well the predictions made by the model match those that\nwere expected.\nThere are different ways in which a test set can be constructed from a dataset, but the\nsimplest is to use what is referred to as a hold-out test set . A hold-out test set is created by\nrandomly sampling a portion of the data in the ABT we created in the Data Preparation\nphase. This random sample is never used in the training process but reserved until after\nthe model has been trained, when we would like to evaluate its performance. Figure 9.1[536]\nillustrates this process.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":590,"page_label":"536","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"536 Chapter 9 Evaluation\nFigure 9.1\nThe process of building and evaluating a model using a hold-out test set.\nUsing a hold-out test set avoids the issue of peeking , which arises when the performance\nof a model is evaluated on the same data used to train it; because the data was used in\nthe training process, the model has already seen this data, so it is probable that it will\nperform very well when evaluated on this data. An extreme case of this problem happens\nwhen knearest neighbor models are used. If the model is asked to make a prediction about\nan instance that was used to train it, the model will ﬁnd as the nearest neighbor, for this\ninstance, the instance itself. Therefore, if the entire training set is presented to this model,\nits performance will appear to be perfect. Using a hold-out test set avoids this problem,\nbecause none of the instances in the test set will have been used in the training process.\nConsequently, the performance of the model on the test set is a better measure of how\nthe model is likely to perform when actually deployed and shows how well the model can\ngeneralize beyond the instances used to train it. The most important rule in evaluating\nmodels is not to use the same data sample both to evaluate the performance of a predictive\nmodel and to train it.\nFor a ﬁrst example of how to evaluate the performance of a predictive model, let us\nassume that we are dealing with an email classiﬁcation problem with a binary categorical\ntarget feature distinguishing between spam andham emails. When making predictions\nabout categorical targets, we need performance measures that capture how often the model\nmakes correct predictions and the severity of the mistakes that the model makes when it is\nincorrect. Table 9.1[537]shows the expected targets for a small sample test set and a set of\npredictions made by a model trained for this prediction problem (the FP and FN comments\nin the Outcome column will be explained shortly).\nThe simplest performance measure we can use to assess how well this model has per-\nformed for this problem is the misclassiﬁcation rate . The misclassiﬁcation rate is the\nnumber of incorrect predictions made by the model divided by the total number of predic-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":591,"page_label":"537","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set 537\ntions made:\nmisclassi f ication rate “number incorrect predictions\ntotal predictions(9.1)\nIn the example in Table 9.1[537],20predictions are made in total, and out of these, 5\nare incorrect (instances d1,d2,d12,d17, and d20). Therefore, the misclassiﬁcation rate is\ncalculated as5\n20“0.25, which is usually expressed as a percentage: 25%. This tells us\nthat the model is incorrect about a quarter of the time. Misclassiﬁcation rate can assume\nvalues in the range r0,1s, and lower values indicate better performance.\nTable 9.1\nA sample test set with model predictions.\nID Target Pred. Outcome\n1 spam ham FN\n2 spam ham FN\n3 ham ham TN\n4 spam spam TP\n5 ham ham TN\n6 spam spam TP\n7 ham ham TN\n8 spam spam TP\n9 spam spam TP\n10 spam spam TPID Target Pred. Outcome\n11 ham ham TN\n12 spam ham FN\n13 ham ham TN\n14 ham ham TN\n15 ham ham TN\n16 ham ham TN\n17 ham spam FP\n18 spam spam TP\n19 ham ham TN\n20 ham spam FP\nTheconfusion matrix is a very useful analysis tool to capture what has happened in an\nevaluation test in a little more detail and is the basis for calculating many other performance\nmeasures. The confusion matrix calculates the frequencies of each possible outcome of the\npredictions made by a model for a test dataset in order to show, in detail, how the model is\nperforming. For a prediction problem with a binary target feature (where, by convention,\nwe refer to the two levels as positive andnegative ), there are just four outcomes when the\nmodel makes a prediction:\n‚True Positive (TP): an instance in the test set that had a positive target feature value and\nthat was predicted to have a positive target feature value\n‚True Negative (TN): an instance in the test set that had a negative target feature value\nand that was predicted to have a negative target feature value\n‚False Positive (FP): an instance in the test set that had a negative target feature value but\nthat was predicted to have a positive target feature value\n‚False Negative (FN): an instance in the test set that had a positive target feature value\nbut that was predicted to have a negative target feature value","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":592,"page_label":"538","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"538 Chapter 9 Evaluation\nTable 9.2\nThe structure of a confusion matrix.\nPrediction\npositive negative\nTargetpositive T P FN\nnegative FP T N\nThe Outcome column of Table 9.1[537]shows the category to which each prediction made\nby the model belongs. One thing worth keeping in mind is that there are two ways in which\nthe prediction made by a model can be correct—true positive or true negative—and two\nways in which the prediction made by a model can be incorrect—false positive or false\nnegative.2The confusion matrix allows us to capture these different types of correct and\nincorrect predictions made by the model.\nEach cell in a confusion matrix represents one of these outcomes (TP, TN, FP, FN) and\ncounts the number of times this outcome occurred when the test dataset was presented to\nthe model. The structure of a confusion matrix for a simple prediction task with two target\nlevels is shown in Table 9.2[538]. The columns in the table are labeled Prediction-positive\nandPrediction-negative and represent the predictions generated by a model, which is either\npositive or negative. The rows in the table are labeled Target-positive andTarget-negative\nand represent the target feature values that were expected. The top left cell in the confusion\nmatrix, labeled TP, shows the number of instances in a test set that have a positive target\nfeature value that were also predicted by the model to have a positive target feature value.\nSimilarly, the bottom left cell in the matrix, labeled FP, shows the number of instances in a\ntest set that have a negative target feature value that were in fact predicted by the model to\nhave a positive target feature value. TN and FN are deﬁned similarly.\nAt a glance, the confusion matrix can show us that a model is performing well if the\nnumbers on its diagonal, representing the true positives and true negatives, are high. Look-\ning at the other cells within the confusion matrix can show us what kind of mistakes the\nmodel is making. Table 9.3[539]shows the confusion matrix for the set of predictions shown\nin Table 9.1[537](in this case, we refer to the spam target level as the positive level and ham\nas the negative level).3\n2. Statisticians will often refer to false positives as type I errors and false negatives as type II errors . Similarly,\nfalse positives are often also referred to as false alarms , true positives as hits, and false negatives as misses .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":592,"page_label":"538","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"At a glance, the confusion matrix can show us that a model is performing well if the\nnumbers on its diagonal, representing the true positives and true negatives, are high. Look-\ning at the other cells within the confusion matrix can show us what kind of mistakes the\nmodel is making. Table 9.3[539]shows the confusion matrix for the set of predictions shown\nin Table 9.1[537](in this case, we refer to the spam target level as the positive level and ham\nas the negative level).3\n2. Statisticians will often refer to false positives as type I errors and false negatives as type II errors . Similarly,\nfalse positives are often also referred to as false alarms , true positives as hits, and false negatives as misses .\n3. Typically, the level that is of most interest is referred to as the positive level . In email classiﬁcation, identi-\nfying spam emails is the most important issue, so the spam level is referred to as the positive level. Similarly,\nin fraud detection, the fraud events would most likely be the positive level; in credit scoring, the default events\nwould most likely be the positive level; and in disease diagnosis, a conﬁrmation that a patient has the disease\nwould most likely be the positive level. The choice, however, is arbitrary.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":593,"page_label":"539","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.3 Standard Approach: Misclassiﬁcation Rate on a Hold-Out Test Set 539\nTable 9.3\nA confusion matrix for the set of predictions shown in Table 9.1[537].\nPrediction\nspam ham\nTargetspam 6 3\nham 2 9\nIt is clear from the values along the diagonal, the true positives and true negatives, that\nthe model is doing a reasonably good job of making accurate predictions. We can actually\ncalculate the misclassiﬁcation rate directly from the confusion matrix as follows:\nmisclassi f ication rate “pFP`FNq\npT P`T N`FP`FNq(9.2)\nIn the email classiﬁcation example we have been following, the misclassiﬁcation rate\nwould be\nmisclassi f ication rate “p2`3q\np6`9`2`3q“0.25\nFor completeness, it is worth noting that classiﬁcation accuracy is the opposite of mis-\nclassiﬁcation rate. Again, using the confusion matrix, classiﬁcation accuracy is deﬁned\nas\nclassi f ication accuracy “pT P`T Nq\npT P`T N`FP`FNq(9.3)\nClassiﬁcation accuracy can assume values in the range r0,1s, and higher values indicate\nbetter performance. For the email classiﬁcation task, classiﬁcation accuracy would be\nclassi f ication accuracy “p6`9q\np6`9`2`3q“0.75\nWe can also use the confusion matrix to begin to investigate the kinds of mistakes that the\nprediction model is making. For example, the model makes a prediction of ham incorrectly\n3times out of the 9times that the correct prediction should be spam (33.333% of the time),\nwhile it makes a prediction of spam incorrectly just 2times out the 11times that the correct\nprediction should be ham (18.182% of the time). This suggests that when the model makes\nmistakes, it more commonly incorrectly predicts the spam level than the ham level. This\nkind of insight that we can get from the confusion matrix can help in trying to improve a\nmodel, as it can suggest to us where we should focus our work.\nThis section has presented a basic approach to evaluating prediction models. The most\nimportant things to take away from this example are:\n1.It is crucial to use data to evaluate a model that has not been used to train the model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":594,"page_label":"540","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"540 Chapter 9 Evaluation\n2.The overall performance of a model can be captured in a single performance measure,\nfor example, misclassiﬁcation rate.\n3.To fully understand how a model is performing, it can often be useful to look beyond\na single performance measure.\nThere are, however, a range of variations to this standard approach to evaluating predic-\ntion model performance, and the remainder of this chapter covers the most important of\nthese.\n9.4 Extensions and Variations\nWhen evaluating the performance of prediction models, there is always a tension between\nthe need to fully understand the performance of the model and the need to reduce model\nperformance to a single measure that can be used to rank models by performance. For\nexample, a set of confusion matrices gives a detailed description of how a set of models\ntrained on a categorical prediction problem performed and can be used for a detailed com-\nparison of performances. Confusion matrices, however, cannot be ordered and so cannot\nbe used to rank the performance of the set of models. To perform this ranking, we need to\nreduce the information contained in the confusion matrix to a single measure, for example,\nmisclassiﬁcation rate. Any information reduction process will result in some information\nloss, and a single measure of model performance will be designed to emphasize some as-\npects of model performance and de-emphasize, or lose, others. For this reason, there are\na variety of different performance measures and no single approach that is appropriate for\nall scenarios.\nThis section covers a selection of the most important performance measures. We also de-\nscribe different experimental designs for evaluating prediction models and ways to monitor\nmodel performance after a model has been deployed.\n9.4.1 Designing Evaluation Experiments\nAs well as being required to select appropriate performance measures to use when evalu-\nating trained models, we also need to ensure that we are using the appropriate evaluation\nexperiment design. The goal here is to ensure that we calculate the best estimate of how a\nprediction model will perform when actually deployed in the wild . In this section we will\ndescribe the most important evaluation experiment designs and indicate when each is most\nappropriate.\n9.4.1.1 Hold-out sampling In Section 9.3[535]we used a hold-out test set to evaluate\nthe performance of a model. The important characteristic of this test set was that it was not","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":594,"page_label":"540","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"model performance after a model has been deployed.\n9.4.1 Designing Evaluation Experiments\nAs well as being required to select appropriate performance measures to use when evalu-\nating trained models, we also need to ensure that we are using the appropriate evaluation\nexperiment design. The goal here is to ensure that we calculate the best estimate of how a\nprediction model will perform when actually deployed in the wild . In this section we will\ndescribe the most important evaluation experiment designs and indicate when each is most\nappropriate.\n9.4.1.1 Hold-out sampling In Section 9.3[535]we used a hold-out test set to evaluate\nthe performance of a model. The important characteristic of this test set was that it was not\nused in the process of training the model. Therefore, the performance measured on this test\nset should be a good indicator of how well the model will perform on future unseen data\nfor which it will be used to make predictions after deployment. This is an example of using","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":595,"page_label":"541","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 541\nasampling method to evaluate the performance of a model, as we take distinct, random,\nnon-overlapping samples from a larger dataset and use these for training and testing a\nprediction model. When we use a hold-out test set, we take one sample from the overall\ndataset to use to train a model and another separate sample to test the model.\nHold-out sampling is probably the simplest form of sampling that we can use and is\nmost appropriate when we have very large datasets from which we can take samples. This\nensures that the training set and test set are sufﬁciently large to train an accurate model and\nfully evaluate the performance of that model. Hold-out sampling is sometimes extended\nto include a third sample, the validation set . The validation set is used when data outside\nthe training set is required in order to tune particular aspects of a model. For example,\nwhen wrapper-based feature selection techniques are used, a validation set is required\nin order to evaluate the performance of the different feature subsets on data not used in\ntraining. It is important that after the feature selection process is complete, a separate test\nset still exists that can be used to evaluate the expected performance of the model on future\nunseen data after deployment. Figure 9.2[541]illustrates how a large ABT can be divided\ninto a training set , avalidation set , and a test set . There are no ﬁxed recommendations\nfor how large the different datasets should be when hold-out sampling is used, although\ntraining:validation:test splits of 50:20:30or40:20:40are common.\nPredic'on*\nModel*Training**\nSet*Test*\nSet*Valida'on*\nSet*\n(a) A 50:20:30split\nPredic'on*\nModel*Training**\nSet*Test*\nSet*Valida'on*\nSet*\n(b) A 40:20:40split\nFigure 9.2\nHold-out sampling can divide the full data into training, validation, and test sets.\nOne of the most common uses of a validation set is to avoid overﬁtting when using ma-\nchine learning algorithms that iteratively build more and more complex models. The ID3\nalgorithm for building decision trees and the gradient descent algorithm for building re-\ngression models are two examples of this type of approach. As the algorithm proceeds, the\nmodel that it is building will become more and more ﬁtted to the nuances of the training\ndata. We can see this in the solid line in Figure 9.3[542]. This shows how the misclassiﬁ-\ncation rate made by a model on a set of training instances changes as the training process","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":596,"page_label":"542","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"542 Chapter 9 Evaluation\ncontinues. This will continue almost indeﬁnitely as the model becomes more and more\ntuned to the instances in the training set. At some point in this process, however, over-\nﬁtting will begin to occur, and the ability of the model to generalize well to new query\ninstances will diminish.\n0 50 100 150 2000.1 0.2 0.3 0.4 0.5\nTraining Iter ationMisclassification RatePerformance on T raining Set\nPerformance on V alidation Set\nFigure 9.3\nUsing a validation set to avoid overﬁtting in iterative machine learning algorithms.\nWe can ﬁnd the point at which overﬁtting begins to happen by comparing the perfor-\nmance of a model at making predictions for instances in the training dataset used to build\nit versus its ability to make predictions for instances in a validation dataset as the training\nprocess continues. The dashed line in Figure 9.3[542]shows the performance of the model\nbeing trained on a validation dataset. We can see that, initially, the performance of the\nmodel on the validation set falls almost in line with the performance of the model on the\ntraining dataset (we usually expect the model to perform slightly better on the training set).\nAbout halfway through the training process, however, the performance of the model on the\nvalidation set begins to disimprove. This is the point at which we say overﬁtting has begun\nto occur (this is shown by the vertical dashed line, at Training Iteration = 100, in Figure\n9.3[542]). To combat overﬁtting, we allow algorithms to train models beyond this point but\nsave the model generated at each iteration. After the training process has completed, we\nﬁnd the point at which performance on the validation set began to disimprove and revert\nback to the model trained at that point. This process is essentially the same as the decision\ntree post-pruning process described in Section 4.4.4[153].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":597,"page_label":"543","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 543\nTwo issues arise when using hold-out sampling. First, using hold-out sampling requires\nthat we have enough data available to make suitably large training, test, and if required,\nvalidation sets. This is not always the case, and making any of these partitions too small can\nresult in a poor evaluation. Second, performance measured using hold-out sampling can be\nmisleading if we happen to make a lucky split of the data that places the difﬁcult instances\ninto the training set and the easy ones into the test set. This will make the model appear\nmuch more accurate than it will actually be when deployed. An example of a commonly\nused sampling method that attempts to address these two issues is k-fold cross validation .\n9.4.1.2 k-Fold cross validation When k-fold cross validation is used, the available\ndata is divided into kequal-sized folds (or partitions), and kseparate evaluation experi-\nments are performed. In the ﬁrst evaluation experiment, the data in the 1stfold is used as\nthe test set, and the data in the remaining k´1folds is used as the training set. A model\nis trained using the training set, and the relevant performance measures on the test set are\nrecorded. A second evaluation experiment is then performed using the data in the 2ndfold\nas the test set and the data in the remaining k´1folds as the training set. Again the relevant\nperformance measures are calculated on the test set and recorded. This process continues\nuntil kevaluation experiments have been conducted and ksets of performance measures\nhave been recorded. Finally, the ksets of performance measures are aggregated to give one\noverall set of performance measures. Although kcan be set to any value, 10-fold cross val-\nidation is probably the most common variant used in practice. Figure 9.4[543]illustrates how\nthe available data is split during the k-fold cross validation process. Each row represents a\nfold in the process, in which the black rectangles indicate the data used for testing while\nthe white spaces indicate the data used for training.\nFold* 1\"\nFold* 2\"\nFold* 3\"\nFold* 4\"\nFold* 5\"\nFold* 6\"\nFold* 7\"\nFold* 8\"\nFold* 9\"\nFold* 10\"\nFigure 9.4\nThe division of data during the k-fold cross validation process. Black rectangles indicate test data,\nand white spaces indicate training data.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":598,"page_label":"544","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"544 Chapter 9 Evaluation\nTable 9.4\nThe performance measures from the ﬁve individual evaluation experiments and an overall aggregate\nfrom the 5-fold cross validation performed on the chest X-ray classiﬁcation dataset.\nClassiﬁcation\nFold Confusion Matrix Accuracy\n1Prediction\nlateral frontal\nTargetlateral 43 9\nfrontal 10 3881%\n2Prediction\nlateral frontal\nTargetlateral 46 9\nfrontal 3 4288%\n3Prediction\nlateral frontal\nTargetlateral 51 10\nfrontal 8 3182%\n4Prediction\nlateral frontal\nTargetlateral 51 8\nfrontal 7 3485%\n5Prediction\nlateral frontal\nTargetlateral 46 9\nfrontal 7 3884%\nOverallPrediction\nlateral frontal\nTargetlateral 237 45\nfrontal 35 18384%","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":599,"page_label":"545","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 545\nLet’s consider an example. As part of a medical decision making system, a prediction\nsystem that can automatically determine the orientation of chest X-rays (the orientations\ncan be lateral orfrontal ) is built.4Based on a full dataset of 1,000instances, we decide\nto evaluate the performance of this system with classiﬁcation accuracy using 5-fold cross\nvalidation. So, the full dataset is divided into 5folds (each containing 200instances), and\nﬁve evaluation experiments are performed using 1fold as the test set and the remaining\nfolds as the training set. The confusion matrices and class accuracy measures arising from\neach fold are shown in Table 9.4[544].\nThe performance measures for each fold (in this case, a confusion matrix and a class\naccuracy measure) can be aggregated into summary performance measures that capture\nthe overall performance across the 5folds. The aggregate confusion matrix, generated by\nsumming together the corresponding cells in the individual confusion matrices for each\nfold, is shown at the bottom of Table 9.4[544]. The aggregate class accuracy measure can\nthen be calculated from this combined confusion matrix and, in this case, turns out to be\n84%. When different performance measures are used, the aggregates can be calculated in\nthe same way.\nThere is a slight shift in emphasis here from evaluating the performance of one model, to\nevaluating the performance of a set of kmodels. Our goal, however, is still to estimate the\nperformance of a model after deployment. When we have a small dataset (introducing the\npossibility of a lucky split), measuring aggregate performance using a set of models gives\na better estimate of post-deployment performance than measuring performance using a\nsingle model. After estimating the performance of a deployed model using k-fold cross\nvalidation, we typically train the model that will be deployed using all of the available\ndata. This contrasts with the hold-out sampling design, in which we simply deploy the\nmodel that has been evaluated.\n9.4.1.3 Leave-one-out cross validation Leave-one-out cross validation , also known\nasjackkniﬁng , is an extreme form of k-fold cross validation in which the number of folds\nis the same as the number of training instances. This means that each fold of the test\nset contains only one instance, and the training set contains the remainder of the data.\nLeave-one-out cross validation is useful when the amount of data available is too small","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":599,"page_label":"545","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"single model. After estimating the performance of a deployed model using k-fold cross\nvalidation, we typically train the model that will be deployed using all of the available\ndata. This contrasts with the hold-out sampling design, in which we simply deploy the\nmodel that has been evaluated.\n9.4.1.3 Leave-one-out cross validation Leave-one-out cross validation , also known\nasjackkniﬁng , is an extreme form of k-fold cross validation in which the number of folds\nis the same as the number of training instances. This means that each fold of the test\nset contains only one instance, and the training set contains the remainder of the data.\nLeave-one-out cross validation is useful when the amount of data available is too small\nto allow big enough training sets in a k-fold cross validation. Figure 9.5[546]illustrates\nhow the available data is split during the leave-one-out cross validation process. Each row\nrepresents a fold in the process, in which the black rectangles indicate the instance that is\nused for testing while the white spaces indicate the data used for training.\nAt the conclusion of the leave-one-out cross validation process, a performance measure\nwill have been calculated for every instance in the dataset. In the same way as we saw in\n4. Lehmann et al. (2003) discusses building prediction models to perform this task.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":600,"page_label":"546","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"546 Chapter 9 Evaluation\nFold* 1\"\nFold* 2\"\nFold* 3\"\nFold* 4\"\nFold* 5\"\nFold* k\"Fold* k-1\"Fold* k-2\"\nFigure 9.5\nThe division of data during the leave-one-out cross validation process. Black rectangles indicate\ninstances in the test set, and white spaces indicate training data.\nTable 9.4[544]fork-fold cross validation, these performance measures are aggregated across\nall the folds to arrive at an overall measure of model performance.\n9.4.1.4 Bootstrapping The next sampling method we will look at is bootstrapping ,\nand in particular the ϵ0bootstrap . Bootstrapping approaches are preferred over cross\nvalidation approaches in contexts with very small datasets (approximately fewer than 300\ninstances). Similar to k-fold cross validation, the ϵ0bootstrap iteratively performs multiple\nevaluation experiments using sightly different training and test sets each time to evaluate\nthe expected performance of a model. To generate these partitions for an iteration of the\nϵ0bootstrap, a random selection of minstances is taken from the full dataset to generate\na test set, and the remaining instances are used as the training set. Using the training set\nto train a model and the test set to evaluate it, a performance measure (or measures) is\ncalculated for this iteration. This process is repeated for kiterations, and the average of\nthe individual performance measures, the titular ϵ0, gives the overall performance of the\nmodel. Typically, in the ϵ0bootstrap, kis set to values greater than or equal to 200, much\nlarger values than when k-fold cross validation is used. Figure 9.6[547]illustrates how the\ndata is divided during the ϵ0bootstrap process. Each row represents an iteration of the\nprocess, in which the black rectangles indicate the data used for testing while the white\nspaces indicate the data used for training.\n9.4.1.5 Out-of-time sampling The sampling methods discussed in the previous section\nall rely on random sampling from a large dataset in order to create test sets. In some\napplications there is a natural structure in the data that we can take advantage of to form\ntest sets. In scenarios that include a time dimension, this can be particularly effective and\nis often referred to as out-of-time sampling , because we use data from one period to build\na training set and data out of another period to build a test set. For example, in a customer","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":601,"page_label":"547","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 547\nItera'on* 1\"\nItera'on* 2\"\nItera'on* 3\"\nItera'on* k\"Itera'on* k-1\"\nFigure 9.6\nThe division of data during the ϵ0bootstrap process. Black rectangles indicate test data, and white\nspaces indicate training data.\nchurn scenario, we might use details of customer behavior from one year to build a training\nset and details of customer behavior from a subsequent year to build a test set. Figure 9.7[547]\nillustrates the process of out-of-time sampling.\nTraining*Set* Test*Set*\nTime*\nFigure 9.7\nThe out-of-time sampling process.\nOut-of-time sampling is essentially a form of hold-out sampling in which the sampling\nis done in a targeted rather than a random fashion. When using out-of-time sampling, we\nshould be careful to ensure that the times from which the training and test sets are taken\ndo not introduce a bias into the evaluation process, because the two different time samples\nare not really representative. For example, imagine we wished to evaluate the performance\nof a prediction model built to estimate the daily energy demand in a residential building\nbased on features describing the family that lives in the house, the weather on a given\nday, and the time of the year. If the training sample covered a period in the summer and\nthe testing sample covered a period in the winter, the results of any evaluation would not\nprovide a reliable measure of how likely the model might actually perform when deployed.\nIt is important when choosing the periods for out-of-time sampling that the time spans are\nlarge enough to take into account any cyclical behavioral patterns or that other approaches\nare used to account for these.\n9.4.2 Performance Measures: Categorical Targets\nThis section describes the most important performance measures for evaluating the perfor-\nmance of models with categorical target features.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":602,"page_label":"548","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"548 Chapter 9 Evaluation\n9.4.2.1 Confusion matrix-based performance measures Confusion matrices are a\nconvenient way to fully describe the performance of a predictive model when applied to\na test set. They are also the basis for a whole range of different performance measures\nthat can highlight different aspects of the performance of a predictive model. The most\nbasic of these measures are true positive rate (TPR ),true negative rate (TNR ),false\nnegative rate (FNR ), and false positive rate (FPR ), which convert the raw numbers from\nthe confusion matrix into percentages.5These measures are deﬁned as follows:\nT PR“T P\npT P`FNq(9.4)\nT NR“T N\npT N`FPq(9.5)\nFPR“FP\npT N`FPq(9.6)\nFNR“FN\npT P`FNq(9.7)\nThere are strong relationships between these measures, for example: FNR“1´T PR ,\nandFPR“1´T NR .\nAll these measures can have values in the range r0,1s. Higher values of TPR and TNR\nindicate better model performance, while the opposite is the case for FNR and FPR. Con-\nfusion matrices are often presented containing these measures rather than the raw counts,\nalthough we recommend using raw counts so that the number of instances with each of the\ndifferent levels of the target feature remains apparent.\nFor the email classiﬁcation data given in Table 9.1[537], the confusion matrix-based values\ncan be calculated as follows:\nT PR“6\np6`3q“0.667\nT NR“9\np9`2q“0.818\nFPR“2\np9`2q“0.182\nFNR“3\np6`3q“0.333\nThese values immediately suggest that the model is better at predicting the ham level\n(TNR) than it is at predicting the spam level (TPR).\n9.4.2.2 Precision, recall, and F 1measure Precision ,recall , and the F1measure are\nanother frequently used set of performance measures that can be calculated directly from\n5. The terms sensitivity andspeciﬁcity are often used for true positive rate and true negative rate.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":603,"page_label":"549","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 549\nthe confusion matrix. Precision andrecall are deﬁned as follows:\nprecision“T P\npT P`FPq(9.8)\nrecall“T P\npT P`FNq(9.9)\nRecall is equivalent to true positive rate (TPR) (compare Equations (9.4)[548]and (9.9)[549]).\nRecall tells us how conﬁdent we can be that all the instances with the positive target level\nhave been found by the model. Precision captures how often, when a model makes a\npositive prediction, this prediction turns out to be correct. Precision tells us how conﬁdent\nwe can be that an instance predicted to have the positive target level actually has the positive\ntarget level. Both precision and recall can assume values in the range r0,1s, and higher\nvalues in both cases indicate better model performance.\nReturning to the email classiﬁcation example, and assuming again that spam emails are\nthe positive level, precision measures how often the emails marked as spam actually are\nspam, whereas recall measures how often the spam messages in the test set were actually\nmarked as spam . The precision and recall measures for the email classiﬁcation data shown\nin Table 9.1[537]are\nprecision“6\np6`2q“0.750\nrecall“6\np6`3q“0.667\nEmail classiﬁcation is a good application scenario in which the different information\nprovided by precision and recall is useful. The precision value tells us how likely it is\nthat a genuine ham email could be marked as spam and, presumably, deleted: 25% (1´\nprecision ). Recall, on the other hand, tells us how likely it is that a spam email will be\nmissed by the system and end up in our in-box: 33.333% (1´recall ). Having both of\nthese numbers is useful as it allows us to think about tuning the model toward one kind of\nerror or the other. Is it better for a genuine email to be marked as spam and deleted, or for\na spam email to end up in our in-box? The performance recorded in Table 9.1[537]shows\nthat this system is slightly more likely to make the second kind of mistake than the ﬁrst.\nPrecision and recall can be collapsed to a single performance measure known as the F1\nmeasure ,6which offers a useful alternative to the simpler misclassiﬁcation rate. The F 1\n6. The F 1measure is often also referred to as the F measure ,F score , orF1score .","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":604,"page_label":"550","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"550 Chapter 9 Evaluation\nmeasure is the harmonic mean of precision and recall and is deﬁned as\nF1measure“2ˆpprecisionˆrecallq\npprecision`recallq(9.10)\nIn Section A.1[745]we talk about how measures of central tendency attempt to capture the\naverage value of a list of numbers. Although the arithmetic mean andmedian are two of\nthe most commonly known such measures, there are more, including the harmonic mean .\nThe harmonic mean tends toward the smaller values in a list of numbers and so can be\nless sensitive to large outliers than the arithmetic mean, which tends toward higher values.\nThis characteristic is useful in the generation of performance measures like the F 1measure,\nas we typically prefer measures to highlight shortcomings in our models rather than hide\nthem. The F 1measure can assume values in the range p0,1s, and higher values indicate\nbetter performance.\nFor the email classiﬁcation dataset shown in Table 9.1[537], the F 1measure (again assum-\ning that the spam level is the positive level) is calculated as\nF1measure“2ˆ´\n6\np6`2qˆ6\np6`3q¯\n´\n6\np6`2q`6\np6`3q¯\n“0.706\nPrecision, recall, and the F 1measure work best in prediction problems with binary tar-\nget features and place an emphasis on capturing the performance of a prediction model on\nthe positive, or most important, level. These measures place less emphasis on the perfor-\nmance of the model on the negative target level. This is appropriate in many applications.\nFor example, in medical applications, a prediction that a patient has a disease is much\nmore important than a prediction that a patient does not. In many cases, however, it does\nnot make sense to consider one target level as being more important. The average class\naccuracy performance measure can be effective in these cases.\n9.4.2.3 Average class accuracy Classiﬁcation accuracy can mask poor performance.\nFor example, the confusion matrices shown in Tables 9.5[551]and 9.6[551]show the perfor-\nmance of two different models on a test dataset that relates to a prediction problem in\nwhich we would like to predict whether a customer will churn or not. The accuracy for\nthe model associated with the confusion matrix shown in Table 9.5[551]is91%, while for\nthe model associated with the confusion matrix shown in Table 9.6[551], the accuracy is just\n78%. In this example the test dataset is quite imbalanced , containing 90instances with\nthenon-churn level and just 10instances with the churn level. This means that the perfor-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":604,"page_label":"550","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4.2.3 Average class accuracy Classiﬁcation accuracy can mask poor performance.\nFor example, the confusion matrices shown in Tables 9.5[551]and 9.6[551]show the perfor-\nmance of two different models on a test dataset that relates to a prediction problem in\nwhich we would like to predict whether a customer will churn or not. The accuracy for\nthe model associated with the confusion matrix shown in Table 9.5[551]is91%, while for\nthe model associated with the confusion matrix shown in Table 9.6[551], the accuracy is just\n78%. In this example the test dataset is quite imbalanced , containing 90instances with\nthenon-churn level and just 10instances with the churn level. This means that the perfor-\nmance of the model on the non-churn level overwhelms the performance on the churn level\nin the accuracy calculation and illustrates how classiﬁcation accuracy can be a misleading\nmeasure of model performance.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":605,"page_label":"551","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 551\nTable 9.5\nA confusion matrix for a k-NN model trained on a churn prediction problem.\nPrediction\nnon-churn churn\nTargetnon-churn 90 0\nchurn 9 1\nTable 9.6\nA confusion matrix for a naive Bayes model trained on a churn prediction problem.\nPrediction\nnon-churn churn\nTargetnon-churn 70 20\nchurn 2 8\nTo address this issue, we can use average class accuracy7instead of classiﬁcation accu-\nracy.8The average class accuracy is calculated as\naverage class accuracy “1\n|levelsptq|ÿ\nlPlevelsptqrecall l (9.11)\nwhere levelsptqis the set of levels that the target feature, t, can assume; |levelsptq|is the\nsize of this set; and recall lrefers to the recall achieved by a model for level l.9The\naverage class accuracies for the model performances shown in Tables 9.5[551]and 9.6[551]are\n1\n2p1`0.1q“55% and1\n2p0.778`0.8q“78.889% respectively, which would indicate that\nthe second model is actually a better performer than the ﬁrst. This result is contrary to the\nconclusion drawn from classiﬁcation accuracy but is more appropriate in this case due to\nthe target level imbalance present in the data.\nThe average class accuracy measure shown in Equation (9.11)[551]uses an arithmetic\nmean and so can be more fully labeled averageclassaccuracy AM. While this is an im-\nprovement over raw classiﬁcation accuracy, many people prefer to use a harmonic mean10\ninstead of an arithmetic mean when calculating average class accuracy. Arithmetic means\nare susceptible to inﬂuence of large outliers, which can inﬂate the apparent performance of\n7. Sometimes target levels in categorical prediction problems are referred to as classes , which is where this name\ncomes from.\n8. Target level imbalance affects misclassiﬁcation rate in the same way, and average misclassiﬁcation rate can\nalso be calculated to combat this problem.\n9. Whereas previously we referred to recall as something calculated only for the positive level, we can calculate\nrecall for any level as the accuracy of the predictions made for that level.\n10. Remember that a harmonic mean is used in the F 1measure given in Equation (9.10)[550].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":606,"page_label":"552","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"552 Chapter 9 Evaluation\na model. The harmonic mean , on the other hand, emphasizes the importance of smaller\nvalues and so can give a slightly more realistic measure of how well a model is performing.\nThe harmonic mean is deﬁned as follows:\naverage class accuracy HM“1\n1\n|levelsptq|ÿ\nlPlevelsptq1\nrecall l(9.12)\nwhere the notation meanings are the same as for Equation (9.11)[551]. The average class\naccuracy HMfor the model performances shown in Tables 9.5[551]and 9.6[551]are\n1\n1\n2ˆ1\n1.0`1\n0.1˙“1\n5.5“18.2%\nand\n1\n1\n2ˆ1\n0.778`1\n0.800˙“1\n1.268“78.873%\nThe harmonic mean results in a more pessimistic view of model performance than an\narithmetic mean. To further illustrate the difference between arithmetic mean and harmonic\nmean, Figure 9.8[553]shows the arithmetic mean and the harmonic mean of all combinations\nof two features A and B that range from 0to100. The curved shape of the harmonic mean\nsurface shows that the harmonic mean emphasizes the contribution of smaller values more\nthan the arithmetic mean—note how the sides of the surface are pulled down to the base\nof the graph by the harmonic mean. We recommend that, in general, when calculating\naverage class accuracy, the harmonic mean should be used rather than the arithmetic mean.\n9.4.2.4 Measuring proﬁt and loss One of the problems faced by all the performance\nmeasures discussed so far is that they place the same value on all the cells within a con-\nfusion matrix. For example, in the churn prediction example, correctly classifying a cus-\ntomer as likely to churn is worth the same as correctly classifying a customer as not likely\nto churn. It is not always correct to treat all outcomes equally. For example, if a customer\nwho really was not a churn risk is classiﬁed as likely to churn, the cost incurred by the\ncompany because of this mistake is the cost of a small bonus offer that would be given to\nthe customer to entice the customer to stay with the company. On the other hand, misclas-\nsifying a customer who really was a churn risk probably has a much larger cost associated\nwith it because that customer will be lost when a small bonus may have enticed the cus-\ntomer to stay. When evaluating the performance of models, it would be useful to be able\nto take into account the costs of different outcomes.\nOne way in which to do this is to calculate the proﬁt or loss that arises from each pre-\ndiction we make and to use these to determine the overall performance of a model. To do","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":607,"page_label":"553","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 553\n(a)\n (b)\nFigure 9.8\nSurfaces generated by calculating (a) the arithmetic mean and (b) the harmonic mean of all combi-\nnations of features A and B that range from 0to100.\nTable 9.7\nThe structure of a proﬁt matrix.\nPrediction\npositive negative\nTargetpositive T P Pro f it FN Pro f it\nnegative FP Pro f it T N Pro f it\nthis we ﬁrst need to create a proﬁt matrix that records these. Table 9.7[553]shows the struc-\nture of a proﬁt matrix, which is the same as the structure of a confusion matrix. T PPro f it\nrepresents the proﬁt arising from a correct positive prediction, FN Pro f it is the proﬁt arising\nfrom an incorrect negative prediction, and so on (note that proﬁt can refer to a positive\nor a negative value). The actual values in a proﬁt matrix are determined through domain\nexpertise.\nTo see the use of a proﬁt matrix in action, consider a prediction problem in which a pay-\nday loan company has built a credit scoring model to predict the likelihood that a borrower\nwill default on a loan. Based on a set of descriptive features extracted from the loan appli-\ncation (e.g., A GE, OCCUPATION , and A SSETS ), the model will classify potential borrowers","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":608,"page_label":"554","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"554 Chapter 9 Evaluation\nTable 9.8\nThe proﬁt matrix for the payday loan credit scoring problem.\nPrediction\ngood bad\nTargetgood 140´140\nbad´700 0\nas belonging to one of two groups: good borrowers, who will repay their loans in full; and\nbadborrowers, who will default on some portion of their loans. The company can run this\nmodel whenever a new loan application is made and only extend credit to those borrowers\npredicted to belong to the good target level. Table 9.8[554]shows the proﬁt matrix for this\nproblem.\nThe values in this matrix are based on historical data that the company has on loans given\nout in the past. The typical value of a loan is $1,000, and the interest rate charged is 14%.\nSo, when a loan is repaid in full, the proﬁt made by the company is usually $140 . There-\nfore, the proﬁt arising from correctly predicting the good level for a potential borrower is\n$140 . Incorrectly predicting the badlevel for a potential borrower who would have re-\npaid the loan in full will result in a negative proﬁt (or loss) of ´$140 , as the company\nhas forgone potential interest payments. Correctly predicting the badlevel for a potential\nborrower results in no proﬁt as no money is loaned.11Incorrectly predicting the good level\nfor a potential borrower who goes on to default on the loan, however, results in a loan not\nbeing repaid. Based on historical examples, the expected loss in this case, referred to as\ntheloss given default , is$700 (most borrowers will repay some of their loan before de-\nfaulting). The values in Table 9.8[554]are based on these ﬁgures. It is clear that the different\noutcomes have different proﬁt and loss associated with them. In particular, extending a\nloan to a borrower who turns out to be badis a very costly mistake.\nTables 9.9(a)[555]and 9.9(b)[555]show confusion matrices for two different prediction mod-\nels, a k-NN model and a decision tree model, trained for the payday loans credit scoring\nproblem. The average class accuracy (using a harmonic mean) for the k-NN model is\n83.824% and for the decision tree model is 80.761% , which suggests that the k-NN model\nis quite a bit better than the decision tree.\nWe can, however, use the values in the proﬁt matrix to calculate the overall proﬁt as-\nsociated with the predictions made by these two models. This is achieved by multiplying\nthe values in the confusion matrix by the corresponding values in the proﬁt matrix and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":608,"page_label":"554","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"loan to a borrower who turns out to be badis a very costly mistake.\nTables 9.9(a)[555]and 9.9(b)[555]show confusion matrices for two different prediction mod-\nels, a k-NN model and a decision tree model, trained for the payday loans credit scoring\nproblem. The average class accuracy (using a harmonic mean) for the k-NN model is\n83.824% and for the decision tree model is 80.761% , which suggests that the k-NN model\nis quite a bit better than the decision tree.\nWe can, however, use the values in the proﬁt matrix to calculate the overall proﬁt as-\nsociated with the predictions made by these two models. This is achieved by multiplying\nthe values in the confusion matrix by the corresponding values in the proﬁt matrix and\nsumming the results. Tables 9.10(a)[555]and 9.10(b)[555]show this calculation for the k-NN\nand the decision tree models. The overall proﬁt for the k-NN model is $560 , while it is\n11. This is always an interesting category to determine a value for. Some people might argue that some proﬁt\narises as no loss was made.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":609,"page_label":"555","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 555\nTable 9.9\n(a) The confusion matrix for a k-NN model trained on the payday loan credit scoring problem\n(average class accuracy HM“83.824% ); and (b) the confusion matrix for a decision tree model\ntrained on the payday loan credit scoring problem ( average class accuracy HM“80.761% ).\n(a)k-NN model\nPrediction\ngood bad\nTargetgood 57 3\nbad 10 30(b) decision tree\nPrediction\ngood bad\nTargetgood 43 17\nbad 3 37\nTable 9.10\n(a) Overall proﬁt for the k-NN model using the proﬁt matrix in Table 9.8[554]and the confusion matrix\nin Table 9.9(a)[555]; and (b) overall proﬁt for the decision tree model using the proﬁt matrix in Table\n9.8[554]and the confusion matrix in Table 9.9(b)[555].\n(a)k-NN model\nPrediction\ngood bad\nTargetgood 7,980´420\nbad´7,000 0\nProﬁt 560(b) decision tree\nPrediction\ngood bad\nTargetgood 6,020´2,380\nbad´2,100 0\nProﬁt 1,540\n$1,540for the decision tree model. As well as showing that it is hard to make money in\nthe payday loans business, this reverses the ordering implied using the average class accu-\nracy. The predictions made by the decision tree model result in a higher proﬁt than those\nmade by the k-NN model. This is because the k-NN makes the mistake of misclassifying a\nbadborrower as good more often than the decision tree model, and this is the more costly\nmistake. Ranking the models by proﬁt, we are able to take this into account, which is\nimpossible using classiﬁcation accuracy or average class accuracy.\nIt is worth mentioning that to use proﬁt as a performance measure, we don’t need to\nquantify the proﬁt associated with each outcome as completely as we have done in this\nexample. The minimal amount of information we need is the relative proﬁt associated with\neach of the different outcomes (TP, TN, FP, or FN) that can arise when a model makes a\nprediction. For example, in the spam ﬁltering problem described previously, all we need\nto use are the relative proﬁts of classifying a ham email as spam , classifying a spam email\nasham, and so on.\nWhile using proﬁt might appear to be the ideal way to evaluate model performance for\ncategorical targets, unfortunately, this is not the case. It is only in very rare scenarios that\nwe can accurately ﬁll in a proﬁt matrix for a prediction problem. In many cases, although","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":610,"page_label":"556","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"556 Chapter 9 Evaluation\nit may be possible to say that some outcomes are more desirable than others, it is simply\nnot possible to quantify this. For example, in a medical diagnosis problem, we might\nconﬁdently say that a false negative (telling a sick patient that they do not have a disease)\nis worse than a false positive (telling a healthy patient that they do have a disease), but it\nis unlikely that we will be able to quantify this as twice as bad, or four times as bad, or\n10.75times as bad. When a proﬁt matrix is available, however, proﬁt is a very effective\nperformance measure to use.\n9.4.3 Performance Measures: Prediction Scores\nCareful examination of the workings of the different classiﬁcation models that we have dis-\ncussed in Chapters 4[117]to 7[311]shows that none of them simply produces a target feature\nlevel as its output. In all cases, a prediction score (or scores) is produced, and a thresh-\nold process is used to convert this score into one of the levels of the target feature. For\nexample, the naive Bayes model produces probabilities that are converted into categorical\npredictions using the maximum a posteriori probability approach, and logistic regression\nmodels produce a probability for the positive target level that is converted into a categorical\nprediction using a threshold. Even in decision trees, the prediction is based on the majority\ntarget level at a leaf node, and the proportion of this level gives us a prediction score. In\na typical scenario with two target levels, a prediction score in the range r0,1sis generated\nby a model, and a threshold of 0.5is used to convert this score into a categorical prediction\nas follows:\nthresholdpscore,0.5q“#\npositive ifscoreě0.5\nnegative otherwise(9.13)\nTo illustrate this, Table 9.11[557]shows the underlying scores that the predictions shown in\nTable 9.1[537]were based on, assuming a threshold of 0.5—that is, instances with a predic-\ntion score greater than or equal to 0.5were given predictions of the spam (positive) level,\nand those with prediction scores less than 0.5were given predictions of the ham (negative)\nlevel. The instances in this table have been sorted by these scores in ascending order; as\na result, the thresholding on the scores to generate predictions is very much apparent. An\nindication of the performance of the model is also evident from this ordering—the Target\ncolumn shows that the instances that actually should get predictions of the ham level gen-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":610,"page_label":"556","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"To illustrate this, Table 9.11[557]shows the underlying scores that the predictions shown in\nTable 9.1[537]were based on, assuming a threshold of 0.5—that is, instances with a predic-\ntion score greater than or equal to 0.5were given predictions of the spam (positive) level,\nand those with prediction scores less than 0.5were given predictions of the ham (negative)\nlevel. The instances in this table have been sorted by these scores in ascending order; as\na result, the thresholding on the scores to generate predictions is very much apparent. An\nindication of the performance of the model is also evident from this ordering—the Target\ncolumn shows that the instances that actually should get predictions of the ham level gen-\nerally have lower scores, and those that should get predictions of the spam level generally\nhave higher scores.\nA range of performance measures use this ability of a model, to rank instances that should\nget predictions of one target level higher than the other, to better assess how well a predic-\ntion model is performing. The basis of most of these approaches is measuring how well\nthe distributions of scores produced by the model for different target levels are separated.\nFigure 9.9[557]illustrates this: assuming that prediction scores are normally distributed, the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":611,"page_label":"557","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 557\nTable 9.11\nA sample test set with model predictions and scores.\nID Target Prediction Score Outcome\n7 ham ham 0.001 TN\n11 ham ham 0.003 TN\n15 ham ham 0.059 TN\n13 ham ham 0.064 TN\n19 ham ham 0.094 TN\n12 spam ham 0.160 FN\n2 spam ham 0.184 FN\n3 ham ham 0.226 TN\n16 ham ham 0.246 TN\n1 spam ham 0.293 FNID Target Prediction Score Outcome\n5 ham ham 0.302 TN\n14 ham ham 0.348 TN\n17 ham spam 0.657 FP\n8 spam spam 0.676 TP\n6 spam spam 0.719 TP\n10 spam spam 0.781 TP\n18 spam spam 0.833 TP\n20 ham spam 0.877 FP\n9 spam spam 0.960 TP\n4 spam spam 0.963 TP\ndistributions of the scores for the two target levels are shown for two different classiﬁca-\ntion models. The prediction score distributions shown in Figure 9.9(a)[557]are much better\nseparated than those in Figure 9.9(b)[557]. We can use the separation of the prediction score\ndistributions to construct performance measures for categorical prediction models.\nPrediction Score\n(a)\nPrediction Score (b)\nFigure 9.9\nPrediction score distributions for two different prediction models. The distributions in (a) are much\nbetter separated than those in (b).\nIf the distributions of prediction scores from predictive models perfectly followed a nor-\nmal distribution, similar to those in Figure 9.9[557], calculating the degree of separation be-\ntween distributions would be very simple and only involve a simple comparison of means\nand standard deviations. Unfortunately, this is not the case, as the distribution of prediction\nscores for a model can follow any distribution. For example, the density histograms in Fig-\nure 9.10[558]show the distributions of prediction scores for the spam andham target levels\nbased on the data in Table 9.11[557]. There are a number of performance measures based on","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":612,"page_label":"558","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"558 Chapter 9 Evaluation\nthe idea of comparing prediction score distributions that attempt to cater to the peculiarities\nof real data. This section describes some of the most important of these measures.\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.5 1.0 1.5 2.0 2.5\n(a)spam\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.5 1.0 1.5 2.0 2.5 (b)ham\nFigure 9.10\nPrediction score distributions for the (a) spam and (b) ham target levels based on the data in Table\n9.11[557].\n9.4.3.1 Receiver operating characteristic curves Thereceiver operating character-\nistic index (ROC index ), which is based on the receiver operating characteristic curve\n(ROC curve ),12is a widely used performance measure that is calculated using prediction\nscores. We saw in Section 9.4.2[547]how the true positive rate (TPR) and true negative\nrate (TNR) can be calculated from a confusion matrix. These measures, however, are in-\ntrinsically tied to the threshold used to convert prediction scores into target levels. The\npredictions shown in Table 9.11[557]and in the confusion matrix in Table 9.3[539]are based\non a prediction score threshold of 0.5. This threshold can be changed, however, which\nleads to different predictions and a different confusion matrix. For example, if we changed\nthe threshold used to generate the predictions shown in Table 9.11[557]from 0.5to0.75, the\npredictions for instances d17,d8, and d6would change from spam toham, resulting in their\noutcomes changing to TN, FN, and FN respectively. This would mean that the confusion\nmatrix would change to that shown in Table 9.12(a)[559]and, in turn, that the TPR and TNR\nmeasures would change to 0.5and0.833respectively.\n12. The slightly strange name receiver operating characteristic comes from the fact that this approach was ﬁrst\nused for tuning radar signals in World War II.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":613,"page_label":"559","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 559\nTable 9.12\nConfusion matrices for the set of predictions shown in Table 9.11[557]using (a) a prediction score\nthreshold of 0.75and (b) a prediction score threshold of 0.25.\n(a) Threshold: 0.75\nPrediction\nspam ham\nTargetspam 4 4\nham 2 10(b) Threshold: 0.25\nPrediction\nspam ham\nTargetspam 7 2\nham 4 7\nSimilarly, if we changed the threshold from 0.5to0.25, the predictions for instances d14,\nd5, and d1would change from ham tospam , resulting in their outcomes changing to FP,\nFP, and TP respectively. This would mean that the confusion matrix would change to that\nshown in Table 9.12(b)[559]and, in turn, that the TPR and TNR measures would change to\n0.777and0.636respectively.\nFor every possible value of the threshold, in the range r0,1s, there are corresponding\nTPR and TNR values. The pattern that is evident in the two examples presented above\ncontinues as the threshold value is modiﬁed: as the threshold increases, TPR decreases and\nTNR increases, and as the threshold decreases, the opposite occurs. Table 9.13[560]shows\nhow the predictions made for test instances change as the threshold changes. Also shown\nare the resulting TPR, TNR, FPR, and FNR values, as well as the misclassiﬁcation rate for\neach threshold. We can see that the misclassiﬁcation rate doesn’t change that much as the\nthreshold changes. This is due to the trade-offs between false positives and false negatives.\nFigure 9.11(a)[561]shows the changing values for TPR and TNR for the prediction scores\nshown in Table 9.13[560]as the threshold is varied from 0to1.13This graph shows that\nchanging the value of the threshold results in a trade-off between accuracy for predictions\nof positive target levels and accuracy for predictions of negative target levels. Capturing\nthis trade-off is the basis of the ROC curve.\nTo plot an ROC curve, we create a chart with true positive rate on the vertical access and\nfalse positive rate (or 1´true negative rate) on the horizontal axis.14The values for these\nmeasures, when any threshold value is used on a collection of score predictions, gives a\npoint on this plot, or a point in receiver operating characteristic space (ROC space ).\nFigure 9.11(b)[561]shows three such points in ROC space and associated confusion matrices\nfor the email classiﬁcation dataset for thresholds of 0.25,0.5, and 0.75.\n13. The staircase nature of this graph arises from the fact that there are ranges for the threshold in which no","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":613,"page_label":"559","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"this trade-off is the basis of the ROC curve.\nTo plot an ROC curve, we create a chart with true positive rate on the vertical access and\nfalse positive rate (or 1´true negative rate) on the horizontal axis.14The values for these\nmeasures, when any threshold value is used on a collection of score predictions, gives a\npoint on this plot, or a point in receiver operating characteristic space (ROC space ).\nFigure 9.11(b)[561]shows three such points in ROC space and associated confusion matrices\nfor the email classiﬁcation dataset for thresholds of 0.25,0.5, and 0.75.\n13. The staircase nature of this graph arises from the fact that there are ranges for the threshold in which no\ninstances occur (for example, from 0.348to0.657), during which the TPR and TNR values do not change. Larger\ntest sets cause these curves to smoothen signiﬁcantly.\n14. ROC curves are often plotted with sensitivity on the vertical axis and 1´speciﬁcity on the horizontal axis.\nRecall that sensitivity is equal to TPR, and speciﬁcity is equal to TNR, so these are equivalent.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":614,"page_label":"560","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"560 Chapter 9 Evaluation\nTable 9.13\nA sample test set with prediction scores and resulting predictions based on different threshold values.\nPred. Pred. Pred. Pred. Pred.\nID Target Score (0.10) (0.25) (0.50) (0.75) (0.90)\n7 ham 0.001 ham ham ham ham ham\n11 ham 0.003 ham ham ham ham ham\n15 ham 0.059 ham ham ham ham ham\n13 ham 0.064 ham ham ham ham ham\n19 ham 0.094 ham ham ham ham ham\n12 spam 0.160 spam ham ham ham ham\n2 spam 0.184 spam ham ham ham ham\n3 ham 0.226 spam ham ham ham ham\n16 ham 0.246 spam ham ham ham ham\n1 spam 0.293 spam spam ham ham ham\n5 ham 0.302 spam spam ham ham ham\n14 ham 0.348 spam spam ham ham ham\n17 ham 0.657 spam spam spam ham ham\n8 spam 0.676 spam spam spam ham ham\n6 spam 0.719 spam spam spam ham ham\n10 spam 0.781 spam spam spam spam ham\n18 spam 0.833 spam spam spam spam ham\n20 ham 0.877 spam spam spam spam ham\n9 spam 0.960 spam spam spam spam spam\n4 spam 0.963 spam spam spam spam spam\nMisclassiﬁcation Rate 0.300 0.300 0.250 0.300 0.350\nTrue Positive Rate (TPR) 1.000 0.778 0.667 0.444 0.222\nTrue Negative rate (TNR) 0.455 0.636 0.818 0.909 1.000\nFalse Positive Rate (FPR) 0.545 0.364 0.182 0.091 0.000\nFalse Negative Rate (FNR) 0.000 0.222 0.333 0.556 0.778\nThe ROC curve is drawn by plotting a point for every feasible threshold value and joining\nthem. Figure 9.12(a)[562]shows a complete ROC curve for the email predictions in Table\n9.13[560]. A line along the diagonal of ROC space from p0,0qtop1,0q, shown as a dotted line\nin Figure 9.12(a)[562], is a reference line representing the expected performance of a model\nthat makes random predictions. We always expect the ROC curve for a trained model to be\nabove this random reference line.15In fact, as the strength of a predictive model increases,\n15. If an ROC curve appears below the diagonal random reference line, this means that the model is consistently\nmaking predictions of the positive level for instances that should receive predictions of the negative level and vice\nversa, and that it could actually be quite a powerful model. This usually arises when a transcription error of some\nkind has been made and should be investigated.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":615,"page_label":"561","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 561\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nThresholdValue\nTrue Positive Rate\nTrue Negative Rate\n(a)\nThresh = 0.25 spam hamPrediction\nspam\nhamTarget42 7\n7\nThresh = 0.5 spam hamPrediction\nspam\nhamTarget23 6\n9\nThresh = 0.75 spam hamPrediction\nspam\nhamTarget22 4\n10\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive RateTrue Positive Rate (b)\nFigure 9.11\n(a) The changing values of TPR and TNR for the test data shown in Table 9.13[560]as the threshold is\naltered; and (b) points in ROC space for thresholds of 0.25,0.5, and 0.75.\nthe ROC curve moves farther away from the random line toward the top left-hand corner\nof ROC space—toward a TPR of 1.0and an FPR of 0.0. So, the ROC curve gives us an\nimmediate visual indication of the strength of a model—the closer the curve is to the top\nleft, the more predictive the model.\nOften the ROC curves for multiple predictive models will be plotted on a single ROC\nplot, allowing easy comparison of their performance. Figure 9.12(b)[562]shows ROC curves\nfor four models tested on a version of the email classiﬁcation test set in Table 9.13[560],\ncontaining many more instances than the one we have been discussing so far, which is\nwhy the curves are so much smoother than the curve shown in Figure 9.12(a)[562]. These\nsmoother curves are more representative of the kind of ROC curves we typically encounter\nin practice. In this example, Model 1 approaches perfect performance, Model 4 is barely\nbetter than random guessing, and Models 2 and 3 sit somewhere in between these two\nextremes.\nAlthough it is useful to visually compare the performance of different models using an\nROC curve, it is often preferable to have a single numeric performance measure with which\nmodels can be assessed. Fortunately, there is an easy calculation that can be made from the\nROC curve that achieves this. The ROC index orarea under the curve (AUC ) measures","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":616,"page_label":"562","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"562 Chapter 9 Evaluation\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive RateTrue Positive Rate\n(a)\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive RateTrue Positive Rate\nModel 1 (0.996)\nModel 2 (0.887)\nModel 3 (0.764)\nModel 4 (0.595) (b)\nFigure 9.12\n(a) A complete ROC curve for the email classiﬁcation example; and (b) a selection of ROC curves\nfor different models trained on the same prediction task.\nthe area underneath an ROC curve. Remembering that the perfect model will appear in the\nvery top left-hand corner of ROC space, it is fairly intuitive that curves with higher areas\nwill be closer to this maximum possible value. The area under an ROC curve is calculated\nas the integral of the curve. Because ROC curves are discrete and stepped in nature, ﬁnding\ntheir integrals is actually very easily done using the trapezoidal method . The ROC index\ncan be calculated as\nROC index“\n|T|ÿ\ni“2pFPRpTrisq´FPRpTri´1sqqˆpT PRpTrisq`T PRpTri´1sqq\n2(9.14)\nwhere Tis a set of thresholds, |T|is the number of thresholds tested, and T PRpTrisqand\nFPRpTrisqare the true positive and false positive rates at threshold irespectively.\nThe ROC index can take values in the range r0,1s(although values less than 0.5are un-\nlikely and indicative of a target labeling error), and larger values indicate better model per-\nformance. So, for example, the ROC index for the ROC curve shown in Figure 9.12(a)[562]is\n0.798, and the ROC indices for Models 1 to 4 in Figure 9.12(b)[562]are0.996,0.887,0.764,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":617,"page_label":"563","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 563\nand0.595(as shown in the legend). While there are no hard and fast rules about what\nconstitutes an acceptable value for the ROC index, and this is really an application-speciﬁc\ndecision, a good rule of thumb is that a value above 0.7indicates a strong model, while a\nvalue below 0.6indicates a weak model. The ROC index is quite robust in the presence\nof imbalanced data, which makes it a common choice for practitioners, especially when\nmultiple modeling techniques are being compared to one another.\nThe ROC index can be interpreted probabilistically as the probability that a model will\nassign a higher rank to a randomly selected positive instance than to a randomly selected\nnegative instance.16TheGini coefﬁcient17is another commonly used performance mea-\nsure that is just a linear rescaling of the ROC index:\nGini coefﬁcient “p2ˆROC indexq´1 (9.15)\nThe Gini coefﬁcient can take values in the range r0,1s, and higher values indicate better\nmodel performance. The Gini coefﬁcient for the model shown in Figure 9.12(a)[562]is\n0.596, and the Gini coefﬁcients for the four models shown in Figure 9.12(a)[562]are0.992,\n0.774,0.527, and 0.190. The Gini coefﬁcient is very commonly used in ﬁnancial modeling\nscenarios such as credit scoring.\n9.4.3.2 Kolmogorov-Smirnov statistic TheKolmogorov-Smirnov statistic (K-S statis-\ntic) is another performance measure that captures the separation between the distribution of\nprediction scores for the different target levels in a classiﬁcation problem. To calculate the\nK-S statistic, we ﬁrst determine the cumulative probability distributions of the prediction\nscores for the positive and negative target levels. This is done as follows:\nCPppositive,psq“num positive test instances with score ďps\nnum positive test instances(9.16)\nCPpnegative,psq“num negative test instances with score ďps\nnum negative test instances(9.17)\nwhere psis a prediction score value, CPppositive,psqis the cumulative probability distri-\nbution of positive value scores, and CPpnegative,psqis the cumulative probability distri-\nbution of negative value scores. These cumulative probability distributions can be plotted\non a Kolmogorov-Smirnov chart (K-S chart ). Figure 9.13[564]shows the K-S chart for\nthe test set predictions shown in Table 9.11[557]. We can see how the cumulative likelihood\nof ﬁnding a ham (or negative) instance increases much more quickly than that of ﬁnding a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":617,"page_label":"563","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"CPppositive,psq“num positive test instances with score ďps\nnum positive test instances(9.16)\nCPpnegative,psq“num negative test instances with score ďps\nnum negative test instances(9.17)\nwhere psis a prediction score value, CPppositive,psqis the cumulative probability distri-\nbution of positive value scores, and CPpnegative,psqis the cumulative probability distri-\nbution of negative value scores. These cumulative probability distributions can be plotted\non a Kolmogorov-Smirnov chart (K-S chart ). Figure 9.13[564]shows the K-S chart for\nthe test set predictions shown in Table 9.11[557]. We can see how the cumulative likelihood\nof ﬁnding a ham (or negative) instance increases much more quickly than that of ﬁnding a\nspam (or positive) instance. This makes sense because if a model is performing accurately,\n16. The ROC index is in fact equivalent to the Wilcoxon-Mann-Whitney statistic used in signiﬁcance testing.\n17. The Gini coefﬁcient should not be confused with the Gini index described in Section 4.4.1[142]. Their only\nconnection is that they are both named after the Italian statistician Corrado Gini.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":618,"page_label":"564","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"564 Chapter 9 Evaluation\nwe would expect negative instances to have low scores (close to 0.0) and positive instances\nto have high scores (close to 1.0).\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCumulative Probability\nCP(spam, ps)\nCP(ham, ps)/uni25CF/uni25CF\nFigure 9.13\nThe K-S chart for the email classiﬁcation predictions shown in Table 9.11[557].\nThe K-S statistic is calculated by determining the maximum difference between the cu-\nmulative probability distributions for the positive and negative target levels. This can be\ngiven formally as\nK-S“max\npspCPppositive,psq´CPpnegative,psqq (9.18)\nwhere CPppositive,psqandCPpnegative,psqare as described above. This distance is\nindicated by the vertical dotted line in Figure 9.13[564], from which it is clear that the K-S\nstatistic is the largest distance between the positive and negative cumulative distributions.\nThe K-S statistic ranges from 0to1, and higher values indicate better model performance,\nreﬂecting the fact that there is a clear distinction between the distributions of the scores\npredicted by the model for the negative and the positive instances.\nIn practice, the simplest way to calculate a K-S statistic for the predictions made by a\nmodel for a test dataset is to ﬁrst tabulate the positive and negative cumulative probabil-\nities for the scores predicted for each instance in the test dataset, in ascending order by\nprediction score. For the score predicted by the model for each instance in the test set, the\ndistance between the positive and negative cumulative probabilities at that score can then\nbe calculated. The K-S statistic is the maximum of these distances. Table 9.14[565]shows an\nexample for the email classiﬁcation problem predictions given in Table 9.11[557]. We have\nhighlighted in bold and marked with a ˚the instance that results in the maximum distance","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":619,"page_label":"565","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 565\nbetween CPpspam,psqandCPpham,psq. This distance is 0.576, which is the K-S statistic\nfor this example.\nTable 9.14\nTabulating the workings required to generate a K-S statistic.\nPositive Negative Positive Negative\n(spam ) ( ham) ( spam ) ( ham)\nPrediction Cumulative Cumulative Cumulative Cumulative\nID Score Count Count Probability Probability Distance\n7 0.001 0 1 0.000 0.091 0.091\n11 0.003 0 2 0.000 0.182 0.182\n15 0.059 0 3 0.000 0.273 0.273\n13 0.064 0 4 0.000 0.364 0.364\n19 0.094 0 5 0.000 0.455 0.455\n12 0.160 1 5 0.111 0.455 0.343\n2 0.184 2 5 0.222 0.455 0.232\n3 0.226 2 6 0.222 0.545 0.323\n16 0.246 2 7 0.222 0.636 0.414\n1 0.293 3 7 0.333 0.636 0.303\n5 0.302 3 8 0.333 0.727 0.394\n14 0.348 3 9 0.333 0.818 0.485\n17 0.657 3 10 0.333 0.909 *0.576\n8 0.676 4 10 0.444 0.909 0.465\n6 0.719 5 10 0.556 0.909 0.354\n10 0.781 6 10 0.667 0.909 0.242\n18 0.833 7 10 0.778 0.909 0.131\n20 0.877 7 11 0.778 1.000 0.222\n9 0.960 8 11 0.889 1.000 0.111\n4 0.963 9 11 1.000 1.000 0.000\n* marks the maximum distance, which is the K-S statistic.\nTo illustrate how a K-S statistic and K-S chart can give insight into model performance,\nFigure 9.14[566]shows a series of charts for the four different prediction models trained on\nthe email classiﬁcation task and evaluated on a large test set. The charts are a histogram\nof the spam scores predicted by the model, a histogram of the ham scores predicted by the\nmodel, and the resulting K-S chart with the K-S statistic highlighted.\nThe resulting K-S statistics are 0.940,0.631,0.432, and 0.164. These results show that\nModel 1 is doing a much better job of separating the two target levels than the other models.\nWe can see this in the score histograms and the K-S charts, but it is also nicely captured in\nthe K-S statistics.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":620,"page_label":"566","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●●\n(a) Model 1\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (b) Model 2\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (c) Model 3\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (d) Model 4\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\nPrediction ScoreDensity\n0.0 0.2 0.4 0.6 0.8 1.00 1 2 3\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCumulative Probability\nCP(spam, ps)\nCP(ham, ps)/uni25CF/uni25CF\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCumulative Probability\nCP(spam, ps)\nCP(ham, ps)/uni25CF/uni25CF\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCumulative Probability\nCP(spam, ps)\nCP(ham, ps)/uni25CF/uni25CF\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCumulative Probability\nCP(spam, ps)\nCP(ham, ps)/uni25CF/uni25CF\nFigure 9.14\nA series of charts for different model performance on the same large email classiﬁcation test set used to generate the ROC curves in Figure\n9.12(b)[562]. Each column from top to bottom: a histogram of the ham scores predicted by the model, a histogram of the spam scores predicted\nby the model, and the K-S chart.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":621,"page_label":"567","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 567\n9.4.3.3 Measuring gain and lift In scenarios in which we have a positive target level\nthat we are especially interested in (for example, spam emails, fraudulent transactions, or\ncustomers who will respond to an offer), it can often be useful to focus in on how well a\nmodel is making predictions for just those instances, rather than how well the model is dis-\ntinguishing between two target levels. This is a subtle difference but can lead to a change in\nthe ordering of models compared to other performance measures. Two useful performance\nmeasures in this regard are gain andlift(we will see that the related performance measures\nofcumulative gain andcumulative lift are also useful).\nThe basic assumption behind both gain and lift is that if we were to rank the instances in a\ntest set in descending order of the prediction scores assigned to them by a well-performing\nmodel, we would expect the majority of the positive instances to be toward the top of this\nranking. The gain and lift measures attempt to measure to what extent a set of predictions\nmade by a model meet this assumption.\nTo calculate gain and lift, we ﬁrst rank the predictions made for a test set in descending\norder by prediction score and then divide them into deciles .18A decile is a group contain-\ning10% of a dataset. Table 9.15[568]shows the data from Table 9.11[557]divided into deciles.\nThere are 20instances, so each decile contains just 2instances. The ﬁrst decile contains\ninstances 9and4, the second decile contains instances 18and20, and so on.\nGain is a measure of how many of the positive instances in the overall test set are found\nin a particular decile. To ﬁnd this, we count the number of positive instances (based on the\nknown target values) found in each decile and divide these by the total number of positive\ninstances in the test set. So, the gain in a given decile is calculated as\ngainpdecq“num positive test instances in decile dec\nnum positive test instances(9.19)\nwhere decrefers to a particular decile. Table 9.16[568]shows how gain is calculated for each\ndecile in the email classiﬁcation test set. The number of positive and negative instances in\neach decile is shown. Based on these numbers, the gain for each decile is calculated using\nEquation (9.19)[567](the calculation of some other measures are also included in this table,\nand these will be explained shortly).\nFigure 9.15(a)[569]graphs the gain for each decile to produce a gain chart . We can see","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":621,"page_label":"567","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"known target values) found in each decile and divide these by the total number of positive\ninstances in the test set. So, the gain in a given decile is calculated as\ngainpdecq“num positive test instances in decile dec\nnum positive test instances(9.19)\nwhere decrefers to a particular decile. Table 9.16[568]shows how gain is calculated for each\ndecile in the email classiﬁcation test set. The number of positive and negative instances in\neach decile is shown. Based on these numbers, the gain for each decile is calculated using\nEquation (9.19)[567](the calculation of some other measures are also included in this table,\nand these will be explained shortly).\nFigure 9.15(a)[569]graphs the gain for each decile to produce a gain chart . We can see\nfrom this chart that the gain is higher for the lower deciles, which contain the instances with\nthe highest scores. This is indicative of the fact that the model is performing reasonably\nwell. Cumulative gain is calculated as the fraction of the total number of positive instances\nin a test set identiﬁed up to a particular decile (i.e., in that decile and all deciles below it):\ncumulative gain pdecq“num positive test instances in all deciles up to dec\nnum positive test instances(9.20)\n18. Any percentiles (see Section A.1[745]) can be used, but deciles are particularly common.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":622,"page_label":"568","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"568 Chapter 9 Evaluation\nTable 9.15\nThe test set with model predictions and scores from Table 9.11[557]extended to include deciles.\nDecile ID Target Prediction Score Outcome\n1st 9 spam spam 0.960 TP\n4 spam spam 0.963 TP\n2nd 18 spam spam 0.833 TP\n20 ham spam 0.877 FP\n3rd 6 spam spam 0.719 TP\n10 spam spam 0.781 TP\n4th 17 ham spam 0.657 FP\n8 spam spam 0.676 TP\n5th 5 ham ham 0.302 TN\n14 ham ham 0.348 TN\n6th 16 ham ham 0.246 TN\n1 spam ham 0.293 FN\n7th 2 spam ham 0.184 FN\n3 ham ham 0.226 TN\n8th 19 ham ham 0.094 TN\n12 spam ham 0.160 FN\n9th 15 ham ham 0.059 TN\n13 ham ham 0.064 TN\n10th 7 ham ham 0.001 TN\n11 ham ham 0.003 TN\nTable 9.16\nTabulating the workings required to calculate gain, cumulative gain, lift, and cumulative lift for the\ndata given in Table 9.11[557].\nDecilePositive\n(spam )\nCountNegative\n(ham)\nCount GainCum.\nGain LiftCum.\nLift\n1st2 0 0.222 0.222 2.222 2.222\n2nd1 1 0.111 0.333 1.111 1.667\n3rd2 0 0.222 0.556 2.222 1.852\n4th1 1 0.111 0.667 1.111 1.667\n5th0 2 0.000 0.667 0.000 1.333\n6th1 1 0.111 0.778 1.111 1.296\n7th1 1 0.111 0.889 1.111 1.270\n8th1 1 0.111 1.000 1.111 1.250\n9th0 2 0.000 1.000 0.000 1.111\n10th0 2 0.000 1.000 0.000 1.000","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":623,"page_label":"569","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 569\nThe cumulative gain for each decile of the email classiﬁcation dataset is shown in Table\n9.16[568]. Figure 9.15(b)[569]shows a cumulative gain chart of this data. That cumulative\ngain chart allows us to understand how many of the positive instances in a complete test\nset we can expect to have identiﬁed at each decile of the dataset. So, for example, Figure\n9.15(b)[569]shows that by the 4thdecile ( 40% of the test data), 66.667% of the spam emails\nin the entire test set will have been identiﬁed. This is evidence of just how well the model is\nperforming. The dotted diagonal line on the cumulative gain chart shows the performance\nwe would expect from random guessing, and the closer the cumulative gain line is to the\ntop left-hand corner of the chart, the better the model is performing.\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF/uni25CF /uni25CF /uni25CF\n/uni25CF /uni25CF0.00 0.05 0.10 0.15 0.20\nDecileGain\n1st 3rd 5th 6th 7th 8th 9th\n(a)\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF /uni25CF/uni25CF/uni25CF/uni25CF /uni25CF /uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th (b)\nFigure 9.15\nThe (a) gain and (b) cumulative gain at each decile for the email predictions given in Table 9.11[557].\nThegain in a particular decile can be interpreted as a measure of how much better than\nrandom guessing the predictions made by a model are. Lift captures this more formally.\nIf a model were performing no better than random guessing, we would expect that within\neach decile, the percentage of positive instances should be the same as the percentage of\npositive instances overall in the complete dataset. Lift tells us how much higher the actual\npercentage of positive instances in a decile decis than the rate expected. So, the lift at\ndecile decis the ratio between the percentage of positive instances in that decile and the\npercentage of positive instances overall in the population:\nli f tpdecq“% of positive test instances in decile dec\n% of positive test instances(9.21)\nIn the email classiﬁcation example, the percentage of positive ( spam ) instances in the full\ntest dataset is9\n20“0.45. Therefore, the lift at each decile decis the percentage of spam","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":624,"page_label":"570","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"570 Chapter 9 Evaluation\ninstances in that decile divided by 0.45. Table 9.16[568]shows the lift for each decile for the\npredictions shown in Table 9.11[557]for the email classiﬁcation problem. If we compare the\nvisualization of lift for these predictions shown in Figure 9.16(a)[570]to the gain chart for\nthe same set of predictions in Figure 9.15(a)[569], we can see that the shapes are the same.\nFor a well-performing model, the lift curve should start well above 1.0and cross 1.0at one\nof the lower deciles. Lift can take values in the range r0,8s, and higher values indicate\nthat a model is performing well at a particular decile.\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF/uni25CF /uni25CF /uni25CF\n/uni25CF /uni25CF0.0 0.5 1.0 1.5 2.0\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th\n(a)\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF1.0 1.2 1.4 1.6 1.8 2.0 2.2\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th (b)\nFigure 9.16\nThe (a) lift and (b) cumulative lift at each decile for the email predictions given in Table 9.11[557].\nIn the same way we calculated cumulative gain, we can calculate lift cumulatively. The\ncumulative lift at decile decis deﬁned as\ncumulative li f t pdecq“% of positive instances in all deciles up to dec\n% of positive test instances(9.22)\nTable 9.16[568]shows the cumulative lift for each decile for the predictions shown in Table\n9.11[557]for the email classiﬁcation problem, and these values are plotted in a cumulative\nlift curve in Figure 9.16(b)[570].\nFigure 9.17[571]shows cumulative gain, lift, and cumulative lift charts (the gain chart is\nnot shown as it is essentially the same as the lift chart) for four different sets of model\npredictions for the larger version of the email classiﬁcation test set (these are the same\npredictions for which ROC charts and K-S charts were plotted in Figures 9.12(b)[562]and\n9.14[566]). Focusing on the cumulative gain charts, we can see that for Model 1, 80% of the\nspam messages are identiﬁed in the top 40% of the model predictions. For Model 2, we\nneed to look almost as far as the top 50% of predictions to ﬁnd the same percentage of","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":625,"page_label":"571","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●●\n(a) Model 1\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (b) Model 2\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (c) Model 3\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nScoreCount\nP_Spam(s)\nP_NonSpam(s)●● (d) Model 4\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF /uni25CF /uni25CF /uni25CF /uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF /uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th\n/uni25CF /uni25CF /uni25CF /uni25CF\n/uni25CF\n/uni25CF\n/uni25CF /uni25CF /uni25CF /uni25CF0.0 0.5 1.0 1.5 2.0\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF /uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF0.5 1.0 1.5 2.0\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF/uni25CF0.5 1.0 1.5\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF /uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF/uni25CF0.8 0.9 1.0 1.1 1.2 1.3 1.4\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF /uni25CF /uni25CF /uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF1.0 1.2 1.4 1.6 1.8 2.0\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF1.0 1.2 1.4 1.6 1.8\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF1.0 1.1 1.2 1.3 1.4 1.5\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th\n/uni25CF/uni25CF /uni25CF\n/uni25CF\n/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF1.00 1.05 1.10 1.15 1.20 1.25\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th\nFigure 9.17\nCumulative gain, lift, and cumulative lift charts for four different models for the extended email classiﬁcation test set.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":626,"page_label":"572","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"572 Chapter 9 Evaluation\nspam messages. For Models 3 and 4, we need to go as far as 60% and75% respectively.\nThis indicates that Model 1 distinguishes between the target levels most effectively.\nCumulative gain is especially useful in customer relationship management (CRM )\napplications such as cross-sell andupsell models. The cumulative gain tells us how many\ncustomers we need to contact in order to reach a particular percentage of those who are\nlikely to respond to an offer, which is an incredibly useful piece of information to know\nwhen planning customer contact budgets.\n9.4.4 Performance Measures: Multinomial Targets\nAll the performance measures described in the previous section assumed that the prediction\nproblem being evaluated had only two target levels. Many of the prediction problems for\nwhich we build models are multinomial , that is, there are multiple target levels. When\nwe deal with multinomial prediction problems, we need a different set of performance\nmeasures. This section describes the most common of these. We begin by discussing how\nthe confusion matrix can be extended to handle multiple target levels.\nIf we have multiple target levels, the structure of the confusion matrix shown in Figure\n9.2[538]no longer ﬁts the data. Similarly, the notion of thinking about a positive level and\na negative level doesn’t apply any more. The confusion matrix can, however, be easily\nextended to handle multiple target levels by including a row and column for each one. Table\n9.17[572]shows the structure of a confusion matrix for a multinomial prediction problem in\nwhich the target feature has llevels.\nTable 9.17\nThe structure of a confusion matrix for a multinomial prediction problem with ltarget levels.\nPrediction\nlevel 1 level 2 level 3 ¨¨¨ level l Recall\nTargetlevel 1 - - - - -\nlevel 2 - - - - -\nlevel 3 - - - - -\n.........\nlevel l - - - - -\nPrecision - - - ¨¨¨ -\nIn Table 9.17[572]we have included precision and recall measures for each target level.\nPrecision and recall are calculated in almost exactly the same way for multinomial prob-\nlems as for binary problems. Abandoning the notion of positive and negative target levels,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":627,"page_label":"573","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 573\nwe get\nprecisionplq“T Pplq\nT Pplq`FPplq(9.23)\nrecallplq“T Pplq\nT Pplq`FNplq(9.24)\nwhere T Pplqrefers to the number of instances correctly given a prediction of the target\nlevel l,FPplqrefers to the number of instances that are incorrectly given a prediction of\ntarget level l, and FNplqrefers to the number of instances that should have been given a\nprediction of target level lbut were given some other prediction.\nTable 9.18\nA sample test set with model predictions for a bacterial species identiﬁcation problem.\nID Target Prediction\n1 durionis fructosus\n2 ﬁculneus fructosus\n3 fructosus fructosus\n4 ﬁculneus ﬁculneus\n5 durionis durionis\n6 pseudo. pseudo.\n7 durionis fructosus\n8 ﬁculneus ﬁculneus\n9 pseudo. pseudo.\n10 pseudo. fructosus\n11 fructosus fructosus\n12 ﬁculneus ﬁculneus\n13 durionis durionis\n14 fructosus fructosus\n15 fructosus ﬁculneusID Target Prediction\n16 ﬁculneus ﬁculneus\n17 ﬁculneus ﬁculneus\n18 fructosus fructosus\n19 durionis durionis\n20 fructosus fructosus\n21 fructosus fructosus\n22 durionis durionis\n23 fructosus fructosus\n24 pseudo. fructosus\n25 durionis durionis\n26 pseudo. pseudo.\n27 fructosus fructosus\n28 ﬁculneus ﬁculneus\n29 fructosus fructosus\n30 fructosus fructosus\nTable 9.18[573]shows the expected targets and a set of model predictions for a multinomial\nprediction problem in which the species of a bacteria present in a sample is determined us-\ning the results of spectrography performed on the sample.19In this example, we are trying\nto distinguish between four species of the bacterial genus Fructobacillus , namely, durio-\nnis,ﬁculneus ,fructosus , and pseudoﬁculneus (abbreviated as pseudo. in all tables). Table\n9.19[574]shows the associated confusion matrix for these predictions, including measures of\nprecision and recall.\n19. See De Bruyne et al. (2011) for an example of machine learning models being used for this task.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":628,"page_label":"574","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"574 Chapter 9 Evaluation\nTable 9.19\nA confusion matrix for a model trained on the bacterial species identiﬁcation problem.\nPrediction\ndurionis ﬁculneus fructosus pseudo. Recall\nTargetdurionis 5 0 2 0 0.714\nﬁculneus 0 6 1 0 0.857\nfructosus 0 1 10 0 0.909\npseudo. 0 0 2 3 0.600\nPrecision 1.000 0 .857 0 .667 1 .000\nWhile the overall classiﬁcation accuracy for this set of predictions is 80%,20the individ-\nual recall scores for each target level show that the performance of the model is not the\nsame for all four levels: the accuracy on the ﬁculneus andfructosus levels is quite high\n(85.714% and90.909% respectively), while for the durionis andpseudoﬁculneus levels,\nthe accuracy is considerably lower ( 71.429% and60.000% ). The averageclassaccuracy HM\nperformance measure can be applied to multinomial prediction problems and is an effective\noption for measuring performance. Using Equation (9.12)[552], we can calculate the average\nclass accuracy for this problem:\n1\n1\n4ˆ1\n0.714`1\n0.857`1\n0.909`1\n0.600˙“1\n1.333“75.000%\nIt is not easy to apply the measures based on prediction scores to multinomial problems.\nAlthough there are some examples of doing it, there is no broad consensus in the com-\nmunity on how it should best be done in all cases, so we do not discuss it further in this\nbook.\n9.4.5 Performance Measures: Continuous Targets\nAll the performance measures that we have discussed so far focus on prediction problems\nwith categorical targets. When evaluating the performance of prediction models built for\ncontinuous targets, there are fewer options to choose from. In this section we describe the\nmost popular performance measures used for continuous targets. The basic process is the\nsame as for categorical targets. We have a test set containing instances for which we know\n20. It is important to remember that for a prediction problem with four target levels, uniform random guessing\nwill give an accuracy of just 25%.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":629,"page_label":"575","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 575\nthe correct target values, and we have a set of predictions made by a model. We would like\nto measure how accurately the predicted values match the correct target values.\n9.4.5.1 Basic measures of error In Section 7.2.2[315], when covering error-based learn-\ning, we discussed the basis of the most common performance measure for continuous\ntargets: sum of squared errors . The sum of squared errors function, L2, for a set of\npredictions made by a model, M, is deﬁned as\nsum o f squared errors “1\n2nÿ\ni“1pti´Mpdiqq2(9.25)\nwhere t1...tnis a set of nexpected target values, and Mpd1q...Mpdnqis a set of npredic-\ntions for a set of test instances, d1...dn. We modify this very slightly to give us the mean\nsquared error performance measure, which captures the average difference between the\nexpected target values in the test set and the values predicted by the model. The mean\nsquared error (MSE ) performance measure is deﬁned as\nmean squared error “nÿ\ni“1pti´Mpdiqq2\nn(9.26)\nThe mean squared error allows us to rank the performance of multiple models on a predic-\ntion problem with a continuous target. Mean squared error values fall in the range r0,8s,\nand smaller values indicate better model performance.\nTable 9.20[576]shows the expected target values for a test set, the predictions made by\ntwo different models (a multivariable linear regression model and a k-NN model), and the\nresulting errors based on these predictions (the additional error measures will be explained\nshortly). The prediction problem in this case is to determine the dosage of a blood-thinning\ndrug (in milligrams) that should be given to a patient in order to achieve a particular level of\nblood-thinning. The descriptive features in this case would be the level of blood-thinning\ndesired, demographic details for the patient, and the results of various medical tests per-\nformed on the patient. Doctors could use the outputs of such a system to help them make\nbetter dosing decisions.21The mean squared error for the multivariable linear regression\nmodel is 1.905and for the k-NN model is 4.394. This indicates that the regression model\nis more accurately predicting the correct drug dosages than the nearest neighbor model.\nOne complaint that is often leveled against mean squared error is that, although it can\nbe used to effectively rank models, the actual mean squared error values themselves are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":629,"page_label":"575","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"blood-thinning. The descriptive features in this case would be the level of blood-thinning\ndesired, demographic details for the patient, and the results of various medical tests per-\nformed on the patient. Doctors could use the outputs of such a system to help them make\nbetter dosing decisions.21The mean squared error for the multivariable linear regression\nmodel is 1.905and for the k-NN model is 4.394. This indicates that the regression model\nis more accurately predicting the correct drug dosages than the nearest neighbor model.\nOne complaint that is often leveled against mean squared error is that, although it can\nbe used to effectively rank models, the actual mean squared error values themselves are\nnot especially meaningful in relation to the scenario that a model is being used for. For\nexample, in the drug dosage prediction problem, we cannot say by how many milligrams\n21. A nice example of building machine learning models for drug dosage prediction can be found in Mac Namee\net al. (2002).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":630,"page_label":"576","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"576 Chapter 9 Evaluation\nTable 9.20\nThe expected target values for a test set, the predictions made by a model, and the resulting errors\nbased on these predictions for a blood-thinning drug dosage prediction problem.\nLinear Regression k-NN\nID Target Prediction Error Prediction Error\n1 10.502 10.730 0.228 12.240 1.738\n2 18.990 17.578 -1.412 21.000 2.010\n3 20.000 21.760 1.760 16.973 -3.027\n4 6.883 7.001 0.118 7.543 0.660\n5 5.351 5.244 -0.107 8.383 3.032\n6 11.120 10.842 -0.278 10.228 -0.892\n7 11.420 10.913 -0.507 12.921 1.500\n8 4.836 7.401 2.565 7.588 2.752\n9 8.177 8.227 0.050 9.277 1.100\n10 19.009 16.667 -2.341 21.000 1.991\n11 13.282 14.424 1.142 15.496 2.214\n12 8.689 9.874 1.185 5.724 -2.965\n13 18.050 19.503 1.453 16.449 -1.601\n14 5.388 7.020 1.632 6.640 1.252\n15 10.646 10.358 -0.288 5.840 -4.805\n16 19.612 16.219 -3.393 18.965 -0.646\n17 10.576 10.680 0.104 8.941 -1.634\n18 12.934 14.337 1.403 12.484 -0.451\n19 10.492 10.366 -0.126 13.021 2.529\n20 13.439 14.035 0.596 10.920 -2.519\n21 9.849 9.821 -0.029 9.920 0.071\n22 18.045 16.639 -1.406 18.526 0.482\n23 6.413 7.225 0.813 7.719 1.307\n24 9.522 9.565 0.043 8.934 -0.588\n25 12.083 13.048 0.965 11.241 -0.842\n26 10.104 10.085 -0.020 10.010 -0.095\n27 8.924 9.048 0.124 8.157 -0.767\n28 10.636 10.876 0.239 13.409 2.773\n29 5.457 4.080 -1.376 9.684 4.228\n30 3.538 7.090 3.551 5.553 2.014\nMSE 1.905 4 .394\nRMSE 1.380 2 .096\nMAE 0.975 1 .750\nR20.889 0 .776","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":631,"page_label":"577","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 577\nwe expect the model to be incorrect based on the mean squared error values. This is due\nto the use of the squared term in the mean squared error calculation but can easily be\naddressed by using root mean squared error instead. The root mean squared error\n(RMSE ) for a set of predictions made by a model on a test set is calculated as\nroot mean squared error “gffffenÿ\ni“1pti´Mpdiqq2\nn(9.27)\nwhere the terms have the same meaning as before. Root mean squared error values are in\nthe same units as the target value and so allow us to say something more meaningful about\nwhat the error for predictions made by the model will be. For example, for the drug dosage\nprediction problem, the root mean squared error value is 1.380for the regression model\nand2.096for the nearest neighbor model. This means that we can expect the predictions\nmade by the regression model to be 1.38mg out on average, whereas those made by the\nnearest neighbor model will be, on average, 2.096mg out.\nDue to the inclusion of the squared term, the root mean squared error tends to overes-\ntimate error slightly as it overemphasizes individual large errors. An alternative measure\nthat addresses this problem is the mean absolute error (MAE ), which does not include a\nsquared term.22Mean absolute error is calculated as\nmean absolute error “nÿ\ni“1abspti´Mpdiqq\nn(9.28)\nwhere the terms in the equation have the same meaning as before, and absrefers to the\nabsolute value. Mean absolute error values fall in the range r0,8s, and smaller values\nindicate better model performance.\nFor the drug dosage predictions given in Table 9.20[576], the mean absolute error is 0.975\nfor the regression model and 1.750for the nearest neighbor model. Mean absolute er-\nrors are in the same units as the predictions themselves, so we can say that, based on\nmean absolute error, we can expect the regression model to make errors of approximately\n0.9575 mg in each of its predictions and the nearest neighbor model to be out by approxi-\nmately 4.020mg. These are not massively different from the values calculated using root\nmean squared error. As we recommended the use of harmonic mean over arithmetic mean\nwhen calculating average class accuracy, we recommend the use of root mean squared\nerror over mean absolute error because it is better to be pessimistic when estimating the\nperformance of models.\n22. This is very similar to the difference between Euclidean distance andManhattan distance discussed in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":631,"page_label":"577","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"rors are in the same units as the predictions themselves, so we can say that, based on\nmean absolute error, we can expect the regression model to make errors of approximately\n0.9575 mg in each of its predictions and the nearest neighbor model to be out by approxi-\nmately 4.020mg. These are not massively different from the values calculated using root\nmean squared error. As we recommended the use of harmonic mean over arithmetic mean\nwhen calculating average class accuracy, we recommend the use of root mean squared\nerror over mean absolute error because it is better to be pessimistic when estimating the\nperformance of models.\n22. This is very similar to the difference between Euclidean distance andManhattan distance discussed in\nSection 5.2.2[184].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":632,"page_label":"578","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"578 Chapter 9 Evaluation\n9.4.5.2 Domain independent measures of error The fact that root mean squared error\nand mean absolute error are in the same units as the target feature itself can be attractive,\nas it gives a very intuitive measure of how well a model is performing—for example, a\nmodel is typically 1.38mg out in its dosage predictions. The disadvantage of this, however,\nis that these types of measures by themselves are not sufﬁcient to judge whether a model\nis making accurate predictions without deep knowledge of a domain. For example, how\ncan we judge whether a drug dosage prediction model that has a root mean squared error\nof1.38mg is actually making accurate predictions without also understanding the domain\nof drug dosage prediction. To make these judgments it is necessary to have a normalized,\ndomain independent measure of model performance.\nTheR2coefﬁcient is a domain independent measure of model performance that is fre-\nquently used for prediction problems with a continuous target. The R2coefﬁcient compares\nthe performance of a model on a test set with the performance of an imaginary model that\nalways predicts the average values from the test set. The R2coefﬁcient is calculated as\nR2“1´sum o f squared errors\ntotal sum o f squares(9.29)\nwhere the sum of squared errors is computed using Equation (9.25)[575], and the total sum\nof squares is given by\ntotal sum o f squares “1\n2nÿ\ni“1`\nti´t˘2(9.30)\nR2coefﬁcient values fall in the range r0,1qand larger values indicate better model per-\nformance. A useful interpretation of the R2coefﬁcient is as the amount of variation in the\ntarget feature that is explained by the descriptive features in the model.\nThe average target value for the drug dosage prediction test set given in Table 9.20[576]\nis11.132. Using this, the R2coefﬁcient for the regression model can be calculated as\n0.889and for the nearest neighbor model as 0.776. This leads to the same conclusion\nwith regard to model ranking as the root mean squared error measures: namely, that the\nregression model has better performance on this task than the nearest neighbor model. The\nR2coefﬁcient, however, has the advantage that it allows assessment of model performance\nin a domain-independent way.\n9.4.6 Evaluating Models after Deployment\nPredictive models are based on the assumption that the patterns learned in the training data\nwill be relevant to unseen instances that are presented to the model in the future. Data,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":632,"page_label":"578","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is11.132. Using this, the R2coefﬁcient for the regression model can be calculated as\n0.889and for the nearest neighbor model as 0.776. This leads to the same conclusion\nwith regard to model ranking as the root mean squared error measures: namely, that the\nregression model has better performance on this task than the nearest neighbor model. The\nR2coefﬁcient, however, has the advantage that it allows assessment of model performance\nin a domain-independent way.\n9.4.6 Evaluating Models after Deployment\nPredictive models are based on the assumption that the patterns learned in the training data\nwill be relevant to unseen instances that are presented to the model in the future. Data,\nhowever, like everything else in the world, is not constant. People grow older, inﬂation\ndrives up salaries, the content of the spam emails changes, and the way people use tech-\nnologies changes. This phenomenon is often referred to as concept drift . Concept drift\nmeans that almost all the predictive models that we build will at some point go stale , and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":633,"page_label":"579","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 579\nthe relationships that they have learned between descriptive features and target features will\nno longer apply. It is important that once a model is deployed, we put in place an ongoing\nmodel validation scheme to monitor the model to catch the point at which it begins to go\nstale. If we can catch this point, we can take appropriate action.\nTo monitor the ongoing performance of a model, we need a signal that indicates that\nsomething has changed. There are three sources from which we can extract such a signal:\n‚The performance of the model measured using appropriate performance measures\n‚The distributions of the outputs of a model\n‚The distributions of the descriptive features in query instances presented to the model\nOnce a signal has identiﬁed that concept drift has occurred and that a model has indeed\ngone stale, corrective action is required. The nature of this corrective action depends on\nthe application and the type of model being used. In most cases, however, corrective action\ninvolves gathering a new labeled dataset and restarting the model building process using\nthis new dataset.\n9.4.6.1 Monitoring changes in performance measures The simplest way to get a sig-\nnal that concept drift has occurred is to repeatedly evaluate models with the same perfor-\nmance measures used to evaluate them before deployment. We can calculate performance\nmeasures for a deployed model and compare these to the performance achieved in eval-\nuations before the model was deployed. If the performance changes signiﬁcantly, this is\na strong indication that concept drift has occurred and that the model has gone stale. For\nexample, if we had used root mean squared error on a hold-out test set to evaluate the\nperformance of a model before deployment, we could collect all the query instances pre-\nsented to the model for a period after deployment and, once their true target feature values\nbecame available, calculate the root mean squared error on this new set of query instances.\nAlarge change in the root mean squared error value would ﬂag that the model had gone\nstale. One of the drawbacks of using this method to detect that a model has gone stale is\nthat estimating how large this change needs to be in order to signal that the model has gone\nstale is entirely domain dependent.23\nAlthough monitoring changes in the performance of a model is the easiest way to tell\nwhether it has gone stale, this method makes the rather large assumption that the correct","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":633,"page_label":"579","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sented to the model for a period after deployment and, once their true target feature values\nbecame available, calculate the root mean squared error on this new set of query instances.\nAlarge change in the root mean squared error value would ﬂag that the model had gone\nstale. One of the drawbacks of using this method to detect that a model has gone stale is\nthat estimating how large this change needs to be in order to signal that the model has gone\nstale is entirely domain dependent.23\nAlthough monitoring changes in the performance of a model is the easiest way to tell\nwhether it has gone stale, this method makes the rather large assumption that the correct\ntarget feature value for a query instance will be made available shortly after the query\nhas been presented to a deployed model. There are many scenarios in which this is the\ncase. For example, for churn models, customers will either churn or not churn; for credit\nscoring models, customers will either repay their loans or not; and for models predicting\nathlete performance, athletes will either match expectations or not. There are many more\n23. Systems like the Western Electric rules (Montgomery, 2004), used widely in process engineering to detect\nout-of-control processes, can be useful in this regard.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":634,"page_label":"580","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"580 Chapter 9 Evaluation\nscenarios, however, in which the correct target feature values either never become available\nor do not become available early enough to be useful for ongoing model validation. In these\nscenarios, this approach to ongoing model validation simply doesn’t work.\n9.4.6.2 Monitoring model output distribution changes An alternative to using chang-\ning model performance is to use changes in the distribution of model outputs as a signal\nfor concept drift. If the distribution of model outputs changes dramatically, for example,\nif a model that previously made positive predictions 80% of the time is suddenly mak-\ning positive predictions only 20% of the time, then we can assume that there is a strong\npossibility that concept drift has occurred and that the model has gone stale. In order to\ncompare distributions, we measure the distribution of model outputs on the test set that\nwas used to originally evaluate a model and then repeat this measurement on new sets of\nquery instances collected during periods after the model has been deployed. We then use\nan appropriate measure to calculate the difference between the distributions collected after\ndeployment and the original distribution. One of the most commonly used measures for\nthis is the stability index . The stability index is calculated as\nstability index “ÿ\nlPlevelsptqˆˆ|At“l|\n|A|´|Bt“l|\n|B|˙\nˆlogeˆ|At“l|\n|A|{|Bt“l|\n|B|˙˙\n(9.31)\nwhere|A|refers to the size of the test set on which performance measures were originally\ncalculated,|At“l|refers to the number of instances in the original test set for which the\nmodel made a prediction of level lfor target t,|B|and|Bt“l|refer to the same measure-\nments on the newly collected dataset, and logeis the natural logarithm .24In general,\n‚If the value of the stability index is less than 0.1, then the distribution of the newly\ncollected test set is broadly similar to the distribution in the original test set.\n‚If the value of the stability index is between 0.1and0.25, then some change has occurred\nand further investigation may be useful.\n‚A stability index value greater than 0.25suggests that a signiﬁcant change has occurred\nand corrective action is required.\nTable 9.21[581]shows an example of how the stability index could be calculated for two\ndifferent sets of query instances collected at two different times after model deployment\nbased on the bacterial species identiﬁcation problem given in Table 9.18[573]. For the origi-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":634,"page_label":"580","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"‚If the value of the stability index is less than 0.1, then the distribution of the newly\ncollected test set is broadly similar to the distribution in the original test set.\n‚If the value of the stability index is between 0.1and0.25, then some change has occurred\nand further investigation may be useful.\n‚A stability index value greater than 0.25suggests that a signiﬁcant change has occurred\nand corrective action is required.\nTable 9.21[581]shows an example of how the stability index could be calculated for two\ndifferent sets of query instances collected at two different times after model deployment\nbased on the bacterial species identiﬁcation problem given in Table 9.18[573]. For the origi-\nnal test set and the two new test sets, referred to as New Sample 1 and New Sample 2, the\ncount and percentage for each target value is given (note that the tests sets do not have to be\nthe same size because relative distributions are used). The original baseline target frequen-\ncies are based on the predictions in Table 9.18[573]and are visualized in Figure 9.18(a)[582].\n24. The natural logarithm of a value a,logepaq, is the logarithm of ato the base e, where eisEuler’s number ,\nequal to approximately 2.718.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":635,"page_label":"581","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 581\nTable 9.21\nCalculating the stability index for the bacterial species identiﬁcation problem given new test data for\ntwo periods after model deployment.\nOriginal New Sample 1 New Sample 2\nTarget Count % Count % SI t Count % SI t\ndurionis 7 0.233 12 0 .267 0.004 12 0 .200 0.005\nﬁculneus 7 0.233 8 0 .178 0.015 9 0 .150 0.037\nfructosus 11 0.367 16 0 .356 0.000 14 0 .233 0.060\npseudo. 5 0.167 9 0 .200 0.006 25 0 .417 0.229\nSum 30 45 0 .026 60 0 .331\nThe frequency and percentage of each target level are shown for the original test set and for two samples collected\nafter deployment. The column marked SI tshows the different parts of the stability index sum based on Equation\n(9.31)[580].\nFigures 9.18(b)[582]and 9.18(c)[582]show the target distributions for the two points in time\nafter deployment for which the stability index is to be calculated. These bar plots show\nthat the distribution of target levels for New Sample 1 is similar to the original test set, but\nthat the distribution of target levels for New Sample 2 is quite different. This is reﬂected\nin the stability index calculations in Table 9.21[581], which are determined using Equation\n(9.31)[580]. For example, for New Sample 1, the stability index is\nstability index “ˆ7\n30´12\n45˙\nˆlogeˆ7\n30{12\n45˙\n`ˆ7\n30´8\n45˙\nˆlogeˆ7\n30{8\n45˙\n`ˆ11\n30´16\n45˙\nˆlogeˆ11\n30{16\n45˙\n`ˆ5\n30´9\n45˙\nˆlogeˆ5\n30{9\n45˙\n“0.026\nwhere the counts come from Table 9.21[581]. The stability index for New Sample 2, calcu-\nlated in the same way, is 0.331. This suggests that at the point in time at which New Sample\n1 was collected, the outputs produced by the model followed much the same distribution\nas when the model was originally evaluated, but that when New Sample 2 was collected,\nthe distribution of the outputs produced by the model had changed signiﬁcantly.\nTo monitor models for the occurrence of concept drift, it is important that the stability\nindex be continuously tracked over time. Figure 9.18(d)[582]shows how the stability index\ncould be tracked for the bacterial species identiﬁcation problem every month for a period\nof 12 months after model deployment. The dotted line indicates a stability index value","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":636,"page_label":"582","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"582 Chapter 9 Evaluation\nof0.1, above which a model should be closely monitored, and the dashed line indicates a\nstability index of 0.25, above which corrective action is recommended.\ndurionis ficulneus fructosus pseudo .\nTargetDensity\n0.0 0.1 0.2 0.3 0.4\n(a) Original\ndurionis ficulneus fructosus pseudo .\nTargetDensity\n0.0 0.1 0.2 0.3 0.4 (b) New Sample 1\ndurionis ficulneus fructosus pseudo .\nTargetDensity\n0.0 0.1 0.2 0.3 0.4\n(c) New Sample 2\n1HZ\u0003\n6DPSOH\u0003\u0014\n1HZ\u0003\n6DPSOH\u0003\u0015\n6WDELOLW\\\u0003,QGH[\n0RQWK\n (d) Monitoring Over Time\nFigure 9.18\nThe distributions of predictions made by a model trained for the bacterial species identiﬁcation prob-\nlem for (a) the original evaluation test set, and for (b) and (c) two periods of time after model deploy-\nment; (d) shows how the stability index can be tracked over time to monitor for concept drift.\nThe stability index can be used for both categorical and continuous targets. When a\nmodel predicts a continuous target, the target range is divided into bins, and the distribution\nof values into these bins is used in the calculation. It is particularly common to use deciles\nfor this task. The same can actually be done for models that predict binary categorical\ntargets by dividing the prediction scores into deciles. The stability index is just one measure","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":637,"page_label":"583","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.4 Extensions and Variations 583\nof the difference between two different distributions, and there are many other options that\ncan be used. For example, for categorical targets, the χ2statistic is often used, and for\ncontinuous targets, the K-S statistic can also be used.\nThe advantage of using evaluation approaches based on comparing the distribution of a\nmodel’s output, such as the stability index, is that they do not require that the true targets for\nquery instances become available shortly after predictions have been made. The downside,\nhowever, is that such measures do not directly measure the performance of the model,\nand consequently, a high stability index may reﬂect a change in the underlying population\nrather than a change in model performance. So, relying solely on a stability index can lead\nto models being rebuilt when it is not required.\n9.4.6.3 Monitoring descriptive feature distribution changes In the same way we can\ncompare the distributions of model outputs between the time that the model was built and\nafter deployment, we can also make the same type of comparison for the distributions of the\ndescriptive features used by the model. We can use any appropriate measure that captures\nthe difference between two different distributions for this, including the stability index, the\nχ2statistic, and the K-S statistic.\nThere is, however, a challenge here, as usually, there are a large number of descriptive\nfeatures for which measures need to be calculated and tracked. Furthermore, it is unlikely\nthat a change in the distribution of just one descriptive feature in a multi-feature model\nwill have a large impact on model performance. For this reason, unless a model uses a\nvery small number of descriptive features (generally fewer than 10), we do not recommend\nthis approach. Measuring the difference in descriptive feature distributions can be useful,\nhowever, in understanding what has changed to make a model go stale. So, we recommend\nthat if a model has been ﬂagged as having gone stale using either performance measure\nmonitoring or output distribution monitoring, then the distributions of the descriptive fea-\ntures at the time that the model was built and the distributions of the features at the time\nthat the model went stale should be compared in an effort to understand what has changed.\nThis information should help if the model is to be rebuilt to address the fact that it has gone\nstale.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":637,"page_label":"583","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"very small number of descriptive features (generally fewer than 10), we do not recommend\nthis approach. Measuring the difference in descriptive feature distributions can be useful,\nhowever, in understanding what has changed to make a model go stale. So, we recommend\nthat if a model has been ﬂagged as having gone stale using either performance measure\nmonitoring or output distribution monitoring, then the distributions of the descriptive fea-\ntures at the time that the model was built and the distributions of the features at the time\nthat the model went stale should be compared in an effort to understand what has changed.\nThis information should help if the model is to be rebuilt to address the fact that it has gone\nstale.\n9.4.6.4 Comparative experiments using a control group At the beginning of this\nchapter, we emphasized that it is important that the evaluation of prediction models not\njust focus on predictive power but also take into account the suitability of the model for\nthe task to which it will be deployed. As part of this type of broader evaluation, the use of\ncomparative experiments that include a control group can be quite effective. The idea\nof a control group might be familiar to readers from reading about medical trials. To test a\nnew medicine, doctors typically assemble a group of patients who suffer from the problem\nthat the medicine is designed to address. During a trial period, half of the patients, the\ntreatment group , are given the new drug, and the other half, the control group, are given","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":638,"page_label":"584","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"584 Chapter 9 Evaluation\nTable 9.22\nThe number of customers who left the mobile phone network operator each week during the com-\nparative experiment from both the control group (random selection) and the treatment group (model\nselection).\nControl Group Treatment Group\nWeek (Random Selection) (Model Selection)\n1 21 23\n2 18 15\n3 28 18\n4 19 20\n5 18 15\n6 17 17\n7 23 18\n8 24 20\n9 19 18\n10 20 19\n11 18 13\n12 21 16\nMean 20.500 17.667\nStd. Dev. 3.177 2.708\naplacebo (essentially a fake drug that has no actual medical effect). Patients are not aware\nwhich group they have been assigned to during the trial (hence the need for the placebo).\nAs long as both the treatment group and the control group are representative of the overall\npopulation, at the end of the trial period, the doctors running the trial can be conﬁdent that\nany improvement they see in the patients in the treatment group that they do not see in the\ncontrol group is due to the new medicine.\nWe can use exactly the same idea to evaluate the impact of predictive models. It is\nimportant to note here that we use control groups not to evaluate the predictive power\nof the models themselves, but rather to evaluate how good they are at helping with the\nbusiness problem when they are deployed. If we have developed a predictive model that is\nused in a particular business process, we can run that business process in parallel both with\nthe predictive model, the treatment group, and without the predictive model, the control\ngroup, in order to evaluate how much the use of the predictive model has improved the\nbusiness process.\nFor example, consider a mobile phone network operator that has built a churn prediction\nmodel to help address a problem with customers leaving to join other networks. The com-\npany would like to evaluate how well the model is helping to address the churn problem.\nBefore the churn model was put in place, every week the company would randomly se-\nlect1,000customers from their customer base and have their customer contact center call","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":639,"page_label":"585","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.5 Summary 585\nthese customers to discuss how satisﬁed they were with the network’s performance and\noffer assistance with any issues. This was based on the assumption that such a call made to\ncustomers considering switching to a different network would encourage them to stay with\ntheir current network. The churn model replaced the random selection of customers by\nassigning every customer in the company’s customer base a churn risk score and selecting\nthe1,000customers with the highest churn risk scores to receive a call from the customer\ncontact center. Everything else about the process was the same as before.\nIn order to evaluate the effect this model was having on the company’s churn problem,\nthey performed a comparative experiment. The company’s entire customer base was di-\nvided randomly into two groups, the treatment group and the control group—and each\ngroup contained approximately 400,000customers. For the customers in the treatment\ngroup, the company applied the process using the predictive model to determine which\ncustomers to contact regarding customer satisfaction. For the customers in the control\ngroup, the random selection process was used. These two approaches ran in parallel for\n12 weeks, and at the end of this period, the company measured the number of customers\nwithin each group who had left the company to join another network. Table 9.22[584]shows\nthe number of customers who churned from each of these two groups during the 12 weeks\nof the trial, and the associated means and standard deviations. These ﬁgures show that, on\naverage, fewer customers churn when the churn prediction model is used to select which\ncustomers to call. This tells us not only something about how accurate the churn prediction\nmodel is but, more important, that using the model actually made a difference in the busi-\nness problem that the company was trying to address.25\nIn order to use control groups in evaluation, we need to be able to divide a population\ninto two groups, run two versions of a business process in parallel, and accurately measure\nthe performance of the business process. Therefore, using control groups is not suitable in\nall scenarios, but when it is applicable, it adds an extra dimension to our evaluations that\ntakes into account not just how well a model can make predictions, but also how much the\npredictive model helps to address the original business problem.\n9.5 Summary","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":639,"page_label":"585","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"model is but, more important, that using the model actually made a difference in the busi-\nness problem that the company was trying to address.25\nIn order to use control groups in evaluation, we need to be able to divide a population\ninto two groups, run two versions of a business process in parallel, and accurately measure\nthe performance of the business process. Therefore, using control groups is not suitable in\nall scenarios, but when it is applicable, it adds an extra dimension to our evaluations that\ntakes into account not just how well a model can make predictions, but also how much the\npredictive model helps to address the original business problem.\n9.5 Summary\nThis chapter covers a range of approaches for evaluating the performance of prediction\nmodels. The choice of the correct performance measure for a particular problem depends\non a combination of the nature of the prediction problem (e.g., continuous versus categori-\ncal), the characteristics of the dataset (e.g., balanced versus imbalanced), and the needs of\nthe application (e.g., medical diagnosis versus marketing response prediction). This last is-\nsue is interesting because sometimes particular performance measures become especially\npopular in certain industries, and in many cases, this dictates the choice of performance\n25. A formal test for statistical signiﬁcance could easily be used to reinforce this conclusion.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":640,"page_label":"586","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"586 Chapter 9 Evaluation\nmeasure. For example, in ﬁnancial credit scoring, the Gini coefﬁcient is almost always\nused to evaluate model performance.\nFor those struggling to choose an appropriate performance measure, in the absence of\nother information, we recommend:\n‚For categorical prediction problems, use average class accuracy based on a harmonic\nmean .\n‚For continuous prediction problems, use the R2coefﬁcient.\nThere are also a number of different ways in which evaluation experiments can be per-\nformed, as described in Section 9.4.1[540]. The choice of which one to use mostly depends\non how much data is available. The following rules of thumb may be useful (although\nthe usual caveats that all scenarios are slightly different apply). In cases with very small\ndatasets (approximately fewer than 300instances), bootstrapping approaches are preferred\nover cross validation approaches. Cross validation approaches are generally preferred un-\nless datasets are very large, in which case the likelihood of the lucky split becomes very\nlow, and hold-out approaches can be used. As with everything else, there is an application-\nspeciﬁc component to the selection of an experimental design—for example, out-of-time\nsampling is a good choice in scenarios where a time dimension is important.\n9.6 Further Reading\nThe evaluation of machine learning models is a live research issue, and a large body of\nmaterial addresses all the questions that have been discussed in this chapter. For a detailed\ndiscussion of the issues associated with evaluating models for categorical prediction prob-\nlems (and model evaluation in general), Japkowicz and Shah (2011) is excellent. David\nHand has also written extensively on the appropriateness of different evaluation measures\nand is always worth reading. For example, Hand and Anagnostopoulos (2013) discusses\nissues with the use of the ROC index.\nJapkowicz and Shah (2011) also discusses the issue of performing statistical signiﬁcance\ntests to compare the performance of multiple models. Demsar (2006) gives another ex-\ncellent overview of comparing multiple modeling types and has been the basis for much\ndiscussion in the machine learning community. This is slightly more of a concern to ma-\nchine learning researchers who are interested in comparing the overall power of different\nmachine learning algorithms. By contrast, in most predictive analytics projects, our focus\nis on determining the best model for a speciﬁc problem.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":640,"page_label":"586","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"and is always worth reading. For example, Hand and Anagnostopoulos (2013) discusses\nissues with the use of the ROC index.\nJapkowicz and Shah (2011) also discusses the issue of performing statistical signiﬁcance\ntests to compare the performance of multiple models. Demsar (2006) gives another ex-\ncellent overview of comparing multiple modeling types and has been the basis for much\ndiscussion in the machine learning community. This is slightly more of a concern to ma-\nchine learning researchers who are interested in comparing the overall power of different\nmachine learning algorithms. By contrast, in most predictive analytics projects, our focus\nis on determining the best model for a speciﬁc problem.\nThe design of model evaluation experiments is an example of the application of tech-\nniques from the larger discipline of experimental design, which is used extensively in the\nmanufacturing industry amongst others. Montgomery (2012) is an excellent reference for\nthis topic and well worth reading.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":641,"page_label":"587","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.6 Further Reading 587\nFinally, for those interested in experimenting with different evaluation measures, the\nROCR package (Sing et al., 2005) for the R programming language includes a wide range\nof measures.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":642,"page_label":"588","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"588 Chapter 9 Evaluation\n9.7 Exercises\n1.The table below shows the predictions made for a categorical target feature by a model\nfor a test dataset. Based on this test set, calculate the evaluation measures listed below.\nID Target Prediction\n1 false false\n2 false false\n3 false false\n4 false false\n5 true true\n6 false false\n7 true trueID Target Prediction\n8 true true\n9 false false\n10 false false\n11 false false\n12 true true\n13 false false\n14 true trueID Target Prediction\n15 false false\n16 false false\n17 true false\n18 true true\n19 true true\n20 true true\n(a)A confusion matrix and the misclassiﬁcation rate\n(b)The average class accuracy (harmonic mean)\n(c)The precision, recall, and F 1measure\n2.The table below shows the predictions made for a continuous target feature by two\ndifferent prediction models for a test dataset.\nModel 1 Model 2\nID Target Prediction Prediction\n1 2,623 2 ,664 2 ,691\n2 2,423 2 ,436 2 ,367\n3 2,423 2 ,399 2 ,412\n4 2,448 2 ,447 2 ,440\n5 2,762 2 ,847 2 ,693\n6 2,435 2 ,411 2 ,493\n7 2,519 2 ,516 2 ,598\n8 2,772 2 ,870 2 ,814\n9 2,601 2 ,586 2 ,583\n10 2,422 2 ,414 2 ,485\n11 2,349 2 ,407 2 ,472\n12 2,515 2 ,505 2 ,584\n13 2,548 2 ,581 2 ,604\n14 2,281 2 ,277 2 ,309\n15 2,295 2 ,280 2 ,296Model 1 Model 2\nID Target Prediction Prediction\n16 2,570 2 ,577 2 ,612\n17 2,528 2 ,510 2 ,557\n18 2,342 2 ,381 2 ,421\n19 2,456 2 ,452 2 ,393\n20 2,451 2 ,437 2 ,479\n21 2,296 2 ,307 2 ,290\n22 2,405 2 ,355 2 ,490\n23 2,389 2 ,418 2 ,346\n24 2,629 2 ,582 2 ,647\n25 2,584 2 ,564 2 ,546\n26 2,658 2 ,662 2 ,759\n27 2,482 2 ,492 2 ,463\n28 2,471 2 ,478 2 ,403\n29 2,605 2 ,620 2 ,645\n30 2,442 2 ,445 2 ,478\n(a)Based on these predictions, calculate the evaluation measures listed below for each\nmodel.\ni.The sum of squared errors\nii.The R2measure\n(b)Based on the evaluation measures calculated, which model do you think is per-\nforming better for this dataset?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":643,"page_label":"589","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.7 Exercises 589\n3.A credit card issuer has built two different credit scoring models that predict the\npropensity of customers to default on their loans. The outputs of the ﬁrst model for a\ntest dataset are shown in the table below.\nID Target Score Prediction\n1 bad 0.634 bad\n2 bad 0.782 bad\n3 good 0.464 good\n4 bad 0.593 bad\n5 bad 0.827 bad\n6 bad 0.815 bad\n7 bad 0.855 bad\n8 good 0.500 good\n9 bad 0.600 bad\n10 bad 0.803 bad\n11 bad 0.976 bad\n12 good 0.504 bad\n13 good 0.303 good\n14 good 0.391 good\n15 good 0.238 goodID Target Score Prediction\n16 good 0.072 good\n17 bad 0.567 bad\n18 bad 0.738 bad\n19 bad 0.325 good\n20 bad 0.863 bad\n21 bad 0.625 bad\n22 good 0.119 good\n23 bad 0.995 bad\n24 bad 0.958 bad\n25 bad 0.726 bad\n26 good 0.117 good\n27 good 0.295 good\n28 good 0.064 good\n29 good 0.141 good\n30 good 0.670 bad\nThe outputs of the second model for the same test dataset are shown in the table below.\nID Target Score Prediction\n1 bad 0.230 bad\n2 bad 0.859 good\n3 good 0.154 bad\n4 bad 0.325 bad\n5 bad 0.952 good\n6 bad 0.900 good\n7 bad 0.501 good\n8 good 0.650 good\n9 bad 0.940 good\n10 bad 0.806 good\n11 bad 0.507 good\n12 good 0.251 bad\n13 good 0.597 good\n14 good 0.376 bad\n15 good 0.285 badID Target Score Prediction\n16 good 0.421 bad\n17 bad 0.842 good\n18 bad 0.891 good\n19 bad 0.480 bad\n20 bad 0.340 bad\n21 bad 0.962 good\n22 good 0.238 bad\n23 bad 0.362 bad\n24 bad 0.848 good\n25 bad 0.915 good\n26 good 0.096 bad\n27 good 0.319 bad\n28 good 0.740 good\n29 good 0.211 bad\n30 good 0.152 bad\nBased on the predictions of these models, perform the following tasks to compare their\nperformance.\n(a)The image below shows an ROC curve for each model. Each curve has a point\nmissing.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":644,"page_label":"590","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"590 Chapter 9 Evaluation\nCalculate the missing point in the ROC curves for Model 1 and Model 2. To\ngenerate the point for Model 1, use a threshold value of 0.51. To generate the\npoint for Model 2, use a threshold value of 0.43.\n(b)The area under the ROC curve (AUC) for Model 1 is 0.955 and for Model 2 is\n0.851. Which model is performing best?\n(c)Based on the AUC values for Model 1 and Model 2, calculate the Gini coefﬁcient\nfor each model.\n4.A retail supermarket chain has built a prediction model that recognizes the household\nthat a customer comes from as being one of single ,business , orfamily . After deploy-\nment, the analytics team at the supermarket chain uses the stability index to monitor\nthe performance of this model. The table below shows the frequencies of predictions\nof the three different levels made by the model for the original validation dataset at the\ntime the model was built, for the month after deployment, and for a monthlong period\nsix months after deployment.\nOriginal 1stNew 2ndNew\nTarget Sample Sample Sample\nsingle 123 252 561\nbusiness 157 324 221\nfamily 163 372 827","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":645,"page_label":"591","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.7 Exercises 591\nBar plots of these three sets of prediction frequencies are shown in the following im-\nages.\nOriginal Sample 1stNew Sample 2ndNew Sample\nsingle business family\nTargetDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nsingle business family\nTargetDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nsingle business family\nTargetDensity\n0.0 0.1 0.2 0.3 0.4 0.5\nCalculate the stability index for the two new periods and determine whether the model\nshould be retrained at either of these points.\n˚5.Explain the problem associated with measuring the performance of a predictive model\nusing a single accuracy ﬁgure.\n˚6.A marketing company working for a charity has developed two different models that\npredict the likelihood that donors will respond to a mailshot asking them to make a\nspecial extra donation. The prediction scores generated for a test set for these two\nmodels are shown in the table below.\nModel 1 Model 2\nID Target Score Score\n1 false 0.1026 0.2089\n2 false 0.2937 0.0080\n3 true 0.5120 0.8378\n4 true 0.8645 0.7160\n5 false 0.1987 0.1891\n6 true 0.7600 0.9398\n7 true 0.7519 0.9800\n8 true 0.2994 0.8578\n9 false 0.0552 0.1560\n10 false 0.9231 0.5600\n11 true 0.7563 0.9062\n12 true 0.5664 0.7301\n13 true 0.2872 0.8764\n14 true 0.9326 0.9274\n15 false 0.0651 0.2992Model 1 Model 2\nID Target Score Score\n16 true 0.7165 0.4569\n17 true 0.7677 0.8086\n18 false 0.4468 0.1458\n19 false 0.2176 0.5809\n20 false 0.9800 0.5783\n21 true 0.6562 0.7843\n22 true 0.9693 0.9521\n23 false 0.0275 0.0377\n24 true 0.7047 0.4708\n25 false 0.3711 0.2846\n26 false 0.4440 0.1100\n27 true 0.5440 0.3562\n28 true 0.5713 0.9200\n29 false 0.3757 0.0895\n30 true 0.8224 0.8614\n(a)Using a classiﬁcation threshold of 0.5, and assuming that trueis the positive target\nlevel, construct a confusion matrix for each of the models.\n(b)Calculate the simple accuracy and average class accuracy (using an arithmetic\nmean ) for each model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":646,"page_label":"592","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"592 Chapter 9 Evaluation\n(c)Based on the average class accuracy measures, which model appears to perform\nbest at this task?\n(d)Generate a cumulative gain chart for each model.\n(e)The charity for which the model is being built typically has only enough money\nto send a mailshot to the top 20% of its contact list. Based on the cumulative gain\nchart generated in the previous part, would you recommend that Model 1 or Model\n2 would perform best for the charity?\n˚7.A prediction model is going to be built for in-line quality assurance in a factory that\nmanufactures electronic components for the automotive industry. The system will be\nintegrated into the factory’s production line and determine whether components are of\nan acceptable quality standard based on a set of test results. The prediction subject is\na component, and the descriptive features are a set of characteristics of the component\nthat can be gathered on the production line. The target feature is binary and labels\ncomponents as good orbad.\nIt is extremely important that the system not in any way slow the production line and\nthat the possibility of defective components being passed by the system be minimized\nas much as possible. Furthermore, when the system makes a mistake, it is desirable\nthat the system can be retrained immediately using the instance that generated the\nmistake. When mistakes are made, it would be useful for the production line operators\nto be able to query the model to understand why it made the prediction that led to a\nmistake. A large set of historical labeled data is available for training the system.\n(a)Discuss the different issues that should be taken into account when evaluating the\nsuitability of different machine learning approaches for use in this system.\n(b)For this task, discuss the suitability of the decision tree, knearest neighbor, naive\nBayes, and logistic regression models. Which one do you think would be most\nappropriate?\n˚8.The following matrices list the confusions matrices for two models and the proﬁt\nmatrix for this prediction task. Calculate the overall proﬁt for each model.\nM1“«\n50 10\n20 80ﬀ\nM2“«\n35 25\n40 60ﬀ\nPro f it“«\n`100´20\n´110`10ﬀ\n˚9.The following table lists the scores returned by a prediction model for a test set of 12\nexamples. The prediction task is a binary classiﬁcation task, and the instances in the\ntest set are labeled as belonging to the positive ornegative class. For ease of reading,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":646,"page_label":"592","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"suitability of different machine learning approaches for use in this system.\n(b)For this task, discuss the suitability of the decision tree, knearest neighbor, naive\nBayes, and logistic regression models. Which one do you think would be most\nappropriate?\n˚8.The following matrices list the confusions matrices for two models and the proﬁt\nmatrix for this prediction task. Calculate the overall proﬁt for each model.\nM1“«\n50 10\n20 80ﬀ\nM2“«\n35 25\n40 60ﬀ\nPro f it“«\n`100´20\n´110`10ﬀ\n˚9.The following table lists the scores returned by a prediction model for a test set of 12\nexamples. The prediction task is a binary classiﬁcation task, and the instances in the\ntest set are labeled as belonging to the positive ornegative class. For ease of reading,\nthe instances have been ordered in descending order of the score the model assigned\nto each instance.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":647,"page_label":"593","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"9.7 Exercises 593\nID Target Score\n1 positive 0.75\n2 positive 0.72\n3 positive 0.64\n4 negative 0.62\n5 negative 0.55\n6 positive 0.48\n7 negative 0.45\n8 negative 0.44\n9 negative 0.38\n10 negative 0.35\n11 negative 0.32\n12 negative 0.31\n(a)Calculate the ROC index for this model using the trapezoidal method and the\nfollowing set of thresholds: 1.0,0.5, and 0.0.\n(b)The following table lists the scores returned by the same prediction model for a\nnew test set of 12 examples. Again, the prediction task is a binary classiﬁcation\ntask, and the instances in the test set are labeled as belonging to the positive or\nnegative class. For ease of reading, the instances have been ordered in descending\norder of the score the model assigned to each instance. Calculate the ROC index\nfor this model using the trapezoidal method and the following set of thresholds:\n1.0,0.5, and 0.0.\nID Target Score\n1 positive 0.71\n2 positive 0.70\n3 positive 0.66\n4 positive 0.65\n5 positive 0.62\n6 positive 0.60\n7 negative 0.58\n8 positive 0.48\n9 positive 0.34\n10 negative 0.30\n11 negative 0.28\n12 negative 0.25\n(c)The ROC index is insensitive to changes in class distribution within the test set.\nThis means that if the proportion of positive to negative instances changes in a\ntest set, the ROC index will remain the same if the performance of the models on\neach class is constant. Consequently, the ROC index is robust to class imbalance\nor skew in the test set. Why do you think this is the case?26\n26. We recommend Fawcett (2006) as an excellent introduction and overview to ROC analysis that covers the\ntopic of imbalance in the test set. Note also that in very highly imbalanced data where there is a very large","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":648,"page_label":"594","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"594 Chapter 9 Evaluation\n˚10.As part of a natural language processing project, a company is creating a dictionary\nof idiomatic phrases.27The company has used an automatic process to extract a set of\n50,000 candidate idioms from a large corpus and now are planning to use a machine\nlearning model to ﬁlter this set of candidates before presenting them to a human an-\nnotator who decides whether a candidate phrase should be added to the dictionary or\nnot. In order to evaluate which machine learning model to use as the pre-annotator\nﬁlter, the company created a test set of 10 phrases extracted at random from the set of\n50,000 candidates.\n(a)The following table presents the scoring by two models of the test set of candidate\nidioms. Which model would be chosen to ﬁlter candidate idioms if the decision\nwere taken on the basis of the F1score for each model, assuming both models use\na threshold ofą0.5for classifying a candidate as an idiom.\nModel Scoring\nID Idiom M1 M2\n1 true 0.70 0.80\n2 true 0.56 0.80\n3 true 0.55 0.70\n4 true 0.54 0.45\n5 true 0.45 0.44\n6 false 0.73 0.55\n7 false 0.72 0.54\n8 false 0.35 0.40\n9 false 0.34 0.38\n10 false 0.33 0.30\n(b)There is a cost associated with each item presented to the human annotator, and\nthe company wants to maximize the number of items that end up in the dictionary.\nThe company estimates that it has an annotation budget that will cover the human\nannotation of 20,000 phrases (i.e., 40% of the set of candidate phrases). Calculate\nthecumulative gain of each of the models for the 4thdecile. Again, assume both\nmodels use a threshold of ą0.5for the idiom class. Finally, on the basis of\ncumulative gain scores, which model would you recommend the company use for\nthe pre-annotation ﬁltering task?\nnumber of negative examples, the false positive rate is not very sensitive to changes in the number of false\npositives (because the denominator is so large) and in these contexts, if the focus of the model is on detecting the\npositive class, it is probably advisable to use precision as the evaluation metric, as it focuses more on the ability to\ndetect the positive class, rather than on the ability to distinguish between classes (which the ROC index captures).\n27. This question is inspired by the work reported in Klubi ˇcka et al. (2018).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":649,"page_label":"595","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"III BEYOND PREDICTION","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":651,"page_label":"597","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10 Beyond Prediction: Unsupervised Learning\n“Dick: I guess it looks as if you’re reorganizing your records. What is this though? Chronological?\nRob: No...\nDick: Not alphabetical...\nRob: Nope...\nDick: What?\nRob: Autobiographical. ”\n—Nick Hornby, High Fidelity\nThe examples throughout this book so far have focused on supervised machine learning\nmethods for building predictive models. In this chapter we shift the focus to unsuper-\nvised machine learning methods. We begin by illustrating the key differences between\nsupervised and unsupervised machine learning before describing clustering, one of the\nmain applications of unsupervised learning; and the standard approach for clustering, the\nk-means clustering algorithm. We then explain some variations on the standard algorithm\nas well as how clustering can be evaluated and interpreted. Finally, we cover a second\nuse case of unsupervised learning, to generate representations that will be used in other\nmachine learning approaches rather than as an end result in their own right. Unsupervised\nlearning is a huge topic in its own right, and so the goal of this chapter is to give a ﬂavor of\nthe most important approaches involved. Suggestions for further reading are given at the\nend of the chapter.\n10.1 Big Idea\nLike lots of other families, the Murphys have a set of magnetic letters on the refrigerator in\ntheir kitchen. Struggling to entertain his daughter one afternoon, Mr. Murphy asked little\nAbigail to tidy them up. A shorter time later than Mr. Murphy would have liked, Abigail\nskipped back in to her father to say that she was ﬁnished. After going along to admire her\nwork, Mr. Murphy congratulated his daughter on organizing the letters so well. Not long\nafterward, however, Mr. Murphy’s son, Andrew, ran in to say that he had ﬁxed the letters in\nthe kitchen because they had been so badly organized. Mr. Murphy walked to the kitchen","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":652,"page_label":"598","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"598 Chapter 10 Beyond Prediction: Unsupervised Learning\nto have a look and agreed with his son that he too had done a great job of organizing the\nletters. Having encouraged his two younger children to play outside, Mr. Murphy had\njust sat down to read the newspaper when his third child Amalia ran in to say that she had\nﬁxed the letters on the fridge because they were all out of order. Once again he went to\nthe kitchen to inspect and agreed that Amalia had also done a great job of organizing the\nletters. Not long afterward the three Murphy children came to their father together to ask\nhow it was that he had told each of them that they had done a great job of arranging the\nletters when they had all done something different.\n(a) The fridge\n (b) Abigail\n (c) Andrew\n (d) Amalia\nFigure 10.1\nThe three different arrangements of the magnetic letters made by the Murphy children on the Murphy\nfamily refrigerator.\nFigure 10.1[598]shows how the three Murphy children organized the letters on the fridge.\nFrom the disorganized letters at the beginning, Abigail divided the letters into six different-\ncolored groups, Andrew organized them into a lowercase group and an uppercase group,\nand Sarah made a group for each letter of the alphabet. All are correct, although they\nfocus on different characteristics of the letter magnets—color, case, and character. This\nillustrates the big idea behind unsupervised learning. In contrast with supervised learning,\nthere is no target feature that we build a model to predict; rather, we perform modeling to\nﬁnd structure within a set of instances deﬁned by descriptive features alone.\n10.2 Fundamentals\nIn Chapter 1[3]of this book we described supervised machine learning techniques as auto-\nmatically learning a model of the relationship between a set of descriptive features and\natarget feature on the basis of a set of historical instances. In contrast with this, un-\nsupervised machine learning techniques are used in the absence of a target feature and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":653,"page_label":"599","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.2 Fundamentals 599\nmodel the underlying structure within the descriptive features in a dataset. This structure\nis typically captured in new generated features that can be appended to the original dataset\nand so augment orenrich it. Figure 10.2[599]illustrates this.\nFigure 10.2\nUnsupervised machine learning as a single-step process.\nThere are two key use cases for unsupervised learning: clustering and representation\nlearning. Clustering is a technique that partitions the instances in a dataset into groups,\norclusters , that are similar to each other. The end result of clustering is a single new\ngenerated feature that indicates the cluster that an instance belongs to, and the generation\nof this new feature is typically the end goal of the clustering task. One of the most common\napplications of clustering is customer segmentation with which organizations attempt to\ndiscover meaningful groupings into which they can group their customers so that targeted\noffers or treatments can be designed.\nInrepresentation learning the goal of unsupervised machine learning is to create a new\nway to represent the instances in a dataset, usually with the expectation that this new repre-\nsentation will be more useful for a later, usually supervised, machine learning process. The\norigins of deep learning discussed in Chapter 8[381]lie in this application of unsupervised\nmachine learning.\nThe fundamentals of unsupervised learning have already largely been covered in previous\nchapters. The clustering methods discussed in this chapter use the ideas of a feature space\nand a distance measure discussed in Chapter 5[181], and the feature generation techniques\nlargely build upon the ideas of error-based learning andneural networks discussed in\nChapters 7[311]and 8[381].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":654,"page_label":"600","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"600 Chapter 10 Beyond Prediction: Unsupervised Learning\nConverting business problems into solutions based on unsupervised machine learning\nalso relies mainly on the techniques discussed in the context of supervised learning in\nChapter 2[23]. CRISP-DM remains an appropriate process to follow, the approach to de-\nsigning data representations based on domain concepts is still very useful, and the project\nﬂow presented in Section 2.5[44]is still appropriate. In all these tasks, the absence of a\ntarget feature makes determining what descriptive features are likely to be useful in an\nABT and evaluating the performance of a proposed solution a little more challenging. For\nthese reasons, analysts often must rely more heavily on domain experts in projects using\nunsupervised learning than in performing supervised machine learning tasks.\nIn the next section we discuss the standard approach to clustering, the k-means clustering\nalgorithm.\n10.3 Standard Approach: The k-Means Clustering Algorithm\nThek-means clustering algorithm is the most well-known approach to clustering. As well\nas being simple to understand and computationally efﬁcient, it is also quite effective and\noften a good solution to clustering problems. The goal of k-means clustering is to take a\ndataset, D, consisting of ninstances, d1todn, where diis a set of mdescriptive features,\nand divide this dataset into kdisjoint clusters, C1toCk. The number of clusters to be\nfound, k, is an input to the algorithm and each instance can belong to only one cluster. The\nalgorithm ﬁnds the division of instances into clusters by minimizing\nnÿ\ni“1min\nc1,...,ckDistpdi,cjq (10.1)\nwhere c1tockare the centers of the kclusters, referred to as cluster centroids ; and Dist\nis a distance measure used to compare instances to centroids. Algorithm 9[601]provides a\npseudocode deﬁnition of the k-means clustering algorithm.1\nThe algorithm begins by randomly selecting kcluster centroids, c1tock, where a clus-\nter centroid is composed of a value for each descriptive feature present in a dataset (these\ninitial cluster centroids are often referred to as seeds ). The values for these cluster cen-\ntroids can be selected randomly following uniform distributions bounded by the minimum\nand maximum values of each feature. The distance of each instance in the dataset to each\nof these cluster centroids is then calculated using the distance measure Dist. It is usual","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":654,"page_label":"600","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is a distance measure used to compare instances to centroids. Algorithm 9[601]provides a\npseudocode deﬁnition of the k-means clustering algorithm.1\nThe algorithm begins by randomly selecting kcluster centroids, c1tock, where a clus-\nter centroid is composed of a value for each descriptive feature present in a dataset (these\ninitial cluster centroids are often referred to as seeds ). The values for these cluster cen-\ntroids can be selected randomly following uniform distributions bounded by the minimum\nand maximum values of each feature. The distance of each instance in the dataset to each\nof these cluster centroids is then calculated using the distance measure Dist. It is usual\n1. There are several different ways to explain the k-means clustering algorithm. For example, as k-means is\nactually a special case of the expectation maximization algorithm (Moon, 1996), it is often described in a\nprobabilisitic context more similar to typical descriptions of that algorithm. To align with Chapter 5[181]we\npresent a similarity-based description of the algorithm.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":655,"page_label":"601","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.3 Standard Approach: The k-Means Clustering Algorithm 601\nAlgorithm 9 Pseudocode description of the k-means clustering algorithm.\nRequire: a dataset Dcontaining ntraining instances, d1,..., dn\nRequire: the number of clusters to ﬁnd k\nRequire: a distance measure, Dist, to compare instances to cluster centroids\n1:Select krandom cluster centroids, c1tock, each deﬁned by values for each descriptive\nfeature, ci“ăcir1s,..., cirmsą\n2:repeat\n3: calculate the distance of each instance, di, to each cluster centroid, c1tock, using\nDist\n4: assign each instance, di, to belong to the cluster, Ci, to whose cluster centroid, ci,\nit is closest\n5: update each cluster centroid, ci, to the average of the descriptive feature values of\nthe instances that belong to cluster Ci\n6:until no cluster reassignments are performed during an iteration\nink-means clustering for Dist to calculate Euclidean distance.2As discussed in Section\n5.4.3[204]in relation to similarity models for predictive modeling, it is important that all\ndescriptive features are normalized before using k-means clustering so that distance con-\ntributions across features are comparable. Each instance in the dataset is then assigned to\nbe a member of the cluster, Ci, to whose cluster centroid, ci, it is closest. The cluster cen-\ntroids are then updated to the average value of the descriptive features of the members of a\ncluster. In this way the cluster centroids are moved to be representative of the members of\nthat cluster.\nThis process repeats until convergence occurs—where convergence is deﬁned as having\noccurred when no cluster memberships change on an iteration of the algorithm and there-\nfore the cluster centroids are stable. Once the algorithm has completed, its two outputs are\na vector of assignments of each instance in the dataset to one of the clusters, C1toCk, and\nthekcluster centroids, c1tock. This ﬁrst output can be used to enrich the original dataset\nwith a new generated feature, the cluster memberships.\n10.3.1 A Worked Example\nTable 10.1[604]shows a simple dataset with just two descriptive features that describe the\naverage number of minutes of calls, C ALL VOLUME , and average megabytes of data,\nDATA USAGE , used per month by customers of a mobile phone company.3From Fig-\n2. Described in Section 5.2.2[184]. Although it is possible to use other distance measures, such as those described\nin Chapter 5[181], ink-means clustering this can break the guarantees that the algorithm makes about convergence.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":655,"page_label":"601","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"a vector of assignments of each instance in the dataset to one of the clusters, C1toCk, and\nthekcluster centroids, c1tock. This ﬁrst output can be used to enrich the original dataset\nwith a new generated feature, the cluster memberships.\n10.3.1 A Worked Example\nTable 10.1[604]shows a simple dataset with just two descriptive features that describe the\naverage number of minutes of calls, C ALL VOLUME , and average megabytes of data,\nDATA USAGE , used per month by customers of a mobile phone company.3From Fig-\n2. Described in Section 5.2.2[184]. Although it is possible to use other distance measures, such as those described\nin Chapter 5[181], ink-means clustering this can break the guarantees that the algorithm makes about convergence.\nThek-medoids clustering algorithm (Kaufman and Rousseeuw, 1990) was developed to address these problems.\n3. The example given here is based on artiﬁcial data generated for the purposes of this book. Performing cus-\ntomer segmentation in this way, however, is very common (see, for example, Berry and Linoff (2004)).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":656,"page_label":"602","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"602 Chapter 10 Beyond Prediction: Unsupervised Learning\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume\n(a)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume (b)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume (c)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume\n(d)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume (e)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume (f)\nFigure 10.3\n(a) A plot of the mobile phone customer dataset given in Table 10.1[604]. (b)–(f) The progress of\nthek-means clustering algorithm, working on the simple customer segmentation dataset. The large\nsymbols represent cluster centroids, and the smaller symbols represent cluster assignments.\nure 10.3(a)[602], which illustrates this dataset, it is clear to a human viewer that three clusters\nexist in this dataset, however, we require an algorithm such as k-means clustering to ﬁnd\nthem automatically (especially in real-world datasets with many more dimensions when\nwe can’t create this type of data visualization).\nFigure 10.3(b)[602]shows the initial randomly selected cluster centroids overlaid onto the\ndataset, where c1“⟨´1.1048,´0.1324⟩,c2“⟨´0.8431,´1.2239⟩, and c3“⟨´1.2744,\n0.2187⟩. After selection of the initial cluster centroids, the next step step in the algorithm\nis to calculate the Euclidean distance from each instance in the dataset to each cluster\ncentroid. In this example we use Euclidean distance as the distance measure. In Table\n10.1[604]the columns labeled Cluster Distances Iter. 1 show the distances between each\ninstance and these initial cluster centroids. On the basis of these distances, each instance\ninDis assigned to one of these clusters, as shown in the column labeled Cluster . These\nmemberships are illustrated in Figure 10.3(c)[602].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":657,"page_label":"603","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.3 Standard Approach: The k-Means Clustering Algorithm 603\nEach cluster centroid is then updated by calculating the mean value of each descriptive\nfeature for all instances that are a member of the cluster. For example, there are nine\nmembers of C1:td1,d2,d3,d6,d8,d11,d13,d20,d24u. So, c1is updated to\nc1rDATA USAGEs “ p´ 0.9531`´1.167`´1.2329`´0.8431`0.9285\n`´1.005`0.2021`´0.7426`´0.3414q{9\n“ ´ 0.5727\nc1rCALL VOLUMEs “ p´ 0.3107`´0.706`´0.4188`0.1811`´0.2168\n`´0.0337`0.4364`0.0119`0.4215q{9\n“ ´ 0.0706\nThe other cluster centroids are updated similarly, so that c1“⟨´0.5727,´0.0706⟩,c2“\n⟨0.8866,´0.7912⟩, and c3“⟨´0.3367,0.6123⟩. These are illustrated in Figure 10.3(d)[602].\nThe previous process is then repeated, and the distances of each instance in Dto these\nupdated cluster centroids are given in Table 10.1[604]in the columns labeled Cluster Dis-\ntances Iter. 2 . The updated cluster memberships based on these distances are shown in the\nrightmost column of Table 10.1[604]. Figures 10.3(e)[602]and 10.3(f)[602]show the remaining\nsteps of the k-means clustering process. In this simple example no cluster reassignments\nare made at the third iteration, and so the process is considered to have converged.\nThe ﬁnal results of the clustering are the cluster centroid values of c1“⟨´1.012,´0.131⟩,\nc2“⟨0.8912,´0.7273⟩, and c3“⟨´0.0491,0.7022⟩; and the cluster assignments of\nC1“td1,d2,d3,d5,d6,d11,d19,d20u\nC2“td4,d8,d9,d10,d15,d17,d18,d21,d22u\nC3“td7,d12,d13,d14,d16,d23,d24u\nThese cluster assignments are also shown in the rightmost column of Table 10.1[604](as\ncluster assignments did not change after the second iteration of the algorithm) in context\nwith the original dataset. A set of cluster assignments is usually referred to as a clustering .\nThe sample dataset used in this section has been purposefully selected to include just\ntwo descriptive features so that the visualizations in Figure 10.3[602]could be easily shown.\nThe algorithm, however, proceeds in exactly the same way for larger datasets and usually\nconverges after tens of iterations. Long after it was ﬁrst proposed, k-means remains a\ncommonly used clustering algorithm; however, there have been many modiﬁcations and\nalternatives proposed over the years. We explore the most important of these modiﬁcations\nin the following sections.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":658,"page_label":"604","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 10.1\nA dataset of mobile phone customers described by their average monthly data (D ATA USAGE ) and call (C ALL VOLUME ) usage. Details of the\nﬁrst two iterations of the k-means clustering algorithm are also shown. The clustering in the second iteration is actually the ﬁnal clustering in\nthis simple example.\nDATA CALL Cluster Distances Iter. 1 Iter. 1 Cluster Distances Iter. 2 Iter. 2\nID U SAGE VOLUME Distpdi,c1q Distpdi,c2q Distpdi,c3q Cluster Distpdi,c1q Distpdi,c2q Distpdi,c3q Cluster\n1 -0.9531 -0.3107 0.2341 0.9198 0.6193 C1 0.4498 1.9014 1.1099 C1\n2 -1.1670 -0.7060 0.5770 0.6108 0.9309 C1 0.87 2.0554 1.558 C1\n3 -1.2329 -0.4188 0.3137 0.8945 0.6388 C1 0.7464 2.152 1.3661 C1\n4 1.0684 -0.4560 2.1972 2.06 2.438 C2 1.6857 0.3813 1.7651 C2\n5 -1.1104 0.1090 0.2415 1.3594 0.1973 C3 0.5669 2.1905 0.923 C1\n6 -0.8431 0.1811 0.4084 1.405 0.4329 C1 0.3694 1.9842 0.6651 C1\n7 -0.3666 0.6905 1.1055 1.9728 1.0231 C3 0.7885 1.9406 0.0837 C3\n8 0.9285 -0.2168 2.0351 2.0378 2.2455 C1 1.5083 0.5759 1.5127 C2\n9 1.1175 -0.6028 2.2715 2.0566 2.529 C2 1.772 0.298 1.895 C2\n10 0.8404 -1.0450 2.1486 1.693 2.4636 C2 1.7165 0.258 2.0328 C2\n11 -1.005 -0.0337 0.1404 1.2012 0.3692 C1 0.4339 2.0376 0.9295 C1\n12 0.2410 0.7360 1.6017 2.2398 1.6013 C3 1.1457 1.6581 0.5908 C3\n13 0.2021 0.4364 1.4253 1.9619 1.4925 C1 0.9259 1.4055 0.5668 C3\n14 0.2153 0.8360 1.6372 2.3159 1.6125 C3 1.2012 1.7602 0.5956 C3\n15 0.8770 -0.2459 1.985 1.9787 2.201 C2 1.4603 0.5454 1.4865 C2\n16 -0.0345 1.0502 1.595 2.4136 1.4929 C3 1.2433 2.0589 0.5321 C3\n17 0.8785 -1.3601 2.3325 1.727 2.6698 C2 1.9413 0.569 2.3167 C2\n18 0.9164 -0.8517 2.1454 1.7984 2.4383 C2 1.6815 0.0674 1.9271 C2\n19 -1.0423 0.1193 0.2593 1.3579 0.2525 C3 0.5065 2.133 0.8608 C1\n20 -0.7426 0.0119 0.3899 1.2399 0.5706 C1 0.1889 1.8164 0.7247 C1\n21 0.6259 -1.1834 2.0248 1.4696 2.3616 C2 1.6355 0.4709 2.0374 C2\n22 0.7684 -0.5844 1.927 1.7338 2.195 C2 1.4362 0.2382 1.6289 C2\n23 -0.2596 0.7450 1.2183 2.0535 1.1432 C3 0.8736 1.9167 0.1535 C3\n24 -0.3414 0.4215 0.9432 1.7202 0.9548 C1 0.5437 1.7259 0.1909 C3","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":659,"page_label":"605","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 605\n10.4 Extensions and Variations\nThek-means clustering algorithm is simple and reasonably effective, but this simple ver-\nsion leaves a number of questions unanswered—can we make the process more efﬁcient,\nhow are clusterings evaluated, how is kchosen, and how are clusters interpreted?—and it\nworks only for clusterings of a certain underlying structure. In this section we look at ex-\ntensions and modiﬁcations of the k-means clustering algorithm that answer these questions\nand address these shortcomings. At the end of the chapter, unsupervised machine learning\nfor feature generation will be discussed.\n10.4.1 Choosing Initial Cluster Centroids\nIn the basic form of k-means clustering, the initial cluster centroids, or seeds, are chosen\nat random uniformly within the feature space. This choice of these initial seeds, unfor-\ntunately, can have a big impact on the performance of the algorithm. Different randomly\nselected starting points can lead to different, often sub-optimal, clusterings. Figure 10.4[606]\nillustrates this, showing other clusterings that are possible given different random initial\ncluster centroids for the mobile phone customer dataset presented in Table 10.1[604]. In Fig-\nures 10.4(a)[606]and 10.4(b)[606], a very different seed from that shown in Figure 10.3(b)[602]\nis shown to lead to the same clustering found previously (Figure 10.3(f)[602]), whereas in\nFigures 10.4(a)[606]to 10.4(h)[606], seeds that lead to very different clusterings are shown.\nIt is clear that the clusterings shown in Figures 10.4(d)[606], 10.4(f)[606], and 10.4(h)[606]are\nquite different from the clustering found previously (shown in Figure 10.3(f)[602]) and are\nsub-optimal compared with the result we would intuitively expect from looking at the vi-\nsualization of this dataset. The clustering in Figure 10.4(h)[606]is particularly unlucky , as\none of the clusters has remained empty!4\nThese sub-optimal clusterings are rare—for this dataset the vast majority of initial cen-\ntroid choices will lead to the clustering in Figure 10.3(f)[602]—but they can occur. In large\nmultivariate datasets they are more common, and the algorithm can very easily arrive in\none—we don’t have the luxury of simply visualizing high-dimensional datasets to check\nagainst an intuitive clustering.\nAn easy way to address this issue is to perform multiple runs of the k-means clustering","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":659,"page_label":"605","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sub-optimal compared with the result we would intuitively expect from looking at the vi-\nsualization of this dataset. The clustering in Figure 10.4(h)[606]is particularly unlucky , as\none of the clusters has remained empty!4\nThese sub-optimal clusterings are rare—for this dataset the vast majority of initial cen-\ntroid choices will lead to the clustering in Figure 10.3(f)[602]—but they can occur. In large\nmultivariate datasets they are more common, and the algorithm can very easily arrive in\none—we don’t have the luxury of simply visualizing high-dimensional datasets to check\nagainst an intuitive clustering.\nAn easy way to address this issue is to perform multiple runs of the k-means clustering\nalgorithm starting from different initial centroids and then aggregate the results. In this way\nthe most common clustering is chosen as the ﬁnal result. There are, however, a number\nof methods designed to more carefully select the initial centroids. These methods offer\ntwo advantages over basic k-means: they can ﬁnd initial centroids less likely to lead to\nsub-optimal clusterings, and they can select initial centroids that allow the algorithm to\nconverge much more quickly than when seeds are randomly chosen. The k-means++\n4. A common modiﬁcation to k-means clustering in which actual instances are chosen as initial centroids, rather\nthan random points in the feature space, easily stops this from happening.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":660,"page_label":"606","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"606 Chapter 10 Beyond Prediction: Unsupervised Learning\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume\n(a) Seed\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (b) Clustering\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (c) Seed\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (d) Clustering\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume\n(e) Seed\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (f) Clustering\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (g) Seed\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (h) Clustering\nFigure 10.4\n(a)–(h) Different clusterings (all with k“3) that can be found for the mobile phone customer dataset\ngiven in Table 10.1[604]when different initial cluster centroids are used.\nalgorithm is a well-known example of this kind of algorithm and is used as the default\nclustering implementation in many machine learning packages and tools.\nAlgorithm 10[607]shows a pseudocode description of the k-means++ algorithm. In this ap-\nproach, an instance is chosen randomly (following a uniform distribution) from the dataset\nas the ﬁrst centroid. Subsequent centroids are then chosen randomly but following a dis-\ntribution deﬁned by the square of the distances between an instance and the nearest cluster\ncentroid out of those found so far. This means that instances far away from the current\nset of centroids are much more likely to be selected than those close to already selected\ncentroids.\nFigure 10.5[607]shows a selection of sets of initial centroids selected by the k-means++\nprocess. From these it is clear that instances from the dataset are being used as cluster\ncentroids and that typically there is good diversity across the feature space in the centroids\nshown. The k-means++ algorithm is still stochastic in the manner in which initial cluster\ncentroids are selected, and it does not completely remove the possibility of a poor starting\npoint that leads to a sub-optimal clustering. Hence, an aggregate across multiple runs\nshould still be used.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":661,"page_label":"607","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 607\nAlgorithm 10 Pseudocode description of the k-means++ algorithm.\nRequire: a dataset Dcontaining ntraining instances, d1,..., dn\nRequire: k, the number of cluster centroids to ﬁnd\nRequire: a distance measure Dist to compare instances to cluster centroids\n1:choose dirandomly (following a uniform distribution) from Dto be the position of the\ninitial centroid, c1, of the ﬁrst cluster, C1\n2:forcluster CjinC2toCkdo\n3: for each instance, di, inDletDistpdiqbe the distance between diand its nearest\ncluster centroid\n4: calculate a selection weight for each instance, di, inDasDistpdiq2\nřn\np“1Distpdpq2\n5: choose dias the position of cluster centroid, cj, for cluster Cjrandomly following\na distribution based on the selection weights\n6:end for\n7:proceed with k-means as normal using tc1,..., ckuas the initial centroids.\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume\n(a)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (b)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (c)\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.50.00.51.0\nData UsageCall V olume (d)\nFigure 10.5\n(a)–(d) Initial centroids chosen using the k-means++ approach (all with k“3) for the mobile phone\ncustomer dataset given in Table 10.1[604].\n10.4.2 Evaluating Clustering\nUnsupervised learning presents us with a more complicated evaluation challenge than su-\npervised learning. All the performance measures described in Chapter 9[533]for supervised\nlearning relied on the existence of ground truth labels to which the predictions made by\na model can be compared to measure its performance. The essence of the unsupervised\nscenario is that no ground truth exists—we don’t know what it is that we are looking for!\nThis means that we typically need to move beyond the performance measures described in\nChapter 9[533].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":662,"page_label":"608","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"608 Chapter 10 Beyond Prediction: Unsupervised Learning\nOne common approach to evaluating clustering is to use an internal criterion to evaluate\nhow well the clustering found by an algorithm matches some idealized notion of what\nagood clustering would look like. Many of these methods make an assumption that a\ngood clustering is a clustering in which the instances that belong to a given cluster are\nvery close together, whereas the instances that belong to different clusters are far apart.\nThat is, a good clustering minimizes intra-cluster distances and maximizes inter-cluster\ndistances . Figure 10.6[608]illustrates these two types of distances along with examples of a\ngood and a badclustering—according to this deﬁnition.\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8 −0.6 −0.4 −0.2 0.0 0.2\nData UsageCall Volume\n(a) Intra-cluster distance\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8 −0.6 −0.4 −0.2 0.0 0.2\nData UsageCall Volume (b) Inter-cluster distance\n−10 −5 0 5 10−10 −5 0 510\nf1f2\n(c) A good clustering\n−10 −5 0 5−10 −5 0 5\nf1f2 (d) A badclustering\nFigure 10.6\n(a) Intra-cluster distance; (b) inter-cluster distance; (c) a good clustering; and (d) a badclustering.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":663,"page_label":"609","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 609\nThesilhouette is a well-known and widely used performance measure that assesses how\nwell a clustering meets these criteria.5Calculating the silhouette for a clustering involves\na reasonable amount of computation. Algorithm 11[610]outlines the steps involved. To\ncalculate the silhouette for a clustering, C, over a dataset, D, we calculate a silhouette\nwidth for each instance, di, inDand average these over all instances in the dataset.\nThe silhouette width for an individual instance is essentially a localized ratio of inter-\ncluster and intra-cluster distances capturing how well the instance dihas been clustered. If\nfordithe average intra-cluster distance, apiq, is much smaller than the average inter-cluster\ndistance to members of the nearest next cluster, bpiq, then the silhouette width, spiq, will be\nclose to 1and we can be conﬁdent that direally belongs to the cluster in which it has been\nplaced. On the other hand, if apiqis much larger than bpiq, then diis closer on average to\nmembers of another cluster than it is on average to the members of its own cluster, and spiq\nwill be close to ´1. This means that didoesn’t really belong to the cluster in which it has\nbeen placed.\nAveraging across these localized values gives an overall measure of the overall quality of\nthe clustering. The silhouette for a clustering will always be in the range from ´1to1. A\nvalue near 1suggests a good clustering, while a value near ´1suggests a poor clustering\n(with the caveat that these scores are based on the assumption about desired characteristics\nof a clustering described previously).\nTable 10.2[611]shows an example of calculating the silhouette for the ﬁnal clustering of\nthe mobile phone customer dataset found using the k-means algorithm (with k“3) (Table\n10.1[604]). The cluster to which each instance has been assigned, the nearest other cluster\nto each instance, the values of apiqandbpiq, and the ﬁnal silhouette value, spiq, are all\nshown. We discuss in detail the calculation of the silhouette width for the ﬁrst instance in\nthe dataset, d1.\nIn this clustering d1is a member of C1. The ﬁrst step is calculating the silhouette value\nford1is to calculate apiq, the average distance between d1and the other members of cluster\nC1. The distance from d1to each other member of C1is\n“d2 d3 d5 d6 d11 d19 d20\nd1 0.45 0.30 0.45 0.50 0.28 0.44 0.39‰\nThe average distance between d1and the other members of C1, the average intra-cluster","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":663,"page_label":"609","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.1[604]). The cluster to which each instance has been assigned, the nearest other cluster\nto each instance, the values of apiqandbpiq, and the ﬁnal silhouette value, spiq, are all\nshown. We discuss in detail the calculation of the silhouette width for the ﬁrst instance in\nthe dataset, d1.\nIn this clustering d1is a member of C1. The ﬁrst step is calculating the silhouette value\nford1is to calculate apiq, the average distance between d1and the other members of cluster\nC1. The distance from d1to each other member of C1is\n“d2 d3 d5 d6 d11 d19 d20\nd1 0.45 0.30 0.45 0.50 0.28 0.44 0.39‰\nThe average distance between d1and the other members of C1, the average intra-cluster\ndistance, is the average of these values, which is equal to 0.401, theapiqvalue for d1given\nin Table 10.2[611].\nThe next step in the algorithm is to calculate the average distance from d1to each member\nof the other two clusters, C2andC3—the inter-cluster distances. The distances from d1to\n5. The silhouette (Rousseeuw, 1987) is just one example of how clusterings can be evaluated. Others include\nthecubic clustering criterion (Sarle, 1983) and the Dunn index (Dunn, 1974); however, these are based on very\nsimilar ideas to the silhouette.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":664,"page_label":"610","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"610 Chapter 10 Beyond Prediction: Unsupervised Learning\nAlgorithm 11 Pseudocode description of the algorithm for calculating the silhouette for\ninternal cluster evaluation.\nRequire: a dataset Dcontaining ntraining instances, d1,..., dn\nRequire: a clustering Cof dataset Dintokclusters, C1,...,Ck\nRequire: a distance measure, Dist, to compare distances between instances\n1:foreach instance diinDdo\n2: letapiqbe the average distance between instance diand all of the other instances\nwithin the cluster to which dibelongs, Cj(average intra-cluster distance )\n3: calculate the average distance between instance diand the members of each of the\nother clusters CzCj\n4: letbpiqbe the lowest average distance between instance diand any other cluster\n(average inter-cluster distance )\n5: calculate the silhouette index for dias\nspiq“bpiq´apiq\nmaxpapiq,bpiqq(10.2)\n6:end for\n7:calculate ﬁnal silhouette for the clustering as s“1\nnnÿ\ni“1spiq\nthe members of C2are\n“d4 d8 d9 d10 d15 d17 d18 d21 d22\nd1 2.03 1.88 2.09 1.94 1.83 2.11 1.95 1.80 1.74‰\nwhich gives an average of 1.931. ForC3the distances are\n“d7 d12 d13 d14 d16 d23 d24\nd1 1.16 1.59 1.38 1.64 1.64 1.26 0.95‰\nwhich gives an average of 1.3743 . Based on these two average distances, C3is the closest\nother cluster to d1and so bpiq“1.3743 , as shown in Table 10.2[611]. The silhouette width\nford1is then calculated using Equation (10.2)[610]\n1.3743´0.401\nmaxp0.401,1.374q“0.7081\nTheapiq,bpiq, and spiqvalues in Table 10.2[611]are calculated similarly for each other\ninstance, and the overall silhouette for the clustering is the average of these values, in this\ncase, 0.656. This suggests a reasonably good clustering.\nThe individual silhouette widths for the instances in a dataset can also be used to produce\na useful visual tool for inspecting a clustering. A silhouette plot shows the silhouette\nwidth for each instance in the dataset grouped by the clusters to which they belong. Figure\n10.7[612]shows the silhouette plot for the clustering of the mobile phone customer dataset.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":665,"page_label":"611","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 611\nTable 10.2\nCalculating the silhouette for the ﬁnal clustering of the mobile phone customer dataset (Table\n10.1[604]) found using the k-means algorithm (with k“3). The overall silhouette index value is\n0.66.\nNearest\nID Cluster Cluster apiq bpiq spiq\n1 C1 C30.401 1.374 0.708\n2 C1 C30.695 1.811 0.616\n3 C1 C30.503 1.644 0.694\n4 C2 C30.484 1.628 0.703\n5 C1 C30.387 1.232 0.686\n6 C1 C30.445 0.970 0.541\n7 C3 C10.452 1.056 0.572\n8 C2 C30.599 1.364 0.561\n9 C2 C30.470 1.768 0.734\n10 C2 C30.504 1.978 0.745\n11 C1 C30.327 1.223 0.732\n12 C3 C10.433 1.537 0.719Nearest\nID Cluster Cluster apiq bpiq spiq\n13 C3 C10.5136 1.3592 0.6221\n14 C3 C10.4349 1.5738 0.7236\n15 C2 C30.5776 1.3480 0.5715\n16 C3 C10.4955 1.5409 0.6784\n17 C2 C10.7369 2.2757 0.6762\n18 C2 C30.4312 1.8473 0.7666\n19 C1 C30.3711 1.1682 0.6823\n20 C1 C30.4334 1.0006 0.5669\n21 C2 C10.6520 1.9710 0.6692\n22 C2 C30.4504 1.5457 0.7086\n23 C3 C10.3954 1.1654 0.6607\n24 C3 C10.5339 0.8880 0.3988\nSilhouette plots show a useful overview of a clustering, including the size of each cluster\nand an indication of how well each instance belongs to its cluster, which can be an easy\nway to identify outliers in clusterings.\nAnother approach to evaluating clustering is to use external criteria in which some mea-\nsure from outside the clustering is used as a proxy ground truth. For example, for the\nsample mobile phone customer data used in this section, the type of tariff that customers\nhad (e.g. gold,silver , orbronze ) could be used as a proxy ground truth, and the ability of\nthe clustering to separate the dataset into groups of the same tariff type could be used as a\nmeasure of the quality of the clustering. This assumes that we should expect the clustering\nto ﬁnd groups with similar tariff types, an assumption that could be based only on detailed\nknowledge of the domain. The performance measures used for classiﬁcation problems de-\nscribed in Chapter 9[533]can be used for this—for example, the F1measure is commonly\nused. There are also speciﬁc measures that are often used for this, for example normal-\nized mutual information (NMI), which is based on information-theoretic ideas including\nentropy , as discussed in Chapter 4[117]; however, these are outside the scope of this chapter.\nOne thing about evaluating unsupervised machine learning approaches that is a little\neasier than the supervised case is that we can do it without the need to divide a dataset","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":665,"page_label":"611","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"to ﬁnd groups with similar tariff types, an assumption that could be based only on detailed\nknowledge of the domain. The performance measures used for classiﬁcation problems de-\nscribed in Chapter 9[533]can be used for this—for example, the F1measure is commonly\nused. There are also speciﬁc measures that are often used for this, for example normal-\nized mutual information (NMI), which is based on information-theoretic ideas including\nentropy , as discussed in Chapter 4[117]; however, these are outside the scope of this chapter.\nOne thing about evaluating unsupervised machine learning approaches that is a little\neasier than the supervised case is that we can do it without the need to divide a dataset\ninto training, testing, and validation partitions. Although techniques such as k-fold cross\nvalidation can be useful in using external criteria for evaluation, in using internal crite-\nria for evaluation we typically just use all the data both for generating the clustering and\nevaluating the clustering.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":666,"page_label":"612","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"612 Chapter 10 Beyond Prediction: Unsupervised Learning\n620219531118152117422910182471323161214\nSilhouette width si0.0 0.2 0.4 0.6 0.8 1.0Silhouette Plot\nAverage silhouette width :  0.66n = 243  clusters   Cj\nj :  nj | avei∈Cj  si\n1 :   7  |  0.62\n2 :   9  |  0.68\n3 :   8  |  0.65\nFigure 10.7\nThe silhouette plot for the ﬁnal clustering of the mobile phone customer dataset (Table 10.1[604])\nfound using the k-means algorithm (with k“3).\n10.4.3 Choosing the Number of Clusters\nThe k-means clustering algorithm, like many other clustering algorithms, requires that\nthe analyst choose the value for kas an input to the algorithm. The absence of ground\ntruth, as discussed in the previous section, makes this choice notoriously difﬁcult. An-\nalysts need to balance the needs of their application—for example, in the mobile phone\ncustomer scenario, how many customer groups could the organization usefully make deci-\nsions about?—with the ability of an algorithm to ﬁnd meaningful groupings within a given\ndataset. The former criterion is very domain speciﬁc, and so it is not discussed further\nhere. The silhouette described in the previous section, however, can be used effectively to\nachieve the latter, and this approach to choosing kis described in this section.\nRegardless of the dataset used, the k-means clustering algorithm will always ﬁnd the\nnumber of clusters requested from it regardless of whether these deﬁne meaningful struc-\nture within the dataset. To determine the best value for kwe can repeatedly cluster the\ndata for all values of kwithin a given range, calculate a clustering performance measure\nfor each clustering found, and then select the value of kthat gives the clustering with the\nhighest performance measure. When the silhouette is used in this approach, it is often re-\nferred to as the silhouette method . Figure 10.8[613]shows each of the clusterings found in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":667,"page_label":"613","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 613\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume\n(a)k“2\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume (b)k“3\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume (c)k“4\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume (d)k“5\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume\n(e)k“6\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volume (f)k“7\n●●\n●\n●\n●\n●●●\n2 3 4 5 6 7 8 90.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nkSilhouette (g) Silhouette summary\nFigure 10.8\n(a)–(f) Different clusterings found for the mobile phone customer dataset in Table 10.1[604]for values\nofkinp2,9q. (g) shows the silhouette for each clustering.\nthe mobile phone customer dataset for values of kinr2,9susing k-means clustering, and\ntheir corresponding silhouettes (Figure 10.8(g)[613]). This shows that the clustering with\nthree clusters has the highest silhouette score and could be chosen as the most appropriate\nclustering given the assumptions that underlie the silhouette.\n10.4.4 Understanding Clustering Results\nAlthough internal measures such as the silhouette and external measures calculated against\na proxy ground truth are useful in measuring how well a clustering matches an ideal ex-\npectation, they do not tell analysts anything about what has been found in the clustering, or\nwhether or not the clustering will be useful for a particular task. Unfortunately, automated\napproaches to answering this question don’t really exist, and it falls on analysts to under-\nstand what a clustering result tells them about a dataset. The fundamental question that\nanalysts must answer is how do the members of a particular cluster differ from the overall\npopulation— what makes the members of this cluster special?\nAlthough fully automated approaches to answering this question do not exist, analysts\ncan use an approach based on the data exploration tools presented in Chapter 3[53]to un-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":668,"page_label":"614","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"614 Chapter 10 Beyond Prediction: Unsupervised Learning\nTable 10.3\nSummary statistics for the three clusters found in the mobile phone customer dataset in Table 10.1[604]\nusing k-means clustering ( k“3). Note, that the % missing and cardinality columns usually used\nare omitted here for legibility as these data quality issues will not arise in this simple example. They\ncould be included when this approach is used on realdatasets.\n1st3rdStd.\nFeature Cluster Count Min. Qrt. Mean Median Qrt. Max Dev.\nDATA\nUSAGEC1 8 -1.2329 -1.1246 -1.0121 -1.0237 -0.9256 -0.7426 0.1639\nC2 9 0.6259 0.8404 0.8912 0.8785 0.9285 1.1175 0.1471\nC3 7 -0.3666 -0.3005 -0.0491 -0.0345 0.2087 0.241 0.2732\nCALL\nVOLUMEC1 8 -0.7060 -0.3377 -0.1310 -0.0109 0.1116 0.1811 0.3147\nC2 9 -1.3601 -1.0450 -0.7273 -0.6028 -0.4560 -0.2168 0.4072\nC3 7 0.4215 0.5635 0.7022 0.7360 0.7905 1.0502 0.2204\nderstand the results of a clustering. The ﬁrst thing that should always be done is a careful\nexamination of a set of summary statistics describing the members of each cluster. This\ncan be done by preparing a data quality report6for each cluster that describes the in-\nstances that belong to that cluster. The second useful thing that can be done is to prepare a\nseries of data visualizations that examine the distribution of each feature for the members\nof each cluster found. This is equivalent to the approach described in Section 3.5.1[72]for\nvisually examining associations between descriptive features and a target feature, in which\nthe cluster that instances belong to are used instead of an actual target feature.\nA ﬁnal useful tool is to rank the importance of each descriptive feature in deﬁning mem-\nbership of each cluster. Some features are particularly important in deﬁning membership\nof certain clusters but not important for deﬁning membership of others. This can help an-\nalysts know where to focus when investigating the data quality reports and visualizations\ncreated describing each cluster. This can easily be done by calculating the information\ngain7for each descriptive feature as a predictor of binary ﬂags indicating membership of\neach cluster and ranking features according to these information gain values.8\nTo illustrate this approach using a simple example, Table 10.3[614]shows a data quality\nreport for each cluster found using k-means clustering (with k“3) on the mobile phone\ncustomer dataset (see Table 10.1[604]). Figure 10.9[615]shows the distributions of each feature","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":668,"page_label":"614","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of certain clusters but not important for deﬁning membership of others. This can help an-\nalysts know where to focus when investigating the data quality reports and visualizations\ncreated describing each cluster. This can easily be done by calculating the information\ngain7for each descriptive feature as a predictor of binary ﬂags indicating membership of\neach cluster and ranking features according to these information gain values.8\nTo illustrate this approach using a simple example, Table 10.3[614]shows a data quality\nreport for each cluster found using k-means clustering (with k“3) on the mobile phone\ncustomer dataset (see Table 10.1[604]). Figure 10.9[615]shows the distributions of each feature\nfor the full population and for each cluster so that the differences can be compared. Finally,\nTable 10.4[616]lists the features in the mobile phone customer dataset in order of importance\nin deﬁning membership of each of the three clusters found.\n6. See Section 3.1[54].\n7. See Section 4.2.3[127].\n8. This is much like the rank and prune approach to feature selection described in Section 5.4.6[223].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":669,"page_label":"615","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Data.UsageDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nclustering = 1\nData.UsageDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5\nclustering = 2\nData.UsageDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5\nclustering = 3\nData.UsageDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5\n(a) D ATA USAGE\nCall.VolumeDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.1 0.2 0.3 0.4 0.5\nclustering = 1\nCall.VolumeDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5\nclustering = 2\nCall.VolumeDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5\nclustering = 3\nCall.VolumeDensity\n−1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5 (b) C ALL VOLUME\nFigure 10.9\n(a)–(b) Visualizations of the distributions of the descriptive features in the mobile phone customer dataset in Table 10.1[604]across the complete\ndataset, and divided by the clustering found using k-means clustering ( k“3).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":670,"page_label":"616","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"616 Chapter 10 Beyond Prediction: Unsupervised Learning\nTable 10.4\nInformation gain for each descriptive feature as a predictor of membership of each cluster based on\nthe clustering of the mobile phone customer dataset in Table 10.1[604]found using k-means clustering\n(k“3).\nC1 C2 C3\nInfo. Info. Info.\nFeature Gain Feature Gain Feature Gain\nDATA.USAGE 0.9183 D ATA.USAGE 0.9544 C ALL.VOLUME 0.8709\nCALL.VOLUME 0.2117 C ALL.VOLUME 0.5488 D ATA.USAGE 0.2479\nTable 10.4[616]shows that membership of clusters C1andC2is most associated with D ATA\nUSAGE , whereas membership of C3is most associated with C ALL VOLUME . In looking\nat the summary statistics and visualizations with these priorities in mind, it is clear that the\ncustomers in C1are characterized most strongly by their low Data Usage , customers in C2\nby their high Data Usage , and customers in C3by their high Call Volume . Although this\nexample is small, the same approach can be used effectively for large multivariate datasets,\nwith the only difference being a need to examine summary statistics and visualizations for\nmany more features. The prioritization given by the information gain values is extremely\nhelpful for dealing with this.\n10.4.5 Agglomerative Hierarchical Clustering\nThek-means clustering algorithm and other algorithms like it impose an expected global\nstructure onto the clustering process—essentially, the k-means algorithm searches for tightly\npacked spherical clusters. Sometimes this expected global structure does not match ac-\ntual patterns within a dataset that we might want to take advantage of. Figure 10.10[617]\nillustrates this using three well-known simple two-dimensional artiﬁcial datasets: blobs ,\ncircles , and half-moons . In each case there is an intuitive clustering that we can observe in\nthe visualizations—for the blobs dataset there are three clusters, one for each blob; for the\ncircles dataset there are two clusters, one for each ring; and for the half-moons dataset there\nare two clusters, one for each half-moon shape. The clustering returned by the k-means\nclustering algorithm, however, can accurately ﬁnd these intuitive clusterings only for the\nblobs dataset. The reason is that the intuitive clusters in the other two datasets do not\nconform to the assumption of spherical clusters that underlies the k-means clustering al-\ngorithm. To ﬁnd these other types of clusterings, it can be more useful to use a clustering","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":670,"page_label":"616","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"circles , and half-moons . In each case there is an intuitive clustering that we can observe in\nthe visualizations—for the blobs dataset there are three clusters, one for each blob; for the\ncircles dataset there are two clusters, one for each ring; and for the half-moons dataset there\nare two clusters, one for each half-moon shape. The clustering returned by the k-means\nclustering algorithm, however, can accurately ﬁnd these intuitive clusterings only for the\nblobs dataset. The reason is that the intuitive clusters in the other two datasets do not\nconform to the assumption of spherical clusters that underlies the k-means clustering al-\ngorithm. To ﬁnd these other types of clusterings, it can be more useful to use a clustering\nalgorithm that is driven more by local relationships in a dataset than an expected global\nstructure.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":671,"page_label":"617","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 617\n−10 −5 0 5 10−10 −5 0 510\nf1f2\n(a) Blobs Dataset\n−10 −5 0 5 10−10 −5 0 510\nf1f2 (b)k-means\n−10 −5 0 5 10−10 −5 0 510\nf1f2 (c) AHC\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nf1f2\n(d) Circles Dataset\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nf1f2 (e)k-means\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nf1f2 (f) AHC\n−1.0−0.50.00.51.01.52.0−1.0−0.50.00.51.01.52.0\nf1f2\n(g) Half-moons Dataset\n−1.0−0.50.00.51.01.52.0−1.0−0.50.00.51.01.52.0\nf1f2 (h)k-means\n−1.0−0.50.00.51.01.52.0−1.0−0.50.00.51.01.52.0\nf1f2 (i) AHC\nFigure 10.10\n(a)–(i) A plot of the blobs ,circles , and half-moons datasets and the clusterings achieved by the k-\nmeans clustering and agglomerative hierarchical clustering algorithms (where kis set to 3,2, and 2,\nrespectively).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":672,"page_label":"618","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"618 Chapter 10 Beyond Prediction: Unsupervised Learning\nThe agglomerative hierarchical clustering (AHC) algorithm is a simple, effective,\nbottom-up clustering approach that is driven by local structure within a dataset rather than\na global expectation of what a cluster structure should be. A pseudocode description of the\nAHC algorithm is given in Algorithm 9[601].\nAlgorithm 12 Pseudocode description of the agglomerative hierarchical clustering al-\ngorithm.\nRequire: a dataset Dcontaining ntraining instances, d1,..., dn\nRequire: a distance measure, Dist, to compare distances between instances\nRequire: a linkage method, L, to compare distances between clusters\n1:initialize the hierarchy level, h“1\n2:divide Dinto a set of ndisjoint clusters, C“tC1,...,Cnu, with one instance in each\ncluster\n3:repeat\n4: using distance measure Dist and linkage method L, ﬁnd the nearest pair of clusters,\nCiandCj, in the current clustering\n5: merge CiandCjto form a new cluster Cn`h\n6: remove the old clusters from the clustering: CÐCztCi,Cju\n7: add the new cluster to the clustering: CÐCYCn`h\n8: hÐh`1\n9:until all the instances join into a single cluster\nThe AHC algorithm begins by considering each instance in a dataset to be the only\nmember of a cluster, which gives ninitial clusters, C1toCn. The two clusters that are\nnearest are then merged (or agglomerated ) to form a new cluster. This process repeats\nover and over until a single cluster containing all instances in the dataset is formed. Figure\n10.10[617]shows how AHC can be used for ﬁnd the intuitive clusterings in the half-moons\nand circles datasets as well as the blobs dataset.\nThis AHC algorithm requires two decisions to be made in order for it to be complete.\nThe ﬁrst is the distance measure, Dist, to be used to compare instances and clusters. Any\nof the distance measures discussed in this book (as well as many others not discussed in\nthis book) can be used for this. The second is the linkage method ,L, that will be used\nto allow distances between whole clusters rather than just single instances to be compared.\nThe challenge is if two clusters, each containing multiple instances, have been found, how\nshould the distance between them be calculated? Should it be based on the distance be-\ntween the centroids of two clusters, or the distance between the two closest instances in\ntwo clusters, or the average distance between all instances in two clusters, or some other","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":672,"page_label":"618","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"The ﬁrst is the distance measure, Dist, to be used to compare instances and clusters. Any\nof the distance measures discussed in this book (as well as many others not discussed in\nthis book) can be used for this. The second is the linkage method ,L, that will be used\nto allow distances between whole clusters rather than just single instances to be compared.\nThe challenge is if two clusters, each containing multiple instances, have been found, how\nshould the distance between them be calculated? Should it be based on the distance be-\ntween the centroids of two clusters, or the distance between the two closest instances in\ntwo clusters, or the average distance between all instances in two clusters, or some other\nmethod? Several different linkage methods for AHC exist in the literature; some of the\nmost common are","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":673,"page_label":"619","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 619\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8−0.6−0.4−0.20.00.2\nData UsageCall V olume\n(a) single\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8−0.6−0.4−0.20.00.2\nData UsageCall V olume\n (b) complete\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8−0.6−0.4−0.20.00.2\nData UsageCall V olume\n(c) centroid\n−1.5 −1.0 −0.5 0.0 0.5 1.0−0.8−0.6−0.4−0.20.00.2\nData UsageCall V olume\n (d) average\nFigure 10.11\n(a)–(d) Different linkage methods that can be used to compare the distances between clusters in\nagglomerative hierarchical clustering. (Arrows for only some indicative distances are shown in the\naverage linkage diagram (d).)\n‚single linkage: the distance between the most similar instances in two clusters is used\nas the overall distance between the clusters;\n‚complete linkage: the distance between the most dissimilar instances in two clusters is\nused as the overall distance between the clusters;\n‚average linkage: the average of the distances between all pairs of instances in two\nclusters is used as the overall distance between the clusters; and\n‚centroid linkage: the distance between the centroids of two clusters is used as the overall\ndistance between the clusters.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":674,"page_label":"620","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"620 Chapter 10 Beyond Prediction: Unsupervised Learning\nTable 10.5\nDistance matrices that detail the ﬁrst three iterations of the AHC algorithm applied to the reduced\nversion of the mobile phone customer dataset in Table 10.1[604].\n(a) A distance matrix for the instances in the dataset.\nd4d15 d8d11 d5d19d24 d7d23\nd40.00\nd150.28 0.00\nd80.28 0.06 0.00\nd112.12 1.89 1.94 0.00\nd52.25 2.02 2.06 0.18 0.00\nd192.19 1.95 2.00 0.16 0.07 0.00\nd241.66 1.39 1.42 0.81 0.83 0.76 0.00\nd71.84 1.56 1.58 0.96 0.94 0.89 0.27 0.00\nd231.79 1.51 1.53 1.08 1.06 1.00 0.33 0.12 0.00(b) The distance matrix after one iteration of AHC.\nd4 C10d11 d5d19d24 d7d23\nd40.00\nC100.28 0.00\nd112.12 1.89 0.00\nd52.25 2.02 0.18 0.00\nd192.19 1.95 0.16 0.07 0.00\nd241.66 1.39 0.81 0.83 0.76 0.00\nd71.84 1.56 0.96 0.94 0.89 0.27 0.00\nd231.79 1.51 1.08 1.06 1.00 0.33 0.12 0.00\n(c) The distance matrix after two iterations of AHC.\nd4 C10d11 C11d24 C12\nd40.00\nC100.28 0.00\nd112.12 1.89 0.00\nC112.19 1.95 0.16 0.00\nd241.66 1.39 0.81 0.76 0.00\nC121.79 1.51 0.97 0.89 0.27 0.00(d) The distance matrix after three iterations of AHC.\nd4 C13 C11d24 C12\nd40.00\nC130.28 0.00\nC112.19 0.16 0.00\nd241.66 0.81 0.76 0.00\nC121.79 0.97 0.89 0.27 0.00\nFigure 10.11[619]illustrates these options. In the AHC examples shown in Figure 10.10[617],\nEuclidean distance and single linkage are used. The choice of linkage method can lead\nto quite different results when AHC is used. For example, using centroid linkage leads to\nresults very similar to a k-means clustering approach, whereas using single linkage leads\nto results much more heavily reliant on local distances within a dataset.\nWe can illustrate how the AHC algorithm works using a reduced version of the mobile\nphone customer dataset from Table 10.1[604]. This dataset contains just nine instances (the\ninstance labels have been kept consistent so that this example can be compared with pre-\nvious ones) and is shown in Figure 10.12(a)[621]. AHC begins by considering each instance\nin the dataset as a simple cluster containing just one instance. The distance between each\nof these single-instance clusters is then calculated (in this example Euclidean distance is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":675,"page_label":"621","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 621\n●●\n●●\n●●\n●●\n●\n−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5−1.0 −0.5 0.0 0.5 1.0\nData UsageCall Volumed5 d19\nd11d7\nd24d23\nd15d8\nd4\n(a)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall V olume (b)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall V olume (c)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall V olume\n(d)\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0\nData UsageCall V olume (e)\nd4\nd15\nd8\nd11\nd5\nd19\nd24\nd7\nd230.0 0.4 0.8 1.2Cluster Dendrogram\nhclust (*, \"single\")InstancesHeight (f)\nFigure 10.12\n(a) A plot of a reduced version of the mobile phone customer dataset given in Table 10.1[604]. (b)\nAt the ﬁrst iteration of the AHC algorithm the ﬁrst pair of instances is combined into a cluster,\nC10. (c) After three iterations of the AHC algorithm, three pairs of instances have been combined\ninto clusters, C10,C11, and C12. (d) At the fourth iteration of AHC, the ﬁrst hierarchical cluster\ncombination is created when a single instance, d11is combined with the cluster C10to create a new\ncluster, C13.\nused). Table 10.5(a)[620]shows these distances and is referred to as a distance matrix .9The\npair of instances in the dataset that are closest together are then selected and combined\ninto a cluster. In this example d15andd8are separated by a distance of just 0.06and are\ncombined together into the cluster C10. This is illustrated in Figure 10.12(b)[621].\n9. Because Euclidean distance between any two points is symmetrical ( distpa,bq“distpb,aq), we show only\nhalf of the distance matrix.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":676,"page_label":"622","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"622 Chapter 10 Beyond Prediction: Unsupervised Learning\nIn the next iteration of the AHC algorithm the distance between this new cluster and all\nother clusters is calculated. In this example single linkage is used and so the distance be-\ntween two clusters is calculated as the minimum distance between their member instances.\nTable 10.5(b)[620]shows an updated distance matrix including C10. Over the next two iter-\nations, two more pairs of instances from the original dataset are combined into clusters:\nd5andd19intoC11, and d7andd23intoC12. This is illustrated in Figure 10.12(c)[621], and\nTable 10.5(b)[620]shows an updated distance matrix including these new clusters.\nThe next iteration of the algorithm is interesting because the smallest distance found in\nthe distance matrix is a distance of 0.16between cluster C10and instance d11. These are\nmerged into a new cluster, C13, as shown in Figure 10.12(d)[621]. This is the clearest indica-\ntion so far of the hierarchical nature of AHC, as a cluster created at a previous iteration of\nthe algorithm will be merged into a larger cluster. Table 10.5(d)[620]shows the distance ma-\ntrix after this new cluster has been created. The algorithm then continues merging clusters\nuntil only a single cluster containing all instances remains. This process is illustrated in\nFigure 10.12(e)[621]. It is also possible to directly illustrate the hierarchical nature of AHC\nusing a dendrogram . In a dendrogram each instance in a dataset is represented by its ID\nlabel at the bottom of the ﬁgure,10and the horizontal linkages indicate where clusters have\nbeen created. The vertical gaps between cluster linkages indicate the distance between\nthe two clusters that have been merged. For example, at the bottom of the hierarchy the\nvertical gaps between clusterings are very small because the clusters are very close, and as\nwe move farther up the hierarchy, the gaps get larger as clusters that are farther apart are\nmerged.\nOne advantage of AHC is that the number of clusters to be found, k, is not a required\ninput for the algorithm. Rather, the algorithm ﬁnds a hierarchical agglomeration (or group-\ning) of the instances in a dataset that can then be used to cluster the instances into any num-\nber of groups. To illustrate this, Figure 10.13(a)[623]returns to the full mobile phone cus-\ntomer dataset from Table 10.1[604]and shows the dendrogram capturing the result of running","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":676,"page_label":"622","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the two clusters that have been merged. For example, at the bottom of the hierarchy the\nvertical gaps between clusterings are very small because the clusters are very close, and as\nwe move farther up the hierarchy, the gaps get larger as clusters that are farther apart are\nmerged.\nOne advantage of AHC is that the number of clusters to be found, k, is not a required\ninput for the algorithm. Rather, the algorithm ﬁnds a hierarchical agglomeration (or group-\ning) of the instances in a dataset that can then be used to cluster the instances into any num-\nber of groups. To illustrate this, Figure 10.13(a)[623]returns to the full mobile phone cus-\ntomer dataset from Table 10.1[604]and shows the dendrogram capturing the result of running\nthe AHC algorithm on this dataset. Once this hierarchical tree structure has been found,\nit can be cutto any level to give a clustering with that many clusters. Figure 10.13(b)[623]\nshows the tree cut for three clusters and the resultant clustering, and Figure 10.13(c)[623]\nshows the same illustrations for six clusters. The techniques described in Section 10.4.2[607]\nfor evaluating clusterings can be used here to set the best value for k.\n10. The instances have been arranged in an order that leads to a nice dendrogram visualization, and this was also\nthe order used in the distance matrices to make it easy to follow the combination of instances into clusters.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":677,"page_label":"623","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"8\n15\n4\n9\n17\n22\n21\n10\n18\n1\n11\n5\n19\n6\n20\n2\n3\n24\n7\n23\n16\n13\n12\n140.0 0.2 0.4 0.6 0.8 1.0Cluster Dendrogram\nhclust (*, \"single\")InstancesHeight(a) AHC result\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume\n(b) Clustering cut at k“3\n−1.0 −0.5 0.0 0.5 1.0−1.0−0.5 0.00.51.0\nData UsageCall V olume\n(c) Clustering cut at k“6\nFigure 10.13\n(a) A plot of the hierarchical grouping of the instances in the mobile phone customer dataset from\nTable 10.1[604]found by the AHC algorithm (using Euclidean distance and single linkage). (b) The\nclustering returned when the tree is cut at k“3. (c) The clustering returned when the tree is cut at\nk“6.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":678,"page_label":"624","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"624 Chapter 10 Beyond Prediction: Unsupervised Learning\nAnother advantage of AHC is that it is deterministic, and doesn’t suffer from the impact\nof different seeds as k-means or k-means++ does. This means that it will give exactly\nthe same result every time it is run on the same dataset and that the issues around ﬁnding\nseeds discussed in Section 10.4.1[605]do not arise. These two advantages are present in most\nhierarchical clustering algorithms.\n10.4.6 Representation Learning with Auto-Encoders\nThe second use case we described for unsupervised learning in Section 10.2[598]wasrepre-\nsentation learning . Representation learning essentially tries to automatically extract new\ndescriptive features from a dataset. This can be an attractive alternative to the manual work\nof designing features that we described in Chapter 2[23]. In this use case, unsupervised\nmachine learning is used as one part of a larger machine learning pipeline. Usually the\noutputs from the unsupervised work are consumed in a supervised machine learning task,\nor another unsupervised machine learning task. Neural network models are particularly\nuseful for representation learning, and this is a large part of the promise of deep learning ,\nas discussed in Chapter 8[381]. In this section we look at how auto-encoder models, a par-\nticular type of neural network, can be used to learn a new feature representation that can\nbe a useful step as part of a larger machine learning process.\nAnauto-encoder model is a special type of feedforward neural network that is trained\nto reproduce its inputs at its output layer. This may seem like a trivial task—it is essen-\ntially an identity function —but it is made more interesting by a network architecture that\ntransforms the data through a series of narrower and narrower layers. This has the impact\nof ﬁrst greatly reducing the dimensionality of the input data, before reproducing the inputs\nfrom this lower-dimensional representation. Figure 10.14[625]shows the architecture of a\ntypical auto-encoder network. The ﬁrst half of an auto-encoder, up to the output of the\nnarrowest layer in the middle, is known as an encoder , and the second half of the network\nis known as a decoder . The narrow middle layer is often referred to as a bottleneck layer .\nFor an auto-encoder network to be able to reproduce the feature values for a query in-\nstance that are presented at its input layer at its output layer, the low-dimensional repre-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":678,"page_label":"624","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"transforms the data through a series of narrower and narrower layers. This has the impact\nof ﬁrst greatly reducing the dimensionality of the input data, before reproducing the inputs\nfrom this lower-dimensional representation. Figure 10.14[625]shows the architecture of a\ntypical auto-encoder network. The ﬁrst half of an auto-encoder, up to the output of the\nnarrowest layer in the middle, is known as an encoder , and the second half of the network\nis known as a decoder . The narrow middle layer is often referred to as a bottleneck layer .\nFor an auto-encoder network to be able to reproduce the feature values for a query in-\nstance that are presented at its input layer at its output layer, the low-dimensional repre-\nsentation at the middle bottleneck layer needs to capture almost all the useful information\ncontained in the original input features. This is what makes auto-encoders useful for rep-\nresentation learning. The outputs of the bottleneck layer in the network can be used as a\nnew representation of the original input features. This type of representation is often re-\nferred to as an embedding because the original features have been embedded into a new\nlower-dimensional space.\nAn auto-encoder model can be trained like any other feedforward neural network using\nthebackpropagation of error algorithm .11The only difference is that rather than having\n11. See Chapter 8[381].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":679,"page_label":"625","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 625\n32 units64 units 64 units\n32 units\n16 units 16 units\n6 units\nInput\nLayerOutput\nLayerBottleneck\nLayer\nEncoder Decoder\nFigure 10.14\nThe architecture of an auto-encoder network made up of an encoder and a decoder connected by a\nbottleneck layer.\n.\na separate target feature vector against which loss is measured, the loss functions used\nto train these measure the ability of the network to reproduce the inputs for particular\ninstances at its output layer. For this type of network, it is common to use rectiﬁed linear\nactivation functions in the units in all layers except for the ﬁnal output layer, at which either\nsigmoid orlinear activation functions are usually used.12Loss in auto-encoder networks is\ntypically measured using mean squared error loss ,13rather than the loss functions more\ncommonly used for classiﬁcation problems.\nTo illustrate the use of an auto-encoder network for feature generation, we use a dataset\nof simple handwritten digits.14This dataset contains a library of 1,797small (8 pixels by 8\npixels) grayscale images of handwritten digits (0–9). Figure 10.15(a)[626]shows a selection\nof these digits. Grayscale images are stored as a matrix of pixel values in the range r0,1s\nwhere 0represents a black pixel and 1represents a white one. The pixels from an 8-by-8\npixel image can be ﬂattened into a single vector of 64values to give an ABT in which each\n12. See Chapter 8[381]for descriptions of these different activation functions.\n13. See Section 9.4.5[574].\n14. This dataset is from the UCI Machine Learning repository Dua and Graff (2017) and was originally described\nby Alimoglu and Alpaydin (1996).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":680,"page_label":"626","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"626 Chapter 10 Beyond Prediction: Unsupervised Learning\n(a) Original images from the handwritten digits dataset\n(b) Auto-encoder image reconstructions before network training\n(c) Auto-encoder image reconstructions after minimal network training (10 epochs)\n(d) Auto-encoder image reconstructions after complete network training ( 1,000epochs)\nFigure 10.15\n(a) A selection of images from the handwritten digits dataset; (b) image reconstructions generated by\nthe auto-encoder network before training; (c) image reconstructions generated by the auto-encoder\nnetwork after minimal training ( 10epochs); and (d) image reconstructions generated by the auto-\nencoder network after complete training ( 1,000epochs).\ndescriptive feature for an instance (an image) represents the grayscale value for a particular\npixel in that image. This gives a large set of low-information descriptive features.\nAn auto-encoder can be trained to learn a more compact representation of these images.\nThe architecture of the auto-encoder used in this example is shown in Figure 10.14[625]. The\nunits in all hidden layers use a rectiﬁed linear activation function. The units in the output\nlayer use a sigmoid activation function. The bottleneck layer contains six units, and this\nis the dimensionality of the new representation, or embedding , generated. The network is\ntrained for 1,000epochs using mini-batch gradient descent with a batch size of 32.15\nFigure 10.15(b)[626]shows examples of the reconstructions generated by the auto-encoder\nnetwork before it has been trained for the digits shown in Figure 10.15(a)[626]. The ini-\ntial weights in the auto-encoder are randomly initialized, and therefore it should not be a\nsurprise that these reconstructions bear no resemblance to the original images and are ba-\nsically noise. Figure 10.15(c)[626]shows reconstructions of the same images after just ﬁve\n15. See Chapter 8[381]for a discussion of mini-batch gradient descent.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":681,"page_label":"627","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.4 Extensions and Variations 627\nepochs of network training. At this stage, these reconstructed images are already beginning\nto resemble the originals on which they are based. Figure 10.15(d)[626]shows the ﬁnal re-\nconstructions generated by the network after 1,000epochs of network training. Although\nthere is some blurring around the edges, these look remarkably similar to the original im-\nages.\nFigure 10.16[627]shows a more detailed view of the reconstructions of the ﬁrst example\nfrom Figure 10.15(a)[626], an image of the digit 2. Reconstructed images and the underlying\npixel values from before, during, and after training are shown, as well as the correspond-\nTraining Image Pixel Values Error\nOriginal\n»\n——————————–0.00 0.19 0.94 1.00 0.88 0.06 0.00 0.00\n0.00 0.12 0.75 0.81 1.00 0.25 0.00 0.00\n0.00 0.00 0.00 0.38 1.00 0.19 0.00 0.00\n0.00 0.00 0.06 0.94 0.62 0.00 0.00 0.00\n0.00 0.00 0.38 1.00 0.25 0.00 0.00 0.00\n0.00 0.12 0.94 0.62 0.00 0.00 0.00 0.00\n0.00 0.25 1.00 0.69 0.50 0.69 0.19 0.00\n0.00 0.19 1.00 1.00 1.00 0.75 0.19 0.00ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\n0 Epochs\n»\n——————————–0.51 0.48 0.49 0.49 0.51 0.50 0.49 0.50\n0.51 0.50 0.49 0.52 0.51 0.51 0.51 0.49\n0.51 0.52 0.50 0.51 0.50 0.49 0.52 0.50\n0.50 0.51 0.51 0.51 0.50 0.50 0.49 0.50\n0.50 0.47 0.50 0.52 0.51 0.50 0.52 0.50\n0.51 0.51 0.50 0.49 0.53 0.50 0.51 0.49\n0.51 0.52 0.49 0.51 0.51 0.50 0.51 0.50\n0.50 0.50 0.48 0.51 0.50 0.51 0.51 0.51ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ0.1876\n10 Epochs\n»\n——————————–0.00 0.00 0.42 0.83 0.76 0.33 0.03 0.00\n0.01 0.12 0.80 0.76 0.74 0.62 0.06 0.00\n0.00 0.15 0.60 0.35 0.54 0.58 0.07 0.00\n0.00 0.09 0.49 0.57 0.68 0.46 0.11 0.00\n0.00 0.08 0.31 0.57 0.68 0.48 0.13 0.01\n0.00 0.05 0.31 0.37 0.41 0.51 0.19 0.00\n0.00 0.02 0.49 0.59 0.59 0.63 0.21 0.01\n0.00 0.01 0.45 0.85 0.74 0.39 0.09 0.01ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ0.0685\n1,000Epochs\n»\n——————————–0.00 0.06 0.71 0.87 0.71 0.10 0.00 0.00\n0.00 0.32 0.70 0.77 0.86 0.30 0.00 0.00\n0.00 0.11 0.09 0.75 0.97 0.24 0.00 0.00\n0.00 0.00 0.00 0.86 0.93 0.08 0.00 0.00\n0.00 0.00 0.02 0.88 0.62 0.00 0.00 0.00\n0.00 0.01 0.68 0.89 0.24 0.04 0.01 0.00\n0.00 0.32 0.91 0.89 0.53 0.51 0.19 0.00\n0.00 0.03 0.78 0.89 0.83 0.69 0.21 0.00ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ0.0179\nFigure 10.16\nAn image of the digit 2and reconstructions of this image by the auto-encoder after various amounts\nof network training. The pixel values of the reconstructed images are shown alongside the images,\nas is the reconstruction error calculated by comparing these to the pixel values of the original image.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":682,"page_label":"628","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"628 Chapter 10 Beyond Prediction: Unsupervised Learning\ning reconstruction errors. It is clear from both the quality of the image shown and the\nreconstruction errors that the quality of the reconstruction improves as training progresses.\nAfter it has been trained to accurately encode its inputs into a low-dimensional space\nand then decode them back into the original feature space, an auto-encoder can be used\nfor feature generation by focusing on the output of the ﬁrst encoder part of the network.\nThe outputs of the ﬁnal layer in the encoder, the bottleneck layer , can be used as a new\ntransformed representation of the original dataset. A full dataset can be passed through the\nencoder network, and the outputs of the bottleneck layer can be saved as new generated\nfeatures. These new generated features can then replace the original descriptive features in\nan ABT for later tasks in a pipeline, for example, training a supervised machine learning\nmodel. This process is illustrated in Figure 10.17[628].\nGenerated Features\n.... .... ....\n.... .... ....\n.... .... ....\n.... .... ....Prediction\nModel\nMachine\nLearning\nAlgorithm\nFigure 10.17\nThe process of using an unsupervised auto-encoder network to generate a feature representation used\nto train a supervised model.\n10.5 Summary\nThis chapter moved away from the supervised machine learning techniques for training\npredictive models discussed in the rest of the book to focus on unsupervised machine\nlearning . Unsupervised machine learning techniques are used in the absence of a target\nfeature and model the underlying structure within the descriptive features in a dataset. We\ncan think of the output of most unsupervised machine learning models as new generated","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":683,"page_label":"629","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.6 Further Reading 629\nfeatures that can be appended to the original dataset to augment orenrich it. There are\ntwo key main use cases for unsupervised learning: clustering and representation learning.\nInclustering , unsupervised algorithms are used to partition the instances in a dataset into\ncoherent groups. The assignment of each instance in a dataset to one of these groups is the\noutput of the clustering process, and a generated feature capturing these assignments can\nbe appended to the original dataset. Two clustering techniques were presented in detail:\nk-means clustering andagglomerative hierarchical clustering (AHC). These are well-\nknown, reasonably effective approaches to clustering. AHC has an advantage over k-means\nthat it does not require kto be set before the algorithm starts. AHC is, however, much more\ncomputationally expensive than k-means, which can be a barrier to using it on very large\ndatasets.\nThe other unsupervised machine learning use case covered in this chapter was repre-\nsentation learning . Here unsupervised machine learning techniques are used to learn new\nsets of generated features to represent instance in a dataset. Neural network models are\nespecially effective in this use case, and the chapter presented an example of using an\nauto-encoder to learn a feature representation that could be used by a supervised machine\nlearning model. Using unsupervised learning to generate feature representations that could\nbe used in later supervised models (e.g., Hinton et al. (2006) and Hinton (2005)) was the\nstarting point for the wave of renewed interest in neural network models that began in the\nearly 2000s and became known as deep learning .\nApplications of unsupervised learning are widespread, including customer segmenta-\ntions (Berry and Linoff, 2004), anomaly detection (Chandola et al., 2009), and analyzing\npeople’s movement patterns (Li et al., 2015). Designing solutions based on unsupervised\nmachine learning techniques can be more creative than designing solutions based on super-\nvised learning because the solutions tend not to follow quite so obvious a pattern. Finally,\nunsupervised learning is a fascinating research area, and it is probably fair to say that it has\nmany more signiﬁcant open research challenges than supervised learning.\n10.6 Further Reading\nFor more detail on unsupervised machine learning algorithms (Friedman et al., 2001) has\na fairly comprehensive unsupervised learning section that gives a broader sweep of ap-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":683,"page_label":"629","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tions (Berry and Linoff, 2004), anomaly detection (Chandola et al., 2009), and analyzing\npeople’s movement patterns (Li et al., 2015). Designing solutions based on unsupervised\nmachine learning techniques can be more creative than designing solutions based on super-\nvised learning because the solutions tend not to follow quite so obvious a pattern. Finally,\nunsupervised learning is a fascinating research area, and it is probably fair to say that it has\nmany more signiﬁcant open research challenges than supervised learning.\n10.6 Further Reading\nFor more detail on unsupervised machine learning algorithms (Friedman et al., 2001) has\na fairly comprehensive unsupervised learning section that gives a broader sweep of ap-\nproaches than those covered in this chapter. As discussed in the introduction, the goal of\nthis chapter is to give a ﬂavor of the most important unsupervised machine learning tech-\nniques, and there are many more clustering algorithms not covered here. For example,\nk-means is a special case of the Gaussian mixture model (Murphy, 2012) approach to\nclustering (assuming spherical distributions), which in turn has been extended into model-\nbased clustering (Scrucca et al., 2016) algorithms, all of which can be very effective. The\nexpectation maximization algorithm sits behind all of these and more than warrants care-\nful study (Moon, 1996). Spectral clustering (Ng et al., 2002) is another approach worth","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":684,"page_label":"630","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"630 Chapter 10 Beyond Prediction: Unsupervised Learning\ninvestigating, and for very large spatial datasets, the DBScan (Ester et al., 1996) can be a\ngood approach.\nBerry and Linoff (2004) provide a good specialized treatment of clustering algorithms\nfor customer segmentation applications. Han et al. (2011) is also very good on describing\nunsupervised machine learning techniques with customer applications in mind.\nThe auto-encoders presented in this chapter are fairly simple, and more sophisticated\napproaches have emerged, for example, convolutional auto-encoders ,de-noising auto-\nencoders , and variational auto-encoders . Guo et al. (2016) provide a readable, coherent\noverview of these different types.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":685,"page_label":"631","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.7 Exercises 631\n10.7 Exercises\n1.The following table shows a small dataset in which each instance describes measure-\nments taken using three sensors when a valve in an oil well was opened. The three\ndescriptive features, P RESSURE , TEMPERATURE , and V OLUME measure character-\nistics of the oil ﬂowing through the valve when it was opened. The k-means clus-\ntering approach is to be applied to this dataset with k“3and using Euclidean\ndistance . The initial cluster centroids for the three clusters C1,C2, and C3are\nc1“⟨´0.929,´1.040,´0.831⟩,c2“⟨´0.329,´1.099,0.377⟩, and c3“⟨´0.672,\n´0.505,0.110⟩. The following table also shows the distance to these three cluster\ncenters for each instance in the dataset.\nCluster Distances Iter. 1\nID P RESSURE TEMPERATURE VOLUME Distpdi,c1q Distpdi,c2q Distpdi,c3q\n1 -0.392 -1.258 -0.666 0.603 1.057 1.117\n2 -0.251 -1.781 -1.495 1.204 1.994 2.093\n3 -0.823 -0.042 1.254 2.314 1.460 1.243\n4 0.917 -0.961 0.055 2.049 1.294 1.654\n5 -0.736 -1.694 -0.686 0.697 1.284 1.432\n6 1.204 -0.605 0.351 2.477 1.611 1.894\n7 0.778 -0.436 -0.220 1.911 1.422 1.489\n8 1.075 -1.199 -0.141 2.125 1.500 1.896\n9 -0.854 -0.654 0.771 1.650 0.793 0.702\n10 -1.027 -0.269 0.893 1.891 1.201 0.892\n11 -0.288 -2.116 -1.165 1.296 1.848 2.090\n12 -0.597 -1.577 -0.618 0.666 1.136 1.298\n13 -1.113 -0.271 0.930 1.930 1.267 0.960\n14 -0.849 -0.430 0.612 1.569 0.879 0.538\n15 1.280 -1.188 0.053 2.384 1.644 2.069\n(a)Assign each instance to its nearest cluster to generate the clustering at the ﬁrst\niteration of k-means on the basis of the initial cluster centroids.\n(b)On the basis of the clustering calculated in Part (a), calculate a set of new cluster\ncentroids.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":686,"page_label":"632","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"632 Chapter 10 Beyond Prediction: Unsupervised Learning\n2.The following table shows details of two different clusterings of the dataset from Ques-\ntion 1—one with k“2and one with k“3—and partial workings to calculate the\nsilhouette for the clusterings.\nk“2clustering\nNearest\nID Cluster Cluster apiqbpiqspiq\nd1 C1 C2 ?? 1.898 ??\nd2 C1 C21.608 2.879 0.442\nd3 C2 C10.624 2.594 0.76\nd4 C1 C21.261 2.142 0.411\nd5 C1 C21.452 2.098 0.308\nd6 C1 ?? ?? ?? ??\nd7 C1 C21.42 2.061 0.311\nd8 C1 C21.272 2.432 ??\nd9 C2 C10.496 2.067 0.76\nd10 C2 C10.344 2.375 ??\nd11 C1 C21.565 2.802 0.441\nd12 C1 C21.338 ?? ??\nd13 C2 C10.379 2.444 0.845\nd14 C2 C1 ?? 2.056 ??\nd15 C1 C21.425 2.53 0.437k“3clustering\nNearest\nID Cluster Cluster apiqbpiqspiq\nd1 C1 C20.732 1.681 0.565\nd2 C1 ?? ?? ?? ??\nd3 C3 C20.624 2.422 0.742\nd4 C2 C10.482 1.884 ??\nd5 C1 C30.619 2.098 0.705\nd6 C2 C30.68 2.24 0.697\nd7 C2 C10.777 1.935 0.598\nd8 C2 C10.558 1.842 0.697\nd9 C3 C10.496 2.04 0.757\nd10 C3 ?? 0.344 ?? ??\nd11 C1 C20.769 2.201 0.651\nd12 C1 C20.592 1.935 0.694\nd13 C3 C10.379 2.436 0.844\nd14 C3 C10.459 2.038 ??\nd15 C2 C10.579 2.101 0.725\n(a)A number of values are missing from these workings (indicated by ??). Calculate\nthe missing values. The distances between each instance in the dataset from Ques-\ntion 1 (using Euclidean distance ) are shown in the following distance matrix, and\nwill be useful for this exercise.\n»\n—————————————————————————–d1d2d3d4d5d6d7d8d9d10d11d12d13d14d15\nd1 0.00\nd2 0.99 0.00\nd3 2.31 3.30 0.00\nd4 1.52 2.11 2.30 0.00\nd5 0.56 0.95 2.55 1.95 0.00\nd6 2.00 2.63 2.29 0.55 2.46 0.00\nd7 1.50 2.12 2.21 0.61 2.02 0.73 0.00\nd8 1.56 1.98 2.62 0.35 1.96 0.78 0.82 0.00\nd9 1.63 2.60 0.78 1.94 1.79 2.10 1.92 2.20 0.00\nd10 1.95 2.93 0.47 2.23 2.15 2.32 2.13 2.52 0.44 0.00\nd11 1.00 0.47 3.23 2.07 0.78 2.61 2.20 1.94 2.49 2.86 0.00\nd12 0.38 0.96 2.43 1.77 0.19 2.26 1.83 1.78 1.69 2.04 0.83 0.00\nd13 2.01 2.98 0.49 2.32 2.19 2.41 2.22 2.61 0.49 0.09 2.91 2.09 0.00\nd14 1.59 2.57 0.75 1.93 1.81 2.08 1.83 2.21 0.28 0.37 2.51 1.70 0.44 0.00\nd15 1.82 2.26 2.68 0.43 2.21 0.66 0.94 0.28 2.31 2.62 2.19 2.03 2.71 2.33 0.00ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\n(b)On the basis of the completed table, calculate the silhouette for each clustering.\n(c)On the basis of the silhouette, would you choose 2or3for the value of kfor this\ndataset?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":687,"page_label":"633","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.7 Exercises 633\n3.A city tax service has performed a clustering of individual taxpayers using k-means\nclustering in order to better understand groups that might exist within their taxpayer\nbase. The clustering has divided the taxpayers into three clusters. Four descriptive\nfeatures have been used to describe each taxpayer:\n‚AGE: The age of the taxpayer.\n‚YEARS INCURRENT EMPLOYMENT : The number of years that the taxpayer has\nbeen in their current job.\n‚TOTAL INCOME : The taxpayer’s total income for the current tax year.\n‚EFFECTIVE TAXRATE: The effective tax rate paid by the taxpayer (this is simply\ntax paid divided by total income).\nThe following table shows summary statistics of the four descriptive features for each\nof the three clusters found.\n1st3rdStd.\nFeature Cluster Min. Qrt. Mean Median Qrt. Max Dev.\nAGEC1 20 28 34.6 34 40 59 7.8\nC2 36 43 45.8 45 48 64 4.5\nC3 20 32 34.9 36 39 52 5.8\nYEARS IN\nCURRENT\nEMPLOYMENTC1 0.50 2.74 7.18 5.11 10.76 27.40 5.56\nC2 8.16 14.25 17.81 17.04 20.71 33.89 4.60\nC3 0.50 2.44 5.73 4.38 9.32 14.12 3.73\nTOTAL INCOMEC1 46 247.70 57 355 .06 68 843 .26 64 977 .64 75 967 .11 175 000 16 387 .77\nC2 11 182.46 24 222 .04 34 711 .67 32 637 .42 44 102 .08 93 800 .98 13 412 .08\nC3 15 505.02 29 636 .07 36 370 .00 36 421 .53 42 912 .04 64 075 .62 8 744 .26\nEFFECTIVE\nTAXRATEC1 0.210 0.256 0.274 0.271 0.291 0.349 0.024\nC2 0.167 0.183 0.204 0.192 0.220 0.321 0.030\nC3 0.147 0.183 0.199 0.194 0.214 0.252 0.021\nThe following table shows the information gain calculated when each descriptive fea-\nture is used to predict the membership of a single cluster versus the rest of the popula-\ntion.\nInformation Gain\nFeature C1 C2 C3\nAGE 0.0599 0 .4106 0 .1828\nYEARS INCURRENT EMPLOYMENT 0.0481 0 .5432 0 .3073\nTOTAL INCOME 0.5015 0 .0694 0 .1830\nEFFECTIVE TAXRATE 0.5012 0 .0542 0 .2166\nThe following images show histograms of the values of the four descriptive features\nboth for the full dataset and when divided into the three clusters found.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":688,"page_label":"634","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"634 Chapter 10 Beyond Prediction: Unsupervised Learning\nAGE YEARS INCURRENT EMPLOYMENT TOTAL INCOME EFFECTIVE TAXRATE\nAll\nAgeDensity\n20 30 40 50 600.00 0.02 0.04 0.06\nY earsInCurrentEmploymentDensity\n0 5 10 15 20 25 30 350.00 0.02 0.04 0.06 0.08\nTotalIncomeDensity\n50000 100000 1500000.0e+00 1.0e−05 2.0e−05\nEffectiveTaxRateDensity\n0.15 0.20 0.25 0.30 0.350 5 10 15\nC1\nAgeDensity\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nY earsInCurrentEmploymentDensity\n0 5 10 15 20 25 30 350.00 0.05 0.10 0.15\nTotalIncomeDensity\n50000 100000 1500000e+00 1e−05 2e−05 3e−05\nEffectiveTaxRateDensity\n0.15 0.20 0.25 0.30 0.350 5 10 15 20 25\nC2\nAgeDensity\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nY earsInCurrentEmploymentDensity\n0 5 10 15 20 25 30 350.00 0.05 0.10 0.15\nTotalIncomeDensity\n50000 100000 1500000e+00 1e−05 2e−05 3e−05\nEffectiveTaxRateDensity\n0.15 0.20 0.25 0.30 0.350 5 10 15 20 25\nC3\nAgeDensity\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\nY earsInCurrentEmploymentDensity\n0 5 10 15 20 25 30 350.00 0.05 0.10 0.15\nTotalIncomeDensity\n50000 100000 1500000e+00 1e−05 2e−05 3e−05\nEffectiveTaxRateDensity\n0.15 0.20 0.25 0.30 0.350 5 10 15 20 25\nUsing the information provided, write a description of what it means for a taxpayer to\nbe a member of each of the clusters.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":689,"page_label":"635","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"10.7 Exercises 635\n˚4.The following table shows a customer-item matrix describing items from an online\nstore that customers have bought.\nID I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14\n1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n2 1 1 1 1 0 1 1 1 0 0 0 0 0 0\n3 1 1 0 1 0 1 1 1 1 0 0 0 1 0\n4 1 0 1 0 1 1 1 0 0 0 0 0 0 0\n5 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n6 0 1 0 0 0 0 0 0 1 1 1 1 1 0\n7 0 0 0 1 0 0 1 1 1 1 0 1 0 1\n8 0 1 0 0 0 0 0 0 0 1 0 1 1 1\nThe online store would like to cluster their customers to see if they could deﬁne mean-\ningful groups to whom they could target special offers. The table below shows a dis-\ntance matrix calculated using the Jaccard similarity measure (see Section 5.4.5[211]).\nA number of items have been left out of this matrix (indicated by ??).\n»\n———————————–d1 d2 d3 d4 d5 d6 d7 d8\nd1 0.000\nd2 ?? 0.000\nd3 0.600 ?? 0 .000\nd4 0.429 0.500 0.700 0.000\nd5 1.000 0.923 0.750 1.000 0.000\nd6 0.909 ?? 0 .727 1.000 0.375 0.000\nd7 0.917 0.727 0.636 0.909 0.444 ?? 0 .000\nd8 0.900 0.909 0.818 1.000 0.500 0.429 0.667 0.000ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\n(a)Using the Jaccard similarity index (reproduced here from Section 5.4.5[211])\ndist Jpq,dq“1´CPpq,dq\nCPpq,dq`PApq,dq`APpq,dq\ncalculate these missing distances in the preceding distance matrix (note that be-\ncause this is a distance (or dissimilarity) matrix rather than a similarity matrix, the\nvalues shown are 1´sim Jpq,dq).\n(b)Agglomerative hierarchical clustering (AHC) can easily be applied to this dis-\ntance matrix. If single linkage is used with AHC, which agglomerations will be\nmade in the ﬁrst three iterations of the algorithm?\n(c)Ifaverage linkage were used with AHC instead of single linkage, which agglom-\nerations would be made in the ﬁrst three iterations of the algorithm?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":690,"page_label":"636","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"636 Chapter 10 Beyond Prediction: Unsupervised Learning\n˚5.The following table shows a small dataset used for human activity recognition from a\nwearable accelerometer sensor.16Each instance describes the average acceleration in\nthe X, Y, and Z directions within a short time window. There are no labels, so this\ndata is being clustered in an attempt to recognize different activity from this simple\ndata stream. The k-means clustering approach is to be applied to this dataset with\nk“2and using Euclidean distance . The initial cluster centroids for the two clusters\nC1andC2arec1“⟨´0.235,0.253,0.438⟩andc2“⟨0.232,0.325,´0.159⟩. The\nfollowing table also shows the distance to these three cluster centers for each instance\nin the dataset.\nCluster Distances Iter. 1\nID X Y Z Distpdi,c1q Distpdi,c2q\n1 -0.154 0.376 0.099 0.370 0.467\n2 -0.103 0.476 -0.027 0.532 0.390\n3 0.228 0.036 -0.251 0.858 0.303\n4 0.330 0.013 -0.263 0.932 0.343\n5 -0.114 0.482 0.014 0.497 0.417\n6 0.295 0.084 -0.297 0.922 0.285\n7 0.262 0.042 -0.304 0.918 0.319\n8 -0.051 0.416 -0.306 0.784 0.332\n(a)Assign each instance to its nearest cluster to generate the clustering at the ﬁrst\niteration of k-means on the basis of the initial cluster centroids.\n(b)On the basis of the clustering calculated in Part (a), calculate a set of new cluster\ncentroids.\n(c)Calculate the distances of each instance to these new cluster centers and perform\nanother clustering iteration.\n16. The data in this question has been artiﬁcially created but is inspired by the Human Activity Recognition Us-\ning Smartphones Dataset ﬁrst described by Anguita et al. (2013) and available from the UCI Machine Learning\nRepository (Bache and Lichman, 2013).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":691,"page_label":"637","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11 Beyond Prediction: Reinforcement Learning\n“Live. Die. Repeat. ”\n—Edge of Tomorrow\nThis chapter discusses reinforcement learning , an approach to machine learning that is\nsufﬁciently different from supervised and unsupervised machine learning to be often con-\nsidered the third leg of the machine learning stool. Although reinforcement learning can\nbe used for many different tasks, its most common application is in learning to control the\nbehaviors of autonomous systems—for example, training robots to perform tasks, or auto-\nmated players to play games—and this is the application that this chapter focuses on. The\nother key differentiating factor between reinforcement learning and the other approaches\nwe have looked at in this book is that reinforcement learning relies less on using a dataset\nto drive learning and more on the ability to repeatedly attempt tasks in an environment.\nThis chapter begins by establishing the fundamental setup of the reinforcement learning\nscenario and then describes temporal-difference learning , a common approach to rein-\nforcement learning. The standard reinforcement learning approach, Q-learning , a form of\ntemporal-difference learning, is then described. After discussing some extensions and vari-\nations of this approach, the chapter focuses on how deep learning techniques have been\nrecently integrated into reinforcement learning to impressive effect. The chapter ﬁnishes\nby describing the deep Q network algorithm.\n11.1 Big Idea\nSarah is a young venture scout in training for her pioneering badge. One of the more\nunusual challenges involved in earning this badge is to learn to cross a stream using a set\nof stepping-stones while wearing an electronic blindfold. In this challenge the scout begins\nat the edge of the bank on one side of the stream and has to make a series of steps to get to\nthe other side. The goal is to get across the river in the fewest steps possible without getting\nwet. The blindfold makes this quite challenging! The blindfold, however, is electronic and\ncontrolled by the scout leader administering the test. Before the scout attempts a step,\nthe blindfold is made transparent for 0.5 seconds to give the scout a quick view of their","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":692,"page_label":"638","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"638 Chapter 11 Beyond Prediction: Reinforcement Learning\nenvironment so that they make a decision about which direction they will step in and how\nfar.\nSarah spent days training to get better at this challenge. At ﬁrst she was terrible at it, and\nalmost every step she took led to the disappointing sensation of wet feet. The ﬁrst time that\nSarah took a step that reached a stepping-stone, however, she felt a huge rush of excitement.\nAs time went by, Sarah found herself feeling this excitement more and more often as she\nbecame better and better at quickly assessing her situation when the blindfold was cleared\nand making better decisions about which direction to step in. She even occasionally made\nit all the way across the stream without stepping into the water and experienced the elation\nof landing on the grassy bank of the far side of the river. Still, throughout her training\nSarah would occasionally still step into the water and experience the disappointing feeling\nof soggy feet, which reminded her to be more careful about her decisions next time. On\nother occasions she would take a series of successful steps but then experience a sinking\nfeeling realizing that she had turned back on herself and arrived back on the starting bank.\nSarah’s training for the stepping-stone challenge has many of the characteristics of a\nreinforcement learning problem. Sarah had a task that she wanted to get better at, and so\nshe practiced it many times. In each practice episode, each one of the decisions Sarah made\nled to immediate feedback: either splashing into the river (negative feedback), landing\nsuccessfully on the next stepping-stone (positive feedback), turning back on her tracks\n(very negative feedback), or landing on the far riverbank (very positive feedback). On the\nbasis of this feedback, Sarah learned what constituted a good decision and and became\nbetter and better at assessing her situation and choosing which direction to step in next\nand how far. By optimizing her decisions to maximize these immediate rewards, Sarah\nlearned how to complete the overall task successfully. This is what we try to achieve in\nreinforcement learning.\nBy the time her test came around, Sarah had become an expert at the river-crossing\nchallenge. No matter how awkwardly the scout leaders laid out the stepping-stones, she\ncould quickly assess the situation each time the blindfold was cleared and choose the right","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":692,"page_label":"638","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"(very negative feedback), or landing on the far riverbank (very positive feedback). On the\nbasis of this feedback, Sarah learned what constituted a good decision and and became\nbetter and better at assessing her situation and choosing which direction to step in next\nand how far. By optimizing her decisions to maximize these immediate rewards, Sarah\nlearned how to complete the overall task successfully. This is what we try to achieve in\nreinforcement learning.\nBy the time her test came around, Sarah had become an expert at the river-crossing\nchallenge. No matter how awkwardly the scout leaders laid out the stepping-stones, she\ncould quickly assess the situation each time the blindfold was cleared and choose the right\ndirection to step in. She completed the challenge in record time and was awarded the\npioneering badge.\n11.2 Fundamentals\nThis section introduces the fundamentals of reinforcement learning . We begin by ex-\nplaining the concept of an intelligent agent and then describe the fundamental building\nblocks of reinforcement learning. Markov decision processes (MDPs) are an extremely\nuseful mathematical tool for framing reinforcement learning problems; these are covered\nnext. Finally, the temporal-difference learning approach, which is the basis for the stan-\ndard approach to reinforcement learning that will be discussed in the following chapter, is\nexplained.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":693,"page_label":"639","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 639\n11.2.1 Intelligent Agents\nThe stepping-stone crossing challenge described in Section 11.1[637]nicely illustrates the\nintelligent agent approach that underpins reinforcement learning. We can consider the\nscout to be an intelligent agent (or simply agent ) attempting to complete a task within\nan environment. The goal of the agent is to complete the task as successfully as possible.\nEach attempt at the task is referred to as an episode . At any point in time, t, the agent\nobserves the current state of its environment, ot; considers these observations to select an\naction, at; and takes this action, receiving immediate feedback, rt, from the environment\nabout whether this was a good or bad action to take. We use rtto refer to feedback, as in\nreinforcement learning feedback is more commonly referred to as reward (where reward\ncan be either positive or negative). This gives a series of discrete steps that make up an\nepisode\npo1,a1,r1q,po2,a2,r2q,po3,a3,r3q,...,poe,ae,req (11.1)\nwhere the episode proceeds through time-steps t“1,..., e. At each time-step the agent\nmakes an observation, ot, of the environment, takes an action, at, and receives a reward, rt,\nbased on that action. This cycle is illustrated in Figure 11.1[639].\nFigure 11.1\nAn agent behaving in an environment and the observation, reward, action cycle. The transition from\nobservations of the environment to a state is shown by the state generation function, φ.\nThe sequence of observations, actions and rewards that precede any time-step, t, is re-\nferred to as a history ,Ht. The job of the agent in the environment is to make decisions at\neach time-step, t, about what action to take next on the basis of its current observations of\nthe environment, ot, and the history, Ht.\nMaintaining long histories of actions, rewards, and observations (which are possibly only\nvery slightly different from one iteration to the next) is not a very efﬁcient way to reason","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":694,"page_label":"640","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"640 Chapter 11 Beyond Prediction: Reinforcement Learning\nabout the world, particularly as episodes might cover hundreds or thousands of time-steps.\nInstead, we collapse this information into a single representation, referred to as a state .\nThe state at time-step t,st, should contain all the important information about the envi-\nronment at that time-step, any important information about what has been happening in\nthe environment at preceding time-steps, and any important information about the internal\ncomposition of the agent. For example, for a robot deployed within a hospital to deliver\nequipment to operating theaters, the state might include the robot’s position in the environ-\nment, the positions of people nearby, whether the robot is on the way to collect items or to\ndeliver them, and the current levels of the robot’s batteries.\nIn Figure 11.1[639]we show how the observations made about the environment at time-\nsteptare converted into a state, st, using a state generation function ,φ. In many cases, if\nthe environment is fully observable this function is a simple identity function because the\nobservation fully deﬁnes the state. It is also possible, however, for this function to be more\nelaborate when the observations over multiple time-steps are accumulated into a state.1\nUsing states instead of observations, Equation (11.1)[639]can be restated2\nps1,a1,r1q,ps2,a2,r2q,ps3,a3,r3q,...,pse,ae,req (11.2)\nWe see in subsequent examples that designing good state representations is one of the arts\nof reinforcement learning.\nThe goal of the intelligent agent is to complete a task as successfully as possible. To\nframe the reinforcement learning problem, this needs to be more formally deﬁned—what\ndoes it mean to successfully complete a task? The next section explores this.\n11.2.2 Fundamentals of Reinforcement Learning\nThe fundamental idea underpinning reinforcement learning is that the only goal of an in-\ntelligent agent is to maximize cumulative reward across an episode.3The cumulative\nreward earned across an episode is referred to as the return from the episode and can be\ndeﬁned as\n1. Environments in which the state contains all information about the environment and any agents in it are known\nasfully observable environments. Environments in which this is not the case are known as partially observable\nenvironments . The use of a state generation function allows us to treat some partially observable environments","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":694,"page_label":"640","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"does it mean to successfully complete a task? The next section explores this.\n11.2.2 Fundamentals of Reinforcement Learning\nThe fundamental idea underpinning reinforcement learning is that the only goal of an in-\ntelligent agent is to maximize cumulative reward across an episode.3The cumulative\nreward earned across an episode is referred to as the return from the episode and can be\ndeﬁned as\n1. Environments in which the state contains all information about the environment and any agents in it are known\nasfully observable environments. Environments in which this is not the case are known as partially observable\nenvironments . The use of a state generation function allows us to treat some partially observable environments\nas if they were fully observable and apply the mechanics of reinforcement learning where otherwise it would not\nbe possible.\n2. There is some argument in the reinforcement learning literature about whether the reward that follows an\naction, at, taken in a state, st, should be referred to as rtorrt`1. The argument stems from a disagreement about\nwhen a discrete moment of time ends—after the action completes or after the reward is received? From a compu-\ntational point of view it makes no difference, as long as consistency is maintained in notation and computation.\nThroughout this chapter we use rtto refer to the reward received after taking an action, at, at time-step t.\n3. This is captured in Sutton’s reward hypothesis (Sutton and Barto, 2018): “ That all of what we mean by goals\nand purposes can be well thought of as maximization of the expected value of the cumulative sum of a received\nscalar signal (reward). ”","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":695,"page_label":"641","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 641\nG“rt`rt`1`rt`2`rt`3`...`re (11.3)\nThat intelligent behavior can be driven by the singular goal of maximizing return is a\nbold statement—it is often argued that it is very ambitious to expect sophisticated, long-\nterm behavior to emerge from simple accumulation of instantaneous rewards. Reward is\noften delayed, and the real value of an action is not reﬂected immediately but rather by the\nfact that an action takes us toward a later state that will ultimately allow an agent to earn a\nreward. For example, early moves in a game of chess do not lead to large positive rewards\nbut set the ground for later high-reward moves. Rewards can also often be somewhat\ncontradictory, and an action that gives an immediate positive reward may turn out to be a\nbad one in the longer term. For example, eating cake almost always seems like a good idea\nin the moment, but in terms of long-term health is probably not always a strong choice. It\nhas been shown repeatedly, however, that it is in fact possible to learn sophisticated, long-\nterm behaviors using the maximization of cumulative reward alone. This introduces the\nsecond artof reinforcement learning: the design of effective reward functions.\nThe key decision making component of a reinforcement learning agent is referred to as\napolicy ,π. A policy is simply a mapping from states to actions\nat“πpstq (11.4)\nthat tells the agent which action, at, to take when in a given state, st. We can also deﬁne a\npolicy in probabilistic terms\nPpAt“a|St“sq“πpSt“sq (11.5)\nwhere AtandStare random variables that can be assigned speciﬁc states and actions; and\nthe policy,π, returns a probability distribution across the possible actions that an agent can\ntake in a given state.\nThe policy can be thought of as a simple lookup table that records the action that should\nbe taken in every state, and reinforcement learning problems can be framed as an effort to\nlearn this table directly.4Policies can also be encoded as a rule used to choose an action\nfrom those available in a particular state, and this is the approach we focus on in this\nchapter. For example, we might use a greedy action selection policy that says the agent\nshould always take the action that will give it this highest immediate reward. This would,\nhowever, ignore the fact that sometimes reward is delayed and that taking an action that\ngives a low immediate reward can be a good idea if it leads the agent to a state that could","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":695,"page_label":"641","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"take in a given state.\nThe policy can be thought of as a simple lookup table that records the action that should\nbe taken in every state, and reinforcement learning problems can be framed as an effort to\nlearn this table directly.4Policies can also be encoded as a rule used to choose an action\nfrom those available in a particular state, and this is the approach we focus on in this\nchapter. For example, we might use a greedy action selection policy that says the agent\nshould always take the action that will give it this highest immediate reward. This would,\nhowever, ignore the fact that sometimes reward is delayed and that taking an action that\ngives a low immediate reward can be a good idea if it leads the agent to a state that could\ngive it large positive rewards later on. This suggests the need for a more sophisticated\n4. Approaches taking this approach include policy gradient andevolutionary reinforcement learning ap-\nproaches.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":696,"page_label":"642","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"642 Chapter 11 Beyond Prediction: Reinforcement Learning\nmeasure of the value of taking an action in a given state and leads to the ﬁnal fundamental\ncomponent of a reinforcement learning agent: a value function .\nIn reinforcement learning a value function returns the cumulative reward that an agent\ncan expect to earn if it starts from a particular state, st, and follows a speciﬁc policy, πall\nthe way to the end of an episode. We can write this\nVπpstq“Eπrrt`rt`1`rt`2`rt`3`...`re|sts (11.6)\nwhere Eis the expectation. This value function returns the expected cumulative reward\nthat an agent will earn if it follows policy πstarting from state st.\nWe can also calculate the expected value from the starting point of an agent’s taking a\nspeciﬁc action, at, in a given state, st. This is known as an action-value function and\nreturns the cumulative reward that an agent can expect to earn if it takes an action atin\nstate stand then continues to select actions using policy, π, all the way to the end of an\nepisode. We can write this5\nQπpst,atq“Eπrrt`rt`1`rt`2`rt`3`...`re|st,ats (11.7)\nThe output of the action-value function is referred to as the expected return of pursuing\naction atin state st. We will see that the action-value function formulation of expected\nreturn is the more useful of these two.\nIn the formulation given in Equation (11.7)[642]expected future rewards are considered\nto be as valuable as the immediate reward that the agent will receive from taking the next\nimmediate action, at. Just as we might be more excited about receiving a gift of $100 today\nthan a promise to receive a gift of $100 in a year’s time, it is reasonable when calculating\nexpected return to pay more attention to the immediate reward we expect to receive from\ntaking the next action than to the rewards that we expect to receive in 10or even 100\nactions’ time. This is known as discounted return . We can modify Equation (11.3)[641]to\ndeﬁne discounted return\nGγ“rt`γrt`1`γ2rt`2`γ3rt`3`...`γe´tre (11.8)\nwhereγis adiscount rate and is a value in r0,1s. This implements an exponential dis-\ncounting so that future expected rewards have less and less impact on the value calculated\nfor an action. Choosing a low value for γmakes the action-value function focus heavily on\nthe most immediate rewards. For example, with γ“0.1\nGγ“0.1“rt`0.1ˆrt`1`0.01ˆrt`2`0.001ˆrt`3`...\n5. We use Qhere to be consistent with the framing of Q-learning later in this chapter.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":697,"page_label":"643","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 643\nChoosing a high value for γgives almost equal importance to all rewards. For example,\nwithγ“0.9\nGγ“0.9“rt`0.9ˆrt`1`0.81ˆrt`2`0.729ˆrt`3`...\nTo use discounted return in the action-value function it is restated\nQπpst,atq“Eπrrt`γrt`1`γ2rt`2`γ3rt`3`...`γ3´tre|st,ats (11.9)\nDiscounting makes intuitive sense, makes some of the mathematics associated with rein-\nforcement learning more straightforward, and avoids any issues with circular paths through\nstates that can arise in some scenarios. Discounted return is widely used in reinforcement\nlearning.\nIn summary, in a reinforcement learning scenario, an agent inhabiting an environment\nattempts to achieve a goal by taking a sequence of actions to move it between states .\nOn completion of each action the agent receives an immediate scalar reward indicating\nwhether the outcome of the action was positive or negative, and to what degree. In re-\ninforcement learning the degree to which an agent has achieved a goal is measured only\nby the cumulative rewards it has received from each action taken in pursuit of that goal.\nTo choose which action to take in a given state the agent uses a policy . Policies rely on\nbeing able to assess the expected return of taking an action in a particular state, and an\naction-value function can be used to calculate this.6\nNone of this, however, yet explains how any actual learning takes place! This explana-\ntion appears subsequently, but before discussing reinforcement learning algorithms we will\nexplain Markov decision processes , a useful mathematical framework into which we can\nplace the key components of reinforcement learning to allow learning to take place.\n11.2.3 Markov Decision Processes\nMarkov decision processes (MDPs) are an attractive mathematical framework within\nwhich to reason about decision making scenarios in which outcomes are partly under the\ncontrol of a decision maker, but also partly random. This has made them an attractive\nframework for applications ranging from ﬁnancial modeling, to robot control, to modeling\nthe ﬂow of human conversation. This also makes them ideal for reasoning about reinforce-\nment learning.\n6. We have presented this discussion on the context of episodic, model-free, policy-based reinforcement learn-\ning. There are other framings of the reinforcement learning problem under which different framings are used and\nextra components are added—for example, policy-based reinforcement learning andmodel-based reinforce-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":697,"page_label":"643","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"which to reason about decision making scenarios in which outcomes are partly under the\ncontrol of a decision maker, but also partly random. This has made them an attractive\nframework for applications ranging from ﬁnancial modeling, to robot control, to modeling\nthe ﬂow of human conversation. This also makes them ideal for reasoning about reinforce-\nment learning.\n6. We have presented this discussion on the context of episodic, model-free, policy-based reinforcement learn-\ning. There are other framings of the reinforcement learning problem under which different framings are used and\nextra components are added—for example, policy-based reinforcement learning andmodel-based reinforce-\nment learning . For clarity of explanation, however, we have ignored these in this discussion of the fundamentals\nrequired to understand the approaches to reinforcement learning that are discussed in this chapter. Interested read-\ners are directed to the suggestions for further reading presented in Section 11.6[677], in which these alternatives\nare discussed.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":698,"page_label":"644","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"644 Chapter 11 Beyond Prediction: Reinforcement Learning\nAMarkov process , a more basic framework than an MDP that does not include decision\nmaking, can be used to model a discrete random process that transitions through a ﬁnite\nset of states, S. For example, we could use a Markov process to model how infection\nprogresses in an individual when a disease epidemic breaks out. Individuals can belong to\none of three states: S USCEPTIBLE , INFECTED , or R ECOVERED (these are often referred\nto as S-I-R models ). An individual can belong to only one of these states at a time and\nmoves between them according to a Markov process. Figure 11.2(a)[644]shows these states\nand how an individual can move between them.7\nSI\nR0.980.020.50\n0.50\n0.750.05\n0.20\n(a) S-I-R Markov processP“»\n—–S I R\nS 0.98 0.02 0.00\nI 0.00 0.50 0.50\nR 0.75 0.05 0.20ﬁ\nﬃﬂ\n(b) S-I-R transition matrix\nFigure 11.2\nA simple Markov process to model the evolution of an infectious disease in individuals during an\nepidemic using the S USCEPTIBLE -INFECTED -RECOVERED (S-I-R) model.\nMarkov processes are built on the Markov assumption that the probability of transition-\ning to a particular state at the next time-step relies only on the current state, and does not\nrequire any knowledge of the history of states that came before that, or\nPpSt`1|St,St´1,St´2,...q“PpSt`1|Stq (11.10)\nGiven the Markov assumption, we can write the probability of transitioning between two\nstates\nPps1Ñs2q“PpSt`1“s2|St“s1q (11.11)\n7. This is a simple manufactured example for this book, but this type of model is often used in epidemiology\nHunter et al. (2018).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":699,"page_label":"645","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 645\nwhere StandSt`1are random variables to which the states at time tandt`1are assigned.\nThe full dynamics of a Markov process can be captured in a transition matrix\nP“»\n————–Pps1Ñs1qPps1Ñs2q... Pps1Ñsnq\nPps2Ñs1qPps2Ñs2q... Pps2Ñsnq\n............\nPpsnÑs1qPpsnÑs2q... PpsnÑsnqﬁ\nﬃﬃﬃﬃﬂ\nwhere s1tosnarendifferent states. A Markov process can be fully characterized by the\nset of states, S, and the transition matrix, P.\nThe numbers along the arrows in Figure 11.2[644]show the probabilities of moving be-\ntween the different sates in this model. The transition matrix is also shown. Most people\nwill remain in the S USCEPTIBLE state indeﬁnitely, PpSÑSq “ 0.98, but with a small\nprobability, PpSÑIq“0.02, can transition to the I NFECTED state. Individuals will most\nlikely remain in the I NFECTED state for some time, PpIÑIq“0.50, but will transition\neventually to the R ECOVERED state, PpIÑRq “ 0.50. After remaining in the R ECOV -\nERED state for some time, PpRÑRq “ 0.20, most people will soon transition back to\nthe S USCEPTIBLE state, PpRÑSq“0.75, but a small part of the population will instead\nrelapse and return to the I NFECTED state, PpRÑIq“0.05.\nAMarkov decision process (MDP) extends the Markov process by adding decision\nmaking and rewards. We extend the formulation of the Markov process to include a ﬁnite\nset of actions that can be taken, A, and add actions from this set to the formulation of\ntransition probabilities\nPps1aÝ Ñs2q“PpSt`1“s2|St“s1,At“aq (11.12)\nRetaining the Markov assumption that only the current time-step is required to model\nwhat will happen next, the probability of transitioning to a particular state depends only\non the current state and the action just taken. Each transition to a new state based on a\nparticular action now also carries with it a reward\nRps1aÝ Ñs2q“Eprt|St“s1,St`1“s2,At“aq (11.13)\nTo illustrate MDPs in a little more detail we develop an MDP representation for an agent\ndesigned to play the card game TwentyTwos , a simpliﬁed version of the popular game\nBlackjack , that has been invented for this example. TwentyTwos is a game played between\na single player and a dealer. In TwentyTwos the player tries to collect a hand of cards that\nhave a total value higher than the total value of the cards in the dealer’s hand, but not\nexceeding 22—a player is said to go bust when they exceed 22. Cards are worth their\nnumber value, with picture cards worth 10, and aces always worth 11(this is one of the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":699,"page_label":"645","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"particular action now also carries with it a reward\nRps1aÝ Ñs2q“Eprt|St“s1,St`1“s2,At“aq (11.13)\nTo illustrate MDPs in a little more detail we develop an MDP representation for an agent\ndesigned to play the card game TwentyTwos , a simpliﬁed version of the popular game\nBlackjack , that has been invented for this example. TwentyTwos is a game played between\na single player and a dealer. In TwentyTwos the player tries to collect a hand of cards that\nhave a total value higher than the total value of the cards in the dealer’s hand, but not\nexceeding 22—a player is said to go bust when they exceed 22. Cards are worth their\nnumber value, with picture cards worth 10, and aces always worth 11(this is one of the\nsimpliﬁcations of this version of the game). The player and dealer are initially each dealt\ntwo cards. The player is allowed to see one of the two cards dealt to the dealer, but the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":700,"page_label":"646","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"646 Chapter 11 Beyond Prediction: Reinforcement Learning\ndealer’s other card remains hidden until the player is ﬁnished taking their actions. The\nplayer then repeatedly chooses to either be dealt another card, Twist , or to stop on their\ncurrent total, Stick .8When the player is ﬁnished, the dealer turns over their second card\nand must continue to deal themselves cards until they reach a total value of 17or more, or\ngo bust by exceeding a value of 22. The dealer’s play strictly follows these rules and so\nwe can say that the dealer does not make any decisions during the game. We also assume\ncards are dealt from an inﬁnite deck, which simpliﬁes some of the modeling of the game.9\nThe player wins if their total is less than or equal to 22and is greater than the dealer’s\ntotal, or if the dealer goes bust. In the rare event that the player is dealt two aces, giving\na total of 22, they are awarded a TwentyTwo and win regardless of the cards dealt to the\ndealer. The game is tied if the dealer and the player have the same total, less than or equal to\n22. The player loses in all other cases. The player wins $2in the event of a TwentyTwo (two\naces), wins $1if they beat the dealer in the normal way, neither wins nor loses anything if\nthe game is tied, and loses $1if the dealer wins.10Table 11.1[647]shows some episodes of\nthe TwentyTwos game being played including the the player’s hand, the dealer’s hand, the\nactions the player takes, and the rewards earned by the player.\nThere are lots of options for the state representation that can be used to model an agent\nplaying the game TwentyTwos. Card suits have no impact on the game (for example, 10♦\nis equivalent to 10♥) and because there is an inﬁnite deck, tracking the actual cards that\nhave been dealt gives no advantage to the player. So, the only thing that needs to be taken\ninto account in the state representation is the total value of the cards in the player’s hand\nand the value of the visible card dealt to the dealer. The total value of the initial two cards\ndealt to the player can range from 4(for example, 2♥and2♣) to22(for example, A♠\nandA♥), giving 19unique values. There are 10values that the dealer’s visible card can\nrepresent: 2to11. This means that there are 190different combinations of the values\nof the hands held by the player and the dealer. To fully capture the game dynamics we\ncould represent TwentyTwos using the 190different hand states plus ﬁve more terminal","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":700,"page_label":"646","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"have been dealt gives no advantage to the player. So, the only thing that needs to be taken\ninto account in the state representation is the total value of the cards in the player’s hand\nand the value of the visible card dealt to the dealer. The total value of the initial two cards\ndealt to the player can range from 4(for example, 2♥and2♣) to22(for example, A♠\nandA♥), giving 19unique values. There are 10values that the dealer’s visible card can\nrepresent: 2to11. This means that there are 190different combinations of the values\nof the hands held by the player and the dealer. To fully capture the game dynamics we\ncould represent TwentyTwos using the 190different hand states plus ﬁve more terminal\nstates representing the outcomes of going bust, losing, tying, winning, and winning with a\nTwentyTwo . This would give a total of 195different states. Many of these states, however,\nor not signiﬁcantly different from each other and so it makes sense to discretize the hand\nrepresentation so as to have a smaller number of states. In designing state representations\nthe principle of parsimony applies: we should strive for the simplest representation that\ngives sufﬁcient ﬂexibility to model the important aspects of an environment and task.\n8. Advanced features of Blackjack, like doubling down ,insurance , and splitting pairs , are not allowed in our\nTwentyTwos game.\n9. This is not as extreme an assumption as it might sound, as cards in Blackjack are usually dealt from a shoe\ncontaining 6to8standard playing card decks.\n10. In this simpliﬁed version of Blackjack, the complication of the player having to choose a wager for each hand\nis ignored.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":701,"page_label":"647","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 647\nTable 11.1\nSome episodes of games played by the TwentyTwos agent showing the cards dealt, as well as the\nstates, actions, and rewards. Note that rewards are shown on the row indicating the action that led to\nthem, not the state that followed that action.\nIter Player Hand Dealer Hand State Action Reward\n12♥7♣ p9q8♥ p8qPL-DH Twist 0\n22♥7♣K♣p19q8♥ p8qPH-DH Stick +1\n32♥7♣K♣p19q8♥Q♦p18qWIN\n14♠A♥ p15qQ♥p10qPM-DH Twist -1\n24♠A♥9♣p24qQ♥p10qBUST\n12♦4♦ p6q3♥ p3qPL-DL Twist 0\n22♦4♦3♥ p9q3♥ p3qPL-DL Twist 0\n32♦4♦3♥6♣p15q3♥ p3qPM-DL Twist 0\n42♦4♦3♥6♣6♦p21q3♥ p3qPH-DL Stick 0\n52♦4♦3♥6♣6♦p21q3♥7♥A♠p21qTIE\n1Q♦J♣ p20qA♥p11qPH-DH Stick +1\n2Q♦J♣ p20qA♣5♣Q♠p26qWIN\n1A♦A♥ p22q2♥ p2qPH-DL Stick +2\n2A♦A♥ p22q2♥ p2qTWENTY TWO\nTo make things manageable for this example, we aggressively discretize the representa-\ntion of the value of the cards in the player’s and dealer’s hands. For the player’s hand, just\nthree levels ( low,medium , and high) are modeled\n‚Player Low (PL): 4´14\n‚Player Medium (PM): 15´18\n‚Player High (PH): 19´22\nFor the value of the visible card dealt to the dealer, just two levels ( lowandhigh) are\nmodeled\n‚Dealer Low (DL): 2´7\n‚Dealer High (DH): 8´11\nThis gives a state representation with six states to represent all possible combinations of\nthe value of the cards in the player’s hand and the value of the dealer’s visible card: PL-\nDL, PM-DL, PH-DL, PL-DH, PM-DH, and PH-DH. Adding the ﬁve terminal states—\nBUST, LOSE, TIE, WIN, and T WENTY TWO—gives 11states in total. Figure 11.3[648]shows\na visualization of an MDP for TwentyTwos using this representation where the circles\nrepresent the 11states. Note that in this diagram the B UST state has been included twice\nto make the illustration a little more clear by avoiding too many overlapping arrows.\nIn TwentyTwos the player has two actions available to them in each non-terminal state:\nStick orTwist . Once the player enters one of the terminal states (B UST, LOSE, TIE, W IN,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":702,"page_label":"648","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"648 Chapter 11 Beyond Prediction: Reinforcement Learning\nPL-DLPM-DL\nPH-DLBUST-1\nBUST\n-1LOSE-1\nTIE 0 WIN +1TWENTY\nTWO+2\nPL-DH\nPM-DHPH-DH0.160.320.27 0.06\n0.210.73\n0.020.98\n0.58\n0.100.770.03\n0.750.25\n0.67 0.03\nFigure 11.3\nA Markov decision process representation for TwentyTwos, a simpliﬁed version of the card game\nBlackjack. The eleven states are shown as the nodes in the graph with transitions based on different\nactions represented by the edges (transitions for Twist actions are shown as solid lines and transitions\nforStick actions are shown as dashed lines). For clarity in the graph, rewards at non-terminal nodes\nare left out and rewards are shown just once at each terminal node; the B UST state is shown twice\nto avoid overlapping transition arrows; and only an illustrative selection of transition probabilities\nare shown (the full set of transitions probabilities is shown in the transition matrices in Equations\n(11.15)[650]and (11.14)[650]).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":703,"page_label":"649","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 649\nor T WENTY TWO) the game is over and there are no more actions to take. Figure 11.3[648]\nshows the possible transitions between states in TwentyTwos based on these actions. Tran-\nsitions related to a Twist action are shown as solid lines, and transitions related to a Stick\naction are shown with dashed lines. Choosing to Twist in any of the non-terminal states\nwill take the agent to the same non-terminal state, another non-terminal state representing\na higher player hand value, or—if they are unlucky—the B UST state. After the player\nchooses to Stick , the dealer will reveal their hidden card and keep dealing more cards until\nthey reach a total greater than or equal to 17. Depending on how the value of the dealer’s\nhand that results from this compares to the value of the player’s hand the agent will move\ninto one of the terminal states: L OSE, TIE, W IN, or T WENTY TWO.\nFigure 11.3[648]shows a selection of the transition probabilities between states in the\nTwentyTwos MDP (only a selection of probabilities are shown to make the graph less clut-\ntered). The probabilities associated with each Twist transition shown in Figure 11.3[648]have\nbeen calculated based on what can happen in a game of TwentyTwos, under the assump-\ntion of an inﬁnite deck from which cards are dealt. Consider, for example, the probability\nof an agent in the PM-DH state remaining in that state after choosing the Twist action:\nPpPM-DHTwistÝÝÑ PM-DHq. If the agent is in the PM-DH state, they must have a total\nvalue in their hand of either 15,16,17, or18. If the agent chooses the Twist action they\nwill be dealt one of the 13types of cards in a deck with a value between 2and11(remem-\nber that there are four cards in the deck with a value of 10:10,J,Q, and K). This means\nthat there are 52different scenarios that could occur if a player is in the PM-DH state and\nchooses the Twist action (the player is dealt one of 13possible cards when their hand has\none of 4possible values, so 4ˆ13“52). Only 3of these scenarios, however, will lead to\nthe agent staying in the PM-DH state. The agent can only stay in the PM-DH state if they\nhave a hand value of 15and are dealt a 2or a3; or if they have a hand value of 16and are\ndealt a 2. So, PpPM-DHTwistÝÝÑ PM-DHq“3\n52“0.058. All state transition probabilities\nbased on the Twist action can be calculated in a similar way. For transitions based on the\nStick action a large number of games of TwentyTwos have been simulated and transition","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":703,"page_label":"649","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"that there are 52different scenarios that could occur if a player is in the PM-DH state and\nchooses the Twist action (the player is dealt one of 13possible cards when their hand has\none of 4possible values, so 4ˆ13“52). Only 3of these scenarios, however, will lead to\nthe agent staying in the PM-DH state. The agent can only stay in the PM-DH state if they\nhave a hand value of 15and are dealt a 2or a3; or if they have a hand value of 16and are\ndealt a 2. So, PpPM-DHTwistÝÝÑ PM-DHq“3\n52“0.058. All state transition probabilities\nbased on the Twist action can be calculated in a similar way. For transitions based on the\nStick action a large number of games of TwentyTwos have been simulated and transition\nprobabilities have been calculated based on this simulation.11\n11. Understanding the probabilities associated with the dynamics of card games has a history stretching back to\nthe origins of probability theory (Bernstein, 1996), through early applications of computing technology (Scarne,\n1986), right to the modern day (Brown and Sandholm, 2017).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":704,"page_label":"650","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"650 Chapter 11 Beyond Prediction: Reinforcement Learning\nWe can capture the full dynamics of the TwentyTwos MDP in a pair of transition matri-\nces, one for each possible action. For the Twist action the structure of the state transition\nmatrix, PTwist, is\nPTwist“»\n———–PpPL-DLTwistÝÝÑ PL-DLq PpPL-DLTwistÝÝÑ PM-DLq... PpPL-DLTwistÝÝÑ TWENTY TWOq\nPpPM-DLTwistÝÝÑ PL-DLq PpPM-DLTwistÝÝÑ PM-DLq... PpPM-DLTwistÝÝÑ TWENTY TWOq\n............\nPpTWENTY TWOTwistÝÝÑ PL-DLqPpTWENTY TWOTwistÝÝÑ PM-DLq... PpTWENTY TWOTwistÝÝÑ TWENTY TWOqﬁ\nﬃﬃﬃﬂ\nand for the Stick action the structure of the state transition matrix, PStick, is\nPStick“»\n———–PpPL-DLStickÝ Ý Ñ PL-DLq PpPL-DLStickÝ Ý Ñ PM-DLq... PpPL-DLStickÝ Ý Ñ TWENTY TWOq\nPpPM-DLStickÝ Ý Ñ PL-DLq PpPM-DLStickÝ Ý Ñ PM-DLq... PpPM-DLStickÝ Ý Ñ TWENTY TWOq\n............\nPpTWENTY TWOStickÝ Ý Ñ PL-DLqPpTWENTY TWOStickÝ Ý Ñ PM-DLq... PpTWENTY TWOStickÝ Ý Ñ TWENTY TWOqﬁ\nﬃﬃﬃﬂ\nThe completed transition matrix for the Twist action, PTwist, is\nPTwist“»\n————————————————–PL-DL PM-DL PH-DL PL-DH PM-DH PH-DH B UST LOSE TIE WIN TWENTY TWO\nPL-DL 0.16 0.32 0.34 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00\nPM-DL 0.00 0.06 0.29 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00\nPH-DL 0.00 0.00 0.08 0.00 0.00 0.00 0.92 0.00 0.00 0.00 0.00\nPL-DH 0.00 0.00 0.00 0.16 0.32 0.34 0.17 0.00 0.00 0.00 0.00\nPM-DH 0.00 0.00 0.00 0.00 0.06 0.29 0.65 0.00 0.00 0.00 0.00\nPH-DH 0.00 0.00 0.00 0.00 0.00 0.08 0.92 0.00 0.00 0.00 0.00\nBUST 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nLOSE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTIE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nWIN 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTWENTY TWO 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ(11.14)\nThe completed transition matrix for the Stick action, PStick, is\nPStick“»\n————————————————–PL-DL PM-DL PH-DL PL-DH PM-DH PH-DH B UST LOSE TIE WIN TWENTY TWO\nPL-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.68 0.00 0.32 0.00\nPM-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.58 0.06 0.36 0.00\nPH-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.10 0.68 0.03\nPL-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.81 0.00 0.19 0.00\nPM-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.73 0.06 0.21 0.00\nPH-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.22 0.15 0.60 0.03\nBUST 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nLOSE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTIE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":704,"page_label":"650","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ(11.14)\nThe completed transition matrix for the Stick action, PStick, is\nPStick“»\n————————————————–PL-DL PM-DL PH-DL PL-DH PM-DH PH-DH B UST LOSE TIE WIN TWENTY TWO\nPL-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.68 0.00 0.32 0.00\nPM-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.58 0.06 0.36 0.00\nPH-DL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.10 0.68 0.03\nPL-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.81 0.00 0.19 0.00\nPM-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.73 0.06 0.21 0.00\nPH-DH 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.22 0.15 0.60 0.03\nBUST 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nLOSE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTIE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nWIN 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTWENTY TWO 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00ﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ(11.15)\nSome of these probabilities are highlighted along the edges in Figure 11.3[648].\nThe ﬁnal component of the MDP shown in Figure 11.3[648]is the reward associated with\neach transition to a new state based on taking a particular action. The rewards are shown","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":705,"page_label":"651","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 651\nas the large numbers beside some states. In this model, the reward for all transitions to\nnon-terminal states (e.g., PL-DL or PM-DH) is 0; these are not shown in Figure 11.3[648].\nThe remaining rewards are based on the winnings earned within the game. Transitions\ninto the B UST or L OSE states return a reward of ´1, transitions into the T IEstate earn a\nreward of 0, transitions into the W INstate return a reward of `1, and transitions into the\nTWENTY TWOstate return a reward of `2. Although it makes sense to do it, the rewards\ndo not have to be so closely tied to the winnings within the game, and different reward\nstructures could be designed to encourage different playing strategies—for example, more\nconservative or more aggressive play.\nThis MDP captures the dynamics of the TwentyTwos game, and it also hints toward\nsome strategies for successful play. For example, choosing to Stick when the value of the\nplayer’s hand is low and the value of the dealer’s hand is high, state PL-DH, rarely leads\nto the player winning, PpPL-DHStickÝÝÑ WINq “ 0.19. The MDP alone, however, is not\nsufﬁcient to describe optimal behavior for successfully playing the game. The next section\ndescribes how an MDP can be used as the basis for reasoning about optimal behavior.\n11.2.4 The Bellman Equations\nAlthough an MDP tells us everything we need to know about how an agent can take actions\nto move between states in an environment, it does not tell us anything about what actions\nthe agent should take to be most successful. However, the action-value function given\nin Equation (11.9)[643]can be expressed in terms of the components of an MDP to do just\nthis. Before presenting the formal version of this, it is worth stating the intuition. We can\ncalculate the value of taking a particular action in a given state as the expected reward for\ntaking the action plus the value of the state that the agent arrives in after taking that action.\nTo restate Equation (11.9)[643]in terms of the components of an MDP, we modify Equa-\ntion (11.9)[643]to sum to inﬁnity rather than just the end of an episode,12and we state the\nequation a little more succinctly\nQπpst,atq “ Eπ“\nrt`γrt`1`γ2rt`2`...`γ3r8|st,at‰\n“Eπ«8ÿ\nk“0γkrt`k|st,atﬀ\n(11.16)\n“Eπ«\nrt`γ8ÿ\nk“0γkrt`k`1|st,atﬀ\n(11.17)\n12. This is a slight departure from Equation (11.9)[643]as it sums to inﬁnity rather than to the end of an episode.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":705,"page_label":"651","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"this. Before presenting the formal version of this, it is worth stating the intuition. We can\ncalculate the value of taking a particular action in a given state as the expected reward for\ntaking the action plus the value of the state that the agent arrives in after taking that action.\nTo restate Equation (11.9)[643]in terms of the components of an MDP, we modify Equa-\ntion (11.9)[643]to sum to inﬁnity rather than just the end of an episode,12and we state the\nequation a little more succinctly\nQπpst,atq “ Eπ“\nrt`γrt`1`γ2rt`2`...`γ3r8|st,at‰\n“Eπ«8ÿ\nk“0γkrt`k|st,atﬀ\n(11.16)\n“Eπ«\nrt`γ8ÿ\nk“0γkrt`k`1|st,atﬀ\n(11.17)\n12. This is a slight departure from Equation (11.9)[643]as it sums to inﬁnity rather than to the end of an episode.\nThis, however, makes no difference in the discussion that follows but is the normal formulation of the Bellman\nEquations.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":706,"page_label":"652","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"652 Chapter 11 Beyond Prediction: Reinforcement Learning\nThis states that the expected return of taking action atin state stis the immediate expected\nreward from taking that action plus the sum of discounted expected rewards that will arise\nif we continue to follow policy π.\nTo restate Equation (11.17)[651]using the components of an MDP we must explicitly rep-\nresent the uncertainty associated with state transitions. This uncertainty is one of the key\nthings that the MDP formulation allows us to model. We don’t know exactly what state an\nagent will arrive in after taking action atfrom state st, but from the transition matrix, Pat,\nwe do know the probability of each possible transition between states, PpstatÝ Ñst`1q. To\ncalculate the expected return of taking action atfrom state stwe can calculate a weighted\nsum across the expected returns that the agent could receive in every state, st`1, that the\nagent could reach after taking action atin state st. The weights in this weighted sum are\nthe probabilities of the state transitions13\nQπpst,atq“ÿ\nst`1PpstatÝ Ñst`1q«\nRpstatÝ Ñst`1q`γEπ«8ÿ\nk“0γkrt`k`1|st`1ﬀﬀ\n(11.18)\nRecall that RpstatÝ Ñst`1qis the reward received when state st`1is reached from state st\nafter taking action at.\nEquation (11.18)[652]still includes the expected return that arises from taking all of the\nactions after atessentially as it was stated before in Equation (11.17)[651]. In fact, the ﬁnal\npart of Equation (11.18)[652]is almost identical to Equation (11.16)[651]except for the explicit\nreference to an action in the latter. By adding an explicit reference to action at`1, we can\narrive at a very elegant recursive deﬁnition of the action-value function. First the expected\nreturn can be written as a sum across the expected returns of all possible actions at`1that\ncould be taken in state st`1\nQπpst,atq“ÿ\nst`1PpstatÝ Ñst`1q«\nRpstatÝ Ñst`1q`γÿ\nat`1Eπ«8ÿ\nk“0γkrt`k`1|st`1,at`1ﬀﬀ\n(11.19)\nThe calculation of expected return for actions at`1and beyond refers to the policy, π, that\nis used to select the action that will be taken in each state (as did all other calculations of\nexpected return). We can deﬁne the policy as a function that returns a probability distribu-\ntion over the set of possible actions that can be taken from a state, as described in Equation\n13. This is simply the deﬁnition of expectation as deﬁned in probability theory for any random variable :\nErXs “řk\ni“1x1p1`x2p2`x3p3`...`xkpk, where Xis a random variable, x1toxkare the possible","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":706,"page_label":"652","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"could be taken in state st`1\nQπpst,atq“ÿ\nst`1PpstatÝ Ñst`1q«\nRpstatÝ Ñst`1q`γÿ\nat`1Eπ«8ÿ\nk“0γkrt`k`1|st`1,at`1ﬀﬀ\n(11.19)\nThe calculation of expected return for actions at`1and beyond refers to the policy, π, that\nis used to select the action that will be taken in each state (as did all other calculations of\nexpected return). We can deﬁne the policy as a function that returns a probability distribu-\ntion over the set of possible actions that can be taken from a state, as described in Equation\n13. This is simply the deﬁnition of expectation as deﬁned in probability theory for any random variable :\nErXs “řk\ni“1x1p1`x2p2`x3p3`...`xkpk, where Xis a random variable, x1toxkare the possible\nvalues of the random variable, and p1topkare the probabilities of these different values occurring.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":707,"page_label":"653","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 653\n(11.5)[641]. Using this deﬁnition we can rewrite Equation (11.19)[652]\nQπpst,atq“ÿ\nst`1PpstatÝ Ñst`1q«\nRpstatÝ Ñst`1q`γÿ\nat`1πpst`1,at`1qQπpst`1,at`1qﬀ\n(11.20)\nwhereπpst`1,at`1qreturns the likelihood of taking action at`1from state st`1under policy\nπ, and Qπpst`1,at`1qis a recursive call to the action-value function itself. This is one of\ntheBellman Equations ,14which are the foundation stones of reinforcement learning. This\nﬁnal formulation of the action-value function simply states that the expected return from\ntaking action atin state stis the expected reward for taking that action plus the expected\nreturn from all of the subsequent actions that the agent will take as it moves between states\nif it continues to follow the policy π.\nGiven that it is the policy, π, that determines which action an agent will take in a given\nstate, it is reasonable to assume that if an agent follows different policies, then the agent\ncan expect to earn different levels of return—some higher and some lower. It can be shown,\nhowever, that for any MDP an optimal policy, π˚, exists, where following π˚will lead to\nreturns as good as or better than the returns that would be earned following any other policy.\nUsing this idea and Equation (11.19)[652]theBellman optimality equation for action-value\nfunctions can be written\nQ˚pst,atq“ÿ\nst`1PpstatÝ Ñst`1q„\nRpstatÝ Ñst`1q`γmax\nat`1Q˚pst`1,at`1q\n(11.21)\nwhich states that the maximum cumulative return following an action attaken in a state st\nis earned by continuing to take the action that will return the maximum return. The Bell-\nman optimality equation for action-value functions can be used to establish a non-linear\nseries of simultaneous equations (one for each pair of states and actions, pst,atq) which, if\nsolved, will give the true values of Q˚pst,atqfor all states and actions. After these values\nhave been found, a simple policy that gives a zero probability to any action that does not\ngive the maximum return available from a state and a non-zero probability to any action\n(there might be more than one) that does lead to the maximum return available from that\nstate is an optimal policy to use with any MDP. Directly solving the large series of simul-\ntaneous equations that arise from the Bellman optimality equation for MDPs of interesting\nscale, however, is computationally very expensive and requires full knowledge of the dy-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":707,"page_label":"653","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"series of simultaneous equations (one for each pair of states and actions, pst,atq) which, if\nsolved, will give the true values of Q˚pst,atqfor all states and actions. After these values\nhave been found, a simple policy that gives a zero probability to any action that does not\ngive the maximum return available from a state and a non-zero probability to any action\n(there might be more than one) that does lead to the maximum return available from that\nstate is an optimal policy to use with any MDP. Directly solving the large series of simul-\ntaneous equations that arise from the Bellman optimality equation for MDPs of interesting\nscale, however, is computationally very expensive and requires full knowledge of the dy-\nnamics of an MDP for a given scenario that can be unavailable in practice. In the optimal\ncontrol domain in which the Bellman optimality equations were originally conceived, so-\nlutions are calculated using dynamic programming . In modern reinforcement learning,\n14. The Bellman Equations were ﬁrst introduced by Richard Bellman in the 1950s (Bellman, 1957a,b) as part of\nvery early work on reinforcement learning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":708,"page_label":"654","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"654 Chapter 11 Beyond Prediction: Reinforcement Learning\nhowever, iterative approaches that calculate approximate solutions are typically used. The\nnext section describes one of the most important of these, temporal-difference learning .\n11.2.5 Temporal-Difference Learning\nTemporal-difference learning is a simple, iterative, tabular approach to learning the action-\nvalue function, Qπpst,atq, that is quite effective. We say that temporal-difference learning\nis tabular because an action-value table is used to store estimates of the expected return\nfrom taking each available action, at, in each possible state, st—the value of Qπpst,atq.\nRecall that the expected return is the cumulative reward that the agent expects to receive\nif it takes action atin state stand then follows the policy πall the way to the end of the\nepisode. All entries in the action-value table are ﬁrst initialized to random values (or some-\ntimes zeros). Table 11.2[654]shows an example action-value table for the TwentyTwos play-\ning agent discussed in Section 11.2.3[643]. Note that there is an entry for each action-state\ncombination and that the terminal states always have a value of 0.000for every action.\nTable 11.2\nAn action-value table for an agent trained to play the card game TwentyTwos (the simpliﬁed version\nof Blackjack described in Section 11.2.3[643]).\nState Action Value\nPL-DL Twist 0.039\nPL-DL Stick´0.623\nPM-DL Twist´0.597\nPM-DL Stick´0.574\nBUST Twist 0.000\nBUST Stick 0.000\nLOSE Twist 0.000\nLOSE Stick 0.000State Action Value\nPH-DL Twist´0.666\nPH-DL Stick 0.940\nPL-DH Twist´0.159\nPL-DH Stick´0.379\nTIE Twist 0.000\nTIE Stick 0.000State Action Value\nPM-DH Twist´0.668\nPM-DH Stick´0.852\nPH-DH Twist´0.883\nPH-DH Stick 0.391\nWIN Twist 0.000\nWIN Stick 0.000\nTWENTY TWO Twist 0.000\nTWENTY TWO Stick 0.000\nThe goal in temporal-difference learning is to ﬁnd the true values for each entry in this\ntable, and this is achieved by deploying the agent into the environment and updating the\nvalues in the table on the basis of the performance of the agent. Over time, we can expect\nthe table to converge toward optimal values. In temporal-difference learning, the relevant\nvalue in the table is updated after each action that the agent takes. When an agent in state\nsttakes action at, the difference between the current estimated return in the action-value\ntable for that action in that state, Qπpst,atq, and the actual return received after taking the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":708,"page_label":"654","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"PH-DH Stick 0.391\nWIN Twist 0.000\nWIN Stick 0.000\nTWENTY TWO Twist 0.000\nTWENTY TWO Stick 0.000\nThe goal in temporal-difference learning is to ﬁnd the true values for each entry in this\ntable, and this is achieved by deploying the agent into the environment and updating the\nvalues in the table on the basis of the performance of the agent. Over time, we can expect\nthe table to converge toward optimal values. In temporal-difference learning, the relevant\nvalue in the table is updated after each action that the agent takes. When an agent in state\nsttakes action at, the difference between the current estimated return in the action-value\ntable for that action in that state, Qπpst,atq, and the actual return received after taking the\naction is calculated. On the basis of this difference, the estimate for the action-value entry\nQπpst,atqis updated slightly. If the current estimate is higher than the actual return, then\nthe estimate is lowered slightly, and vice versa. A learning rate is used to control the size\nof the changes that are made to the action-value estimates at each iteration. We can write","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":709,"page_label":"655","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.2 Fundamentals 655\nthis action-value table update rule\nQπpst,atqÐQπpst,atq`αpGpst,atq´Qπpst,atqloooooooooooomoooooooooooon\ndifference between actual\nand expected returnsq (11.22)\nwhere Gpst,atqis the actual return received from the point of taking action atto the end of\nthe episode. By repeatedly applying this update rule, the values in the action-value table\nslowly converge to good estimates of their true values.15\nCareful readers might note a contradiction in the paragraph above, however. Equation\n(11.22)[655]states that the action-value table is updated using the actual return received by\nan agent across a full episode, whereas we also said that this update takes place after every\naction. It is not possible to know the actual return that will be earned by the agent across the\nentire episode until the episode is complete, and so it would not be possible to apply this\nupdate rule after early actions in the episode. To overcome this contradiction temporal-\ndifference learning uses an approach known as bootstrapping . Bootstrapping uses the\nexisting estimates of expected returns in the action-value table to make action-value table\nupdates rather than waiting for an episode to ﬁnish to discover what the actual return is.\nWe can see bootstrapping quite clearly in the deﬁnition of the actual temporal-difference\nlearning update rule\nQπpst,atqÐQπpst,atq`αprt`γQπpst`1,at`1qlooooooooooomooooooooooon\nactualreturn´Qπpst,atqloooomoooon\nexpected\nreturnq (11.23)\nRather than waiting for the episode to complete to ﬁnd out the actual return of taking\naction atin state st, the update rule uses the current estimate of the expected return,\nQπpst`1,at`1q. This formulation of temporal-difference learning that performs an action-\nvalue table update after every action is known as TD(0) . As TD(0) updates an entry in the\naction-value table every time an action has been taken, it has the advantage that an agent\ncan start to learn very quickly; however, this approach can take a long time to converge\ntoward the optimal values in the action-value table.16\nTo allow learning to take place in temporal-difference learning, we need to revisit the\nidea of a policy, and use policies that allow a balance of exploration andexploitation .\nTo help illustrate this, imagine Conor has gone to a small Gaeltacht town in Ireland on a\ntwo-week business trip. In Gaeltacht towns people speak the native Irish language rather","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":709,"page_label":"655","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"value table update after every action is known as TD(0) . As TD(0) updates an entry in the\naction-value table every time an action has been taken, it has the advantage that an agent\ncan start to learn very quickly; however, this approach can take a long time to converge\ntoward the optimal values in the action-value table.16\nTo allow learning to take place in temporal-difference learning, we need to revisit the\nidea of a policy, and use policies that allow a balance of exploration andexploitation .\nTo help illustrate this, imagine Conor has gone to a small Gaeltacht town in Ireland on a\ntwo-week business trip. In Gaeltacht towns people speak the native Irish language rather\nthan English (spoken in the rest of the country). It is important for Conor to have a good\nmeal each evening so that he is ready for work again the next day, and he is worried about\n15. It is worth noting that the temporal-difference learning approach to reinforcement learning takes a very similar\napproach to the gradient descent algorithm described in Section 7.3[319].\n16. An alternative TD( λ) uses an approach known as eligibility traces to take account of the actual rewards\nreceived from multiple actions.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":710,"page_label":"656","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"656 Chapter 11 Beyond Prediction: Reinforcement Learning\ngetting things he doesn’t like in the hotel restaurant. Unfortunately, Conor doesn’t speak\nany Irish and the staff in the hotel restaurant don’t speak any English. Conor does, however,\nrecognize the Irish word for chicken, sic´ın, on the hotel restaurant menu, and he does like\nchicken. The menu is quite simple, displaying only ﬁve words: sic´ın,mairteoil ,muiceoil ,\nmuisiri ´un, and brad ´an. So what would be a reasonable policy for Conor to use to order his\ndinner each evening?\nConor could take the approach of always pointing to sic´ınon the menu, as he knows\nwhat this is and he likes chicken. This is an example of a greedy action selection policy .\nConor knows the expected reward for ordering chicken is high compared with the expected\nreward for ordering the other unknown items (which he assumes to be low), and so this\npolicy will return a consistently high reward. A greedy policy exploits current knowledge\nof the rewards that actions are expected to return. Following this policy will lead to the\nbest outcomes on the basis of current knowledge, but does not allow any opportunities\nfor learning. Perhaps one of the other items on the menu is even better than chicken, but\nfollowing a greedy policy will never allow Conor the opportunity to learn this.\nLearning requires an opportunity to explore as well as to exploit. To ensure maximum\nexploration Conor could randomly pick one of the menu items each evening. This would\nallow him to sample every item and update his knowledge about what he likes and what he\ndoesn’t like. Sometimes Conor would be very happy with his choices (his reward would\nbe high), but sometimes he would be disappointed (his reward would be low). While this\nrandom action selection policy is good for learning, pursuing it into the second week\nof Conor’s stay would seem like a bad idea. He would have no opportunity to exploit\nthe knowledge he established in the ﬁrst week about the rewards associated with ordering\ndifferent items.\nInstead, the best policy for Conor to take for learning is to balance exploration and ex-\nploitation. Some days Conor should order the item he knows he likes best, and some days\nhe should choose something new. This will allow Conor to make good choices based on his\ncurrent knowledge, but also to explore new items and learn about the rewards associated\nwith them. The ϵ-greedy action selection policy is a simple action selection policy that","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":710,"page_label":"656","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"random action selection policy is good for learning, pursuing it into the second week\nof Conor’s stay would seem like a bad idea. He would have no opportunity to exploit\nthe knowledge he established in the ﬁrst week about the rewards associated with ordering\ndifferent items.\nInstead, the best policy for Conor to take for learning is to balance exploration and ex-\nploitation. Some days Conor should order the item he knows he likes best, and some days\nhe should choose something new. This will allow Conor to make good choices based on his\ncurrent knowledge, but also to explore new items and learn about the rewards associated\nwith them. The ϵ-greedy action selection policy is a simple action selection policy that\nbalances exploration and exploitation. When an ϵ-greedy policy is used, an agent chooses\nthe best action most of the time, but occasionally—with a probability of ϵ—selects a ran-\ndom action uniformly from those available. So, if on alternate evenings Conor swapped\nbetween choosing his favorite menu item and choosing a menu item at random ( ϵ“0.5),\nby the end of the two weeks he will have enjoyed a whole selection of meals from the hotel\nrestaurant’s dishes of chicken ,beef,pork,mushrooms , and salmon .17\n17. If we extended this example so that the restaurant was slightly hip and prepared different dishes based on the\nmain ingredient chosen every night , some that Conor likes more than others (for example, chicken satay one night\nand chicken pie the next), this would become an example of a multi-armed bandit problem . Multi-armed bandit","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":711,"page_label":"657","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning 657\nIn reinforcement learning scenarios where episodes involve long sequences of actions,\nbalancing exploration and exploitation is even more important. Agents need to make se-\nquences of good decisions in order to get the opportunity to explore deeper into action\nsequences and so need to exploit before exploring in some episodes. For example, if a\nchess-playing agent never took good early moves, the agent would never have a chance\nto learn how to behave in potentially game-winning situations. In talking about reinforce-\nment learning policies, it is worthwhile to distinguish between a behavior policy that an\nagent uses during learning and a target policy that an agent will use when deployed into\nthe world after learning has taken place. After deployment it can often make sense to re-\nstrict an agent to greedy action selection and allow no further exploration. This will ensure\nthat the agent never takes potentially dangerous random actions but limits the ability of\nan agent to continue to learn after being deployed, which could be useful if aspects of the\nenvironment changed over time.18\nOne ﬁnal thing worth noting is that temporal-difference learning is model-free . The\nagent does not need to know anything about the dynamics of the environment in which\nit is acting. The only knowledge an agent using temporal-difference learning requires is\na list of states that exist in the environment, a way to recognize what state it is in, and a\nlist of the actions that it is possible for the agent to take. With this minimal information\ntemporal-difference learning can be used to learn sophisticated, long-term behaviors. Note\nespecially that we do not need to know the full dynamics of an MDP, in particular the state\ntransition probabilities that are captured in a transition matrix. This is useful because often\nin real-life applications it can be difﬁcult to capture these values.\nIn the next section we combine the ideas of temporal-difference learning and a speciﬁc\nbehavior policy to deﬁne the standard approach to reinforcement learning: Q-learning .\n11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning\nAlgorithm 13[658]shows the algorithm for the Q-learning approach to temporal-difference\nlearning. In this approach an agent is deployed into the world and acts sequentially, ob-\nserving the state of the world and taking actions that move it to new states and generate","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":711,"page_label":"657","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"transition probabilities that are captured in a transition matrix. This is useful because often\nin real-life applications it can be difﬁcult to capture these values.\nIn the next section we combine the ideas of temporal-difference learning and a speciﬁc\nbehavior policy to deﬁne the standard approach to reinforcement learning: Q-learning .\n11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning\nAlgorithm 13[658]shows the algorithm for the Q-learning approach to temporal-difference\nlearning. In this approach an agent is deployed into the world and acts sequentially, ob-\nserving the state of the world and taking actions that move it to new states and generate\nreward. This algorithm assumes an episodic scenario in which the agent will repeat mul-\ntiple episodes of the task that is performing (for example, multiple iterations of a game or\nattempts to navigate an environment). During learning, after completing an episode the\nagent will return to the initial state and start again. This will repeat for some pre-speciﬁed\nnumber of episodes after which the expectation is that the agent will have learned to per-\nproblems are a common framework for solving optimization problems where choices have uncertain outcomes\nand can be viewed as a very simple form of reinforcement learning.\n18. This is related to the idea of concept drift discussed in Section 9.4.6[578].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":712,"page_label":"658","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"658 Chapter 11 Beyond Prediction: Reinforcement Learning\nform the task well. The ϵ-greedy policy is often used together with Q-learning for choosing\nactions that balance exploration and exploitation and will be used in this section.19\nAlgorithm 13 Pseudocode description of the Q-learning algorithm for off-policy temporal-\ndifference learning.\nRequire: a behavior policy, π, that chooses actions\nRequire: an action-value function Qthat performs a lookup into an action-value table\nwith entries for every possible action, a, and state, s\nRequire: a learning rate, α, a discount-rate, γ, and a number of episodes to perform\n1:initialize all entries in the action-value table to random values (except for terminal\nstates which receive a value of 0)\n2:foreach episode do\n3: reset stto the initial agent state\n4: repeat\n5: select an action, at, based on policy, π, current state, st, and action-value func-\ntion, Q\n6: take action atobserving reward, rt, and new state st`1\n7: update the record in the action-value table for the action, at, just taken in the\nlast state, st, using:\nQpst,atqÐQpst,atq`αˆ\nrt`γmax\nat`1Qpst`1,at`1q´Qpst,atq˙\n(11.24)\n8: letst“st`1\n9: until agent reaches a terminal state\n10:end for\nThe learning in Q-learning takes place when an entry in the action-value table is updated\nafter an action has taken place using Equation (11.24)[658]at Line 7.20The intuition behind\nthis update is that the value of taking a speciﬁc action in a speciﬁc state, Qpst,atq, should\nincrease if that action leads to an immediate positive reward and/or it takes the agent to\na state from which the agent can expect to receive a positive future return. The value of\ntaking the action in the state should decrease if the reward and/or future return are negative.\n19. There are many other action-selection policies that can be used together with temporal-difference learning.\nFor example, a simple modiﬁcation to the ϵ-greedy policy is to reduce ϵover time so that the amount of explo-\nration that the agent performs reduces over time. Boltzmann action selection is another commonly used alterna-\ntive that uses the action-value function values, Qpst,atq, for each possible action to build a softmax probability\ndistribution, πpst,atq“eQpst,atq{ř\nateQpst,atq. Actions are then selected randomly following this distribution.\n20. This is just a slight modiﬁcation of Equation (11.23)[655].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":713,"page_label":"659","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning 659\nQ-learning is referred to as off-policy learning, as when the update to the value function\nis made the behavior policy (for example, ϵ-greedy action selection) to select the action\nused to calculate the expected future return. Rather, the agent uses a greedy policy—it is\nassumed that the agent will always take the best possible next action (based on the current\naction-value table). This leads Q-learning to be optimistic about what will happen in the\nfuture.\nThe values in Table 11.2[654]actually show the entries in the action-value table learned for\nTwentyTwos after 100,000episodes of Q-learning.21Examining the actions with the max-\nimum expected return in each state shows that the target policy learned is overwhelmingly\ntoStick , with Twist being the best action only in the PL-DL, PL-DH, and PM-DH states.\nEven with this cautious strategy, in an evaluation in which an agent plays a bout of 1,000\nhands of TwentyTwos 100,000times, this player will on average earn a proﬁt of $198˘24\nin1,000hands. This compares favorably to a random player that will expect to lose on\naverage $197˘25out of 1,000hands. Given that the agent started with no knowledge\nof the game and has learned playing strategy through maximization of cumulative reward\nalone in a simple state space, this is pretty impressive.22\n11.3.1 A Worked Example\nGrid worlds have long been used as a good way to illustrate the operation of reinforcement\nlearning algorithms, and we will use this type of example in this section. In a grid world\nscenario an agent must learn to navigate an environment from a start position to a goal\nposition, often avoiding obstacles along the way. A grid world environment is deﬁned by\na rectangular grid in which an agent occupies a single cell and can move horizontally or\nvertically, one cell at a time, across the world. The agent has no map of the environment\nand begins with knowledge only of the actions that can be taken— left,right ,up, ordown —\nand its current state. The state representation is simple, with each cell that an agent can\noccupy in the grid world deﬁning a state. Grid worlds are an excellent environment in\nwhich to study reinforcement learning because, as we will see, it is easy to examine how\nthe action-value table evolves over time.\nFigure 11.4[660]shows a grid world in which an agent must traverse an environment","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":713,"page_label":"659","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"position, often avoiding obstacles along the way. A grid world environment is deﬁned by\na rectangular grid in which an agent occupies a single cell and can move horizontally or\nvertically, one cell at a time, across the world. The agent has no map of the environment\nand begins with knowledge only of the actions that can be taken— left,right ,up, ordown —\nand its current state. The state representation is simple, with each cell that an agent can\noccupy in the grid world deﬁning a state. Grid worlds are an excellent environment in\nwhich to study reinforcement learning because, as we will see, it is easy to examine how\nthe action-value table evolves over time.\nFigure 11.4[660]shows a grid world in which an agent must traverse an environment\nfraught with dangers. The starting position is shown marked with an Sand the goal po-\nsition is marked with a G. The cells marked with an frepresent ﬁery ground that will\ndamage the agent if it is crossed. The agent’s task is to navigate the environment from the\nstart state to the goal state as quickly as possible while avoiding damage from ﬁery ground.\nTo achieve this a reward structure has been designed. The reward for moving between any\n21. Because of the random nature of state transitions in TwentyTwos, a lot of episodes are needed for learning to\nconverge.\n22. Note that TwentyTwos is a much easier game to win than Blackjack, in which it is well known that the house\nalways wins!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":714,"page_label":"660","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"660 Chapter 11 Beyond Prediction: Reinforcement Learning\ntwo normal cells is ´1, the reward for arriving at the goal is 50, and the reward for entering\na ﬁery cell is´20. The agent always starts in the start cell marked with an S. To demon-\nstrate the workings of the Q-learning algorithm, we examine the process of using it to train\nan agent to navigate this environment. In this example the ϵ-greedy policy with ϵ“0.1is\nused throughout, and the hyper-parameters αandγare set toα“0.2andγ“0.9.23\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6S\nf f\nf f\nf f\nG\nFigure 11.4\nA simple grid world. The start position is annotated with an Sand the goal with a G. The squares\nmarked fdenote ﬁre, which is very damaging to an agent.\nThe Q-learning algorithm (Algorithm 13[658])) starts by randomly initializing the action-\nvalue table (Line 1[658]). In this example all entries have been initialized to random numbers\ninr´1,1s. There are 196entries in the full action-value table—one for each of the four\nactions that can be taken in each of the 49states that make up the grid world. Table 11.3[661]\nshows a portion of the action-value table.\nAt the beginning of the ﬁrst episode of the Q-learning process (Line 3[658]), the agent is\nplaced in the starting cell in the grid world ( s0“0-3). From Table 11.3[661]we can see the\nQvalues of the actions available to the agent from the starting state: Qp0-3,upq“´ 0.308,\nQp0-3,downq“0.247,Qp0-3,leftq“0.963, and Qp0-3,rightq“0.455. The leftaction\nis the most attractive in this instance on the basis of its Qvalue. The agent chooses the\nnext action to take, however, using its policy (Line 5[658]), in this case ϵ-greedy with ϵ“\n0.1. Following the ϵ-greedy policy, the agent generates a random number (uniformly from\nr0,1s) of0.634, which is greater than ϵ, and the agent uses greedy action selection and\n23. These values are recommended by Sutton and Barto (2018) but can be tuned through experimentation.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":715,"page_label":"661","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning 661\nTable 11.3\nA portion of the action-value table for the grid world example at its ﬁrst initialization.\nState Action Value\n0-0 up 0.933\n0-0 down´0.119\n0-0 left´0.985\n0-0 right 0.822\n0-1 up 0.879\n0-1 down 0.164\n0-1 left 0.343\n0-1 right´0.832\n0-2 up 0.223\n0-2 down 0.582\n0-2 left 0.672\n0-2 right 0.084\n0-3 up´0.308\n0-3 down 0.247\n0-3 left 0.963\n0-3 right 0.455\n0-4 up´0.634\n...State Action Value\n...\n2-0 left´0.691\n2-0 right 0.668\n2-1 up´0.918\n2-1 down´0.228\n2-1 left´0.301\n2-1 right´0.317\n2-2 up 0.633\n2-2 down´0.048\n2-2 left 0.566\n2-2 right´0.058\n2-3 up 0.635\n2-3 down 0.763\n2-3 left´0.121\n2-3 right 0.562\n2-4 up 0.629\n2-4 down´0.409\n...State Action Value\n...\n6-2 right 0.201\n6-3 up´0.588\n6-3 down 0.038\n6-3 left 0.859\n6-3 right´0.085\n6-4 up 0.000\n6-4 down 0.000\n6-4 left 0.000\n6-4 right 0.000\n6-5 up 0.321\n6-5 down´0.793\n6-5 left´0.267\n6-5 right 0.588\n6-6 up´0.870\n6-6 down´0.720\n6-6 left 0.811\n6-6 right 0.176\nchooses a0“left. The agent performs this action (Line 6[658]), moving to the left, and\nrecords the next state, s1“0-2, and the reward received, r0“´1.\nOn the basis of the action it has taken, the agent now makes an update to Qp0-3,leftq\n(Line 7[658]) using Equation (11.24)[658]. The Qvalues of the actions available to the agent\nfrom state 0-2 are Qp0-2,upq“0.223,Qp0-2,downq“0.582,Qp0-2,leftq“0.672, and\nQp0-2,rightq“0.084(based on Table 11.3[661]). The action with the highest Qvalue from\nstate 0-2 is therefore leftand this is the one used in the update equation. It is this selection\nof the best action, rather than one selected using the behavior policy, that makes Q-learning\nanoff-policy approach. Equation (11.24)[658]is used to perform the update as follows:\nQp0-3,leftqÐQp0-3,leftq`αˆpRp0-3,leftq`γˆQp0-2,leftq´Qp0-3,leftqq\n0.963`0.2ˆp´1`0.9ˆ0.672´0.963q\n0.691\nAs the experience of moving left from state 0-3 to state 0-2 has not led to a large positive\nreward or opened up the potential for a new action that will give signiﬁcant positive return,\nthe value of this action in this state has been reduced. The agent then updates its current\nstate (Line 8[658]),s0“s1“0-2, and returns to the beginning of the algorithm to choose\nthe next action according to the policy. Again, following the ϵ-greedy policy a random","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":716,"page_label":"662","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"662 Chapter 11 Beyond Prediction: Reinforcement Learning\nnumber is generated, 0.073, which this time is below ϵ, and so a random action from those\navailable from state 0-2 is selected. The random action selected in this case is down . Note\nthat this action is different from the action used in the update equation previously ( left).\nThis is because the action is now selected using the ϵ-greedy policy, whereas the action\nused during the update was selected off-policy .\nThe agent then continues to move across the environment making sequential updates to\nthe action-value table until eventually it reaches the goal state after making 150moves\nand accumulating a total reward of ´891. Figure 11.5(a)[663]shows a representation of the\naction-value table after this ﬁrst episode. A grid for each action ( up,down ,left, and right )\nis shown where the shading in the grid illustrates the current estimate of the expected return\nof that action in the state corresponding to the grid cell position (darker shading indicates\nhigher expected return with lighter shading indicating lower expected return).\nThe action-value table is still fairly sparse after the ﬁrst iteration with most combinations\nhaving changed little from their randomly initialized values. There are a couple of inter-\nesting things that are evident, however. At the end of the ﬁrst episode the agent reached\nthe goal cell, state 6-4, by moving leftfrom state 6-5. This led to a large positive reward\nof50. Before this action Qp6-5,leftqgave an expected return from the action-value table\nof´0.267. Following this action it was updated as follows:\nQp6-5,leftqÐQp6-5,leftq`αˆpRp6-5,leftq`γˆQp6-4,upq´Qp6-5,leftqq\n´0.267`0.2ˆp50`0.9ˆ0´´0.267q\n9.786\nThis value can be seen in the dark shading of the cell corresponding to state 6-5 in the\nright grid in Figure 11.5(a)[663]. There are also some very lightly shaded cells in Figure\n11.5(a)[663]—for example, cell 1-2 in the down panel—illustrating large negative values\nresulting from trips into the ﬁery cells and subsequent negative reward.\nThe reason that only the states immediately surrounding those that led to large positive or\nnegative rewards reﬂect these rewards is that Q-learning, and temporal-difference learning\nin general, uses bootstrapping and updates Qvalues immediately after actions rather than\nwaiting until the end of an episode. So the large positive reward achieved when moving to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":716,"page_label":"662","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"´0.267`0.2ˆp50`0.9ˆ0´´0.267q\n9.786\nThis value can be seen in the dark shading of the cell corresponding to state 6-5 in the\nright grid in Figure 11.5(a)[663]. There are also some very lightly shaded cells in Figure\n11.5(a)[663]—for example, cell 1-2 in the down panel—illustrating large negative values\nresulting from trips into the ﬁery cells and subsequent negative reward.\nThe reason that only the states immediately surrounding those that led to large positive or\nnegative rewards reﬂect these rewards is that Q-learning, and temporal-difference learning\nin general, uses bootstrapping and updates Qvalues immediately after actions rather than\nwaiting until the end of an episode. So the large positive reward achieved when moving to\nthe terminal state can only impact the Qvalue of the state that immediately preceded it. As\nlearning continues, however, values start to propagate across the states in the environment\non subsequent episodes. This is clear in the visualization of the action-value table after 35\nepisodes of Q-learning have elapsed that is shown in Figure 11.5(b)[663]. We can see here\nthat the large positive Qvalues from actions taken in states near the goal state have started\nto propagate through the environment, although there are not yet large positive values near\nthe start state. The emergence of knowledge about how to traverse the environment is also\nreﬂected in the fact that the cumulative reward earned by the agent at episode 35 is ´115\nbased on a journey taking 50actions.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":717,"page_label":"663","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.3 Standard Approach: Q-Learning, Off-Policy Temporal-Difference Learning 663\n40\n20\n02040\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6S\nf f\nf f\nf f\nGworld\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6up\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6down\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6left\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6right\n(a) Action-value table after 1 episode\n40\n20\n02040\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6S\nf f\nf f\nf f\nGworld\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6up\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6down\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6left\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6right\n(b) Action-value table after 35 episodes\n40\n20\n02040\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6S\nf f\nf f\nf f\nGworld\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6up\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6down\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6left\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6right\n(c) Action-value table after 350 episodes\n(d) Cumulative Reward\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6\n (e) Policy\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6\n (f) Ofﬂine Path\nFigure 11.5\n(a)–(c) The evolution of the entries in the action-value table over episodes of Q-learning off-policy\ntemporal-difference learning across the grid world. (d) The cumulative reward earned from each\nepisode. (e) An illustration of the target policy learned by the agent after 350episodes. The arrows\nshow the direction with the highest entry in the action-value table for each state. (f) The path the\nagent will take from the start state to the goal state when greedily following the target policy.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":718,"page_label":"664","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"664 Chapter 11 Beyond Prediction: Reinforcement Learning\nFigure 11.5(d)[663]shows the total cumulative reward (or return) earned by the agent for\neach of 350learning episodes that the agent performs in this grid world environment (to\nmake it easier to see the overall trend in this graph a rolling mean across 10 episodes is\nshown). This graph shows that the agent’s performance initially declined, and it started\nto perform quite badly, until after about 40 episodes its performance began to improve.\nPerformance then became stable with a return of about 40after the agent learned a useful\npath through the environment. Figure 11.5(c)[663]visualizes the action-value table after 350\nepisodes have elapsed. By this time a clear optimal route through the graph has emerged\nrunning straight down from the top to the bottom, and we can say that the agent has learned\nto perform the navigation task. Table 11.4[665]shows the same portion of the action-value\ntable shown in Table 11.3[661]after the ﬁnal episode has completed. Extracting the action\nwith the highest Qvalue in each state from the action-value table gives the greedy target\npolicy that the agent, now trained to complete the task, would use after deployment. This\nis shown in Figure 11.5(e)[663]. The path that the agent would take from the start state to the\ngoal state following this policy is shown in Figure 11.5(f)[663].\nIt is worth reﬂecting for a moment that the agent has learned to navigate this grid world\nwithout any knowledge of the overall structure of the environment or indication about\nwhat it should do. Rather, the agent—equipped with only knowledge of the states in the\nworld and the actions that it can take—was able to learn a long-term strategy to complete a\nsomewhat sophisticated task using only the instantaneous rewards that is received for each\nmove that it made.\n11.4 Extensions and Variations\nIn this section we cover two important extensions to the temporal-difference learning ap-\nproach introduced in the previous section. The ﬁrst is the SARSA on-policy modiﬁcation\nto temporal-difference learning. The second is an extension that uses a predictive machine\nlearning model to replace the action-value table to accommodate environments in which\nthe state-action space is too large for tabular methods to work. Speciﬁcally we present the\ndeep Q network (DQN) algorithm.\n11.4.1 SARSA, On-Policy Temporal-Difference Learning","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":718,"page_label":"664","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"somewhat sophisticated task using only the instantaneous rewards that is received for each\nmove that it made.\n11.4 Extensions and Variations\nIn this section we cover two important extensions to the temporal-difference learning ap-\nproach introduced in the previous section. The ﬁrst is the SARSA on-policy modiﬁcation\nto temporal-difference learning. The second is an extension that uses a predictive machine\nlearning model to replace the action-value table to accommodate environments in which\nthe state-action space is too large for tabular methods to work. Speciﬁcally we present the\ndeep Q network (DQN) algorithm.\n11.4.1 SARSA, On-Policy Temporal-Difference Learning\nTheQ-learning algorithm discussed in Section 11.3[657]is referred to as off-policy as the\nbehavior policy is not used to select the action to be taken in the next state when Qvalues\nare updated (a greedy target policy is used instead). On-policy temporal-difference learning\nis an alternative in which the behavior policy is used to select the next action at the update\nstep. SARSA24is the most well-known on-policy temporal-difference learning algorithm,\n24. The name SARSA comes from the terms st,at,rt,st`1, and at`1used in the algorithm and the order in which\nthey appear.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":719,"page_label":"665","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.4 Extensions and Variations 665\nTable 11.4\nA portion of the action-value table for the grid world example after 350episodes of Q-learning have\nelapsed.\nState Action Value\n0-0 up´1.627\n0-0 down´1.255\n0-0 left´1.655\n0-0 right´1.000\n0-1 up 1.302\n0-1 down´1.900\n0-1 left´1.900\n0-1 right 15.173\n0-2 up 13.299\n0-2 down 12.009\n0-2 left 8.858\n0-2 right 18.698\n0-3 up 13.921\n0-3 down 21.886\n0-3 left 15.900\n0-3 right 13.846\n0-4 up 1.637\n...State Action Value\n...\n2-0 left´1.583\n2-0 right´1.217\n2-1 up´1.493\n2-1 down 4.132\n2-1 left´1.643\n2-1 right´36.301\n2-2 up 13.247\n2-2 down´46.862\n2-2 left´0.858\n2-2 right´1.157\n2-3 up 16.973\n2-3 down 29.366\n2-3 left´88.492\n2-3 right´77.447\n2-4 up´1.016\n2-4 down´20.255\n...State Action Value\n...\n6-2 right 40.190\n6-3 up 34.375\n6-3 down 40.206\n6-3 left 24.784\n6-3 right 50.000\n6-4 up 0.000\n6-4 down 0.000\n6-4 left 0.000\n6-4 right 0.000\n6-5 up´0.353\n6-5 down´0.793\n6-5 left 36.823\n6-5 right´0.342\n6-6 up´0.870\n6-6 down´0.720\n6-6 left 1.008\n6-6 right´0.802\nand is described in Algorithm 14[666]. This algorithm is very similar to the Q-learning\nalgorithm in Algorithm 13[658]. The main difference is in the Qvalue update step, where the\nnext action to be taken, at`1, from the new state, st`1, is chosen using the behavior policy\n(Line 7[666]) and then used in updating the value of Qpst,atq(Line 8[666]). This is in contrast\ntoQ-learning, which always assumes the action with the highest expected return will be\nchosen when computing the update. If an ϵ-greedy behavior policy (or any other similar\nbehavior policy that encourages exploration) is used, then occasionally random actions\nrather than the best action will be used in this step.\nThe differences between SARSA and Q-learning can be illustrated by examining the\nbehavior of a SARSA agent in the same grid world environment used in Section 11.3.1[659]\n(we assume the same sequence of random numbers are generated for easy comparison).\nThe algorithm starts by initializing the Qvalues for every action in every state to the same\nvalues used before, shown in Table 11.3[661]. At Line 4[666]the agent selects the ﬁrst action25\nusing theϵ-greedy policy. Assuming that the agent generates the same random number as\nbefore ( 0.634), which is greater than ϵ, greedy action selection will be used and, again,\n25. This action selection is required here to seed the ﬁrst iteration of the loop beginning on Line 5[666].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":720,"page_label":"666","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"666 Chapter 11 Beyond Prediction: Reinforcement Learning\nAlgorithm 14 Pseudocode description of the SARSA algorithm for on-policy temporal-\ndifference learning.\nRequire: a behavior policy, π, that chooses actions\nRequire: an action-value function Qthat performs a lookup into an action-value table\nwith entries for every possible action, a, and state, s\nRequire: a learning rate, α, a discount-rate, γ, and a number of episodes to perform\n1:initialize all entries in the action-value table to random values (except for terminal\nstates which receive a value of 0)\n2:foreach episode do\n3: reset stto the initial agent state\n4: select an action, at, based on policy, π, current state, st, and action-value function,\nQ\n5: repeat\n6: take action atobserving reward, rt, and new state, st`1\n7: select the next action, at`1, based on policy, π, new state, st`1, and action-value\nfunction, Q\n8: update the record in the action-value table for the action, at, just taken in the\nlast state, st, using:\nQpst,atqÐQpst,atq`αprt`γQpst`1,at`1q´Qpst,atqq\n9: letst“st`1andat“at`1\n10: until agent reaches terminal state\n11:end for\na0“leftwill be chosen as it has the highest Qvalue for the start state, 0-3. After taking\nthis action (Line 6[666]) and recording the reward, r0“ ´ 1, and next state, s1“0-2,\nthe behavior policy is used to select the next action that the agent will take (Line 7[666]).\nAssuming again that the same random number as before ( 0.0728 ) that is less than ϵis\ngenerated, random action selection will be used and a1“down will again be selected.\nNow, when updating the value for Qp0-3,leftq, the Qvalue for this action, down , from the\nnext state is used rather than the Qvalue for the best possible action, left, that was used in\nQ-learning. So, Qp0-3,leftqis updated to:\nQp0-3,leftqÐQp0-3,leftq`αˆpRp0-3,leftq`γˆQp0-2,downq´Qp0-3,leftqq\n0.963`0.2ˆp´1`0.9ˆ0.582´0.963q\n0.675","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":721,"page_label":"667","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.4 Extensions and Variations 667\nThe value calculated is not terribly different from the value calculated before. Cumula-\ntively, however, these differences will lead to quite different behavior. Agents trained using\nSARSA tend to learn more conservative strategies than agents trained using Q-learning.\nSince agents using SARSA use a policy with some exploration in their action-value table\nupdate equation, they will often base their estimation of expected return on next actions\nwith quite poor return. This is evident in Figure 11.6[667], which shows a visualization of\nthe ﬁnal action-value table learned by an agent training using SARSA for the grid world.\nHere we see that the SARSA agent favors staying away from any possibility of falling into\none of the dangerous cells rather than taking the more direct route.\n40\n20\n02040\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6S\nf f\nf f\nf f\nGworld\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6up\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6down\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6left\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6right\n(a) Action-value table after 350 episodes\n(b) Cumulative Reward\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6\n (c) Policy\n0 1 2 3 4 5 60\n1\n2\n3\n4\n5\n6\n (d) Ofﬂine Path\nFigure 11.6\n(a) A visualization of the ﬁnal action-value table for an agent trained using SARSA on-policy\ntemporal-difference learning across the grid world after 350episodes. (b) The cumulative reward\nearned from each episode. (c) An illustration of the target policy learned by the agent after 350\nepisodes. The arrows show the direction with the highest entry in the action-value table for each\nstate. (d) The path the agent will take from the start state to the goal state when greedily following\nthe target policy.\nThis preference for caution is often the reason for choosing between SARSA and Q-\nLearning (although this can also be managed through careful design of rewards). On top\nof a preference for caution in a ﬁnal target policy, if training is being undertaken in the real","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":722,"page_label":"668","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"668 Chapter 11 Beyond Prediction: Reinforcement Learning\nworld with potentially expensive equipment, SARSA might be favored over Q-Learning.\nThere is more likelihood of Q-Learning choosing actions that do damage to equipment as\nrandom action selections made by a behavior policy coupled with a target policy favoring\nrisky states could lead to the occasional disaster.\n11.4.2 Deep Q Networks\nIn many environments to which we would like to apply reinforcement learning, it becomes\ndifﬁcult to deﬁne a state representation that captures the nuances of the environment and\ndoes not lead to so many states as to become impossible to use with the tabular approaches\ndescribed so far. Consider, for example, the version of the classic Lunar Lander video\ngame shown in Figure 11.7[669].26In this game the player must land the spacecraft on the\nMoon’s surface without damaging it. The player has just three controls: thrusters that turn\nthe spacecraft left and right, and one that pushes it up. An episode involves an attempt\nto land the spaceship from a position at the top of the screen and ends when either the\nspaceship successfully touches down gently on the landing pad or crashes.\nJust about any state representation we could design that would accurately capture the\ndynamics of this game would result in an action-value table with thousands or hundreds\nof thousands of entries. For example, we could represent state using a vector storing the\nposition of the spaceship (in xandycoordinates), the velocity of the spaceship (again in x\nandydirections), the angular velocity of the spaceship, the spaceship’s altitude above the\nsurface, and the slope of the line connecting the spaceship to the landing pad. Even if we\ndiscretized each of these 7 characteristics allowing just 5 levels ( very low ,low,medium ,\nhigh, and very high ), which would be a gross approximation, this would lead to 75“\n16,807possible states. Adding 4 possible actions ( left,right ,up, and none ) to this would\ngive an action-value table with 67,228entries. Dealing with an action-value table of this\nscale is computationally troublesome, but more important, such a large state space would\nmost likely mean that any training process would inevitably leave much of the action-value\ntable unexplored.\nFor large problems like this, instead of implementing the action-value function, Qπpst,atq,\nas a table storing the value of every action in every state, it would be better to learn a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":722,"page_label":"668","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"discretized each of these 7 characteristics allowing just 5 levels ( very low ,low,medium ,\nhigh, and very high ), which would be a gross approximation, this would lead to 75“\n16,807possible states. Adding 4 possible actions ( left,right ,up, and none ) to this would\ngive an action-value table with 67,228entries. Dealing with an action-value table of this\nscale is computationally troublesome, but more important, such a large state space would\nmost likely mean that any training process would inevitably leave much of the action-value\ntable unexplored.\nFor large problems like this, instead of implementing the action-value function, Qπpst,atq,\nas a table storing the value of every action in every state, it would be better to learn a\ngeneralized version of the action-value function from observation of a small number of\nstate and action pairs. Approaches that do this are referred to as approximate methods .\nLearning generalized functions from a small training dataset is exactly what predictive\nmodeling does, and we can use predictive models, trained using a combination of ideas\nform supervised machine learning and reinforcement learning, for this task! A predictive\n26. The implementation of Lunar Lander used in this example comes from the Gym toolkit developed by OpenAI\n(Brockman et al., 2016). This is a fantastic resource for experimenting with reinforcement learning agents.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":723,"page_label":"669","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.4 Extensions and Variations 669\nFigure 11.7\nThe Lunar Lander environment. The aim of the game is to control the spaceship starting from the\ntop of the world and attempting to land on the landing pad.\nModel\nState: sQ(s, a1 )Q(s, a2 )Q(s, a3 ) Q(s, am )\nFigure 11.8\nFraming the action-value function as a prediction problem.\nmodel can be trained to learn the action-value function\nMpst,atq≈Qπpst,atq (11.25)\nIf aneural network (see Chapter 8[381]) is used for this task (and that is what we will use\nin the approach described in this section), we can output the value of all actions in a given\nstate at the same time across the output layer of a network. So, the problem is reframed\nMpstq≈Qπpst,atq (11.26)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":724,"page_label":"670","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"670 Chapter 11 Beyond Prediction: Reinforcement Learning\nwhere for a given state the model outputs the value of the action-value function for every\naction that could be taken in that state. To simplify notation we refer to action-value func-\ntions implemented as predictive models as QM. Figure 11.8[669]illustrates this framing of\nthe action-value function learning problem.\nNeural networks are a useful modeling approach to use for this prediction problem be-\ncause, as long as a loss function can be formulated, we can use iterative approaches like the\ngradient descent algorithm27to train them. The link between temporal-difference learning\nand gradient descent was already made in Section 11.2.5[654]. Temporal-difference learning\nis based on calculating the error between a predicted value for the expected return from\ntaking an action in a given state and the actual return that is earned when the agent takes\nthat action (Equation (11.23)[655]). We can use exactly this idea to deﬁne a loss function that\ncan be used to train a neural network\nLpQMWpstqq“pti´QMWpst,atqq2(11.27)\n“ˆ\nrt`γmax\nat`1QMWpst`1,at`1q´QMWpst,atq˙2\n(11.28)\nwhere Ware the network weights; and ti, the target feature value, is deﬁned as the actual\nreturn earned from taking an action. We can calculate a gradient of this function\nBLpQMWpst,atqq\nBW“ˆ\nrt`γmax\nat`1QMWpst`1,at`1q´QMWpst,atq˙BQMWpst,atq\nBW\n(11.29)\nThe naive approach to training a neural network using this loss function would be to\nuse the backpropagation of error algorithm with stochastic gradient descent. This would\nmean that every time the agent took an action, at, to move from sttost`1, accumulating\nreward rt, a loss would be calculated using Equation (11.28)[670]and the gradient of this loss\nEquation (11.29)[670]would be backpropagated through the network to update the weights.\nAlgorithm 15[671]outlines this naive approach.\nIn practice, unfortunately, this doesn’t work, for a number of reasons. First, recall that\nwhen gradient descent was discussed, the instances in the training data were shufﬂed at the\nbeginning of each epoch. Gradient-descent-like algorithms for training neural networks\nassume training instances are independent from each other. In this naive approach each\ninstance presented to the network would be highly correlated with the previous instance\npresented (similar states would follow each other on the basis of actions taken) and in-\ndependence would no longer be the case. This would make the training process likely to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":724,"page_label":"670","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Algorithm 15[671]outlines this naive approach.\nIn practice, unfortunately, this doesn’t work, for a number of reasons. First, recall that\nwhen gradient descent was discussed, the instances in the training data were shufﬂed at the\nbeginning of each epoch. Gradient-descent-like algorithms for training neural networks\nassume training instances are independent from each other. In this naive approach each\ninstance presented to the network would be highly correlated with the previous instance\npresented (similar states would follow each other on the basis of actions taken) and in-\ndependence would no longer be the case. This would make the training process likely to\nbecome stuck in something approaching a local minimum. The other problem is that the\n27. See Section 7.3[319].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":725,"page_label":"671","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.4 Extensions and Variations 671\nAlgorithm 15 Pseudocode description of the naive neural Q-learning algorithm.\n1:initialize weights, W, in action-value function network, QM, to random values\n2:foreach episode do\n3: reset stto the initial agent state\n4: repeat\n5: select action, at, based on policy, π, the current state, st, and action-value net-\nwork output, QMpst,atq\n6: take action atand observing reward, rt, and new state, st`1\n7: generate a target feature\nt“rt`γmax\nat`1QMpst`1,at`1q\n8: perform an iteration of stochastic gradient descent using a single training in-\nstanceăst,tą\n9: until agent reaches terminal state\n10:end for\nnetwork being used to generate targets is the actual network being trained. This network is\nchanging frequently during the training process, which can cause the training to oscillate\nwildly. The deep Q network (DQN) algorithm addresses these issues using two key ideas:\nexperience replay andnetwork freezing .\nWhen experience replay is used, each time an agent uses an action-value network QMW\nto select and take an action, at, in a state, st, earning a reward, rt, and moving the agent\nto a new state, st`1, an instance of the form⟨s“st,a“at,r“rt,s1“st`1⟩is added\nto a replay memory ,D. After taking the action, instead of performing a single step\nof stochastic gradient descent, the agent randomly selects a random sample of binstances\nfrom the replay memory, and performs an iteration of mini-batch gradient descent28using\nthis sample as the mini-batch. The target feature values for the instances in the mini-batch\nare generated as described in Algorithm 15[671]. This means that the training process is\nusing its experience of the environment much more efﬁciently because each step is used\nin network training multiple times. Furthermore, the correlations between instances are\nbroken because mini-batches are randomly selected from the replay memory. The replay\nmemory is given a maximum size, N(usually greater than 10,000), and when it reaches\nthis the oldest instances are dropped as new ones are added. Figure 11.9[672]illustrates this\nprocess.\nIn the naive approach described in Algorithm 15[671]the network being trained is also\nbeing used to generate target feature values (Line 6). This can cause the network training\n28. See Chapter 8[381].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"672 Chapter 11 Beyond Prediction: Reinforcement Learning\nTarget Network\nˆQ<latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit>None\nUp\nRightLeft\nBehavior Network","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Up\nRightLeft\nBehavior Network\nQ<latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit>None\nUp\nRightLeft\ns a r s’\nLeft -0.3\nRight -0.3\nNone +100Replace     with","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"every CstepsˆQ<latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit><latexit sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit> Q<latexit","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sha1_base64=\"0PXp9Px2pMFVVX2Xek0u3pTe5OM=\">AAACA3icbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0pWTSTBuayQzJHaEMXfoBbvUT3IlbP8Qv8DfMtLOwrQcCh3PuK8ePpTDout9OYWNza3unuFva2z84PCofn7RMlGjGmyySke741HApFG+iQMk7seY09CVv+5P7zG8/cW1EpB5xGvN+SEdKBIJRtFK7N6aYNmaDcsWtunOQdeLlpAI56oPyT28YsSTkCpmkxnQ9N8Z+SjUKJvms1EsMjymb0BHvWqpoyE0/nZ87IxdWGZIg0vYpJHP1b0dKQ2OmoW8rQ4pjs+pl4n9eN8Hgtp8KFSfIFVssChJJMCLZ38lQaM5QTi2hTAt7K2FjqilDm9DSlmy2NoHJkvFWc1gnrauq51a9xnWldpdnVIQzOIdL8OAGavAAdWgCgwm8wCu8Oc/Ou/PhfC5KC07ecwpLcL5+AbdUmPU=</latexit> Q<latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit><latexit sha1_base64=\"Nc5FV2LLR+hQzPG7Pg8FM3pExX4=\">AAAB/XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuiG5ct2Ae0Q8mkd9rQTGZIMkIpxQ9wq5/gTtz6LX6Bv2GmnYVtPRA4nHNfOUEiuDau++0UNja3tneKu6W9/YPDo/LxSUvHqWLYZLGIVSegGgWX2DTcCOwkCmkUCGwH4/vMbz+h0jyWj2aSoB/RoeQhZ9RYqdHolytu1Z2DrBMvJxXIUe+Xf3qDmKURSsME1brruYnxp1QZzgTOSr1UY0LZmA6xa6mkEWp/Oj90Ri6sMiBhrOyThszVvx1TGmk9iQJbGVEz0qteJv7ndVMT3vpTLpPUoGSLRWEqiIlJ9msy4AqZERNLKFPc3krYiCrKjM1maUs2W+lQz2wy3moO66R1VfXcqte4rtTu8oyKcAbncAke3EANHqAOTWCA8AKv8OY8O+/Oh/O5KC04ec8pLMH5+gW3FZYo</latexit>","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":726,"page_label":"672","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Replay Memory State: s Action: a\nEnvironmentReward: rNext state: s’\nFigure 11.9\nAn illustration of the DQN algorithm including experience replay and target network freezing.\nprocess to become unstable as small change in the outputs of the action-value network can\nlead to sudden changes in the policy as a different action is suddenly favored in a type\nof state. Target network freezing is used to address this. Two different networks are\nused in the training process: an action-value behavior network that is used to predict the\nvalues of actions for making decisions and an action-value target network that is used to\npredict the value of taking subsequent actions in subsequent states when generating target\nfeature values. The action-value target network is frozen and not updated at each iteration\nof the algorithm. It does, however, need to be updated occasionally because otherwise\nthe estimated values used in the loss function will be inaccurate. Therefore, after every C\nsteps the current action-value target network is replaced with a copy of the action-value\nbehavior network. This is also illustrated in Figure 11.9[672]. Target network freezing makes\nthe training process more stable and leads to faster convergence. A pseudocode description\nof the deep Q network algorithm is given in Algorithm 16[673].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":727,"page_label":"673","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.4 Extensions and Variations 673\nAlgorithm 16 Pseudocode description of the deep Q network (DQN) algorithm.\n1:initialize replay memory Dwith Nsteps based on random actions\n2:initialize weights, Win behavior action-value function network, QM, to random values\n3:initialize weights, xWin target action-value function network, xQMtoW\n4:foreach episode do\n5: reset stto the initial agent state\n6: repeat\n7: select action, at, based on agent’s policy, π, the current state, st, and behavior\nnetwork output, QMpst,atq\n8: take action atand observe the resulting reward, rt, and new state, st`1\n9: add tuple⟨s“st,a“at,r“rt,s1“st`1⟩as a new instance in D\n10: randomly select a mini-batch of binstances from Dto give Db\n11: generate target feature values for each instance,⣨\nsi,ai,ri,s1\ni⟩\ninDbas:\nti“ri`γmax\na1xQMps1\ni,a1q\n12: perform an iteration of mini-batch gradient descent using Db\n13: every Csteps letxQM“QM\n14: until agent reaches terminal state\n15:end for\nThe deep Q network algorithm can be used with any state representation that can be in-\nput into a neural network, and can use different neural network architectures. The simplest\nversion of this would be a numeric state vector input into a multi-layer perceptron feed-\nforward network. The algorithm was ﬁrst proposed, however, as an approach to playing\nvideo games in which the only inputs were screenshots of the game. To best handle image\ninputs a convolutional neural network29was used. A single screenshot of a game does\nnot contain sufﬁcient information about the state of an environment and an agent for the\nenvironment to be considered fully observable , and so the Markov assumption does not\nhold. For example, in the single screenshot of the Lunar Lander environment in Figure\n11.7[669], it is not possible to tell at what velocity the spaceship is moving. To overcome\nthis, sequences of the last kscreenshots stacked together can be used as the state represen-\ntation. This is an example of using a state generation function as discussed in Section\n11.2[638]. Usually small stacks of screenshots (e.g. k“4) provide enough information to\ncapture the state.\nIt is difﬁcult to provide a detailed worked example of the DQN algorithm because the\nnumber of weights to be learned and steps required for anything interesting is too large for\n29. See Chapter 8[381].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":728,"page_label":"674","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"674 Chapter 11 Beyond Prediction: Reinforcement Learning\nclear presentation. Instead, to illustrate the DQN algorithm we will examine at a higher\nlevel how an automated player of the Lunar Lander game can be trained. As mentioned\nbefore this game has four actions available to the agent: None ,Up,Left, and Right . State\ncan be represented as a stack of the last 4frames in the game. This is illustrated in Figure\n11.9[672]. There are two ways that an episode can end: an agent can either land successfully\nor crash. The agent earns a reward of `100for landing successfully and a reward of ´100\nfor crashing. During landing the agent receives a reward of `10each time one of its legs\ntouches the ground gently. For every step that the agent is ﬁring one of its thrusters it\nreceives a reward of ´0.3.\nAconvolutional neural network was used as the action-value network. Input images\nwere scaled to 84ˆ84and the network contained hidden convolutional layers with 32,64\nand64units.30Filter sizes were 8ˆ8(stride 4),4ˆ4(stride 3), and 3ˆ3(stride 1).\nRectiﬁed linear activation functions were used in all hidden layer units. A ﬁnal hidden\nlayer ﬂattened the outputs of the previous convolutional layer and contained 512 fully\nconnected units with rectiﬁed linear activations. The output layer was a fully connected\nlayer with 4outputs (one per action) using linear activations . Figure 11.9[672]illustrates\nthis architecture. The behavior policy used was ϵgreedy , but linear annealing was also\nused. Linear annealing allows the value for ϵused inϵgreedy policy to change over time.\nAt the beginning, a large value 0.9is used and this slowly moves down toward a small\nvalue 0.05. During DQN training the size of the replay memory was 50,000and the target\naction-value function network, xQM, was replaced every 10,000steps.\nFigure 11.10(c)[675]shows the changing cumulative reward for the DQN agent as it learns\nto play the Lunar Lander game. The steady increase of return clearly shows that the agent\nwas improving its performance over time. Figure 11.10(a)[675]shows a series of screens\nfrom an episode early in the training process in which the agent performed quite poorly.\nThis can be compared to the screens in Figure 11.10(b)[675]which are from an episode much\nlater in training at which the agent is performing quite well.\nThis example demonstrates that, using the DQN algorithm, it is possible to train agents to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":728,"page_label":"674","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"action-value function network, xQM, was replaced every 10,000steps.\nFigure 11.10(c)[675]shows the changing cumulative reward for the DQN agent as it learns\nto play the Lunar Lander game. The steady increase of return clearly shows that the agent\nwas improving its performance over time. Figure 11.10(a)[675]shows a series of screens\nfrom an episode early in the training process in which the agent performed quite poorly.\nThis can be compared to the screens in Figure 11.10(b)[675]which are from an episode much\nlater in training at which the agent is performing quite well.\nThis example demonstrates that, using the DQN algorithm, it is possible to train agents to\nperform very sophisticated tasks using modern deep neural networks combined with very\nbasic state representations—in this case just screenshots from the game. This ability led to\na resurgence of interest in reinforcement learning in the 2010s after somewhat of a quiet\nperiod. New methods that expand on the ideas described in this section are being proposed\nall the time and some of the most promising are discussed in section 11.6[677].\n11.5 Summary\nThis chapter introduced reinforcement learning , an alternative approach to supervised\norunsupervised machine learning. Some argue that reinforcement learning is closer to\n30. The architecture followed the architecture described by Mnih et al. (2013).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":729,"page_label":"675","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.5 Summary 675\n(a) Poor performance in the Lunar Lander environment early in the learning process.\n(b) Good performance in the Lunar Lander environment after 30,000learning steps.\n0 200 400 600 800 1000 1200 1400\nEpisode1000\n800\n600\n400\n200\n0200Cumulative reward\n(c) Changing return during training.\nFigure 11.10\n(a) Frames from an episode early in the training process in which the agent performs poorly. (b)\nFrames from an episode near the end of the learning process where the agent is starting to be very\neffective. (c) Changing episode returns during DQN training. The gray line shows a 50-episode\nmoving average to better highlight the trend.\nsupervised learning because the reward signal is a form of supervision. The key differ-\nence, however, is that reinforcement learning does not require a labeled dataset containing\nexamples of correct behavior to learn from. Instead, a reinforcement learning agent can be\ndeployed into an environment and learn from experimenting within that environment. This\nis particularly attractive for problems in which automated systems are to be trained to per-\nform a control task—for example, robotics or automated game playing. To use supervised\nlearning for these types of tasks would require an expert controler to operate the system in\norder to generate a dataset containing examples of correct behavior. This is often very time","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":730,"page_label":"676","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"676 Chapter 11 Beyond Prediction: Reinforcement Learning\nconsuming and expensive to generate. Moreover, reinforcement learning agents can often\nﬁnd exceptional solutions to problems that a human operator would not be aware of. This\nhas been the case in the recent deep reinforcement learning successes at games like go,\nchess, and poker at which reinforcement learning agents are playing at world class level\nand use play strategies that are a total surprise to human players.\nIn a reinforcement learning scenario an agent inhabiting an environment attempts to\nachieve a goal by taking a sequence of actions to move it between states . On comple-\ntion of each action the agent receives an immediate scalar reward indicating whether the\noutcome of the action was positive or negative and to what degree. In reinforcement learn-\ning, the degree to which an agent has achieved a goal is measured only by the cumulative\nrewards it has received from each action taken in pursuit of that goal. To choose which\naction to take in a given state the agent uses a policy . Policies rely on being able to assess\ntheexpected return of taking an action in a particular state, and an action-value function\nis used to calculate this. Markov decision processes are particularly useful for formaliz-\ning this structure and provide the scaffolding for reasoning about reinforcement learning\nproblems.\nThe learning approaches described in this chapter are value-based andmodel-free . That\nis, they optimize the action-value function and do not rely on having a model of how the\nworld behaves. This means that they can learn to behave effectively in an environment with\nonly minimal knowledge of that environment—the states that can be occupied and the ac-\ntions that the agent can take are all that the agent needs to know. Temporal-difference\nlearning , and its Q-learning (off-policy ) and SARSA (on-policy ) variants, are standard\napproaches to reinforcement learning and have been used effectively in a variety of en-\nvironments. They take a bootstrapping approach where estimates of action-value are\niteratively improved on the basis of other estimates of action-value. The advantage of this\nis that learning can happen quickly as updates are made after each action the agent takes.\nThe disadvantage is that these approaches can struggle to learn very long-term strategies\nin which reward does not accrue until long after actions have been taken.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":730,"page_label":"676","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tions that the agent can take are all that the agent needs to know. Temporal-difference\nlearning , and its Q-learning (off-policy ) and SARSA (on-policy ) variants, are standard\napproaches to reinforcement learning and have been used effectively in a variety of en-\nvironments. They take a bootstrapping approach where estimates of action-value are\niteratively improved on the basis of other estimates of action-value. The advantage of this\nis that learning can happen quickly as updates are made after each action the agent takes.\nThe disadvantage is that these approaches can struggle to learn very long-term strategies\nin which reward does not accrue until long after actions have been taken.\nThe SARSA and Q-learning approaches are both tabular methods which are limited in\nterms of the size of state space they can handle. Approximate methods are an alternative\nto tabular approaches to reinforcement learning that learn a generalized version of the\naction-value function (or the value function) and can handle much larger state spaces than\ntabular methods. These methods have been in existence since at least the early 1990s but\nhave seen a resurgence of interest in the 2010s with the emergence of deep learning .Deep\nQ networks are a temporal-difference based approach that use a deep neural network to\nlearn a generalize action-value function. These networks have been shown to be especially\neffective, particularly when states are stored in very low-level representations, such as the\narrangement of pieces on a game board or a screenshot of a game.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":731,"page_label":"677","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.6 Further Reading 677\nOne overarching point about reinforcement learning that is worth mentioning is that it\ncomes at the cost of hugely increased computation. Training reinforcement learning agents\nfor sophisticated tasks in complex, dynamic environments can required large computa-\ntional resources for signiﬁcant amounts of time. This is especially the case when approxi-\nmate methods based on deep neural networks are used.\n11.6 Further Reading\nReinforcement learning is built upon the framework of an intelligent agent . The intelligent\nagent view of artiﬁcial intelligence overlaps to a large extent with machine learning but is\na vibrant ﬁeld in its own right. Wooldridge and Jennings (1995) remains a key introduction\nto the ﬁeld and the more recent Wooldridge (2009) adds useful information about systems\nwhere multiple agents compete or cooperate is very useful. Mac Namee (2009) provides\nan overview of the dominant approaches in using intelligent agent systems in games and\nentertainment applications.\nThe ideas of reinforcement learning and Markov decision processes stem from early\nwork by Howard (1960) and Bellman (1957a,b). One fascinating early example is by\nMichie (1961, 1963) who built an automated tic-tac-toe player based on ideas of reinforce-\nment learning. Due to a lack of access to computing resources, however, this was built\nusing over two hundred matchboxes ﬁlled with colored marbles rather than in software!\nSutton and Barto’s textbook has been the deﬁnitive work on reinforcement learning since\nit was ﬁrst published in (Sutton and Barto, 1998), and their recent 2ndedition (Sutton\nand Barto, 2018) is an excellent update. This covers a deep theoretical framing of the\nreinforcement learning problem, as well as a collection of approaches including dynamic\nprogramming, Monte Carlo methods, temporal-difference learning, and other extensions.\nThe book is detailed, rigorous, and easy to read. Bertsekas (2017) is also an excellent and\ndetailed textbook that covers the fundamentals of reinforcement learning, with a leaning\ntoward solutions based on dynamic programming .\nFor a more broad discussion, Isaac Asimov’s I, Robot (Asimov, 1950), in which the\nThree Laws of Robotics ﬁrst appear, is a fun exploration of the challenges of deﬁning re-\nward and utility functions. Similarly, Bostrom’s paperclip maximizer thought experiment\n(Bostrom, 2003) is an interesting exploration of the potential negative consequences of a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":731,"page_label":"677","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"programming, Monte Carlo methods, temporal-difference learning, and other extensions.\nThe book is detailed, rigorous, and easy to read. Bertsekas (2017) is also an excellent and\ndetailed textbook that covers the fundamentals of reinforcement learning, with a leaning\ntoward solutions based on dynamic programming .\nFor a more broad discussion, Isaac Asimov’s I, Robot (Asimov, 1950), in which the\nThree Laws of Robotics ﬁrst appear, is a fun exploration of the challenges of deﬁning re-\nward and utility functions. Similarly, Bostrom’s paperclip maximizer thought experiment\n(Bostrom, 2003) is an interesting exploration of the potential negative consequences of a\nhighly functional intelligent agent pursuing cumulative reward.\nReinforcement learning approaches have been used in automated game playing since\nTD-Gammon (Tesauro, 1994) was developed in the 1990s. This trend has continued and\nrecent, high-proﬁle successes include Deep Q Learning for Atari video games (Mnih et al.,\n2013), AlphaGo for Go (Silver et al., 2017), AlphaZero for chess (Silver et al., 2018), and\nthe OpenAI Five player for DOTA 2 (McCandlish et al., 2018). For more recent advances\nat the junction of reinforcement learning and deep learning, Sejnowski (2018) gives a nice","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":732,"page_label":"678","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"678 Chapter 11 Beyond Prediction: Reinforcement Learning\noverview of some key application areas and recent developments, and again, Sutton and\nBarto (2018) provides a good overview.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":733,"page_label":"679","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.7 Exercises 679\n11.7 Exercises\n1.An agent in an environment completes an episode and receives the following rewards:\ntr0“´33,r1“´11,r2“´12,r3“27,r4“87,r5“156u\n(a)Calculate the discounted return at time t“0on the basis of this sequence of\nrewards using a discounting factor of 0.72.\n(b)Calculate the discounted return at time t“0on the basis of this sequence of\nrewards using a discount rate of 0.22.\n2.To try to better understand the slightly bafﬂing behavior of her new baby, Maria—a\nscientiﬁcally minded new mother—monitored her baby girl over the course of a day\nrecording her activity at 20 minute intervals. The activity stream looked like this (with\ntime ﬂowing down through the columns):\nSLEEPING SLEEPING SLEEPING CRYING SLEEPING SLEEPING\nCRYING SLEEPING HAPPY HAPPY CRYING HAPPY\nSLEEPING SLEEPING CRYING HAPPY SLEEPING HAPPY\nSLEEPING CRYING SLEEPING HAPPY SLEEPING HAPPY\nSLEEPING CRYING SLEEPING HAPPY SLEEPING HAPPY\nHAPPY SLEEPING HAPPY HAPPY SLEEPING HAPPY\nHAPPY SLEEPING HAPPY SLEEPING HAPPY HAPPY\nHAPPY HAPPY HAPPY SLEEPING HAPPY SLEEPING\nSLEEPING SLEEPING HAPPY SLEEPING HAPPY SLEEPING\nSLEEPING HAPPY HAPPY SLEEPING SLEEPING SLEEPING\nSLEEPING HAPPY HAPPY SLEEPING HAPPY SLEEPING\nSLEEPING CRYING CRYING SLEEPING SLEEPING SLEEPING\nMaria noticed that her baby could occupy one of three states—H APPY , CRYING , or\nSLEEPING —and moved quite freely between them.\n(a)On the basis of this sequence of states, calculate a transition matrix that gives the\nprobability of moving between each of the three states.\n(b)Draw a Markov process diagram to capture the behavior of a small baby as de-\nscribed.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":734,"page_label":"680","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"680 Chapter 11 Beyond Prediction: Reinforcement Learning\n3.The following table shows the action-value table for a reinforcement learning agent\nlearning to play the TwentyTwos game after 20 episodes of training have elapsed.\nState Action Value\nPL-DL Twist 0.706\nPL-DL Stick 0.284\nPM-DL Twist´0.985\nPM-DL Stick 0.589\nBUST Twist 0.000\nBUST Stick 0.000\nLOSE Twist 0.000\nLOSE Stick 0.000State Action Value\nPH-DL Twist´0.038\nPH-DL Stick 0.164\nPL-DH Twist 0.386\nPL-DH Stick´0.832\nTIE Twist 0.000\nTIE Stick 0.000State Action Value\nPM-DH Twist 0.533\nPM-DH Stick´0.526\nPH-DH Twist 0.154\nPH-DH Stick 0.103\nWIN Twist 0.000\nWIN Stick 0.000\nTWENTY TWO Twist 0.000\nTWENTY TWO Stick 0.000\nIn the answers to the following questions, assume that after the initial cards have been\ndealt to the player and the dealer, the following cards are coming up next in the deck:\n10♥,2♣,7♣,K♥,9♦.\n(a)At the beginning of the ﬁrst episode the player is dealt p2♥,K♣q, the dealer is\ndealtpA♦,3♦q, and the dealer’s visible card is the A♦. Given these cards, what\nstate is the TwentyTwos playing agent in?\n(b)Assuming that the next action that the agent will take is selected using a greedy\naction selection policy , what action will the agent choose to take ( Stick orTwist )?\n(c)Simulate taking the action that the agent selected in Part (b) and determine the\nstate that the agent will move to following this action and the reward that they will\nreceive. ( Note: If cards need to be dealt to the player or dealer, use cards from the\nlist given at the beginning of this question.)\n(d)Assuming that Q-learning is being used with α“0.2andγ“0.9, update the\nentry in the action-value table for the action simulated in Part (c).\n(e)Assuming that a greedy action selection policy is used again and that Q-learning\nis still being used with α“0.2andγ“0.9, select the next action that the agent\nwill perform, simulate this action, and update the entry in the action-value table\nfor the action. ( Note: If cards need to be dealt to the player or dealer, continue to\nuse cards from the list given at the beginning of this question.)\n(f)On the basis of the changes made to the TwentyTwos playing agent’s action-value\ntable following the two actions taken in the previous parts of this question, how\nhas the agent’s target policy changed?","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":735,"page_label":"681","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"11.7 Exercises 681\n˚4.As part of a project to develop a self-driving taxi system, the behavior of a taxi driver\nhas been observed over a work day. During their shift the taxi driver can C RUISE\nlooking for work, wait for a fare at a taxi R ANK, take a F ARE and deliver a passenger\nto their destination, or take a B REAK . The behavior of the taxi driver over the shift\nlooked like this (with time ﬂowing down through the columns):\nCRUISE RANK RANK FARE CRUISE CRUISE\nFARE CRUISE BREAK FARE FARE BREAK\nCRUISE CRUISE CRUISE FARE FARE RANK\nCRUISE FARE CRUISE FARE FARE RANK\nCRUISE FARE CRUISE FARE CRUISE RANK\nFARE CRUISE FARE FARE CRUISE FARE\nFARE RANK FARE CRUISE FARE FARE\nFARE FARE FARE CRUISE FARE CRUISE\nCRUISE CRUISE FARE CRUISE FARE CRUISE\nCRUISE FARE FARE CRUISE CRUISE CRUISE\nCRUISE FARE FARE CRUISE FARE CRUISE\nCRUISE RANK FARE CRUISE CRUISE CRUISE\n(a)On the basis of this behavior sequence, calculate a transition matrix that gives the\nprobability of moving between all the four states.\n(b)Draw a Markov process diagram to capture the behavior of the taxi driver as\ndescribed.\n˚5.The following image labeled (a) shows a simple schematic of a system used to train\na self-driving car to drive on a four-lane highway. The car has sensors on the front,\nthe rear, and the sides that indicate the presence of other cars or lane barriers in the\narea immediately surrounding the car. The shaded cells in Image (a) below show the\nregion that these sensors cover. The region around the car is divided into cells that\ncan be empty, occupied by another car, or occupied by a barrier. Cars occupy an area\ncovered by two cells (one above the other as shown in Image (a)). Images (b) and (c)\nshow the car in other positions where other cars and barriers are sensed by the car, but\nother cars and barriers are out of range of the sensors.\n(a) (b) (c)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":736,"page_label":"682","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"682 Chapter 11 Beyond Prediction: Reinforcement Learning\nThe car can move at three speeds: stationary ,slow, and fast. When moving fast,\nthe car moves forward two cells per time-step; when moving slowly the car moves\nforward one cell per time-step; and when stationary does not move forward at all. The\nactions that the car can take are to (1) maintain its current speed, (2) increase its speed\n(move up one level in the speed categories stationary, slow, and fast), (3) decrease its\nspeed (move down one level in the speed categories), (4) move to the left, and (5)\nmove to the right.\nWhen the car changes speed, the action has an immediate effect on the car’s progress\nin the current time-step. If the car is stationary, taking the action to move left or right\nhas no effect on the car’s position. If the car is moving slowly, then moving left or\nright moves the car one cell in that direction at that time-step, but not any distance\nforward. If the car is moving fast, then moving left or right moves the car one cell in\nthat direction at that time-step and one cell forward.\nThe goal that the agent is being trained to achieve is to learn to drive as far as possible\nin the shortest amount of time possible without crashing. The car moves forward along\nan inﬁnite highway, and an episode ends if the car crashes into a barrier or another car.\n(a)Design a state representation for the car agent in this scenario. How many states\nwill exist in the representation?\n(b)Given the state representation that you have deﬁned in Part (a) and the actions\navailable to the agent, how many entries would the action-value function table for\na tabular reinforcement learning agent trained for this task have?\n(c)Design a reward function for this scenario.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":737,"page_label":"683","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"IV CASE STUDIES AND CONCLUSIONS","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":739,"page_label":"685","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12 Case Study: Customer Churn\n“There is only one boss. The customer. And he can ﬁre everybody in the company from the chairman\non down, simply by spending his money somewhere else. ”\n—Sam Walton\nAcme Telephonica (AT) is a mobile phone operator that has customers across every state of\nthe USA. Like every telecommunications company, AT struggles with customer churn —\ncustomers leaving AT for other mobile phone operators. AT is always looking for new ways\nto address the churn issue and in 2008 founded a customer retention team. The customer\nretention team monitors the number of calls made to the AT customer support center by\neach customer and identiﬁes the customers who make a large number of customer support\ncalls as churn risks. The customer retention team contacts these customers with special\noffers designed to entice them to stay with AT. This approach, however, has not proved\nparticularly successful, and churn has been steadily increasing over the last ﬁve years.\nIn 2010 AT hired Ross, a predictive data analytics specialist, to take a new approach to\nreducing customer churn. This case study describes the work carried out by Ross when he\ntook AT through the CRISP-DM process1in order to develop a predictive data analytics\nsolution to this business problem. The remainder of this chapter will discuss how each\nphase of the CRISP-DM process was addressed in this project.\n12.1 Business Understanding\nAs is the case in most predictive data analytics projects, AT did not approach Ross with a\nwell-speciﬁed predictive analytics solution. Instead, the company approached him with a\nbusiness problem—reducing customer churn. Therefore, Ross’s ﬁrst goal was to convert\nthis business problem into a concrete analytics solution. Before attempting this conversion,\nRoss had to fully understand the business objectives of AT. This was reasonably straightfor-\nward as AT management had stated that their goal was to reduce their customer churn rates.\nThe only factor left unspeciﬁed was what the magnitude of that reduction was expected to\n1. See Section 1.6[15].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":740,"page_label":"686","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"686 Chapter 12 Case Study: Customer Churn\nbe. Based on previous projects he had worked on, the current approach to customer reten-\ntion that AT was taking, and AT’s historical data, Ross agreed with AT management that a\ntarget reduction from the current high of approximately 10% to approximately 7.5% was\nrealistic and probably achievable. Ross did stress to AT management that until he actually\nexamined the data, he could not know how useful a model he would be able to build.\nRoss’s next task was to fully assess the current situation within AT. In particular, Ross\nneeded to understand the current analytics capability of the company and its readiness\nto take action in response to the insights that an analytics solution would provide. AT\nalready had a customer retention team proactively making interventions in an effort to\nreduce customer churn. Furthermore, this team was already using data from within the\norganization to choose which customers to target for intervention, which suggested that\nthe team members were in a position to use predictive data analytics models.\nRoss spent a signiﬁcant amount of time meeting with Kate, the leader of the customer\nretention team, in order to understand how they worked. Kate explained that at the end of\nevery month, a call list was generated, capturing the customers who had made more than\nthree calls to the AT customer support service in the previous two months. These customers\nwere deemed to be at risk of churning in the coming month, so the customer retention team\nset about contacting them with a special offer. Typically, the offer was a reduced call rate\nfor the next three months, although retention team members had the freedom to make other\noffers.\nRoss also spoke to the chief technology ofﬁcer (CTO) at AT, Grace, in order to un-\nderstand the available data resources. Ross learned that AT had reasonably sophisticated\ntransactional systems for recording recent call activity and billing information. Historic\ncall and bill records as well as customer demographic information were stored in a data\nwarehouse. Grace had played a signiﬁcant role in developing the process that had made in-\nformation about customer support contacts available to the customer retention team. Ross\nhoped that this would make his task a little easier because Grace was the main gatekeeper\nto all the data resources at AT, and having her support for the project would be impor-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":740,"page_label":"686","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"offers.\nRoss also spoke to the chief technology ofﬁcer (CTO) at AT, Grace, in order to un-\nderstand the available data resources. Ross learned that AT had reasonably sophisticated\ntransactional systems for recording recent call activity and billing information. Historic\ncall and bill records as well as customer demographic information were stored in a data\nwarehouse. Grace had played a signiﬁcant role in developing the process that had made in-\nformation about customer support contacts available to the customer retention team. Ross\nhoped that this would make his task a little easier because Grace was the main gatekeeper\nto all the data resources at AT, and having her support for the project would be impor-\ntant. Other parts of the business that Ross spent signiﬁcant time interviewing included the\nbilling department, the sales and marketing team, and the network management.\nThroughout the early stages of the project, Ross had been consciously working on de-\nveloping his situational ﬂuency . Through his discussions with the AT management team,\nKate, and Grace, he had learned a lot about the mobile phone industry. The basic structure\nof the AT business was that customers had a contract for call services that AT provided.\nThese contracts did not have a ﬁxed time and were essentially renewed every month when\na customer paid a ﬁxed recurring charge for that month. Paying the recurring charge en-\ntitled a customer to a bundle of minutes of call time that were offered at a reduction to the\nstandard call rate. For different recurring fees, customers received different-sized bundles\nof call time. When a customer used up all the call time in his or her bundle, subsequent","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":741,"page_label":"687","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.1 Business Understanding 687\ncall time was referred to as over bundle minutes . These tended to be more expensive than\nthe minutes included as part of a customer’s bundle. At AT, all calls were classiﬁed as\neither peak time calls oroff-peak time calls . Peak time was 08:00 to 18:00 from Monday\nto Friday, and calls made during peak time were more expensive than calls made during\noff-peak times.\nBased on his assessment of the current situation within AT, Ross developed a list of\nways that predictive analytics could help address the customer churn problem at AT. These\nincluded ﬁnding answers to the following questions:\n‚What is the overall lifetime value of a customer? A model could be built to predict the\noverall value that AT was likely to receive from a particular customer over the person’s\nentire customer lifecycle. This could be used to identify customers who currently did\nnot look valuable but that were likely to be valuable customers later in their customer\nlifecycles (college students often fall into this category). By offering these customers\nincentives now to prevent them from churning, AT would ensure that it received the full\nvalue from these customers in the future.\n‚Which customers were most likely to churn in the near future? A prediction model\ncould be trained to identify the customers from the AT customer base who were most\nlikely to churn in the near future. The retention team could focus their retention efforts\non these customers. The process that the AT retention team had in place at the beginning\nof the project to identify customers likely to churn took a single feature approach to\nthis identiﬁcation—they looked only at how many calls a customer had made to the AT\ncustomer support service. It was likely that a machine learning model that looked at\nmultiple features would do a better job of identifying customers likely to churn.\n‚What retention offer would a particular customer best respond to? A system could\nbe built to predict which offer, from a set of possible retention offers, a particular cus-\ntomer would be most likely respond to when contacted by the AT retention team. This\ncould help the retention team convince more customers to stay with AT.\n‚Which pieces of the network infrastructure were likely to fail in the near future?\nUsing information about network loads, network usage, and equipment diagnostics, a\npredictive model could be built to ﬂag upcoming equipment failures so that pre-emptive","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":741,"page_label":"687","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"customer support service. It was likely that a machine learning model that looked at\nmultiple features would do a better job of identifying customers likely to churn.\n‚What retention offer would a particular customer best respond to? A system could\nbe built to predict which offer, from a set of possible retention offers, a particular cus-\ntomer would be most likely respond to when contacted by the AT retention team. This\ncould help the retention team convince more customers to stay with AT.\n‚Which pieces of the network infrastructure were likely to fail in the near future?\nUsing information about network loads, network usage, and equipment diagnostics, a\npredictive model could be built to ﬂag upcoming equipment failures so that pre-emptive\naction could be taken. Network outages are a driver of customer dissatisfaction and\nultimately customer churn, so reducing these could have a positive impact on churn rates.\nAfter discussion with the AT executive team, it was decided that the analytics solution\nmost appropriate to focus on was predicting which customers are most likely to churn in\nthe near future. There were a number of reasons this project was selected:\n‚Ross’s previous discussions with Grace, the AT CTO, had established that the data re-\nquired to build a churn prediction model were likely to be available and reasonably easily\naccessible.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":742,"page_label":"688","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"688 Chapter 12 Case Study: Customer Churn\n‚The prediction model could be easily integrated with AT’s current business processes.\nAT already had a retention team in place that was making proactive interventions to help\nprevent churn, albeit using a very simple system to identify which customers to contact.\nBy creating a more sophisticated model to identify those customers, this existing process\nwould be improved.\n‚Building a churn prediction model was also attractive to the AT executive team, as they\nhoped that as well as being useful in reducing churn rates, it would help to explain\nthe main drivers behind customer churn. A better understanding of the main drivers of\ncustomer churn would be useful to many other parts of the AT business.\nBy contrast, the other analytics solutions developed suffered from a lack of available data\n(e.g., AT had no data available on the success or otherwise of various retention offers\nmade); from being too signiﬁcant a change in the business processes used by AT to be\nconsidered achievable at the time (e.g., generating a prediction of the overall lifetime value\nof a customer); or from not being based on sufﬁciently well-grounded assumptions (e.g.,\nthe fact that customer churn is heavily inﬂuenced by network failures).\nOnce the analytics solution had been deﬁned, the next step was to agree on the expected\nperformance of the new analytics model. Based on a recent evaluation of historical per-\nformance, AT management believed at the time of this project that their current system\nfor identifying likely churners had an accuracy of approximately 60%, so any newly devel-\noped system would have to perform considerably better than this to be deemed worthwhile.\nIn consultation with the members of the AT executive team and the retention team, Ross\nagreed that his goal would be to create a churn prediction system that would achieve a\nprediction accuracy in excess of 75%.\n12.2 Data Understanding\nDuring the process of determining which analytics solution was most suitable for the cur-\nrent situation at AT, Ross had already begun to understand the data resources available.\nHis next task was to add much more depth to this understanding, following the process\ndescribed in Section 2.3[28]. This involved working very closely with Grace to understand\nwhat data was available, the formats that the data was kept in, and where the data resided.\nThis understanding would form the basis of Ross’s work on designing the domain con-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":742,"page_label":"688","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"agreed that his goal would be to create a churn prediction system that would achieve a\nprediction accuracy in excess of 75%.\n12.2 Data Understanding\nDuring the process of determining which analytics solution was most suitable for the cur-\nrent situation at AT, Ross had already begun to understand the data resources available.\nHis next task was to add much more depth to this understanding, following the process\ndescribed in Section 2.3[28]. This involved working very closely with Grace to understand\nwhat data was available, the formats that the data was kept in, and where the data resided.\nThis understanding would form the basis of Ross’s work on designing the domain con-\ncepts anddescriptive features that would make up the analytics base table (ABT ), which\nwould drive the creation of the predictive model. This was an iterative process in which\nRoss moved back and forth between Kate at the AT retention team, Grace, the CTO, and\nother parts of the business identiﬁed as having insight into the data associated with cus-\ntomer churn. It quickly became apparent that the key data resources within AT that would\nbe important for this project were\n‚The customer demographic records from the AT data warehouse","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":743,"page_label":"689","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.2 Data Understanding 689\n‚The customer billing records stored in the AT billing database, where records stretch\nback over a time horizon of ﬁve years\n‚The transactional record of calls made by individuals, stretching back over a time horizon\nof 18 months\n‚The sales team’s transactional database, containing details of phone handsets issued to\ncustomers\n‚The retention team’s simple transactional database, containing all the contacts they had\nmade with customers, and the outcomes of these contacts, stretching back to a time\nhorizon of 12 months\nBefore going any further, Ross had to deﬁne the prediction subject for the ABT and the\ntarget feature. The goal was to develop a model that would predict whether a customer\nwould churn in the coming months. This meant that the prediction subject in this case was\na customer, so the ABT would need to be built to contain one row per customer.\nPredicting churn is a form of propensity modeling ,2where the event of interest in this\ncase is a customer making the decision to churn. Consequently, Ross needed to agree\nwith the business (in particular the customer retention team) on a deﬁnition of churn. The\ndeﬁnition would be used to identify churn events in AT’s historical data and, consequently,\nwas fundamental to building the ABT for the project. The business agreed that a customer\nwho had been inactive for one month (i.e., had not made any calls or paid a bill) or who had\nexplicitly canceled or not renewed a contract would be considered to have churned. Ross\nalso needed to deﬁne the lengths of the observation period and the outcome period for\nthe model. He decided that the observation period , during which he would collect data\non customer behavior, would stretch back for 12 months. This was a decision made based\non the data available and Ross’s expectation that anything farther back than this was likely\nto have little impact on predicting churn. With regard to deﬁning the outcome period , the\ncompany agreed that it would be most useful to make a prediction that a customer was\nlikely to churn three months before the churn event took place, as this gave them time to\ntake retention actions. Consequently, the outcome period was deﬁned as three months.3\nWith the target feature suitably deﬁned, Ross’s next task was to determine the domain\nconcepts that would underpin the design of the ABT. The domain concepts are those ar-\neas that the business believes have an impact on a customer’s decision to churn. The","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":743,"page_label":"689","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"on the data available and Ross’s expectation that anything farther back than this was likely\nto have little impact on predicting churn. With regard to deﬁning the outcome period , the\ncompany agreed that it would be most useful to make a prediction that a customer was\nlikely to churn three months before the churn event took place, as this gave them time to\ntake retention actions. Consequently, the outcome period was deﬁned as three months.3\nWith the target feature suitably deﬁned, Ross’s next task was to determine the domain\nconcepts that would underpin the design of the ABT. The domain concepts are those ar-\neas that the business believes have an impact on a customer’s decision to churn. The\ndomain concepts were developed through a series of workshops with representatives of\nvarious parts of the AT business—in particular the retention team, but also sales and mar-\nketing and billing. AT believed that the main concepts that affected churn were underlying\n2. See Section 2.4.3[36].\n3. Obviously, churn events will happen on different dates for different customers; therefore, to build the ABT, the\nobservation and outcome periods for different customers would have to be aligned. This situation is an example\nof the propensity model scenario illustrated in Figure 2.6[39]in Section 2.4.3[36].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":744,"page_label":"690","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"690 Chapter 12 Case Study: Customer Churn\ncustomer demographics (e.g., perhaps younger customers were more likely to churn); cus-\ntomer billing information, and in particular changes in billing patterns (e.g., perhaps cus-\ntomers whose bill suddenly increased were more likely to churn); the details of a cus-\ntomer’s handset (e.g., perhaps customers who have had a handset for a long time are more\nlikely to churn); the interactions a customer has had with AT customer care (e.g., perhaps\ncustomers who are making a large number of calls to customer care are having difﬁculties\nwith the AT network and so are likely to churn); and the actual calls the customer is mak-\ning, in particular, changing call patterns (e.g., perhaps customers who have started making\ncalls to new groups of people are more likely to churn). This set of domain concepts was\nfelt to be extensive enough to cover all the characteristics that were likely to contribute to\na customer’s likelihood to churn and is shown in Figure 12.1[690].\nChurn Prediction\nCustomer\nDemographic sBilling\nInformationHandset\nInformationCustomer Care\nInteractionCall UsageChurn\nIndicator\nBilling\nAmountsBill\nChangeSocial Network\nChangeCall TypesCall Types\nChangeCall\nPerformance\nFigure 12.1\nThe set of domain concepts for the Acme Telephonica customer churn prediction problem.\nFrom these domain concepts, Ross worked on deriving a set of descriptive features.\nSome of the descriptive features were simply copies of available raw data. For example,\ntheAGE,GENDER ,CREDIT RATING , and OCCUPATION columns from the customer demo-\ngraphics data warehouse could be directly included as descriptive features in the ABT to\ncapture the C USTOMER DEMOGRAPHICS domain concept. The more interesting descrip-\ntive features were ones that had to be derived from the raw data sources. For example, Ross\nlearned that the retention team believed that one of the main reasons customers churned\nwas the availability of new, high-end handsets at other networks. To try to capture the\nHANDSET INFORMATION domain concept, Ross designed three descriptive features:\n‚SMART PHONE : This feature indicated whether the customer’s current handset was a\nsmartphone, which was derived from the customer’s most recent handset entry.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":745,"page_label":"691","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.3 Data Preparation 691\n‚NUM HANDSETS : This was a count of how many different handsets the customer had\nhad in the past three years. This was derived from a count of all the handset entries for a\nparticular customer.\n‚HANDSET AGE: Based on a customer’s latest handset entry, this feature captured the\nnumber of days that the customer had had his or her current handset.\nIn churn analysis, and in any sort of propensity modeling, change is usually a key driver\nof customer behavior. For this reason, and based on discussions with the AT team, Ross\nincluded the B ILLCHANGE and S OCIAL NETWORK CHANGE domain concepts. It was\nunderstood by the AT retention team that customers often made a decision to churn if their\nbill increased signiﬁcantly due to changing call patterns, or when they began to make large\nnumbers of calls to new friends or colleagues on other networks. For these reasons, Ross\ndesigned the following descriptive features:\n‚CALL MINUTES CHANGE PCT: Derived from the raw call data, this feature captured the\namount by which the number of minutes a customer used had changed that month com-\npared to the previous month.\n‚BILL AMOUNT CHANGE PCT: Derived from the raw call data, this feature captured the\namount by which a customer’s bill had changed that month compared to previous month.\n‚NEW FREQUENT NUMBERS : Derived from analysis of the actual numbers dialed in the\nraw call data, this feature attempted to capture how many new numbers a customer has\nbegun calling frequently that month. A frequent number was deﬁned as a number that\nconstituted more than 15% of a customer’s total calls.\nOften descriptive features that are likely to be very useful cannot be implemented due to\nthe unavailability of data. For example, the AT team felt that a customer beginning to fre-\nquently call other networks would be a good indicator of churn, but a suitable data feature\ncould not be extracted to capture this. In its call records, AT did not include information\nabout which network calls are made to, and with the free movement of numbers among\noperators, numbers themselves were no longer a reliable indicator of network.\nThe full set of descriptive features Ross developed, along with a short description of\neach, is shown in Table 12.1[692].\n12.3 Data Preparation\nWith help from Grace to implement the actual data manipulation and data integration\nscripts using the tools available at AT, Ross populated an ABT containing all the fea-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":745,"page_label":"691","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"quently call other networks would be a good indicator of churn, but a suitable data feature\ncould not be extracted to capture this. In its call records, AT did not include information\nabout which network calls are made to, and with the free movement of numbers among\noperators, numbers themselves were no longer a reliable indicator of network.\nThe full set of descriptive features Ross developed, along with a short description of\neach, is shown in Table 12.1[692].\n12.3 Data Preparation\nWith help from Grace to implement the actual data manipulation and data integration\nscripts using the tools available at AT, Ross populated an ABT containing all the fea-\ntures listed in Table 12.1[692]. Ross sampled data from the period 2008 to 2013. Using\nthe deﬁnition of churn as a customer who had not made any calls or paid a bill for one\nmonth, Ross was able to identify churn events throughout this time period. To collect in-\nstances of customers who had not churned, Ross randomly sampled customers who did not\nmatch the churn deﬁnition but who also could be deemed active customers. Working with","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":746,"page_label":"692","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"692 Chapter 12 Case Study: Customer Churn\nTable 12.1\nThe descriptive features in the ABT developed for the Acme Telephonica churn prediction task.\nFeature Description\nBILL AMOUNT CHANGE PCT The percentage by which the customer’s bill has changed from last month to\nthis month\nCALL MINUTES CHANGE PCTThe percentage by which the call minutes used by the customer has changed\nfrom last month to this month\nAVGBILL The average monthly bill amount\nAVGRECURRING CHARGE The average monthly recurring charge paid by the customer\nAVGDROPPED CALLS The average number of customer calls dropped each month\nPEAK RATIO CHANGE PCT The percentage by which the customer’s peak calls to off-peak calls ratio has\nchanged from last month to this month\nAVGRECEIVED MINS The average number of calls received each month by the customer\nAVGMINS The average number of call minutes used by the customer each month\nAVGOVERBUNDLE MINS The average number of out-of-bundle minutes used by the customer each month\nAVGROAM CALLS The average number of roaming calls made by the customer each month\nPEAK OFFPEAKRATIO The ratio between peak and off-peak calls made by the customer this month\nNEW FREQUENT NUMBERS How many new numbers the customer is frequently calling this month\nCUSTOMER CARECALLS The number of customer care calls made by the customer last month\nNUM RETENTION CALLS The number of times the customer has been called by the retention team\nNUM RETENTION OFFERS The number of retention offers the customer has accepted\nAGE The customer’s age\nCREDIT RATING The customer’s credit rating\nINCOME The customer’s income level\nLIFE TIME The number of months the customer has been with AT\nOCCUPATION The customer’s occupation\nREGION TYPE The type of region the customer lives in\nHANDSET PRICE The price of the customer’s current handset\nHANDSET AGE The age of the customer’s current handset\nNUM HANDSETS The number of handsets the customer has had in the past 3 years\nSMART PHONE Whether the customer’s current handset is a smartphone\nCHURN The target feature\nKate, Ross deﬁned an active customer as a current customer who made at least ﬁve calls\nper week and who had been a customer for at least six months.4This deﬁnition ensured\n4. The fact that active customers were deﬁned as current customers means that they were all active on the same\ndate—namely, whatever day the ABT was generated. This could be problematic: a model trained on this data","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":746,"page_label":"692","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"REGION TYPE The type of region the customer lives in\nHANDSET PRICE The price of the customer’s current handset\nHANDSET AGE The age of the customer’s current handset\nNUM HANDSETS The number of handsets the customer has had in the past 3 years\nSMART PHONE Whether the customer’s current handset is a smartphone\nCHURN The target feature\nKate, Ross deﬁned an active customer as a current customer who made at least ﬁve calls\nper week and who had been a customer for at least six months.4This deﬁnition ensured\n4. The fact that active customers were deﬁned as current customers means that they were all active on the same\ndate—namely, whatever day the ABT was generated. This could be problematic: a model trained on this data\nmight ignore seasonal effects such as Christmas. The alternative is to deﬁne active customers as any customer\nin the AT data that was active at some point. Such a deﬁnition, however, has the complication that the same\ncustomer could appear in the ABT as both an active and a churn customer, although admittedly the descriptive\nfeatures for these two instances would be calculated over different periods of time.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":747,"page_label":"693","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.3 Data Preparation 693\nthat the non-churn instances in the dataset would include only customers with a relatively\nnormal behavior proﬁle and for which there was a long enough data history that realistic\ndescriptive features could be calculated for them.\nThe ﬁnal ABT contained 10,000instances equally split between customers who churned\nand customers who did not churn. In the raw data, customers who did not churn out-\nnumber those who churned at a ratio of over 10to1. This is an example of an imbal-\nanced dataset , in which the different levels of the target feature—in this case, churners\nand non-churners—are not equally represented in the data. Some of the machine learning\napproaches we have discussed in the preceding chapters perform better when a balanced\nsample is used to train them, and this is why Ross created an ABT with equal numbers of\ninstances with each target level.5\nRoss then developed a full data quality report for the ABT including a range of data\nvisualizations. The data quality report tables are shown in Table 12.2[694]. Ross ﬁrst assessed\nthe level of missing values within the data. Within the continuous features, only AGE\nstood out with 11.47% of values missing. This could be handled reasonably easily using an\nimputation approach,6but Ross held off on performing this at this stage. The REGION TYPE\nand OCCUPATION categorical features both suffered from a signiﬁcant number of missing\nvalues— 74% and47.8%respectively. Ross strongly considered removing these features\nentirely.\nWhen he considered cardinality , Ross noticed that a number of the continuous features\nhad very low cardinality—for example, INCOME ,AGE,NUM HANDSETS ,HANDSET PRICE ,\nand NUM RETENTION CALLS . In most cases, Ross conﬁrmed with Kate and Grace that\nthese were valid because the range of values that the features could take was naturally\nlow. For example, HANDSET PRICE can take only a small number of values—e.g., 59.99,\n129.99,499.99, and so on. The INCOME feature stood out as unusual with only 10dis-\ntinct values (the histogram for this feature conﬁrmed this; see Figure 12.2(a)[695]). Grace\nexplained to Ross that incomes were actually recorded in bands rather than as exact values,\nso this was really a categorical feature. The cardinality of the CREDIT CARD and REGION -\nTYPEcategorical features were higher than expected (the histograms for these features are\nshown in Figures 12.2(b)[695]and 12.2(c)[695]). The issue was that some levels had multiple","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":747,"page_label":"693","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"these were valid because the range of values that the features could take was naturally\nlow. For example, HANDSET PRICE can take only a small number of values—e.g., 59.99,\n129.99,499.99, and so on. The INCOME feature stood out as unusual with only 10dis-\ntinct values (the histogram for this feature conﬁrmed this; see Figure 12.2(a)[695]). Grace\nexplained to Ross that incomes were actually recorded in bands rather than as exact values,\nso this was really a categorical feature. The cardinality of the CREDIT CARD and REGION -\nTYPEcategorical features were higher than expected (the histograms for these features are\nshown in Figures 12.2(b)[695]and 12.2(c)[695]). The issue was that some levels had multiple\nrepresentations—for example, for the REGION TYPE feature, towns were represented as\ntown and as t. Ross easily corrected this issue by mapping the levels of the feature to one\nconsistent labeling scheme.\n5. We return to this discussion in Section 12.5[698]and Section 13.4.1[719].\n6. See Section 3.4[69].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":748,"page_label":"694","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"694 Chapter 12 Case Study: Customer Churn\nTable 12.2\nA data quality report for the Acme Telephonica ABT.\n(a) Data quality report for continuous features\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nAGE 10,000 11.47 40 0.00 0.00 30.32 34.00 48.00 98.00 22.16\nINCOME 10,000 0.00 10 0.00 0.00 4.30 5.00 7.00 9.00 3.14\nNUM HANDSETS 10,000 0.00 19 1.00 1.00 1.81 1.00 2.00 21.00 1.35\nHANDSET AGE 10,000 0.00 1,923 52.00 590.00 905.52 887.50 1,198.00 2,679.00 453.75\nHANDSET PRICE 10,000 0.00 16 0.00 0.00 35.73 0.00 59.99 499.99 57.07\nAVGBILL 10,000 0.00 5,588 0.00 33.33 58.93 49.21 71.76 584.23 43.89\nAVGMINS 10,000 0.00 4,461 0.00 150.63 521.17 359.63 709.19 6,336.25 540.44\nAVGRECURRING CHARGE 10,000 0.00 1,380 0.00 30.00 46.24 44.99 59.99 337.98 23.97\nAVGOVERBUNDLE MINS 10,000 0.00 2,808 0.00 0.00 40.65 0.00 37.73 513.84 81.12\nAVGROAM CALLS 10,000 0.00 850 0.00 0.00 1.19 0.00 0.26 177.99 6.05\nCALL MINUTES CHANGE PCT10,000 0.00 10,000 -16.422 -1.49 0.76 0.50 2.74 19.28 3.86\nBILL AMOUNT CHANGE PCT 10,000 0.00 10,000 -31.67 -2.63 2.96 1.96 7.56 42.89 8.51\nAVGRECEIVED MINS 10,000 0.00 7,103 0.00 7.69 115.27 52.54 154.38 2,006.29 169.98\nAVGOUTCALLS 10,000 0.00 524 0.00 3.00 25.29 13.33 33.33 610.33 35.66\nAVGINCALLS 10,000 0.00 310 0.00 0.00 8.37 2.00 9.00 304.00 17.68\nPEAK OFFPEAKRATIO 10,000 0.00 8,307 0.00 0.78 2.22 1.40 2.50 160.00 3.88\nPEAK RATIO CHANGE PCT 10,000 0.00 10,000 -41.32 -6.79 -0.05 0.01 6.50 37.78 9.97\nAVGDROPPED CALLS 10,000 0.00 1,479 0.00 0.00 0.50 0.00 0.00 9.89 1.41\nLIFE TIME 10,000 0.00 56 6.00 11.00 18.84 17.00 24.00 61.00 9.61\nCUSTOMER CARECALLS 10,000 0.00 109 0.00 0.00 1.74 0.00 1.33 365.67 5.76\nNUM RETENTION CALLS 10,000 0.00 5 0.00 0.00 0.05 0.00 0.00 4.00 0.23\nNUM RETENTION OFFERS 10,000 0.00 5 0.00 0.00 0.02 0.00 0.00 4.00 0.155\nNEW FREQUENT NUMBERS 10,000 0.00 4 0.00 0.00 0.20 0.00 0.00 3.00 0.64\n(b) Data quality report for categorical features\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nOCCUPATION 10,000 74.00 8 professional 1,705 65.58 crafts 274 10.54\nREGION TYPE 10,000 47.80 8 suburb 3,085 59.05 town 1,483 28.39\nMARRIAGE STATUS 10,000 0.00 3 unknown 3,920 39.20 yes 3,594 35.94\nCHILDREN 10,000 0.00 2 false 7,559 75.59 true 2,441 24.41\nSMART PHONE 10,000 0.00 2 true 9,015 90.15 false 985 9.85\nCREDIT RATING 10,000 0.00 7 b 3,785 37.85 c 1,713 17.13\nHOME OWNER 10,000 0.00 2 false 6,577 65.77 true 3,423 34.23\nCREDIT CARD 10,000 0.00 6 true 6,537 65.37 false 3,146 31.46","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":748,"page_label":"694","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"NEW FREQUENT NUMBERS 10,000 0.00 4 0.00 0.00 0.20 0.00 0.00 3.00 0.64\n(b) Data quality report for categorical features\n2nd2nd\n% Mode Mode 2ndMode Mode\nFeature Count Miss. Card. Mode Freq. % Mode Freq. %\nOCCUPATION 10,000 74.00 8 professional 1,705 65.58 crafts 274 10.54\nREGION TYPE 10,000 47.80 8 suburb 3,085 59.05 town 1,483 28.39\nMARRIAGE STATUS 10,000 0.00 3 unknown 3,920 39.20 yes 3,594 35.94\nCHILDREN 10,000 0.00 2 false 7,559 75.59 true 2,441 24.41\nSMART PHONE 10,000 0.00 2 true 9,015 90.15 false 985 9.85\nCREDIT RATING 10,000 0.00 7 b 3,785 37.85 c 1,713 17.13\nHOME OWNER 10,000 0.00 2 false 6,577 65.77 true 3,423 34.23\nCREDIT CARD 10,000 0.00 6 true 6,537 65.37 false 3,146 31.46\nCHURN 10,000 0.00 2 false 5,000 50.00 true 5,000 50.00","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":749,"page_label":"695","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.3 Data Preparation 695\nincomeDensity\n0 2 4 6 80.00 0.05 0.10 0.15 0.20 0.25\n(a) INCOME\nTRUE t no f yes\ncreditCardDensity\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nFALSE (b) CREDIT CARD\ntownabsentt r\nregionT ypeDensity\n0.0 0.1 0.2 0.3 0.4\nmissing suburb\nrurals (c) REGION TYPE\ncurrentHandsetPr iceDensity\n0 100 200 300 400 5000.000 0.005 0.010 0.015 0.020\n(d) HANDSET PRICE\navgMinsDensity\n0 1000 2000 3000 4000 5000 60000.0000 0.0005 0.0010 0.0015 (e) AVGMINS\navgReceiv edMinsDensity\n0 500 1000 1500 20000.000 0.002 0.004 0.006 0.008\n(f)AVGRECEIVED MINS\navgOv erBundleMinsDensity\n0 100 200 300 400 5000.00 0.01 0.02 0.03 0.04 (g) AVGOVERBUNDLE MINS\nFigure 12.2\n(a)–(c) Histograms for the features from the AT ABT with irregular cardinality; (d)–(g) histograms\nfor the features from the AT ABT that are potentially suffering from outliers.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":750,"page_label":"696","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"696 Chapter 12 Case Study: Customer Churn\nFour continuous features stood out as possibly suffering from the presence of outliers :\nHANDSET PRICE , with a minimum value of 0, which seemed unusual; AVGMINS, with a\nmaximum of 6,336.25, which was very different from the mean and the 3rdquartile val-\nues for that feature; AVGRECEIVED MINS, with a maximum of 2,006.29, which was also\nvery different from the mean and the 3rdquartile values for that feature; and AVGOVER-\nBUNDLE MINS, with minimum, 1stquartile, and median values of 0compared to a mean of\n40. Figure 12.2[695]presents the histograms for these features. Ross conﬁrmed with Grace\nand Kate that these were valid outliers—for example, some handsets are given away for\nfree, and some customers just make a lot of calls. They did spend some time, however,\ndiscussing the AVGOVERBUNDLE MINS. The histogram for this feature has an unusual\nshape that results in the unusual minimum, 1stquartile, and median values (see Figure\n12.2(g)[695]). By examining the data for this feature more closely, they eventually explained\nthis shape by the fact that most customers did not go over the number of minutes in their\nbundle, which accounts for the large bar for 0in this histogram. The values above zero\nseem to follow something close to a wide normal distribution, and the large number of,\nalbeit valid, zero values account for the unusual minimum, 1stquartile, and median values.\nAt this point Ross just made note of these outliers as something he might have to deal with\nduring the modeling phase.\nRoss then turned his attention to examining the data visualizations of the relationship\nbetween each descriptive feature and the target feature. No individual feature stood out as\nhaving a very strong relationship, but the evidence of connections between the descriptive\nfeatures and the target feature could be seen. For example, Figure 12.3(a)[697]shows a\nslightly higher propensity of people in rural areas to churn. Similarly, Figure 12.3(b)[697]\nshows that customers who churned tended to make more calls outside their bundle than\nthose who did not.\nOnce Ross had reviewed the full data quality report in detail, he made the following de-\ncisions regarding the problematic features he had identiﬁed. First, he decided to delete the\nAGE and OCCUPATION features because of the level of missing values in each of these fea-\ntures. He decided to keep the REGION TYPE feature, however, because it appeared to have","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":750,"page_label":"696","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"having a very strong relationship, but the evidence of connections between the descriptive\nfeatures and the target feature could be seen. For example, Figure 12.3(a)[697]shows a\nslightly higher propensity of people in rural areas to churn. Similarly, Figure 12.3(b)[697]\nshows that customers who churned tended to make more calls outside their bundle than\nthose who did not.\nOnce Ross had reviewed the full data quality report in detail, he made the following de-\ncisions regarding the problematic features he had identiﬁed. First, he decided to delete the\nAGE and OCCUPATION features because of the level of missing values in each of these fea-\ntures. He decided to keep the REGION TYPE feature, however, because it appeared to have\nsome relationship with the target. He also applied the planned mapping of the REGION -\nTYPE values to a consistent labeling scheme: ts|suburbu Ñ suburb ;tt|townu Ñ town ;\ntmissing|absentuÑ missing .\nRoss further divided this dataset into three randomly sampled partitions—a training par-\ntition ( 50%), a validation partition ( 20%) and a test partition ( 30%). The training partition\nwas used as the core training data for the prediction models built. The validation partition\nwas used for tuning tasks, and the test partition was used for nothing other than a ﬁnal test\nof the model to evaluate its performance.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":751,"page_label":"697","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.4 Modeling 697\n01000 3000\nr s t\nabsent\nregion Typechurn\nFALSE\nTRUE0 0.5 1\nruralsuburb\ntown\nmissingFrequency Percentage\n(a) REGION TYPE\nchurn = FALSE\navgOv erBundleMinsDensity\n0 100 200 300 400 5000.00 0.01 0.02 0.03 0.04 0.05churn = TR UE\navgOv erBundleMinsDensity\n0 100 200 300 400 5000.00 0.01 0.02 0.03 0.04 0.05 (b) AVGOVERBUNDLE MINS\nFigure 12.3\n(a) A stacked bar plot for the REGION TYPE feature; and (b) histograms for the AVGOVER-\nBUNDLE MINSfeature by target feature value.\n12.4 Modeling\nThe requirements for this model were that it be accurate, that it be capable of being in-\ntegrated into the wider AT processes, and, possibly, that it act as a source of insight into\nthe reasons people might churn. In selecting the appropriate model type to use, all these\naspects, along with the structure of the data, should be taken into account. In this case,\nthe ABT was composed of a mixture of continuous and categorical descriptive features\nand had a categorical target feature. The categorical target feature, in particular, makes\ndecision trees a suitable choice for this modeling task. Furthermore, decision tree algo-\nrithms are capable of handling both categorical and continuous descriptive features as well\nas handling missing values and outliers without any need to transform the data. Finally,\ndecision trees are relatively easy to interpret, which means that the structure of the model\ncan give some insight into customer behavior. All these factors taken together indicated\nthat decision trees were an appropriate modeling choice for this problem.\nRoss used the ABT to train, tune, and test a series of decision trees to predict churn\ngiven the set of descriptive features. The ﬁrst tree that Ross built used an entropy-based\ninformation gain as the splitting criterion, limited continuous splits to binary choices, and\nno pruning. Ross had decided, again in consultation with the business, that a simple classi-\nﬁcation accuracy rate was the most appropriate evaluation measure for this task. The ﬁrst","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":752,"page_label":"698","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"698 Chapter 12 Case Study: Customer Churn\ntree constructed achieved an average class accuracy7of74.873% on the hold-out test set,\nwhich was reasonably encouraging.\nCHURN\nCHURN NON-CHURN CHURN\nCHURN NON-CHURN\nNON-CHURN NON-CHURN CHURN\nCHURN\nCHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nCHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN\nCHURN CHURN NON-CHURN CHURN\nNON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN\nNON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nCHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN NON-CHURN NON-CHURN NON-CHURN CHURN\nCHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN genTree372\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN\nCHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN\nCHURN CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":752,"page_label":"698","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN\nCHURN CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN\nCHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nFigure 12.4\nAn unpruned decision tree built for the AT churn prediction problem (shown only to indicate its size\nand complexity). The excessive complexity and depth of the tree are evidence that overﬁtting has\nprobably occurred.\nThis tree is shown in Figure 12.4[698], and the lack of pruning is obvious in its complexity.\nThis complexity and the excessive depth of the tree suggest overﬁtting. In the second tree\nthat he built, Ross employed post-pruning using reduced error pruning ,8which used the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":752,"page_label":"698","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN CHURN NON-CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nNON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN NON-CHURN CHURN\nFigure 12.4\nAn unpruned decision tree built for the AT churn prediction problem (shown only to indicate its size\nand complexity). The excessive complexity and depth of the tree are evidence that overﬁtting has\nprobably occurred.\nThis tree is shown in Figure 12.4[698], and the lack of pruning is obvious in its complexity.\nThis complexity and the excessive depth of the tree suggest overﬁtting. In the second tree\nthat he built, Ross employed post-pruning using reduced error pruning ,8which used the\nvalidation partition that was created from the initial dataset. The reasonably large dataset\nthat Ross had to begin with, which in turn led to a reasonably large validation partition,\nmeant that reduced error pruning was appropriate in this case.9Figure 12.5[699]shows the\ntree resulting from this training iteration. It should be clear that this is a much simpler\ntree than the previous one. The features used at the top levels of both trees, and deemed\nmost informative by the algorithm, were the same: AVGOVERBUNDLE MINS,BILL AM-\nOUNT CHANGE PCT, and HANDSET AGE.\nUsing pruning, Ross was able to increase the average class accuracy on the hold-out test\nset to 79.03%, a signiﬁcant improvement over the previous model. Table 12.3[699]shows the\nconfusion matrix from this test. The confusion matrix shows that this model was slightly\nmore accurate when classifying instances with the non-churn target level than with the\nchurn target level. Based on these, results Ross was conﬁdent that this tree was a good\nsolution for the AT churn prediction problem.\n12.5 Evaluation\nThe model evaluations based on misclassiﬁcation rate described in the previous section are\nthe ﬁrst step in evaluating the performance of the prediction model created. The classiﬁ-\ncation accuracy of 79.03% is well above the target agreed on with the business. This is\n7. All average class accuracies used in this section use a harmonic mean .\n8. See Section 4.4.4[153].\n9. If data had been more scarce, pruning using a statistical test, such as χ2, would have been a more sensible\nroute to take.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":753,"page_label":"699","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.5 Evaluation 699\navgOverBundleMins\nchurn≥99.2\nbillAmountChangePct<99.2 or Missing\nchurn≥9.52\nhandsetAge<9.52 or Missing\nbillAmoutChangePct<1,240.5\nhandsetAge≥1,240.5\n<-13.97  ≥-13.97 or Missing  <1,598.5 or Missing     ≥1,598.5\nnon-churn non-churn non-churn\nchurn churn churn churn\nnon-churn churn churn churn non-churn churn\nchurn non-churn non-churn non-churn churn non-churn churn\nnon-churn churn churn non-churn non-churn churn churn non-churn\nchurn churn\nnon-churn churn non-churn\nchurn non-churn\nFigure 12.5\nA pruned decision tree built for the AT churn prediction problem. Gray leaf nodes indicate a churn\nprediction, and clear leaf nodes indicate a non-churn prediction. For space reasons, we show only\nthe features tested at the top-level nodes.\nTable 12.3\nThe confusion matrix from the test of the AT churn prediction stratiﬁed hold-out test set using the\npruned decision tree in Figure 12.5[699].\nPrediction\nchurn non-churn Recall\nTargetchurn 1,058 442 70.53\nnon-churn 152 1,348 89.86\nmisleading, however. This performance is based on a stratiﬁed hold-out test set, which\ncontains the same number of churners and non-churners. The underlying distribution of\nchurners and non-churners within the larger AT customer base, however, is much different.\nRather than a 50:50split of churners to non-churners, the actual underlying ratio is, in\nfact, closer to 10:90. For this reason, it is very important to perform a second evaluation\nin which the test data reﬂect the actual distribution of target feature values in the business\nscenario.\nRoss had AT generate a second data sample (which did not overlap with the sample taken\npreviously) that was not stratiﬁed according to the target feature values. The confusion","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":754,"page_label":"700","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"700 Chapter 12 Case Study: Customer Churn\nTable 12.4\nThe confusion matrix from the test of the AT churn prediction non-stratiﬁed hold-out test set.\nPrediction\nchurn non-churn Recall\nTargetchurn 1,115 458 70.88\nnon-churn 1,439 12,878 89.95\nmatrix illustrating the performance of the prediction model on this test set is shown in\nTable 12.4[700].\nThe average class accuracy on the non-stratiﬁed hold-out test set was 79.284% . Ross\nalso generated cumulative gain ,lift, and cumulative lift charts for the dataset.10These\nare shown in Figure 12.6[700]. The cumulative gain chart in particular shows that if AT were\nto call just 40% of their customer base, they would identify approximately 80% of the\ncustomers who are likely to churn, which is strong evidence that the model is doing a good\njob of distinguishing between different customer types.\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF /uni25CF /uni25CF /uni25CF /uni25CF0.0 0.2 0.4 0.6 0.8 1.0\nDecileCum ulativ e Gain\n1st 3rd 5th 7th 9th\n(a) Cumulative gain\n/uni25CF /uni25CF /uni25CF /uni25CF\n/uni25CF\n/uni25CF\n/uni25CF /uni25CF /uni25CF /uni25CF0.0 0.5 1.0 1.5 2.0\nDecileLift\n1st 3rd 5th 6th 7th 8th 9th (b) Lift\n/uni25CF /uni25CF /uni25CF /uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF1.0 1.2 1.4 1.6 1.8 2.0\nDecileCum ulativ e Lift\n1st 3rd 5th 6th 7th 8th 9th (c) Cumulative lift\nFigure 12.6\n(a) Cumulative gain, (b) lift, and (c) cumulative lift charts for the predictions made on the large test\ndata sample.\nGiven these good results Ross decided that it was appropriate to present the model to\nother parts of the business. This was an important step in gaining credibility for the model.\nThe tree shown in Figure 12.5[699]is reasonably straightforward to interpret, but when taken\nout to other parts of the business, it may be hard for people to deal with this much informa-\n10.Cumulative gain ,lift, and cumulative lift are introduced in Section 9.4.3.3[565].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":755,"page_label":"701","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"12.5 Evaluation 701\ntion, so Ross decided to create a purposefully stunted version of the decision tree, with only\na small number of levels shown for the presentation of the model to the business (although\nhe intended to use the larger pruned tree for actual deployment). The idea behind this was\nthat stunting the tree made it more interpretable. The fact that the most informative features\noccupy berths toward the top of a tree means that stunted trees usually capture the most\nimportant information. Many machine learning tools will allow the maximum depth of a\ntree to be speciﬁed as a parameter, which allows for the creation of such stunted trees .\nFigure 12.7[701]shows the stunted tree Ross generated for the churn problem, where the\ndepth of the tree is limited to ﬁve layers. This tree results in a slightly lower classiﬁcation\naccuracy on the test partition, 78.5%, but is very easy to interpret—the key features in\ndetermining churn are clearly AVGOVERBUNDLE MINS,BILL AMOUNT CHANGE PCT, and\nHANDSET AGE. It seems, from this data, that customers are most likely to churn when their\nbill changes dramatically, when they begin to exceed the bundled minutes in their call\npackage, or when they have had a handset for a long time and are considering changing\nto something newer. This is useful information that the business can use to attempt to\ndevise other churn handling strategies in parallel to using this model to create call lists\nfor the retention team. The business was interested in the features that were selected as\nimportant to the tree, and there was a good deal of discussion on the omission of the\nfeatures describing customers’ interactions with AT customer care (these had been the\nbasis of the organization’s previous model).\navgOverBundleMins\nchurn≥99.2\nbillAmountChangePct<99.2 or Missing\nchurn≥9.52\nhandsetAge<9.52 or Missing\nbillAmoutChangePct<1240.5\nhandsetAge ≥1240.5\nchurn<-13.97\nnon-churn ≥-13.97 or Missing\nnon-churn<1,598.5 or Missing\nchurn≥1,598.5\nFigure 12.7\nA pruned and stunted decision tree built for the Acme Telephonica churn prediction problem.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":756,"page_label":"702","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"702 Chapter 12 Case Study: Customer Churn\nTo further support his model, Ross organized a control group test (see Section 9.4.6[578])\nin which for two months, the AT customer base was randomly divided into two groups, and\ncall lists for the retention team were selected from the ﬁrst group using the old approach\nbased on calls to customer care, and for the second group using the new decision tree\nmodel. It was shown after two months that the churn rate within the sample for which\nthe retention team used the new model to build their call list was approximately 7.4%,\nwhile for the group using the old model, it was over 10%. This experiment showed the\nAT executive team that the new decision tree model could signiﬁcantly reduce churn rates\nwithin the AT customer base.\n12.6 Deployment\nBecause AT was already using a process in which its retention team generated call lists\nbased on collected data, deployment of the new decision tree model was reasonably straight-\nforward. The main challenge was a return to the Data Preparation phase to make the rou-\ntines used to extract the data for the ABT robust and reliable enough to be used to generate\nnew query instances every month. This involved working with the AT IT department to\ndevelop deployment-ready extract-transform-load (ETL) routines. Code was then writ-\nten to replace the previous simple rule about customer care contacts with the decision tree\nwhen retention call lists were generated.\nThe last step in deployment was to put in place an ongoing model validation plan to\nraise an alarm if evidence arose indicating that the deployed model had gone stale . In this\nscenario, feedback on the performance of the model implicitly arises within a reasonably\nshort amount of time after predictions are made—churn predictions can be easily compared\nto actual customer behavior (taking into account interventions made by the business). The\nmonitoring system that Ross put in place generated a report at the end of every quarter that\nevaluated the performance of the model in the previous quarter by comparing how many\nof the people not contacted by the retention team actually churned. If this number changed\nsigniﬁcantly from what was seen in the data used to build the model, the model would be\ndeemed stale, and retraining would be required.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":757,"page_label":"703","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13 Case Study: Galaxy Classiﬁcation\n“The history of astronomy is a history of receding horizons. ”\n—Edwin Powell Hubble\nAstronomy has gone through a revolution in recent years as the reducing costs of digital\nimaging has made it possible to collect orders of magnitude more data than ever before.\nLarge-scale sky scanning projects are being used to map the whole of the night sky in\nintricate detail. This offers huge potential for new science based on this massive data\ncollection effort. This progress comes at a cost, however, as all this data must be labeled,\ntagged, and cataloged. The old approach of doing all this manually has become obsolete\nbecause the volume of data involved is just too large.\nTheSloan Digital Sky Survey (SDSS ) is a landmark project that is cataloging the night\nsky in intricate detail and is facing exactly the problem described above.1The SDSS\ntelescopes collect over 175GB of data every night, and for the data collected to be fully\nexploited for science, each night sky object captured must be identiﬁed and cataloged\nwithin this data in almost real time. Although the SDSS has been able to put in place\nalgorithmic solutions to identifying certain objects within the images collected, there have\nbeen a number of difﬁculties. In particular, it has not been possible for the SDSS to develop\na solution to automatically categorize galaxies into the different morphological groups—\nfor example, spiral galaxies or elliptical galaxies.\nThis case study2describes the work undertaken when, in 2011, the SDSS hired Jocelyn,\nan analytics professional, to build a galaxy morphology classiﬁcation model to include in\ntheir data processing pipeline. The remainder of this chapter describes the work undertaken\nby Jocelyn on this project within each phase of the CRISP-DM process.\n1. Full details of the SDSS project, which is fascinating, are available at www.sdss.org.\n2. Although this case study is based on real data downloaded from the SDSS, the case study itself is entirely\nﬁctitious and developed only for the purposes of this book. Very similar work to that described in this section\nhas, however, actually been undertaken, and details of representative examples are given in Section 13.6[727].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":758,"page_label":"704","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"704 Chapter 13 Case Study: Galaxy Classiﬁcation\n13.1 Business Understanding\nWhen Jocelyn ﬁrst arrived at SDSS, she was pleased to ﬁnd that the business problem\nshe was being asked to help with was already pretty well deﬁned in predictive analytics\nterms. The SDSS pipeline takes the data captured by the SDSS instruments and processes\nit, before storing the results of this processing in a centrally accessible database. At the\ntime Jocelyn arrived, the SDSS pipeline included rule-based systems that could classify\nnight sky objects into broad categories—for example, stars and galaxies. SDSS scien-\ntists, however, were struggling to build rule-based systems that could accurately perform\nmore ﬁne-grained classiﬁcations. In particular, the SDSS scientists wanted a system that\ncould reliably classify galaxies into the important morphological (i.e., shape) types: ellip-\ntical galaxies andspiral galaxies . Classifying galaxies according to galaxy morphology\nis standard practice in astronomy,3and morphological categories have been shown to be\nstrongly correlated with other important galaxy features. So, grouping galaxies by morpho-\nlogical type is a fundamentally important step in analyzing the characteristics of galaxies.\nThis was the challenge that the SDSS had hired Jocelyn to address. The scientists at\nSDSS wanted Jocelyn to build a machine learning model that could examine sky objects\nthat their current rule-based system had ﬂagged as being galaxies and categorize them as\nbelonging to the appropriate morphological group. Although there remained some details\nleft to agree on, the fact that the SDSS had deﬁned their problem in terms of analytics meant\nthat Jocelyn very easily completed the important step of converting a business problem\ninto an analytics solution. Edwin was assigned to Jocelyn as her key scientiﬁc contact\nfrom SDSS and was eager to answer any questions Jocelyn had as he saw real value in the\nmodel she was developing.\nThe ﬁrst detail that Jocelyn needed to agree on with Edwin was the set of categories into\nwhich sky objects should be categorized. The scientists at SDSS listed two key galaxy\nmorphologies of interest: elliptical andspiral . The spiral category further divided into\nclockwise spiral andanti-clockwise spiral subcategories. Figure 13.1[705]shows illustrations\nof these different galaxy types. Jocelyn suggested that she would ﬁrst work on the coarse\nclassiﬁcation of galaxies into elliptical and spiral categories, and then, depending on how","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":758,"page_label":"704","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"into an analytics solution. Edwin was assigned to Jocelyn as her key scientiﬁc contact\nfrom SDSS and was eager to answer any questions Jocelyn had as he saw real value in the\nmodel she was developing.\nThe ﬁrst detail that Jocelyn needed to agree on with Edwin was the set of categories into\nwhich sky objects should be categorized. The scientists at SDSS listed two key galaxy\nmorphologies of interest: elliptical andspiral . The spiral category further divided into\nclockwise spiral andanti-clockwise spiral subcategories. Figure 13.1[705]shows illustrations\nof these different galaxy types. Jocelyn suggested that she would ﬁrst work on the coarse\nclassiﬁcation of galaxies into elliptical and spiral categories, and then, depending on how\nthis model performed, look at classifying spirals into the more ﬁne-grained categories.\nJocelyn also suggested that a third other category be included to take into account the fact\nthat all the sky objects labeled as galaxies in the previous step in the SDSS may not actually\nbe galaxies. Edwin agreed with both of these suggestions.\nThe second detail that Jocelyn needed to agree on with Edwin was the target accuracy\nthat would be required by the system she would build in order for it to be of use to scientists\nat SDSS. It is extremely important that analytics professionals manage the expectations of\ntheir clients during the business understanding process, and agreeing on expected levels of\n3. This practice was ﬁrst systematically applied by Edwin Hubble in 1936 (Hubble, 1936).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":759,"page_label":"705","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.1 Business Understanding 705\n(a) Elliptical\n (b) Clockwise spiral\n (c) Anti-clockwise spiral\nFigure 13.1\nExamples of the different galaxy morphology categories into which SDSS scientists categorize\ngalaxy objects. Credits for these images belong to the Sloan Digital Sky Survey, www.sdss3.org.\nmodel performance is one of the easiest ways in which to do this. This avoids disappoint-\nment and difﬁculties at later stages in a project. After lengthy discussion, both Jocelyn\nand Edwin agreed that in order for the system to be useful, a classiﬁcation accuracy of\napproximately 80% would be required. Jocelyn stressed that until she had looked at the\ndata and performed experiments, she could not make any predictions as to what classiﬁ-\ncation accuracy would be possible. She did, however, explain to Edwin that because the\ncategorization of galaxy morphologies is a somewhat subjective task (even human experts\ndon’t always fully agree on the category that a night sky object should belong to), it was\nunlikely that classiﬁcation accuracies beyond 90% would be achievable.\nFinally, Edwin and Jocelyn discussed how fast the model built would need to be to allow\nits inclusion in the existing SDSS pipeline. Fully processed data from the SDSS pipeline\nis available to scientists approximately one week after images of night sky objects are\ncaptured by the SDSS telescopes.4The system that Jocelyn built would be added to the end\nof this pipeline because it would require outputs from existing data processing steps. It was\nimportant that the model Jocelyn deployed not add a large delay to data becoming available\nto scientists. Based on the expected volumes of images that would be produced by the\nSDSS pipeline, Jocelyn and Edwin agreed that the model developed should be capable of\nperforming approximately 1,000classiﬁcations per second on a dedicated server of modest\nspeciﬁcation.\n4. In an interesting example of the persistence of good solutions using older technology, the data captured by\nthe telescopes at the SDSS site in New Mexico is recorded onto magnetic tapes that are then couriered to the\nFeynman Computing Center at Fermilab in Illinois, over 1,000miles away. This is the most effective way to\ntransport the massive volumes of data involved!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":760,"page_label":"706","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"706 Chapter 13 Case Study: Galaxy Classiﬁcation\n13.1.1 Situational Fluency\nThe notion of situational ﬂuency5is especially important when dealing with scientiﬁc\nscenarios. It is important that analytics professionals have a basic grasp of the work their\nscientiﬁc partners are undertaking so that they can converse ﬂuently with them. The real\nskill in developing situational ﬂuency is determining how much knowledge about the ap-\nplication domain the analytics professional requires in order to complete the project suc-\ncessfully. It was not reasonable, nor necessary, to expect that Jocelyn would become fully\nfamiliar with the intricacies of the SDSS and the astronomy that it performs. Instead, she\nneeded enough information to understand the key pieces of equipment involved, the impor-\ntant aspects of the night sky objects that she would be classifying, and the key terminology\ninvolved.\nWhile complex scientiﬁc scenarios can make this process more difﬁcult than is the case\nfor more typical business applications, there is also the advantage that scientiﬁc projects\ntypically produce publications clearly explaining their work. These kinds of publications\nare an invaluable resource for an analytics professional trying to come to grips with a new\ntopic. Jocelyn read a number of publications by the SDSS team6before spending several\nsessions with Edwin discussing the work that he and his colleagues did. The following\nshort summary of the important things she learned illustrates the level of situational ﬂuency\nrequired for this kind of scenario.\nThe SDSS project captures two distinct kinds of data—images of night-sky objects and\nspectrographs of night sky objects—using two distinct types of instrument, an imaging\ncamera and a spectrograph.\nThe SSDS imaging camera captures images in ﬁvedistinct photometric bands :7ultra-\nviolet ( u), green ( g), red ( r), far-red ( i), and near infrared ( z). The raw imaging data\ncaptured from the SDSS telescopes is passed through a processing pipeline that identi-\nﬁes individual night sky objects and extracts a number of properties for each object. For\ngalaxy classiﬁcation, the most important properties extracted from the images are bright-\nness, color, and shape. The measure of brightness used in the SDSS pipeline is referred to\nasmagnitude .Flux is another measure that attempts to standardize measures of bright-\nness, taking into account how far away different objects are from the telescope. Measures","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":760,"page_label":"706","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"camera and a spectrograph.\nThe SSDS imaging camera captures images in ﬁvedistinct photometric bands :7ultra-\nviolet ( u), green ( g), red ( r), far-red ( i), and near infrared ( z). The raw imaging data\ncaptured from the SDSS telescopes is passed through a processing pipeline that identi-\nﬁes individual night sky objects and extracts a number of properties for each object. For\ngalaxy classiﬁcation, the most important properties extracted from the images are bright-\nness, color, and shape. The measure of brightness used in the SDSS pipeline is referred to\nasmagnitude .Flux is another measure that attempts to standardize measures of bright-\nness, taking into account how far away different objects are from the telescope. Measures\nof ﬂux and magnitude are made in each of the ﬁve photometric bands used by the SDSS\nimaging system. To measure the color of night sky objects, the ﬂux measured in differ-\n5. See Chapter 2[23].\n6. Stoughton et al. (2002) provides an in-depth discussion of the data collected by the SDSS. A shorter overview\nis provided at skyserver.sdss3.org/dr9/en/sdss/data/data.asp.\n7. Most consumer digital cameras capture full-color images by capturing separate images on red, green, and\nblue imaging sensors and combining these. The colors red, green, and blue are known as photometric bands .\nThe photometric bands captured by the SDSS imaging camera are the same as these bands; they are just deﬁned\non different parts of the spectrum.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":761,"page_label":"707","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.2 Data Understanding 707\nent photometric bands is compared. The image-based measures of overall galaxy shape\nare extracted from the images using morphological andmoment image processing opera-\ntions. These measures capture how well objects match template shapes—although none is\naccurate enough to actually perform the galaxy morphology prediction itself.\nAspectrograph is a device that disperses the light emitted by an object into different\nwavelengths and measures the intensity of the emission of each wavelength—this set of\nmeasures is referred to as a spectrogram . The SDSS spectrographs perform this task for\nmanually identiﬁed night sky objects and produce spectrograms across wavelengths from\nvisible blue light to near-infrared light. Spectrography data may be useful in galaxy classi-\nﬁcation because different galaxy types are likely to emit different amounts of different light\nwavelengths, so spectrograms might be a good indicator for galaxy type. Spectrography\nalso allows measurement of redshift , which is used to determine the distance of night sky\nobjects from the viewer.\nOnce Jocelyn felt that she was suitably ﬂuent with the SDSS situation, she proceeded\nto the Data Understanding phase of the CRISP-DM process so as to better understand the\ndata available.\n13.2 Data Understanding\nJocelyn’s ﬁrst step in fully understanding the data available to her was to deﬁne the pre-\ndiction subject . In this case the task was to categorize galaxies according to morphology,\nand therefore galaxy made sense as the prediction subject. The structure of the dataset re-\nquired for this task would contain one row per galaxy, and each row would include a set of\ndescriptive features describing the characteristics of that galaxy object and a target feature\nindicating the morphological category of the galaxy object.\nBased on her understanding of the SDSS process , Jocelyn sketched out the ﬁrst draft of\nthe domain concepts diagram for the galaxy classiﬁcation problem shown in Figure 13.2[708].\nJocelyn felt that the important domain concepts were likely to be the target (galaxy type),\ngalaxy appearance measures (e.g., color), spectrography information (e.g., red shift), and\nposition information (the position of each object in the night sky was also available from the\nSDSS pipeline). Data with which to implement features based on these domain concepts\nwould likely come from the raw camera imaging and spectrograph images themselves, or","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":761,"page_label":"707","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"indicating the morphological category of the galaxy object.\nBased on her understanding of the SDSS process , Jocelyn sketched out the ﬁrst draft of\nthe domain concepts diagram for the galaxy classiﬁcation problem shown in Figure 13.2[708].\nJocelyn felt that the important domain concepts were likely to be the target (galaxy type),\ngalaxy appearance measures (e.g., color), spectrography information (e.g., red shift), and\nposition information (the position of each object in the night sky was also available from the\nSDSS pipeline). Data with which to implement features based on these domain concepts\nwould likely come from the raw camera imaging and spectrograph images themselves, or\nfrom the results of the SDSS processing pipeline.\nJocelyn took this ﬁrst domain concept draft along to a meeting with Ted, the SDSS chief\ndata architect, to discuss the data resources that would be available for model building.\nTed quickly made two observations. First, the spectrograph data collected by the SDSS\ntelescopes was not nearly as extensive as the camera imaging data collected—while there\nwas imaging data for millions of galaxies, there were spectrograms for only hundreds\nof thousands. Collecting spectrographic information involves a much more complicated\nprocess than capturing imaging data, so it is done for a much smaller portion of the sky.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":762,"page_label":"708","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"708 Chapter 13 Case Study: Galaxy Classiﬁcation\nClassiﬁcation\nGalaxy Appearance Galaxy Shape Spectrography PositionGalaxy\nType\nColor Brightness Morphology Moments Red Shift Spectra\nFigure 13.2\nThe ﬁrst draft of the domain concepts diagram developed by Jocelyn for the galaxy classiﬁcation\ntask.\nThis was likely to continue to be the case, so any solution that relied on spectrographic\ndata as well as imaging data to classify galaxy types would work for only a fraction of the\nobservations made by the SDSS telescopes.\nTed’s second observation was that, although there was a huge amount of data available\non past observations of night sky objects, only a tiny fraction of these contained manual\nlabels indicating the morphological category to which they belonged. This meant that the\ndata available at the SDSS did not contain a suitable target feature that Jocelyn could use\nto train prediction models. This is a very common scenario and a real thorn in the side\nof the predictive model builder—although there is often an almost endless amount of data\navailable for training, little or none of it is labeled with the relevant target feature, making\nit effectively useless.\nJocelyn’s options at this stage were (1) to embark on a large-scale manual data labeling\nproject for which she would hire experts to manually label a suitably large set of historical\nnight sky object observations, or (2) to ﬁnd some other data source that she could add to the\nSDSS data to use as a target feature. While the ﬁrst option is often used, Jocelyn was lucky\nthat another data source became available. Through conversations with Edwin, Jocelyn\nbecame aware of a parallel project to the SDSS that offered an intriguing solution to her\nproblem. Galaxy Zoo8is acrowdsourced ,citizen science effort in which people can log\non to a website and categorize images of galaxies—taken from the SDSS—into different\ngroups. The Galaxy Zoo project started in 2007 and since then has collected millions of\nclassiﬁcations of hundreds of thousands of galaxies.\nThe galaxy types that Galaxy Zoo citizen scientists could choose from were elliptical ,\nclockwise spiral ,anti-clockwise spiral ,edge-on disk ,merger , and don’t know . The ﬁrst\nthree types are self-explanatory and match directly with the categories of interest to the\nSDSS project. An edge-on disk is a spiral galaxy viewed from the edge, which makes the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":762,"page_label":"708","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"problem. Galaxy Zoo8is acrowdsourced ,citizen science effort in which people can log\non to a website and categorize images of galaxies—taken from the SDSS—into different\ngroups. The Galaxy Zoo project started in 2007 and since then has collected millions of\nclassiﬁcations of hundreds of thousands of galaxies.\nThe galaxy types that Galaxy Zoo citizen scientists could choose from were elliptical ,\nclockwise spiral ,anti-clockwise spiral ,edge-on disk ,merger , and don’t know . The ﬁrst\nthree types are self-explanatory and match directly with the categories of interest to the\nSDSS project. An edge-on disk is a spiral galaxy viewed from the edge, which makes the\n8. Full details of the Galaxy Zoo project and the data released by it are described in Lintott et al. (2008, 2011).\nThe Galaxy Zoo (www.galaxyzoo.org) project referred to in this example is Galaxy Zoo I.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":763,"page_label":"709","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.2 Data Understanding 709\nTable 13.1\nThe structure of the SDSS and Galaxy Zoo combined dataset.\nName Type Description\nOBJID Continuous Unique SDSS object identiﬁer\nPEL Continuous Fraction of votes for elliptical galaxy category\nPCW Continuous Fraction of votes for clockwise spiral galaxy category\nPACW Continuous Fraction of votes for anti-clockwise spiral galaxy category\nPEDGE Continuous Fraction of votes for edge-on disk galaxy category\nPMG Continuous Fraction of votes for merger category\nPDK Continuous Fraction of votes for don’t know category\ndirection of the spiral arms unclear. A merger is a sky object in which multiple galax-\nies appear grouped together. Examples were labeled as don’t know when a Galaxy Zoo\nparticipant could not place the object in question into one of the other categories.\nThe data from the Galaxy Zoo project was publicly available and therefore easily ac-\ncessible to Jocelyn. Galaxy Zoo labels were available for approximately 600,000SDSS\ngalaxies, which Jocelyn felt would be more than enough to use to train and test a galaxy\nmorphology classiﬁcation model. Conveniently, this also determined the subset of the\nSDSS dataset (those galaxies used in the Galaxy Zoo project) that Jocelyn would use for\nthis project. With the knowledge that the Galaxy Zoo labels would provide her with a target\nfeature, Jocelyn returned to speak with Ted again about getting access to the SDSS data.\nAccessing the results of the SDSS processing pipeline turned out to be reasonably straight-\nforward, as it was already collected into a single large table in the SDSS data repository.\nTed organized a full download of the SDSS photo imaging data repository for all the ob-\njects for which Galaxy Zoo labels existed. This dataset contained 600,000rows and 547\ncolumns,9with one row for each galaxy observation, containing identiﬁers, position infor-\nmation, and measures describing the characteristics of the galaxy.\nJocelyn decided to begin her data exploration work by focusing on the target feature.\nThe structure of the data available from the Galaxy Zoo project is shown in Table 13.1[709].\nThe category of each galaxy is voted on by multiple Galaxy Zoo participants, and the data\nincludes the fraction of these votes for each of the categories.\nThe raw data did not contain a single column that could be used as a target feature, so\nJocelyn had to design one from the data sources that were present. She generated two","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":763,"page_label":"709","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"jects for which Galaxy Zoo labels existed. This dataset contained 600,000rows and 547\ncolumns,9with one row for each galaxy observation, containing identiﬁers, position infor-\nmation, and measures describing the characteristics of the galaxy.\nJocelyn decided to begin her data exploration work by focusing on the target feature.\nThe structure of the data available from the Galaxy Zoo project is shown in Table 13.1[709].\nThe category of each galaxy is voted on by multiple Galaxy Zoo participants, and the data\nincludes the fraction of these votes for each of the categories.\nThe raw data did not contain a single column that could be used as a target feature, so\nJocelyn had to design one from the data sources that were present. She generated two\n9. The fact that the SDSS and Galaxy Zoo make all their data available for free online is a massive con-\ntribution to global science. The data used in this case study can be accessed by performing a simple SQL\nquery at skyserver.sdss3.org/dr9/en/tools/search/sql.asp. The query to select all the camera imaging data from\nthe SDSS data release for each of the objects covered by the Galaxy Zoo project along with the Galaxy\nZoo classiﬁcations is SELECT *FROM PhotoObj AS p JOIN ZooSpec AS zs ON zs.objid =\np.objid ORDER BY p.objid . Full details of all the data tables available from the SDSS are available at\nskyserver.sdss3.org/dr9/en/help/docs/tabledesc.asp.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":764,"page_label":"710","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"710 Chapter 13 Case Study: Galaxy Classiﬁcation\nelliptical spiral             other\nGalaxy Categor yFrequency\n0 100000 200000 300000 400000\n(a)3-level model\nelliptical  spiral_edge spiral_acw spiral_cw     other\nGalaxy Categor yFrequency\n0 100000 300000 (b)5-level model\nFigure 13.3\nBar plots of the different galaxy types present in the full SDSS dataset for the 3-level and 5-level\ntarget features.\npossible target features from the data provided. In both cases, the target feature level was\nset to the galaxy category that received the majority of the votes. In the ﬁrst target feature,\njust three levels were used: elliptical (PELmajority), spiral (PCW,PACW , or PEDGE\nmajority), and other (PMGorPDKmajority). The second target feature allowed three\nlevels for spiral galaxies: spiral cw(PCWmajority), spiral acw (PACW majority), and\nspiral edge (PEDGE majority). Figure 13.3[710]shows bar plots of the frequencies of the\n3-level and the 5-level target features. The main observation that Jocelyn made from these\nis that galaxies in the dataset were not evenly distributed across the different morphology\ntypes. Instead, the elliptical level was much more heavily represented than the others in\nboth cases. Using the 3-level target feature as her initial focus, Jocelyn began to look at the\ndifferent descriptive features in the data downloaded from the SDSS repository that might\nbe useful in building a model to predict galaxy morphology.\nThe SDSS download that Jocelyn had access to was a big dataset—over 600,000rows.\nAlthough modern predictive analytics and machine learning tools can handle data of this\nsize, a large dataset can be cumbersome when performing data exploration operations—\ncalculating summary statistics, generating visualizations, and performing correlation tests\ncan just take too long. For this reason, Jocelyn extracted a small sample of 10,000rows\nfrom the full dataset for exploratory analysis using stratiﬁed sampling .\nGiven that (1) the SDSS data that Jocelyn downloaded was already in a single table; (2)\nthe data was already at the right prediction subject level (one row per galaxy); and (3) many\nof the columns in this dataset would most likely be used directly as features in the ABT that\nshe was building, Jocelyn decided to produce a data quality report on this dataset. Table\n13.2[711]shows an extract from this data quality report. At this point Jocelyn was primarily","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":765,"page_label":"711","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.2 Data Understanding 711\nTable 13.2\nAnalysis of a subset of the features in the SDSS dataset.\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nRUN 10,000 0.00 380 109.00 2,821.00 3,703.45 3,841.00 4,646.00 8,095.00 1,378.82\nRA.1 10,000 0.00 9,964 0.03 151.38 185.26 185.02 220.56 359.99 59.12\nDEC.1 10,000 0.00 9,928 -11.23 9.71 24.87 23.41 39.11 69.83 18.92\nROWC U 10,000 0.00 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nROWC G 10,000 0.00 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nROWC R 10,000 0.00 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nROWC I 10,000 0.00 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nROWC Z 10,000 0.00 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nSKYIVAR U10,000 0.00 9,986 -9,999.00 459.81 78.89 798.27 1,083.65 2,197.09 450.26\nSKYIVAR G10,000 0.00 9,989 -9,999.00 439.55 965.88 2,957.92 6,005.71 9,913.59 2,766.70\nSKYIVAR R10,000 0.00 9,988 -9,999.00 123.31 201.91 1,091.78 3,347.77 4,623.07 1,514.50\nSKYIVAR I10,000 0.00 9,986 -9,999.00 46.02 174.79 434.48 1,825.93 2,527.57 851.42\nSKYIVAR Z10,000 0.00 9,986 -9,999.00 13.60 -234.23 49.57 75.39 205.07 44.51\nPSFMAG U10,000 0.00 9,768 7.47 20.60 21.08 21.13 21.598 26.19 0.85\nPSFMAG G10,000 0.00 9,743 8.30 19.06 19.48 19.54 19.967 26.17 0.78\nPSFMAG R10,000 0.00 9,744 7.45 18.23 18.65 18.68 19.113 26.49 0.76\nPSFMAG I10,000 0.00 9,744 7.33 17.83 18.27 18.26 18.722 25.46 0.80\nPSFMAG Z10,000 0.00 9,747 7.40 17.47 17.93 17.90 18.381 23.92 0.82\nDEVFLUX U10,000 0.00 9,990 -3.68 11.64 43.05 23.07 44.31 28,616.04 194.73\nDEVFLUX G10,000 0.00 9,987 -1,278.28 48.79 143.71 77.06 133.46 614,662.80 2,401.59\nDEVFLUX R10,000 0.00 9,983 -4.37 111.04 267.74 152.75 250.65 137,413.00 993.65\nDEVFLUX I10,000 0.00 9,980 -4.06 160.42 390.98 216.57 351.21 608,862.80 3,041.20\nDEVFLUX Z10,000 0.00 9,983 -14.72 204.72 528.69 276.99 447.45 2,264,700.00 9,073.95\ninterested in understanding the amount of data available, any issues that might arise from\nmissing values, and the types of each column in the dataset.\nJocelyn was surprised that none of the columns had any missing values. Although this\nis not unheard of (particularly in cases like the SDSS project in which data is generated\nin a fully automated process), it is very unusual. The minimum values of ´9,999for the\nSKYIVAR U/G/R/I/Zcolumns (and some others not shown in Table 13.2[711]), which were\nso different from the means for those columns, suggested that maybe there were missing\nvalues after all.10There were also a number of columns, such as ROWC U/G/R/I/Z, that had","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":765,"page_label":"711","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"interested in understanding the amount of data available, any issues that might arise from\nmissing values, and the types of each column in the dataset.\nJocelyn was surprised that none of the columns had any missing values. Although this\nis not unheard of (particularly in cases like the SDSS project in which data is generated\nin a fully automated process), it is very unusual. The minimum values of ´9,999for the\nSKYIVAR U/G/R/I/Zcolumns (and some others not shown in Table 13.2[711]), which were\nso different from the means for those columns, suggested that maybe there were missing\nvalues after all.10There were also a number of columns, such as ROWC U/G/R/I/Z, that had\ncardinality of 1(and standard deviations of zero) indicating that every row had the same.\nThese features contained no actual information, so should be removed from the dataset.\n10. Many systems use values like ´9,999to indicate that values are actually missing.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":766,"page_label":"712","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"712 Chapter 13 Case Study: Galaxy Classiﬁcation\nClassiﬁcation\nGalaxy Appearance Galaxy ShapeGalaxy\nType\nColor Brightness Morphology Moments\nFigure 13.4\nThe revised domain concepts diagram for the galaxy classiﬁcation task.\nHaving performed this initial analysis, Jocelyn met again with Edwin and Ted to discuss\nthe data quality issues and, more generally, to review the domain concepts outlined in\nFigure 13.2[708]so as to begin designing the actual descriptive features that would populate\nthe ABT. Edwin was broadly in agreement with the set of domain concepts that Jocelyn had\ndeveloped and was very positive about the use of Galaxy Zoo classiﬁcations as a source\nfor generating the target feature. He did explain, however, that Jocelyn’s suggestion of\nusing position information was very unlikely to be useful, so that was removed from the\nset of domain concepts. Edwin also agreed that Ted was correct about the unavailability\nof spectrograph data for most objects, so this was also removed. The ﬁnal domain concept\ndiagram is shown in Figure 13.4[712]. Edwin helped Jocelyn align the columns in the raw\nSDSS dataset with the different domain concepts, which generated a good set of descriptive\nfeatures within each domain concept.\nBoth Edwin and Ted were surprised to see missing values in the data, as it was produced\nthrough a fully automated process. Simply through eyeballing the data, Jocelyn uncovered\nthe fact that, in almost all cases, when one suspect ´9,999value was present in a row\nin the dataset, that row contained a number of suspect ´9,999values (this was the case\nfor2%of the rows in the dataset). Although neither Edwin nor Ted could understand\nexactly how this had happened, they agreed that something had obviously gone wrong in\nthe processing pipeline in those cases and that the ´9,999values must refer to missing\nvalues.11Complete case analysis was used to entirely remove any rows containing two\nor more´9,999, or missing, values. Before performing this operation, however, Jocelyn\nﬁrst checked that the percentage of missing values was approximately 2% in each of the 3\nlevels (and in each of the levels in the 5-level model) to ensure that there was no relationship\n11. The co-occurrence of multiple missing values in a row is something that it is hard to ﬁnd through summary\nanalysis of the data and one of the reasons analytics practitioners should always eyeball extracts from a dataset\nduring the data exploration process.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":767,"page_label":"713","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.3 Data Preparation 713\nbetween missing values and galaxy type. There was no obvious relationship, so Jocelyn\nwas conﬁdent that removing rows containing missing values would not affect one target\nlevel more than the others.\nOne of the advantages of working in scientiﬁc scenarios is that there is a body of liter-\nature that discusses how other scientists have addressed similar problems. Working with\nEdwin, Jocelyn reviewed the relevant literature and discovered a number of very infor-\nmative articles discussing descriptive features that were likely to be useful in classifying\ngalaxy morphologies.12In particular, a number of interesting features that could be derived\nfrom the ﬂux and magnitude measurements already in the SDSS dataset were described in\nthe literature. Jocelyn implemented these derived features for inclusion in the ﬁnal ABT.\nIn many instances the SDSS dataset contained the same measurement for a night sky\nobject measured separately for each of the ﬁve photometric bands covered by the SDSS\ntelescope. Because of this, Jocelyn suspected that there would be a large amount of re-\ndundancy in the data as the measurements in the different bands were likely to be highly\ncorrelated. To investigate this idea, she generated SPLOM charts for different photometric\nband versions of a selection of columns from the dataset (see Figure 13.5[714]), and these\nshowed signiﬁcant relationships, which conﬁrmed her suspicion. Jocelyn showed these\ncharts to Edwin. Edwin agreed that it was likely that correlations existed between mea-\nsurements in the different photometric bands but stressed, however, that differences across\nthese bands would exist and might be quite important in predicting galaxy morphology.\nThe existence of a high level of correlation between measurements indicated to Jocelyn\nthat feature selection would be important later during the modeling phase as it had the\npotential to massively reduce the dimensionality of the dataset.\nAt this point the design of the ABT had fallen into place. For the most part, Jocelyn\nwould use descriptive features directly from the raw SDSS data. These would be aug-\nmented with a small number of derived features that the literature review undertaken with\nEdwin had identiﬁed. Jocelyn was now ready to move into the Data Preparation phase,\nduring which she would populate the ABT, analyze its contents in detail, and perform any\ntransformations that were required to handle data quality issues.\n13.3 Data Preparation","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":767,"page_label":"713","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"that feature selection would be important later during the modeling phase as it had the\npotential to massively reduce the dimensionality of the dataset.\nAt this point the design of the ABT had fallen into place. For the most part, Jocelyn\nwould use descriptive features directly from the raw SDSS data. These would be aug-\nmented with a small number of derived features that the literature review undertaken with\nEdwin had identiﬁed. Jocelyn was now ready to move into the Data Preparation phase,\nduring which she would populate the ABT, analyze its contents in detail, and perform any\ntransformations that were required to handle data quality issues.\n13.3 Data Preparation\nAfter removing a large number of the columns from the raw SDSS dataset, introducing\na number of derived features, and generating two target features, Jocelyn generated an\nABT containing 327descriptive features and two target features. Table 13.3[715]lists these\n12. Interested readers might ﬁnd Tempel et al. (2011), Ball et al. (2004) and Banerji et al. (2010) good references\non this topic.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":768,"page_label":"714","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"714 Chapter 13 Case Study: Galaxy Classiﬁcation\n(a) EXPRAD\n (b) DEVRAD\nFigure 13.5\nSPLOM diagrams of (a) the EXPRAD; and (b) DEVRADmeasurements from the raw SDSS dataset.\nEach SPLOM shows the measure across the ﬁve different photometric bands captured by the SDSS\ntelescope ( u,g,r,i, and z).\nfeatures (features that occur over all ﬁve photometric bands are listed as NAME U/G/R/I/Z\nto save space).13\nOnce Jocelyn had populated the ABT, she generated a data quality report (the initial data\nquality report covered the data in the raw SDSS dataset only, so a second one was required\nthat covered the actual ABT) and performed an in-depth analysis of the characteristics of\neach descriptive feature. An extract from this data quality report is shown in Table 13.4[716].\nThe magnitude of the maximum values for the FIBER 2FLUXIVAR Ufeature in compari-\nson to the median and 3rdquartile value was unusual and suggested the presence of outliers.\nThe difference between the mean and median values for the SKYIVAR Rfeature also sug-\ngested the presence of outliers. Similarly, the difference between the mean and median\nvalues for the LNLSTAR Rfeature suggested that the distribution of this feature was heav-\nily skewed and also suggested the presence of outliers. Figure 13.6[717]shows histograms\n13. We direct the interested reader to http://skyserver.sdss3.org/dr9/en/sdss/data/data.asp for a overview of what\nthese features represent.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":769,"page_label":"715","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.3 Data Preparation 715\nTable 13.3\nFeatures from the ABT for the SDSS galaxy classiﬁcation problem.\nFeature Feature Feature\nSKYIVAR U/G/R/I/Z U ERR U/G/R/I/Z EXP FLUX U/G/R/I/Z\nPSFMAG U/G/R/I/Z M E1 U/G/R/I/Z EXP FLUXIVAR U/G/R/I/Z\nPSFMAGERR U/G/R/I/Z M E2 U/G/R/I/Z MODEL FLUXIVAR U/G/R/I/Z\nFIBER MAG U/G/R/I/Z M E1E1E RR U/G/R/I/Z C MODEL FLUX U/G/R/I/Z\nFIBER MAGERR U/G/R/I/Z M E1E2E RR U/G/R/I/Z C MODEL FLUXIVAR U/G/R/I/Z\nFIBER 2MAG U/G/R/I/Z M E2E2E RR U/G/R/I/Z APER FLUX7U/G/R/I/Z\nFIBER 2MAGERR U/G/R/I/Z M RRCCU/G/R/I/Z APER FLUX7IVAR U/G/R/I/Z\nPETRO MAG U/G/R/I/Z M RRCCERR U/G/R/I/Z LN LSTAR U/G/R/I/Z\nPETRO MAGERR U/G/R/I/Z M CR4U/G/R/I/Z LN LEXPU/G/R/I/Z\nPSFFLUX U/G/R/I/Z DE VRAD U/G/R/I/Z LN LDEVU/G/R/I/Z\nPSFFLUXIVAR U/G/R/I/Z DE VRADERR U/G/R/I/Z FRAC DEVU/G/R/I/Z\nFIBER FLUX U/G/R/I/Z DE VAB U/G/R/I/Z DERED U/G/R/I/Z\nFIBER FLUXIVAR U/G/R/I/Z DE VABE RR U/G/R/I/Z DERED DIFF UG\nFIBER 2FLUX U/G/R/I/Z DE VM AG U/G/R/I/Z DERED DIFF GR\nFIBER 2FLUXIVAR U/G/R/I/Z DE VM AGERR U/G/R/I/Z DERED DIFF RI\nPETRO FLUX U/G/R/I/Z DE VFLUX U/G/R/I/Z DERED DIFF IZ\nPETRO FLUXIVAR U/G/R/I/Z DE VFLUXIVAR U/G/R/I/Z PETRO RATIO I\nPETRO RAD U/G/R/I/Z EXP RAD U/G/R/I/Z PETRO RATIO R\nPETRO RADERR U/G/R/I/Z EXP RADERR U/G/R/I/Z A EI\nPETRO R50 U/G/R/I/Z EXP AB U/G/R/I/Z PETRO MAGDIFF UG\nPETRO R50E RR U/G/R/I/Z EXP ABE RR U/G/R/I/Z PETRO MAGDIFF GR\nPETRO R90 U/G/R/I/Z EXP MAG U/G/R/I/Z PETRO MAGDIFF RI\nPETRO R90E RR U/G/R/I/Z EXP MAGERR U/G/R/I/Z PETRO MAGDIFF IZ\nQU/G/R/I/Z C MODEL MAG U/G/R/I/Z GALAXY CLASS 3\nQERR U/G/R/I/Z C MODEL MAGERR U/G/R/I/Z GALAXY CLASS 5\nUU/G/R/I/Z\nfor these features. The problems of outliers and skewed distributions is clearly visible in\nthese distributions. A number of other features exhibited a similar pattern.\nWith Edwin’s help, Jocelyn investigated the actual data in the ABT to determine whether\nthe extreme values in the features displaying signiﬁcant skew or the presence of outliers\nwere due to valid outliers orinvalid outliers . In all cases the extreme values were deter-\nmined to be valid outliers. Jocelyn decided to use the clamp transformation to change\nthe values of these outliers to something closer to the central tendency of the features. Any\nvalues beyond the 1stquartile value plus 2.5times the inter-quartile range were reduced to\nthis value. The standard value of 1.5times the inter-quartile range was changed to 2.5to\nslightly reduce the impact of this operation.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":770,"page_label":"716","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Table 13.4\nA data quality report for a subset of the features in the SDSS ABT.\n% 1st3rdStd.\nFeature Count Miss. Card. Min. Qrt. Mean Median Qrt. Max. Dev.\nSKYIVAR U 640,432 0.00 639,983 0.00 465.53 784.78 793.20 1,079.53 2,190.05 447.36\nSKYIVAR G 640,432 0.00 640,081 0.00 442.55 3,318.72 2,949.62 6,008.31 9,898.47 2,769.84\nSKYIVAR R 640,432 0.00 640,178 0.00 127.18 1,629.86 1,094.93 3,342.65 4,596.46 1,513.38\nSKYIVAR I 640,432 0.00 640,042 0.00 48.28 842.18 436.13 1,825.88 2,515.35 852.73\nSKYIVAR Z 640,432 0.00 640,042 0.00 13.90 52.19 49.76 75.10 205.69 44.19\nME2 G 640,432 0.00 629,246 -0.96 -0.13 0.01 0.01 0.15 0.97 0.28\nFIBER 2FLUXIVAR U 640,432 0.00 639,827 0.00 20.31 27.24 25.96 32.40 170.70 11.02\nPSFMAG U 640,432 0.00 632,604 13.76 20.59 21.05 21.12 21.58 25.56 0.81\nPETRO FLUXIVAR U 640,432 0.00 627,391 0.00 0.16 0.40 0.31 0.53 6.29 0.36\nLNLSTAR R 640,432 0.00 639,690 -218,875.30 -12,623.05 -12,009.95 -6,771.37 -4,308.99 0.00 16,193.73\nPETRO MAG R 640,432 0.00 628,562 11.72 16.76 17.08 17.29 17.61 22.72 0.75\nEXPAB I 640,432 0.00 623,467 0.05 0.49 0.65 0.67 0.81 1.00 0.20\nDERED DIFF UG 640,432 0.00 630,319 -2.47 1.29 1.61 1.67 1.89 6.67 0.40\nDERED DIFF GR 640,432 0.00 631,627 -1.06 0.64 0.82 0.84 0.99 4.70 0.27\nDERED DIFF RI 640,432 0.00 611,597 -4.46 0.36 0.39 0.40 0.44 2.22 0.10\nDERED DIFF IZ 640,432 0.00 615,131 -2.29 0.23 0.28 0.30 0.34 5.33 0.11\nPETRO RATIO I 640,432 0.00 640,432 1.12 2.33 2.67 2.68 3.01 25.52 0.46\nPETRO RATIO R 640,432 0.00 640,432 1.18 2.29 2.63 2.64 2.96 10.05 0.42\nAEI 640,432 0.00 640,432 0.00 0.13 0.27 0.23 0.38 0.90 0.18\nMODEL MAGDIFF UG 640,432 0.00 630,476 -2.45 1.33 1.65 1.71 1.94 6.83 0.40\nMODEL MAGDIFF GR 640,432 0.00 630,437 -1.05 0.68 0.85 0.87 1.03 4.75 0.27\nMODEL MAGDIFF RI 640,432 0.00 613,667 -4.46 0.38 0.41 0.42 0.47 2.25 0.10\nMODEL MAGDIFF IZ 640,432 0.00 615,346 -2.27 0.25 0.29 0.32 0.35 5.34 0.11\nPETRO MAGDIFF GR 640,432 0.00 631,901 -1.99 0.64 0.83 0.84 1.00 5.13 0.28\nPETRO MAGDIFF RI 640,432 0.00 612,827 -3.32 0.35 0.39 0.41 0.45 2.83 0.11\nPETRO MAGDIFF IZ 640,432 0.00 620,422 -4.43 0.19 0.24 0.27 0.33 3.69 0.15","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":771,"page_label":"717","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.3 Data Preparation 717\nfiber2FluxIv ar_uDensity\n0 50 100 1500.00 0.01 0.02 0.03 0.04\n(a) FIBER 2FLUXIVAR U\nskyIv ar_rDensity\n0 1000 2000 3000 40000.0000 0.0005 0.0010 0.0015 (b) SKYLVAR R\nlnLStar_rDensity\n−200000 −100000 −50000 00e+00 2e−05 4e−05 6e−05 (c) LNLSTAR R\nFigure 13.6\nHistograms of a selection of features from the SDSS dataset.\nJocelyn also made the decision to normalize all the descriptive features into standard\nscores .The differences in the ranges of values of the set of descriptive features in the ABT\nwas huge. For example, DEVAB Rhad a range as small as r0.05,1.00swhile APER -\nFLUX7IVAR Uhad a range as large as r´265,862,15,274s. Standardizing the descrip-\ntive feature in this way was likely to improve the accuracy of the ﬁnal predictive models.\nThe only drawback to standardization is that the models become less interpretable. Inter-\npretability, however, was not particularly important for the SDSS scenario (the model built\nwould be added to the existing SDSS pipeline and process thousands of galaxy objects per\nday), so standardization was appropriate.\nJocelyn also performed a simple ﬁrst-pass feature selection using the 3-level model to\nsee which features might stand out as predictive of galaxy morphology. Jocelyn used the\ninformation gain measure to rank the predictiveness of the different features in the dataset\n(for this analysis, missing values were simply omitted). The columns identiﬁed as being\nmost predictive of galaxy morphology were EXPRAD G(0.3908 ),EXPRAD R(0.3649 ),\nDEVRAD G(0.3607 ),EXPRAD I(0.3509 ),DEVRAD R(0.3467 ),EXPRAD Z(0.3457 ),\nand MRRCCG(0.3365 ). Jocelyn generated histograms for all these features compared to\nthe target feature—for example, Figure 13.7[718]shows the histograms for the EXPRAD R\nfeature. It was encouraging that in many cases distinct distributions for each galaxy type\nwere apparent in the histograms. Figure 13.8[718]shows small multiple box plots divided\nby galaxy type for a selection of features from the ABT. The differences between the three\nbox plots in each plot gives an indication of the likely predictiveness of each feature. The\npresence of large numbers of outliers can also be seen.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":772,"page_label":"718","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"718 Chapter 13 Case Study: Galaxy Classiﬁcation\ngalaxy_c lass = elliptical\nexpRad_rDensity\n0 10 20 30 40 50 600.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35galaxy_c lass = other\nexpRad_rDensity\n0 10 20 30 40 50 600.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35galaxy_c lass = spiral\nexpRad_rDensity\n0 10 20 30 40 50 600.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35\nFigure 13.7\nHistograms of the EXPRAD Rfeature split by target feature level.\nelliptical other spiral0.0 0.2 0.4 0.6 0.8aE_i\n(a) AEI\nelliptical other spiral0.2 0.4 0.6 0.8 1.0deVAB_g (b) DEVAB G\nelliptical other spiral0 10 20 30 40fiberFluxIv ar_r (c) FIBER FLUXIVAR R\nFigure 13.8\n(a)–(c) Small multiple box plots (split by the target feature) of some of the features from the SDSS\nABT.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":773,"page_label":"719","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.4 Modeling 719\n13.4 Modeling\nThe descriptive features in the SDSS dataset are primarily continuous. For this reason,\nJocelyn considered trying a similarity-based model, the knearest neighbor , and two error-\nbased models, the logistic regression model and the support vector machine . Jocelyn\nbegan by constructing a simple baseline model using the 3-level target feature.\n13.4.1 Baseline Models\nBecause of the size of the ABT, Jocelyn decided to split the dataset into a training set and\na large hold-out test set . Subsets of the training set would be also used for validation\nduring the model building process. The training set consisted of 30% of the data in the\nABT (approximately 200,000instances), and the test set consisted of the remaining 70%\n(approximately 450,000instances).14Using the training set, Jocelyn performed a 10-fold\ncross validation experiment on models trained to use the full set of descriptive features to\npredict the 3-level target. These would act as baseline performance scores that she would\ntry to improve upon. The classiﬁcation accuracies achieved during the cross validation\nexperiment were 82.912%,86.041%, and 85.942% by the knearest neighbor, logistic re-\ngression, and support vector machine models respectively. The confusion matrices from\nthe evaluation of these models are shown in Table 13.5[720].\nThese initial baseline results were promising; however, one key issue did emerge. It was\nclear that the performance of the models trained using the SDSS data was severely affected\nby the target level imbalance in the data—there were many more examples of the elliptical\ntarget level than either the spiral or, especially, the other target level. Having a dominant\ntarget level, like the elliptical target level in this example, means that models trained on\nthis data can overcompensate for the majority target level and ignore the minority ones.\nFor example, based on the confusion matrix in Table 13.5(c)[720], the misclassiﬁcation rate\nfor the elliptical target level is only 8.756%, while for the spiral target level, it is higher, at\n18.693%, and for the other target level, it is a fairly dire 98.230%. The single classiﬁcation\naccuracy performance measure hides this poor performance on the minority target levels.\nAn average class accuracy performance measure, however, brings this issue to the fore.\nThe average class accuracy scores achieved by the models were 54.663%,62.137%, and","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":773,"page_label":"719","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"target level, like the elliptical target level in this example, means that models trained on\nthis data can overcompensate for the majority target level and ignore the minority ones.\nFor example, based on the confusion matrix in Table 13.5(c)[720], the misclassiﬁcation rate\nfor the elliptical target level is only 8.756%, while for the spiral target level, it is higher, at\n18.693%, and for the other target level, it is a fairly dire 98.230%. The single classiﬁcation\naccuracy performance measure hides this poor performance on the minority target levels.\nAn average class accuracy performance measure, however, brings this issue to the fore.\nThe average class accuracy scores achieved by the models were 54.663%,62.137%, and\n58.107% by the knearest neighbor, logistic regression, and support vector machine models\nrespectively. Jocelyn decided to build a second set of models in which she would address\nthe target level imbalance issue.\n14. It is more common to split an ABT in the opposite proportions ( 70% for the training set and 30% for the test\nset). In this case, however, because the ABT was so large it was more useful to have a very large test sample, as\n200,000instances should be more than enough for the training set.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":774,"page_label":"720","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"720 Chapter 13 Case Study: Galaxy Classiﬁcation\nTable 13.5\nThe confusion matrices for the baseline models.\n(a)knearest neighbor model (classiﬁcation accuracy: 82.912%, average class accuracy: 54.663%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 115,438 10,238 54 91.814%\nspiral 19,831 50,368 18 71.731%\nother 2,905 1,130 18 0.442%\n(b) logistic regression model (classiﬁcation accuracy: 86.041%, average class accuracy:\n62.137%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 115,169 10,310 251 91.600%\nspiral 13,645 56,321 251 80.209%\nother 2,098 1,363 592 14.602%\n(c) support vector machine model (classiﬁcation accuracy: 85.942%, average class accuracy:\n58.107%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 114,721 10,992 18 91.244%\nspiral 13,089 57,092 36 81.307%\nother 2,654 1,327 72 1.770%\nThe target level imbalance in the SDSS dataset arises through relative rarity .15In the\nlarge SDSS dataset, there are plenty of galaxies in the other andspiral categories; there are\njust many more in the elliptical category. In this case, Jocelyn addressed the target level\nimbalance problem by using under-sampling to generate a new training dataset in which\nall three target levels had an equal distribution. This was referred to as the under-sampled\ntraining set . Jocelyn performed the same baseline test on the three model types using this\nnew dataset. The resulting confusion matrices are shown in Table 13.6[721].\n15. Target level imbalance typically arises through either absolute rarity orrelative rarity of the minority target\nlevels. Absolute rarity refers to scenarios in which there simply do not exist many examples of the minority target\nlevels—for example, in automated inspection tasks on production lines, it is often the case that there are simply\nvery few examples of defective products that can be used for training. Relative rarity, on the other hand, refers to\nscenarios in which the proportion of examples of the majority target levels in a dataset is much higher than the\nproportion of examples of the minority target level, but there is actually no shortage of examples of the minority\ntarget level.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":775,"page_label":"721","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.4 Modeling 721\nTable 13.6\nThe confusion matrices showing the performance of models on the under-sampled training set.\n(a)knearest neighbor model (classiﬁcation accuracy: 73.965%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 23,598 4,629 5,253 70.483%\nspiral 4,955 24,734 3,422 74.700%\nother 3,209 4,572 25,628 76.711%\n(b) logistic regression model (classiﬁcation accuracy: 78.805%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 25,571 4,203 3,706 76.378%\nspiral 3,677 26,267 3,166 79.331%\nother 2,684 3,763 26,963 80.705%\n(c) support vector machine model (classiﬁcation accuracy: 78.226%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 24,634 4,756 4,089 73.579%\nspiral 3,763 26,310 3,038 79.460%\nother 2,584 3,550 27,275 81.640%\nThe resulting classiﬁcation accuracies (average class accuracies and classiﬁcation accu-\nracies are the same in this case because the dataset is balanced) from the 10-fold cross\nvalidation experiment were 73.965%,78.805%, and 78.226% for the knearest neighbor,\nlogistic regression, and support vector machine models respectively. The overall perfor-\nmance on this balanced dataset was not as good as the performance on the original dataset;\nhowever, balancing the training set did result in the performance on each target level being\nmore equal. Predictions for the other target level are actually being performed this time,\nwhereas in the previous example, this target level was essentially being ignored. Choosing\nbetween models in this sort of scenario is difﬁcult as it really comes down to balancing the\nneeds of the application—when the system makes errors (as it inevitably will from time to\ntime), what error is least bad? In this example, is it better to classify a galaxy that should\nbeother as an elliptical galaxy or vice versa? Jocelyn discussed this issue and the re-\nsults of these two baseline experiments with Edwin, and both decided that it would be best\nto pursue the optimal performance measured by overall classiﬁcation accuracy because,\nin practice, the important thing for the SDSS system was to classify elliptical andspiral\ngalaxies as accurately as possible.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":776,"page_label":"722","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"722 Chapter 13 Case Study: Galaxy Classiﬁcation\nWith these baseline performance measures established, Jocelyn turned her attention to\nfeature selection in an effort to improve on these performance scores.\n13.4.2 Feature Selection\nIn the SDSS dataset, many of the features are represented multiple times for each of the\nﬁve different photometric bands, and this made Jocelyn suspect that many of these fea-\ntures might be redundant and so ripe for removal from the dataset. Feature selection\napproaches that search through subsets of features (known as wrapper approaches) are\nbetter at removing redundant features than rank and prune approaches because they con-\nsider groups of features together. For this reason, Jocelyn chose to use a step-wise se-\nquential search for feature selection for each of the three model types. In all cases overall\nclassiﬁcation accuracy was used as the ﬁtness function that drove the search. After feature\nselection, the classiﬁcation accuracy of the models on the test set were 85.557%,88.829%,\nand87.188% for the knearest neighbor, logistic regression, and support vector machine\nmodels respectively. The resulting confusion matrices are shown in Table 13.7[723]. In all\ncases performance of the models improved with feature selection. The best performing\nmodel is the logistic regression model. For this model, just 31out of the total 327features\nwere selected.16This was not surprising given the large amount of redundancy within the\nfeature set.\nBased on these results, Jocelyn determined that the logistic regression model trained us-\ning the reduced set of features was the best model to use for galaxy classiﬁcation. This\nmodel gave the best prediction accuracy and offered the potential for very fast classiﬁca-\ntion times, which was attractive for integration into the SDSS pipeline. Logistic regression\nmodels also produce conﬁdences along with the predictions, which was attractive to Edwin\nas it meant that he could build tests into the pipeline that would redirect galaxies with low\nconﬁdence classiﬁcations for manual conﬁrmation of the predictions made by the auto-\nmated system.\n13.4.3 The 5-Level Model\nTo address the ﬁner grained 5-level ( elliptical ,spiral cw,spiral acw,spiral eo, and other )\nclassiﬁcation task, Jocelyn attempted two approaches. First, she used a 5-target-level\nmodel to make predictions. Second, she used a two-stage model . In this case the lo-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":776,"page_label":"722","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tion times, which was attractive for integration into the SDSS pipeline. Logistic regression\nmodels also produce conﬁdences along with the predictions, which was attractive to Edwin\nas it meant that he could build tests into the pipeline that would redirect galaxies with low\nconﬁdence classiﬁcations for manual conﬁrmation of the predictions made by the auto-\nmated system.\n13.4.3 The 5-Level Model\nTo address the ﬁner grained 5-level ( elliptical ,spiral cw,spiral acw,spiral eo, and other )\nclassiﬁcation task, Jocelyn attempted two approaches. First, she used a 5-target-level\nmodel to make predictions. Second, she used a two-stage model . In this case the lo-\ngistic regression model used for the 3-level target feature would ﬁrst be used, and then a\nmodel trained to distinguish only between different spiral galaxy types ( clockwise ,anti-\n16. The features selected were AEI,APER FLUX7IVAR G,APER FLUX7IVAR I,APER FLUX7U,DERED U,\nDEVAB R,DEVRADERR Z,DEVRAD U,DERED DIFF GR,EXPRAD G,EXPRAD R,FIBER 2FLUXIVAR Z,\nFIBER 2MAGERR G,FIBER FLUXIVAR R,FRAC DEVZ,LNLDEVG,LNLDEVR,LNLDEVU,LNLDEVZ,\nMCR4Z,PETRO FLUXIVAR G,PETRO FLUXIVAR I,PETRO R50E RR R,PETRO R50 G,PETRO R90 G,PETRO -\nRATIO R,PSFFLUXIVAR I,PSFMAGERR R,PSFMAG R,SKYIVAR U, and SKYIVAR Z.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":777,"page_label":"723","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.4 Modeling 723\nTable 13.7\nThe confusion matrices for the models after feature selection.\n(a)knearest neighbor model (classiﬁcation accuracy: 85.557%, average class accuracy:\n57.617%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 116,640 9,037 54 92.770%\nspiral 15,833 54,366 18 77.426%\nother 2,815 1,130 108 2.655%\n(b) logistic regression model (classiﬁcation accuracy: 88.829%, average class accuracy:\n67.665%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 117,339 8,302 90 93.326%\nspiral 10,812 59,297 108 84.448%\nother 1,757 1,273 1,022 25.221%\n(c) support vector machine model (classiﬁcation accuracy: 87.188%, average class accuracy:\n60.868%)\nPrediction\nelliptical spiral other Recall\nTargetelliptical 115,152 10,561 18 91.586%\nspiral 11,243 58,938 36 83.938%\nother 2,528 1,237 287 7.080%\nclockwise , and edge-on ) would be used to further classify those galaxy objects classiﬁed\nasspiral by the ﬁrst stage.\nBased on the performance of the logistic regression model on the 3-level classiﬁcation\nproblem, Jocelyn trained a logistic regression classiﬁer on the 5-level dataset and evaluated\nit using a 10-fold cross validation. Following the same approach as in earlier models,\nJocelyn performed feature selection using a step-wise sequential search to ﬁnd the best\nsubset of features for this model. Just 11features from the full set were selected.17The\nresulting classiﬁcation accuracy on the best performing model that Jocelyn could build was\n77.528% (with an average class accuracy of 43.018%). The confusion matrix from this\ntest is shown in Table 13.8[724]. The overall accuracy of this model is somewhat comparable\n17. The features selected were SKYIVAR U,PETRO FLUXIVAR I,PETRO R50E RR G,DEVRAD G,DE-\nVRADERR R,DEVRADERR I,DEVAB G,EXPFLUX Z,APER FLUX7Z,APER FLUX7IVAR R, and MODEL -\nMAGDIFF IZ.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":778,"page_label":"724","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"724 Chapter 13 Case Study: Galaxy Classiﬁcation\nTable 13.8\nThe confusion matrix for the 5-level logistic regression model (classiﬁcation accuracy: 77.528%,\naverage class accuracy: 43.018%).\nPrediction\nelliptical spiral cw spiral acw spiral eo other Recall\nTargetelliptical 120,625 46 1 ,515 3,450 95 95.939%\nspiral cw 7,986 373 4 ,715 2,176 30 2.443%\nspiral acw 8,395 435 4 ,928 2,272 35 30.673%\nspiral eo 8,719 75 1 ,018 28,981 78 74.556%\nother 3,038 30 218 619 148 3.660%\nTable 13.9\nThe confusion matrix for the logistic regression model that distinguished between only the spiral\ngalaxy types (classiﬁcation accuracy: 68.225%, average class accuracy: 56.621%).\nPrediction\nspiral cw spiral acw spiral eo Recall\nTargetspiral cw 5,753 6,214 3,319 37.636%\nspiral acw 6,011 6,509 3,540 40.528%\nspiral eo 1,143 2,084 35,643 91.698%\nwith the overall accuracy of the 3-level model. The classiﬁer accurately predicts the type of\ngalaxies with the elliptical target level and, to a lesser extent, with the spiral eotarget level.\nThe ability of the model to distinguish between clockwise ( spiral cw) and anti-clockwise\n(spiral acw) spiral galaxies, however, is extremely poor.\nTo test the two-stage classiﬁer, Jocelyn extracted a small ABT containing only spiral\ngalaxies from the original ABT. Using this new ABT, Jocelyn trained a logistic regres-\nsion model to distinguish between the three spiral galaxy types ( spiral cw,spiral acw, and\nspiral eo). She used step-wise sequential feature selection again, and this time 32features\nwere chosen.18This model was able to achieve a classiﬁcation accuracy of 68.225% (with\nan average class accuracy of 56.621%). The resulting confusion matrix is shown in Table\n13.9[724]. Although it is evident from the confusion matrix that the model could distin-\nguish between the edge-on spiral galaxies and the other two types, it could not accurately\ndistinguish between the clockwise and anti-clockwise spiral galaxies.\n18. The features selected were AEI,APER FLUX7IVAR R,CMODEL FLUXIVAR U,DEVABE RR G,DEV-\nABE RR Z,DEVAB G,DEVAB I,DEVFLUXIVAR U,DEVM AGERR U,DEVRAD G,DEVRAD U,DERED -\nDIFF UG,EXPABE RR U,EXPAB G,EXPMAG Z,EXPRADERR U,FIBER 2FLUXIVAR R,FIBER 2MAG I,\nFIBER FLUXIVAR G,FIBER FLUX G,FIBER FLUX R,FIBER FLUX Z,LNLDEVR,MCR4Z,ME1E1E RR Z,\nME1 U,MODEL MAGDIFF RI,PETRO MAGDIFF RI,PETRO R90 R,PSFMAG U,SKYIVAR U, and UR.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":779,"page_label":"725","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.5 Evaluation 725\nTable 13.10\nThe confusion matrix for the 5-level two-stage model (classiﬁcation accuracy: 79.410%, average\nclass accuracy: 53.118%).\nPrediction\nelliptical spiral cw spiral acw spiral eo other Recall\nTargetelliptical 117,339 76 2 ,510 5,716 90 93.326%\nspiral cw 2,354 4,859 5,242 2,802 23 31.799%\nspiral acw 2,473 5,079 5,499 2,990 25 34.229%\nspiral eo 5,985 965 1 ,760 30,102 60 77.439%\nother 1,757 98 341 834 1 ,022 25.222%\nIn spite of the model’s difﬁculty distinguishing between the clockwise and anti-clockwise\nspiral galaxies, Jocelyn did perform an evaluation of the two-stage model . This model ﬁrst\nused a 3-level logistic regression model to distinguish between the elliptical ,spiral , and\nother target levels. Any objects classiﬁed as belonging to the spiral target level were then\npresented to a model trained to distinguish between the three different spiral types. The\ntwo-stage model achieved a classiﬁcation accuracy of 79.410%. The resulting confusion\nmatrix is shown in Table 13.10[725].\nAlthough the performance of the two-stage model was better than the performance of the\nsimpler 5-level model, it still did a very poor job of distinguishing between the different\nspiral galaxy types. Jocelyn discussed this model with Edwin, and they both agreed that\nthe performance was not at the level required by the SDSS scientists for inclusion in the\nSDSS processing pipeline. It would most likely be possible to create a model that could\ndistinguish between the clockwise and anti-clockwise spiral galaxies, but this would prob-\nably require the calculation of new features based on the application of image processing\ntechniques to the raw galaxy images. Based on the time available to the project, Jocelyn\ndid not pursue this avenue and, in consultation with Edwin, decided to continue with just\nthe3-level model. The best-performing model was the 3-level logistic regression model\nafter feature selection (the performance of this model is shown in Table 13.7(b)[723]). With\nthis model selected as the best performing approach, Jocelyn was ready to perform the ﬁnal\nevaluation experiment.\n13.5 Evaluation\nThe ﬁnal evaluation that Jocelyn performed was in two parts. In the ﬁrst part, she per-\nformed a performance test of the ﬁnal model selected—the 3-level logistic regression\nmodel using the selected feature subset—on the large test dataset mentioned at the be-\nginning of Section 13.4[719]. This dataset had not been used in the training process, so the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":779,"page_label":"725","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"the3-level model. The best-performing model was the 3-level logistic regression model\nafter feature selection (the performance of this model is shown in Table 13.7(b)[723]). With\nthis model selected as the best performing approach, Jocelyn was ready to perform the ﬁnal\nevaluation experiment.\n13.5 Evaluation\nThe ﬁnal evaluation that Jocelyn performed was in two parts. In the ﬁrst part, she per-\nformed a performance test of the ﬁnal model selected—the 3-level logistic regression\nmodel using the selected feature subset—on the large test dataset mentioned at the be-\nginning of Section 13.4[719]. This dataset had not been used in the training process, so the\nperformance of the model on this dataset should give a fair indication of how well the\nmodel would perform when deployed on real, unseen data. The confusion matrix resulting","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":780,"page_label":"726","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"726 Chapter 13 Case Study: Galaxy Classiﬁcation\nTable 13.11\nThe confusion matrix for the ﬁnal logistic regression model on the large hold-out test set (classiﬁca-\ntion accuracy: 87.979%, average class accuracy: 67.305%).\nPrediction\nelliptical spiral other Recall\nTargetelliptical 251,845 19,159 213 92.857%\nspiral 25,748 128,621 262 83.179%\nother 4,286 2,648 2,421 25.879%\nfrom this test is shown in Table 13.11[726]. The classiﬁcation accuracy was 87.979% (with\nan average class accuracy of 67.305%), which was similar to performance on the training\ndata and well above the target that Jocelyn and Edwin had agreed on at the beginning of\nthe project.\nThe purpose of the second part of the evaluation was to encourage conﬁdence in the\nmodels that Jocelyn had built among the SDSS scientists. In this evaluation, Edwin and\nfour of his colleagues independently examined 200galaxy images randomly selected from\nthe ﬁnal test set and classiﬁed them as belonging to one of the three galaxy types. A single\nmajority classiﬁcation was calculated from the ﬁve manual classiﬁcations for each galaxy.\nJocelyn extracted two key measurements by comparing these manual classiﬁcations to the\nclassiﬁcations made by the model she had built. First, Jocelyn calculated an average class\naccuracy by comparing the predictions made by her model for the same 200galaxies with\nthe manual classiﬁcations made by the SDSS scientists. The average class accuracy was\n78.278%, which was similar to the accuracies measured on the overall test set.\nSecond, Jocelyn calculated an inter-annotator agreement statistic for the manual clas-\nsiﬁcations given by the ﬁve SDSS scientists. Using the Cohen’s kappa19measure of\ninter-annotator agreement to measure how closely the manual classiﬁcations matched\neach other, Jocelyn calculated a measure of 0.6. Jocelyn showed that even the SDSS sci-\nentists themselves disagreed on the types of certain galaxies. This is not uncommon in this\nkind of scenario, in which the classiﬁcations have a certain amount of fuzziness around\ntheir boundaries—e.g., the exact line between an elliptical and a spiral galaxy can be hard\nto deﬁne—and led to very interesting discussions for the scientists!\nTogether the strong performance by the model on the large test dataset and the conﬁdence\nbuilt through the manual annotation exercise meant that Edwin and his colleagues were\nhappy to integrate the 3-level model into the SDSS processing pipeline.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":780,"page_label":"726","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"each other, Jocelyn calculated a measure of 0.6. Jocelyn showed that even the SDSS sci-\nentists themselves disagreed on the types of certain galaxies. This is not uncommon in this\nkind of scenario, in which the classiﬁcations have a certain amount of fuzziness around\ntheir boundaries—e.g., the exact line between an elliptical and a spiral galaxy can be hard\nto deﬁne—and led to very interesting discussions for the scientists!\nTogether the strong performance by the model on the large test dataset and the conﬁdence\nbuilt through the manual annotation exercise meant that Edwin and his colleagues were\nhappy to integrate the 3-level model into the SDSS processing pipeline.\n19. The Cohen’s kappa statistic was ﬁrst described in Cohen (1960). Using the Cohen’s kappa statistic, a value of\n1.0indicates total agreement, while a value of 0.0indicates agreement no better than chance. Values around 0.6\nare typically understood to indicate an acceptable level of agreement, although the exact nature of what is and is\nnot acceptable is very task dependent.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":781,"page_label":"727","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"13.6 Deployment 727\n13.6 Deployment\nOnce Edwin had approved the models that Jocelyn had built, Jocelyn met again with Ted\nto begin the process of integrating the models into the SDSS processing pipeline. This was\na reasonably straightforward process with just a few issues that needed discussion. First,\nJocelyn had put the SDSS data through a preprocessing step, standardizing all descriptive\nfeatures. The standardization parameters (the mean and standard deviation of each feature)\nneeded to be included in the pipeline so that the same preprocessing step could be applied\nto newly arriving instances before presenting them to the models.\nSecond, a process was put in place that allowed manual review by SDSS experts to be\nincluded in the galaxy classiﬁcation process. One of the advantages of using a logistic\nregression model is that along with classiﬁcations, it also produces probabilities. Given\nthat there are three target levels, a prediction probability of approximately 0.333indicates\nthat the prediction made by the model is really quite unsure. A system was put in place in\nthe SDSS processing pipeline to ﬂag for manual review any galaxies given low probability\npredictions.\nLast, a strategy needed to be put in place to monitor the performance of the models over\ntime so that any concept drift that might take place could be ﬂagged. Jocelyn agreed with\nTed to put in place an alert system using the stability index . This would raise an alert\nwhenever the stability index went above 0.25so that someone could consider retraining\nthe model.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":783,"page_label":"729","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14 The Art of Machine Learning for Predictive Data Analytics\n“It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit\ntheories, instead of theories to suit facts. ”\n—Sherlock Holmes\nPredictive data analytics projects use machine learning to build models that capture the\nrelationships in large datasets between descriptive features and a target feature. A spe-\nciﬁc type of learning, called inductive learning, is used, where learning entails inducing a\ngeneral rule from a set of speciﬁc instances. This observation is important because it high-\nlights that machine learning has the same properties as inductive learning. One of these\nproperties is that a model learned by induction is not guaranteed to be correct. In other\nwords, the general rule that is induced from a sample may not be true for all instances in a\npopulation.\nAnother important property of inductive learning is that learning cannot occur unless\nthe learning process is biased in some way. This means that we need to tell the learning\nprocess what types of patterns to look for in the data. This bias is referred to as inductive\nbias. The inductive bias of a learning algorithm comprises the set of assumptions that\ndeﬁne the search space the algorithm explores, and the search process it uses.\nOn top of the inductive bias encoded in a machine learning algorithm, we also bias the\noutcome of a predictive data analytics project in lots of other ways. Consider the following\nquestions:\nWhat is the predictive analytics target? What descriptive features will we include/exclude?\nHow will we handle missing values? How will we normalize our features? How will we\nrepresent continuous features? What types of models will we create? How will we set the\nparameters of the learning algorithms? What evaluation process will we follow? What\nperformance measures will we use?\nThese questions are relevant when building any prediction model, and the answer to\neach one introduces a speciﬁc bias. Often we are forced to answer these questions, and\nothers like them, based on intuition, experience, and experimentation. This is what makes\nmachine learning something of an art, rather than strictly a science. But it is also what\nmakes machine learning such a fascinating and rewarding area in which to work.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":784,"page_label":"730","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"730 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nTable 14.1\nThe alignment between the phases of CRISP-DM, key questions for analytics projects, and the chap-\nters and sections of this book.\nCRISP-DM Open Questions Chapter\nBusiness\nUnderstandingWhat is the organizational problem being ad-\ndressed? In what ways could a prediction model\naddress the organizational problem? Do we\nhave situational ﬂuency? What is the capacity\nof the organization to utilize the output of a pre-\ndiction model? What data is available?Chapter 2[23]\nData\nUnderstandingWhat is the prediction subject? What are the do-\nmain concepts? What is the target feature? What\ndescriptive features will be used?Chapter 2[23]\nData\nPreparationAre there data quality issues? How will we han-\ndle missing values? How will we normalize our\nfeatures? What features will we include?Chapter 3[53]\nModelingWhat types of models will we use? How will we\nset the parameters of the machine learning al-\ngorithms? Have underﬁtting or overﬁtting oc-\ncurred?Chapters 4[117], 5[181],\n6[243], 7[311]and 8[381]\nEvaluationWhat evaluation process will we follow? What\nperformance measures will we use? Is the model\nﬁt for purpose?Chapter 9[533]\nDeploymentHow will we continue to evaluate the model after\ndeployment? How will the model be integrated\ninto the organization?Section 9.4.6[578]and\nChapters 12[685]and\n13[703]\nEn masse all the questions that must be answered to successfully complete a predic-\ntive data analytics project can seem overwhelming. This is why we recommend using the\nCRISP-DM process to manage a project through its lifecycle. Table 14.1[730]shows the\nalignment between the phases of CRISP-DM, some of the key questions that must be an-\nswered during a predictive data analytics project, and the chapters in this book dealing with\nthese questions.\nRemember, an analytics project is often iterative, with different stages of the project\nfeeding back into later cycles. It is also important to remember that the purpose of an\nanalytics project is to solve a real-world problem and to keep focus on this rather than\nbeing distracted by the, admittedly sometimes fascinating, technical challenges of model\nbuilding. We strongly believe that the best way to keep an analytics project focused, and to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":785,"page_label":"731","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14.1 Different Perspectives on Prediction Models 731\nimprove the likelihood of a successful conclusion, is to adopt a structured project lifecycle,\nsuch as CRISP-DM, and we recommend its use.\n14.1 Different Perspectives on Prediction Models\nA key step in any predictive analytics project is deciding which type of predictive analytics\nmodel to use. In this book we have presented some of the most commonly used predic-\ntion models and the machine learning algorithms used to build them. We have structured\nthis presentation around ﬁve approaches to learning: information-based, similarity-based,\nprobability-based, error-based, and deep learning. The mathematical foundation of these\napproaches can be described using ﬁve simple (but important) equations: Claude Shan-\nnon’s model of entropy (Equation (14.1)[731]),Euclidean distance (Equation (14.2)[731]),\nBayes’ Theorem (Equation (14.3)[731]), the sum of squared errors (Equation (14.4)[731]),\nand the application of the chain rule to backpropagate error gradients in a neural network\n(Equation (14.5)[731]).\nHpt,Dq“´ÿ\nlPlevelsptqpPpt“lqˆlog2pPpt“lqqq (14.1)\ndistpq,dq“gffemÿ\ni“1pqris´drisq2 (14.2)\nPpt“l|qq“Ppq|t“lqPpt“lq\nPpqq(14.3)\nL2pMw,Dq“1\n2nÿ\ni“1pti´Mwpdiqq2(14.4)\nBE\nBwi,k“BE\nBaiˆBai\nBziˆBzi\nBwi,k(14.5)\nAn understanding of these ﬁve equations is a strong basis for understanding the math-\nematical fundamentals of many areas of scientiﬁc modeling. Adding an understanding\nof how these ﬁve equations are used in the machine learning algorithms we have de-\nscribed (ID3, knearest neighbor, multivariable linear regression with gradient descent,\nnaive Bayes, and the backpropagation of error algorithm) is a strong foundation on which\nto build a career in predictive data analytics.\nThe taxonomy we have used to distinguish between different machine learning algo-\nrithms is based on human approaches to learning that the algorithms can be said to emulate.\nThis is not the only set of distinctions that can be made between the algorithms and the re-\nsulting models. It is useful to understand some of the other commonly used distinctions,\nbecause this understanding can provide insight into which learning algorithm and related\nmodel is most appropriate for a given scenario.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":786,"page_label":"732","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"732 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nThe ﬁrst distinction between models that we will discuss is the distinction between para-\nmetric andnon-parametric models. This distinction is not absolute, but it generally de-\nscribes whether the size of the domain representation used to deﬁne a model is solely\ndetermined by the number of features in the domain or is affected by the number of in-\nstances in the dataset. In a parametric model the size of the domain representation (i.e., the\nnumber of parameters) is independent of the number of instances in the dataset. Examples\nof parametric models include the naive Bayes and Bayesian network models in Chapter\n6[243]and the simple linear and logistic regression models in Chapter 7[311]. For example, the\nnumber of factors required by a naive Bayes model is only dependent on the number of\nfeatures in the domain and is independent of the number of instances. Likewise, the num-\nber of weights used in a linear regression model is deﬁned by the number of descriptive\nfeatures and is independent of the number of instances in the training data.\nIn a non-parametric model the number of parameters used by the model increases as\nthe number of instances increases. Nearest neighbor models are an obvious example of\na non-parametric model. As new instances are added to the feature space, the size of the\nmodel’s representation of the domain increases. Decision trees are also considered non-\nparametric models. The reason for this is that when we train a decision tree from data, we\ndo not assume a ﬁxed set of parameters prior to training that deﬁne the tree. Instead, the\ntree branching and the depth of the tree are related to the complexity of the dataset it is\ntrained on. If new instances were added to the dataset and we rebuilt the tree, it is likely\nthat we would end up with a (potentially very) different tree. Support vector machines are\nalso non-parametric models. They retain some instances from the dataset—potentially all\nof them, although in practice, relatively few—as part of the domain representation. Hence,\nthe size of the domain representation used by a support vector machine may change as\ninstances are added to the dataset.\nIn general, parametric models make stronger assumptions about the underlying distri-\nbutions of the data in a domain. A linear regression model, for example, assumes that\nthe relationship between the descriptive features and the target is linear (this is a strong","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":786,"page_label":"732","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"trained on. If new instances were added to the dataset and we rebuilt the tree, it is likely\nthat we would end up with a (potentially very) different tree. Support vector machines are\nalso non-parametric models. They retain some instances from the dataset—potentially all\nof them, although in practice, relatively few—as part of the domain representation. Hence,\nthe size of the domain representation used by a support vector machine may change as\ninstances are added to the dataset.\nIn general, parametric models make stronger assumptions about the underlying distri-\nbutions of the data in a domain. A linear regression model, for example, assumes that\nthe relationship between the descriptive features and the target is linear (this is a strong\nassumption about the distribution in the domain). Non-parametric models are more ﬂexi-\nble but can struggle with large datasets. For example, a 1-NN model has the ﬂexibility to\nmodel a discontinuous decision surface; however, it runs into time and space complexity\nissues as the number of instances grows.\nWhen datasets are small, a parametric model may perform well because the strong as-\nsumptions made by the model—if correct—can help the model to avoid overﬁtting. How-\never, as the size of the dataset grows, particularly if the decision boundary between the\nclasses is very complex, it may make more sense to allow the data to inform the predic-\ntions more directly. Obviously the computational costs associated with non-parametric\nmodels and large datasets cannot be ignored. However, support vector machines are an","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":787,"page_label":"733","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14.1 Different Perspectives on Prediction Models 733\nexample of a non-parametric model that, to a large extent, avoids this problem. As such,\nsupport vector machines are often a good choice in complex domains with lots of data.\nThe other important distinction that is often made between classiﬁcation models is whether\nthey are generative ordiscriminative . A model is generative if it can be used to generate\ndata that will have the same characteristics as the dataset from which the model was pro-\nduced. In order to do this, a generative model must learn, or encode, the distribution of the\ndata belonging to each class. The Bayesian network models described in Chapter 6[243]are\nexamples of generative models.1Indeed, Markov chain Monte Carlo methods for estimat-\ning probabilities are based on the fact that we can run these models to generate data that\napproximate the distributions of the dataset from which the model was induced. Because\nthey explicitly model the distribution of the data for each class knearest neighbor models\nare also generative models.\nIn contrast, discriminative models learn the boundary between classes rather than the\ncharacteristics of the distributions of the different classes. Support vector machines and\nthe other classiﬁcation models described in Chapter 7[311]are examples of discriminative\nprediction models. In some cases they learn a hard boundary between the classes; in other\ncases—such as logistic regression—they learn a soft boundary, which takes into account\nthe distance from the boundary. However, all these models learn a boundary. Decision trees\nare also discriminative models. Decision trees are induced by recursively partitioning the\nfeature space into regions belonging to the different classes, and consequently they deﬁne\na decision boundary by aggregating the neighboring regions belonging to the same class.\nDecision tree model ensembles based on bagging andboosting are also discriminative\nmodels. Most of the deep network models described in Chapter 8[381]are also discriminative\nmodels, although the auto-encoder network described in Chapter 10[597]is a nice example\nof a generative neural network.\nThis generative versus discriminative distinction is more than just a labeling exercise.\nGenerative and discriminative models learn different concepts. In probabilistic terms, us-\ningdto represent the vector of descriptive feature values and tlto represent a target level,\na generative model works by","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":787,"page_label":"733","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"a decision boundary by aggregating the neighboring regions belonging to the same class.\nDecision tree model ensembles based on bagging andboosting are also discriminative\nmodels. Most of the deep network models described in Chapter 8[381]are also discriminative\nmodels, although the auto-encoder network described in Chapter 10[597]is a nice example\nof a generative neural network.\nThis generative versus discriminative distinction is more than just a labeling exercise.\nGenerative and discriminative models learn different concepts. In probabilistic terms, us-\ningdto represent the vector of descriptive feature values and tlto represent a target level,\na generative model works by\n1.learning the class conditional densities (i.e., the distribution of the data for each target\nlevel) Ppd|tlqand the class priors Pptlq;\n2.then using Bayes’ Theorem to compute the class posterior probabilities Pptl|dq;2\n3.and then applying a decision rule over the class posteriors to return a target level.\n1. In this discussion, when we categorize models as being generative or discriminative, we are speaking in the\ngeneral case. In fact, all models can be trained in either a generative or a discriminative manner. However, some\nmodels lend themselves to generative training and others to discriminative training, and it is this perspective that\nwe use in this discussion.\n2. We could also formulate the generative model as learning the joint distribution Ppd,tlqdirectly and then\ncomputing the required posteriors from this distribution.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":788,"page_label":"734","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"734 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\n0 2 4 6 8 100.0 0.1 0.2 0.3 0.4 0.5\ndClass Conditional DensityP(d|t=l1) P(d|t=l2)\n(a)\n0 2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0\ndClass P oster iorsP(t=l1|d) P(t=l2|d) (b)\nFigure 14.1\n(a) The class conditional densities for two classes ( l1,l2) with a single descriptive feature d. The\nheight of each curve reﬂects the density of the instances from that class for that value of d. (b) The\nclass posterior probabilities plotted for each class for different values of d. Notice that the class\nposterior probability Ppt“l1|dqis not affected by the multimodal structure of the corresponding\nclass conditional density Ppd|t“l1q. This illustrates how the class posterior probabilities can be\nsimpler than the class conditional densities. The solid vertical line in (b) plots the decision boundary\nfordthat gives the minimum misclassiﬁcation rate assuming uniform prior for the two target levels\n(i.e., Ppt“l1q“Ppt“l2q). This ﬁgure is based on Figure 1.27 from Bishop (2006).\nBy contrast, a discriminative model works by\n1.learning the class posterior probability Pptl|dqdirectly from the data,\n2.and then applying a decision rule over the class posteriors to return a target level.\nThis distinction between what generative and discriminative models try to learn is im-\nportant because the class conditional densities, Ppd|tlq, can be very complex compared to\nthe class posteriors, Pptl|dq(see Figure 14.1[734]). Consequently, generative models try to\nlearn more complex solutions to the prediction problem than discriminative models.\nThe potential difﬁculty in learning the class conditional densities, relative to the pos-\nterior class probabilities, is exacerbated in situations where we have a lot of descriptive\nfeatures because, as the dimensionality of dincreases, we will need more and more data\nto create good estimates for Pptl|dq. So, in complex domains, discriminative models are\nlikely to be more accurate. However, as is so often the case in machine learning, this is\nnot the end of the generative versus discriminative debate. Generative models tend to have\na higher bias—they make more assumptions about the form of the distribution they are\nlearning. For example, as we discussed in Chapter 6[243]on probability, generative models\nencode independence assumptions about the descriptive features in d. This may sound","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":789,"page_label":"735","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14.2 Choosing a Machine Learning Approach 735\nTable 14.2\nA taxonomy of models based on the parametric versus non-parametric and generative versus dis-\ncriminative distinctions.\nParametric/ Generative/\nModel Non-Parametric Discriminative\nkNearest Neighbor Non-Parametric Generative\nDecision Trees Non-Parametric Discriminative\nBagging/Boosting Parametric* Discriminative\nNaive Bayes Parametric Generative\nBayesian Network Parametric Generative\nLinear Regression Parametric Discriminative\nLogistic Regression Parametric Discriminative\nSVM Non-Parametric Discriminative\nNeural networks Parametric** Discriminative\n*Although the individual models in an ensemble could be non-parametric (for example, when decision trees are\nused), the ensemble model itself is considered parametric. **We have classiﬁed neural networks as parametric\nmodels; however, it can be argued that deep networks are non-parametric because they have so many parameters\nthat they have a representational overcapacity.\nlike another problem for generative models. However, in domains where we have good\nprior knowledge of the independence relationships between features, we can encode this\nprior structural information into a generative model. This structural information can bias\nthe model in such as way as to help it avoid overﬁtting the data. As a result, a genera-\ntive model may outperform a discriminative model when trained on a small dataset with\ngood prior knowledge. Conversely, however, as the amount of training data increases. the\nbias imposed on a generative model can become larger than the error of the trained model.\nOnce this tipping point in dataset size has been surpassed, a discriminative model will\noutperform a generative model.\nThe debate regarding the advantages and disadvantages of generative and discriminative\nmodels can be extended beyond model accuracy to include their ability to handle missing\ndata, unlabeled data, and feature preprocessing, among other topics. We will not discuss\nthese topics here. Instead, we will simply note that the appropriate choice of generative\nversus discriminative model is context-dependent, and evaluating a range of different types\nof models is the safest option. Table 14.2[735]summarizes the different perspectives on the\nmodel types that we have presented in this book.\n14.2 Choosing a Machine Learning Approach\nEach of the approaches to machine learning that we have presented in this book induces","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":789,"page_label":"735","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"outperform a generative model.\nThe debate regarding the advantages and disadvantages of generative and discriminative\nmodels can be extended beyond model accuracy to include their ability to handle missing\ndata, unlabeled data, and feature preprocessing, among other topics. We will not discuss\nthese topics here. Instead, we will simply note that the appropriate choice of generative\nversus discriminative model is context-dependent, and evaluating a range of different types\nof models is the safest option. Table 14.2[735]summarizes the different perspectives on the\nmodel types that we have presented in this book.\n14.2 Choosing a Machine Learning Approach\nEach of the approaches to machine learning that we have presented in this book induces\ndistinct types of prediction models with different strengths and weaknesses. This raises the\nquestion of when to use which machine learning approach. The ﬁrst thing to understand","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":790,"page_label":"736","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"736 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nis that there is not one best approach that always outperforms the others. This is known\nas the No Free Lunch Theorem (Wolpert, 1996). Intuitively, this theorem makes sense\nbecause each algorithm encodes a distinct set of assumptions (i.e., the inductive bias of\nthe learning algorithm), and a set of assumptions that are appropriate in one domain may\nnot be appropriate in another domain.\nWe can see the assumptions encoded in each algorithm reﬂected in the distinctive char-\nacteristics of the decision boundaries that they learn for categorical prediction tasks. To\nillustrate these characteristics, we have created three artiﬁcial datasets and trained four dif-\nferent models on each of these datasets. The top row of images in Figure 14.2[737]illustrates\nhow the three artiﬁcial datasets were created. Each of the images in the top row shows a\nfeature space deﬁned by two continuous descriptive features, F1 and F2, partitioned into\ngood andbadregions by three different, artiﬁcially created decision boundaries.3In the\nsubsequent images, we show the decision boundaries that are learned by four different ma-\nchine learning algorithms based on training datasets generated according to the decision\nboundaries shown in the top row. In order from top to bottom, we show decision trees\n(without pruning), nearest neighbor models (with k= 3 and using majority voting), naive\nBayes models (using normal distributions to represent the two continuous feature values),\nand logistic regression models (using a simple linear model). In these images the training\ndata instances are shown as symbols on the feature space (triangles for good and crosses\nforbad), the decision boundaries learned by each algorithm are represented by thick black\nlines, and the underlying actual decision boundaries are shown by the background shading.\nThese examples show two things. First, the decision boundaries learned by each algo-\nrithm are characteristic of that algorithm. For example, the decision boundaries associated\nwith decision trees have a characteristic stepped appearance because of the way feature val-\nues are split in a decision tree, while the decision boundaries associated with k-NN models\nare noticeably jagged because of their local focus. The characteristic appearance of the de-\ncision boundaries is related to the representations used within the models and the inductive","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":790,"page_label":"736","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"forbad), the decision boundaries learned by each algorithm are represented by thick black\nlines, and the underlying actual decision boundaries are shown by the background shading.\nThese examples show two things. First, the decision boundaries learned by each algo-\nrithm are characteristic of that algorithm. For example, the decision boundaries associated\nwith decision trees have a characteristic stepped appearance because of the way feature val-\nues are split in a decision tree, while the decision boundaries associated with k-NN models\nare noticeably jagged because of their local focus. The characteristic appearance of the de-\ncision boundaries is related to the representations used within the models and the inductive\nbiases that the algorithms used to build them encode. The second thing that is apparent\nfrom the images in Figure 14.2[737]is that some of the models do a better job of represent-\ning the underlying decision boundaries than others. The decision boundary learned by the\nlogistic regression model best matches the underlying decision boundary for the dataset\nin the ﬁrst column, the decision tree model seems most appropriate for the dataset in the\nsecond column, and the k-NN model appears best for the dataset in the third column.\n3. This example is partly inspired by the ”machine learning classiﬁer gallery” by Tom Fawcett at home.comcast.\nnet/„tom.fawcett/public html/ML-gallery/pages/","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":791,"page_label":"737","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"F1F2\ngoodbad\nF1F2\ngoodbad\nF1F2\ngoodbad\nDecision trees\n k-NN\n Naive Bayes\n Logistic regression\nFigure 14.2\nAn illustration of the decision boundaries learned by different machine learning algorithms for three\nartiﬁcial datasets.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":792,"page_label":"738","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"738 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nReal predictive data analytics projects use datasets that are much more complex than\nthose shown in Figure 14.2[737]. For this reason selecting which type of model to use should\nbe informed by the speciﬁc priorities of a project and the types of the descriptive and target\nfeatures in the data. Also, in general, it is not a good idea to select just one machine\nlearning approach at the beginning of a project and to exclusively use that. Instead, it is\nbetter to choose a number of different approaches and to run experiments to evaluate which\nis best for the particular project. However, this still requires the selection of a set of initial\napproaches. There are two questions to consider:\n1.Does a machine learning approach match the requirements of the project?\n2.Is the approach suitable for the type of prediction we want to make and the types of\ndescriptive features we are using?\n14.2.1 Matching Machine Learning Approaches to Projects\nIn many cases the primary requirement of a project is to create an accurate prediction\nmodel. Accuracy can often be related to the power of a machine learning algorithm to\ncapture the interaction between descriptive features and the target feature. Caruana and\nNiculescu-Mizil (2006) and Caruana et al. (2008) report empirical evaluations of the ac-\ncuracy of a range of model types across a range of domains. They found that on average,\nensemble models and support vector machines were among the most accurate models.\nA consistent ﬁnding in both of these experiments, however, was the fact that for some do-\nmains, these more powerful models performed quite poorly, and other models, that in other\ndomains were quite weak, achieved the best results. The main conclusions from this, and\nother similar studies, is that no machine learning approach is universally best, and exper-\nimentation with different approaches is the best way to ensure that an accurate model is\nbuilt.\nWhen evaluating models against a particular deployment scenario, model accuracy is not\nthe only issue we need to consider. In order to successfully address a business problem,\na model must be accurate, but it must also meet the other requirements of the business\nscenario. Three issues are important to consider:\n‚Prediction speed: How quickly can a model make predictions? Logistic regression\nmodels, for example, are very fast at making predictions as all that is involved is cal-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":792,"page_label":"738","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"other similar studies, is that no machine learning approach is universally best, and exper-\nimentation with different approaches is the best way to ensure that an accurate model is\nbuilt.\nWhen evaluating models against a particular deployment scenario, model accuracy is not\nthe only issue we need to consider. In order to successfully address a business problem,\na model must be accurate, but it must also meet the other requirements of the business\nscenario. Three issues are important to consider:\n‚Prediction speed: How quickly can a model make predictions? Logistic regression\nmodels, for example, are very fast at making predictions as all that is involved is cal-\nculating the regression equation and performing a thresholding operation. On the other\nhand, knearest neighbor models are very slow to make predictions as they must perform\na comparison of a query instance to every instance in a, typically large, training set. The\ntime differences arising from these different computational loads can have an inﬂuence\non model selection. For example, in a real-time credit card fraud prediction system, it\nmay be required that a model perform thousands of predictions per second. Even if sig-\nniﬁcant computational resources were to be deployed for such a problem, it may not be\npossible for a knearest neighbor model to perform fast enough to meet this requirement.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":793,"page_label":"739","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14.2 Choosing a Machine Learning Approach 739\n‚Capacity for retraining: In Section 9.4.6[578]we discussed approaches that can be used\nto monitor the performance of a model so as to ﬂag the occurrence of concept drift and\nindicate if a model has gone stale. When this occurs, the model needs to be changed\nin some way to adapt to the new scenario. For some modeling approaches this is quite\neasy, while for others it is almost impossible to adapt a model, and the only option is to\ndiscard the current model and train a new one using an updated dataset. Naive Bayes and\nknearest neighbor models are good examples of the former type, while decision trees\nand regression models are good examples of the latter.\n‚Interpretability: In many instances a business will not be happy to simply accept the\npredictions made by a model and incorporate these into their decision making. Rather,\nthey will require the predictions made by a model to be explained and justiﬁed. Different\nmodels offer different levels of explanation capacity and therefore different levels of\ninterpretability. For example, decision trees and linear regression models are very easily\ninterpreted, while support vector machines, ensembles, and deep neural networks are\nalmost entirely uninterpretable (because of this, they are often referred to as a black\nbox).\nIn summary, ensembles, support vector machines, neural networks, and Bayesian net-\nworks are, in general, more powerful machine learning approaches than the others we have\npresented. However, these approaches are more complex, take a longer time to train, and\nare harder to interpret than the simpler approaches that we have presented. Furthermore,\nthe selection of a machine learning approach also depends on the aspects of an application\nscenario described above (speed, capacity for retraining, interpretability), and often, these\nfactors are a bigger driver for the selection of a machine learning approach than prediction\naccuracy.\n14.2.2 Matching Machine Learning Approaches to Data\nWhen matching machine learning approaches to the characteristics of a dataset, it is im-\nportant to remember that almost every approach can be made to work for both continuous\nand categorical descriptive and target features. Certain approaches, however, are a more\nnatural ﬁt for some kinds of data than others, so we can make some recommendations.\nThe ﬁrst thing to consider in regard to data is whether the target feature is continuous or","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":793,"page_label":"739","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"scenario described above (speed, capacity for retraining, interpretability), and often, these\nfactors are a bigger driver for the selection of a machine learning approach than prediction\naccuracy.\n14.2.2 Matching Machine Learning Approaches to Data\nWhen matching machine learning approaches to the characteristics of a dataset, it is im-\nportant to remember that almost every approach can be made to work for both continuous\nand categorical descriptive and target features. Certain approaches, however, are a more\nnatural ﬁt for some kinds of data than others, so we can make some recommendations.\nThe ﬁrst thing to consider in regard to data is whether the target feature is continuous or\ncategorical. Models trained by reducing the sum of squared errors, for example, linear re-\ngression, are the most natural ﬁt for making predictions for continuous target features. Out\nof the different approaches we have considered, the information-based and probability-\nbased approaches are least well suited in this case. If, on the other hand, the target feature\nis categorical, then information-based and probability-based approaches are likely to work\nvery well. Models trained using error-based approaches can become overly complicated\nwhen the number of levels of the target feature goes above two, although deep neural net-\nwork models handle these scenarios easily.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":794,"page_label":"740","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"740 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nIf all the descriptive features in a dataset are continuous, then a similarity-based approach\nis a natural ﬁt, especially when there is also a categorical target feature. Error-based models\nwould be preferred if the target feature is also continuous. When there are many continuous\nfeatures, probability-based and information-based models can become complicated, but if\nall the features in a dataset are categorical, then information-based and probability-based\nmodels are appropriate. Error-based and deep learning models are less suitable in this\ncase as they require categorical features to be converted into sets of binary features, which\ncauses an increase in dimensionality. In many cases datasets will contain both categorical\nand continuous descriptive features. The most naturally suited learning approaches in these\nscenarios are probably those that are best suited for the majority feature type.\nThe last issue to consider in relation to data when selecting machine learning approaches\nis the curse of dimensionality . If there are a large number of descriptive features, then we\nwill need a large training dataset. Feature selection is an important process in any machine\nlearning project and should generally be applied no matter what type of models are being\ndeveloped. That said, some models are more susceptible to the curse of dimensionality than\nothers. Similarity-based approaches are particularly sensitive to the curse of dimensionality\nand can struggle to perform well for a dataset with large numbers of descriptive features.\nDecision tree models have a feature selection mechanism built into the algorithm and so are\nmore robust to this issue. Also, some of the unsupervised learning approaches described\nin Chapter 10[597]can be used to address the curse of dimensionality by learning new more\ncompact representations.\n14.3 Beyond Prediction\nThe majority of this book is focused on building predictive models. Machine learning,\nhowever, can be used for many other tasks. In Chapters 10[597]and 11[637]we described two\nof the major uses of machine learning that go beyond prediction : unsupervised learning\nand reinforcement learning.\nUnsupervised machine learning techniques are used in the absence of a target feature and\nmodel the underlying structure within the descriptive features in a dataset. Usually this is","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":794,"page_label":"740","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"more robust to this issue. Also, some of the unsupervised learning approaches described\nin Chapter 10[597]can be used to address the curse of dimensionality by learning new more\ncompact representations.\n14.3 Beyond Prediction\nThe majority of this book is focused on building predictive models. Machine learning,\nhowever, can be used for many other tasks. In Chapters 10[597]and 11[637]we described two\nof the major uses of machine learning that go beyond prediction : unsupervised learning\nand reinforcement learning.\nUnsupervised machine learning techniques are used in the absence of a target feature and\nmodel the underlying structure within the descriptive features in a dataset. Usually this is\ndone either to divide a dataset into clusters of similar examples, or to generate new features\nthat can be appended to a dataset.\nIn reinforcement learning an agent inhabiting an environment learns to perform a task by\npursuing actions that achieve the highest cumulative reward, where a reward is immediate\nfeedback that follows an action to indicate how successful it was. The main application of\nreinforcement learning is to learn control strategies, for example, in robotics.\nAs we did with predictive modeling we can capture the mathematical foundations of\nthese further approaches to machine learning in two key equations: the function minimized\nby the k-means clustering algorithm (Equation (14.6)[741]) and the action-value function","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":795,"page_label":"741","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"14.4 Your Next Steps 741\nupdate rule used in Q-learning (Equation (14.7)[741]).\nnÿ\ni“1min\nc1,...,ckDistpdi,cjq (14.6)\nQpst,atqÐQpst,atq`αˆ\nrt`γmax\nat`1Qpst`1,at`1q´Qpst,atq˙\n(14.7)\nUnderstanding these two equations, and the core algorithms that they underpin ( k-means\nclustering and Q-learning), is an excellent step toward broadening your knowledge out to\nthe many other uses of machine learning beyond prediction. Most of the advice given in\nthe previous sections on choosing machine learning approaches and completing successful\nprojects, also equally applies to these machine learning approaches, and a good grounding\nin predictive modeling makes adding these approaches to your toolkit relatively straight-\nforward.\n14.4 Your Next Steps\nIn many ways, the easy part of a predictive data analytics project is building the models.\nThe machine learning algorithms tell us how to do this. What makes predictive data an-\nalytics difﬁcult, but also fascinating, is ﬁguring out how to answer all the questions that\nsurround the modeling phase of a project. Throughout the course of a predictive data an-\nalytics project, we are forced to use our intuition and experience, and experimentation,\nto steer the project toward the best solution. To ensure a successful project outcome, we\nshould inform the decisions that we make by\n‚becoming situationally ﬂuent so that we can converse with experts in the application\ndomain;\n‚exploring the data to understand it correctly;\n‚spending time cleaning the data;\n‚thinking hard about the best ways to represent features;\n‚and spending time designing the evaluation process correctly.\nA distinctive aspect of this book is that we have chosen to present machine learning\nin context. In order to do this, we have included topics that are not covered in many\nmachine learning books, including discussions on business understanding, data exploration\nand preparation, and case studies. We have also provided an in-depth introduction to some\nof the most popular machine learning approaches with examples that illustrate how these\nalgorithms work. We believe that this book will provide you with an understanding of the\nbroader context and core techniques of machine learning that will enable you to have a\nsuccessful career in predictive data analytics.\nMachine learning is a huge topic, however, and one book can only be so long. As a result,\nwe have had to sacriﬁce coverage of some aspects of machine learning in order to include","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":796,"page_label":"742","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"742 Chapter 14 The Art of Machine Learning for Predictive Data Analytics\nother topics and worked examples. We believe that this book will give you the knowledge\nand skills that you will need to explore these topics yourself. To help with this, we would\nrecommend Hastie et al. (2001), Bishop (2006), and Murphy (2012) for broad coverage\nof machine learning algorithms, including more in-depth coverage of unsupervised and\nreinforcement learning approaches than included in this book. These books are suitable\nas reference texts for experienced practitioners and postgraduate researchers in machine\nlearning. Some of the other machine learning topics that you might like to explore include\nsemi-supervised learning (Chapelle et al., 2009), multi-label classiﬁcation (Tsoumakas\net al., 2012), and graphical models (Kollar and Friedman, 2009). Finally, we hope that\nyou ﬁnd machine learning as fascinating and rewarding a topic as we do, and we wish you\nthe best in your future learning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":797,"page_label":"743","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"V APPENDICES","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":799,"page_label":"745","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A Descriptive Statistics and Data Visualization for Machine Learning\nIn this appendix we introduce the fundamental statistical measures of central tendency andvaria-\ntion. We also introduce three of the most important and useful data visualization techniques that can\nbe used to visualize a single feature: the bar plot , the histogram , and the box plot .\nA.1 Descriptive Statistics for Continuous Features\nTo understand the characteristics of a continuous feature, there are two things that are important to\nmeasure: the central tendency of the feature and the variation within the feature. These are the\nbasic building blocks of everything else that will follow, so it is important to fully understand them.\nA.1.1 Central Tendency\nThe central tendency of a sample refers to the value that is typical of the sample and therefore can be\nused to summarize it. Measures of central tendency are an approximation of this notional value. The\narithmetic mean (orsample mean or just mean ) is the best-known measure of central tendency.\nThe arithmetic mean of a set of nvalues for a feature ais denoted by the symbol aand is calculated\nas\na“1\nnnÿ\ni“1ai (A.1)\nFigure A.1[746]shows a group of players on a school basketball team and their heights. Using\nEquation (A.1)[745]we can calculate the arithmetic mean of these players’ heights as\nHEIGHT“1\n8ˆp150`163`145`140`157`151`140`149q\n“149.375\nThis mean height is shown by the dashed gray line in Figure A.1[746]. The arithmetic mean is one\nmeasure of the central tendency of a sample (for our purposes, a sample is just a set of values for a\nfeature in an ABT). Because it is easy to calculate and easy to interpret, the mean is commonly used\nas part of the data exploration process as a good estimate of the central tendencies of features in an\nABT.\nAny measure of central tendency is, however, just an approximation, so we must be aware of the\nlimitations of any measure that we use. The arithmetic mean, for example, is very sensitive to very\nlarge or very small values in a sample. For example, suppose our basketball team manages to sign\naringer measuring in at 229cm, as shown in Figure A.2(a)[746]. The arithmetic mean for the full\ngroup is now 158.222cm and, as shown by the dashed gray line in Figure A.2(a)[746], no longer really\nrepresents the central tendency of the group. An unusually large or small value like this is referred\nto as an outlier , and the arithmetic mean is very sensitive to the presence of outliers.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":799,"page_label":"745","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"ABT.\nAny measure of central tendency is, however, just an approximation, so we must be aware of the\nlimitations of any measure that we use. The arithmetic mean, for example, is very sensitive to very\nlarge or very small values in a sample. For example, suppose our basketball team manages to sign\naringer measuring in at 229cm, as shown in Figure A.2(a)[746]. The arithmetic mean for the full\ngroup is now 158.222cm and, as shown by the dashed gray line in Figure A.2(a)[746], no longer really\nrepresents the central tendency of the group. An unusually large or small value like this is referred\nto as an outlier , and the arithmetic mean is very sensitive to the presence of outliers.\nThere are other statistics that we can use to measure central tendency that are not as sensitive to\noutliers. The median is another very useful measure of the central tendency of a sample. The median\nof a set of values can be calculated by ordering the values from lowest to highest and selecting the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":800,"page_label":"746","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"746 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\n150 163 157 140 145 140 151 149 \nFigure A.1\nThe members of a school basketball team. The height of each player is listed below the player. The\ndashed gray line shows the arithmetic mean of the players’ heights.\n150 163 157 140 145 140 151 149 229 \n(a) Mean\n140 140 150 149 145 157 151 163 229 (b) Median\nFigure A.2\nThe members of the school basketball team from Figure A.1[746]with one very tall ringer added: (a)\nthe dashed gray line shows the mean of the players’ heights; and (b) the dashed gray line shows the\nmedian of the players’ heights, with the players ordered by height.\nmiddle value. If there is an even number of values in the sample, then the median is obtained by\ncalculating the arithmetic mean of the middle two values. The median is not as sensitive to outliers\nas the arithmetic mean and therefore can be a more accurate estimate of the central tendency of a set\nof values if outliers exist. In fact, a large difference between the mean and median of a feature is an\nindication that there may be outliers among the feature values.\nFigure A.2(b)[746]shows the extended basketball team ordered from smallest to tallest, with the\nheight of the each player listed below the player. The median value of this set is 150and is shown as\nthe dashed gray line in Figure A.2(b)[746]. In this case the median better captures the central tendency\nof the set of values.\nAnother commonly used measure of central tendency is the mode . The mode is simply the most\ncommonly occurring value in a sample (determined by counting the frequency with which each value\noccurs in the sample). If all values in a sample occur with equal frequency, then there is no mode.\nFor the heights of the players in the extended basketball team in Figure A.2[746], the mode is 140, as it\nis the only value that appears twice. The mode is not particularly effective in this case at measuring\nthe central tendency of the values. Mode is more frequently useful for categorical features than for\ncontinuous ones, but it can be useful for continuous features when the sample is large enough.\nA.1.2 Variation\nHaving used the measures of central tendency to describe where our data is centered, we will now\nturn our attention to the variation in our data. Figure A.3[747]shows a rival school basketball team of\nthat shown in Figure A.1[746]. The height of each player is listed below the player, and the dashed gray","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":801,"page_label":"747","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A.1 Descriptive Statistics for Continuous Features 747\nline shows the arithmetic mean of the players’ heights, which is 149.375, the same as for the original\nteam. The heights of the players in this second team vary much more than those of the ﬁrst team (see\nFigures A.1[746]and A.3[747]). Descriptive statistics provides us with a range of tools that we can use\nto formally measure variation and so distinguish between the sets of heights in the two basketball\nteams. In essence, most of statistics, and in turn, analytics, is about describing and understanding\nvariation.\n192 102 126 165 145 123 154 188 \nFigure A.3\nThe members of a rival school basketball team. Player heights are listed below each player. The\ndashed gray line shows the arithmetic mean of the players’ heights.\nThe most easily calculated measure of variation is range . The range of a sample of nvalues for a\nfeature ais calculated as\nrange“maxpaq´minpaq (A.2)\nThe range of the basketball player heights in Figure A.1[746]is163´140“23and for those in\nFigure A.3[747]is192´102“90. These measures match what we intuitively see in these ﬁgures—\nthe heights of the second team vary much more than those of the ﬁrst team. The main advantage of\nusing the range is the ease with which it is calculated. The major disadvantage of the range, however,\nis that it is highly sensitive to outliers.\nThevariance of a sample is a more useful measure of variation. Variance measures the average\ndifference between each value in a sample and the mean of that sample. The variance of the nvalues\nof a feature ais denoted varpaqand is calculated as\nvarpaq“nÿ\ni“1pai´aq2\nn´1(A.3)\nIn order to allow for the fact that some of the differences between values and the mean will be positive\nand some will be negative, we square each difference.1\nFor the players’ heights given in Figure A.1[746], the mean is 149.375, so the variance can be calcu-\nlated as\nvarpHEIGHTq“p150´149.375q2`p163´149.375q2`...`p149´149.375q2\n8´1\n“63.125\n1. We divide by n´1(as opposed to n) because we are calculating the variance using only a sample, and on\naverage, dividing by n´1gives a better estimate of the population variance than using n.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":802,"page_label":"748","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"748 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\nFor the players’ heights given in Figure A.3[747], the mean is also 149.375, so the variance can be\ncalculated as\nvarpHEIGHTq“p192´149.375q2`p102´149.375q2`...`p188´149.375q2\n8´1\n“1,011.411\nThis example illustrates that the variance also captures the intuition that the heights of the players\nin the second team vary much more than those in the ﬁrst team. It also, however, illustrates an issue\nwith using variance. Due to the fact that the differences are squared, variances are not in the same\nunits as the original values, so they are not especially informative—telling someone that the variance\nof the heights on one team is 63.125and on another is 1,011.411doesn’t give them any particularly\nuseful information other than the fact that the variance of one team is bigger than that of the other.\nThestandard deviation ,sd, of a sample is calculated by taking the square root of the variance of\nthe sample:\nsdpaq“b\nvarpaq\n“gffffenÿ\ni“1pai´aq2\nn´1(A.4)\nThis means that the standard deviation is measured in the original units of the sample, which makes it\nmuch more interpretable than the variance. It is very common to see the mean and standard deviation\nprovided as a full description of a sample.\nThe standard deviation of the heights of the players on the ﬁrst basketball team is 7.945and for the\nsecond team is 31.803. As these measures are in the same units as the heights, they afford us a more\nintuitive understanding of the data and make comparison easier. We can say that, on average, players\non the ﬁrst team vary by almost 8cm from the average of 149.375cm, while on the second team, they\nvary by approximately 32cm.\nPercentiles are another useful measure of the variation of the values for a feature. A proportion\nofi\n100of the values in a sample take values equal to or lower than the ithpercentile of that sample.\nConversely, a proportion of p100´iq{100values in a sample take values larger than the ithpercentile.\nTo calculate the ithpercentile of the nvalues of a feature a, we ﬁrst order the values in ascending\norder and then multiply nbyi\n100to determine the index . If the index is a whole number, we take the\nvalue at that position in the ordered list of values as the ithpercentile. If index is not a whole number,\nthen we interpolate the value for the ithpercentile as\nithpercentile“p1´index fqˆaindex w`index fˆaindex w`1 (A.5)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":802,"page_label":"748","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Percentiles are another useful measure of the variation of the values for a feature. A proportion\nofi\n100of the values in a sample take values equal to or lower than the ithpercentile of that sample.\nConversely, a proportion of p100´iq{100values in a sample take values larger than the ithpercentile.\nTo calculate the ithpercentile of the nvalues of a feature a, we ﬁrst order the values in ascending\norder and then multiply nbyi\n100to determine the index . If the index is a whole number, we take the\nvalue at that position in the ordered list of values as the ithpercentile. If index is not a whole number,\nthen we interpolate the value for the ithpercentile as\nithpercentile“p1´index fqˆaindex w`index fˆaindex w`1 (A.5)\nwhere index wis the whole part of index ,index fis the fractional part of index , and aindex wis the\nvalue in the ordered list at position index w.\nFor example, Figure A.4[749]shows the basketball team from Figure A.3[747]ordered by height. To\ncalculate the 25thpercentile, we ﬁrst calculate index as25\n100ˆ8“2. So, the 25thpercentile is the\nsecond value in the ordered list, which is 123. To calculate the 80thpercentile, we ﬁrst calculate\nindex as80\n100ˆ8“6.4. Because index is not a whole number, we set index wto the whole part of","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":803,"page_label":"749","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A.2 Descriptive Statistics for Categorical Features 749\n192 102 126 165 145 123 154 188 \nFigure A.4\nThe members of the rival school basketball team from Figure A.3[747]ordered by height.\nindex ,6, and index fto the fractional part, 0.4. Then we can calculate the 80thpercentile as\np1´0.4qˆ165`0.4ˆ188“174.2\nusing the 6thand7thvalues in the list, 165and188. We have actually already come across a percentile\nin the measures of central tendency. The median is the 50thpercentile.\nWe can use percentiles to describe another measure of variation known as the inter-quartile range\n(IQR ). The inter-quartile range is calculated as the difference between the 25thpercentile and the 75th\npercentile. These percentiles are also known as the lower quartile (or1stquartile) and the upper\nquartile (or3rdquartile), hence the name inter-quartile range. For the heights of the ﬁrst basketball\nteam, the inter-quartile range is 151´140“11, while for the second team, it is 165´123“42.\nA.2 Descriptive Statistics for Categorical Features\nThe statistics outlined in the previous section work well to describe continuous features, but they do\nnot work for categorical features. For categorical features we are interested primarily in frequency\ncounts andproportions . The frequency count of each level2of a categorical feature is calculated\nby counting the number of times that level appears in the sample. The proportion for each level is\ncalculated by dividing the frequency count for that level by the total sample size. Frequencies and\nproportions are typically presented in a frequency table , which shows the frequency and proportion\nof each level for a particular feature—usually sorted by descending frequency.\nFor example, Table A.1[750]lists the position that each player on a school basketball team plays\nat, and the average training expenses accrued each month by each player on the team. Table A.2[750]\nshows the frequencies and proportions of the positions that players in the team play at, based on\ncounts of the occurrences of the different levels of the P OSITION feature in Table A.1[750]. We can\nsee from this example that the guard level is the most frequent, followed by forward andcenter .\nBased on these frequency counts and proportions, the mode of a categorical feature can be calcu-\nlated. The mode is a measure of the central tendency of a categorical feature and is simply the most\nfrequent level. Based on the counts in Table A.2[750], the mode of the P OSITION feature is guard . We","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":803,"page_label":"749","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"at, and the average training expenses accrued each month by each player on the team. Table A.2[750]\nshows the frequencies and proportions of the positions that players in the team play at, based on\ncounts of the occurrences of the different levels of the P OSITION feature in Table A.1[750]. We can\nsee from this example that the guard level is the most frequent, followed by forward andcenter .\nBased on these frequency counts and proportions, the mode of a categorical feature can be calcu-\nlated. The mode is a measure of the central tendency of a categorical feature and is simply the most\nfrequent level. Based on the counts in Table A.2[750], the mode of the P OSITION feature is guard . We\noften also calculate a second mode , which is just the second most common level of a feature. In this\nexample, the second mode is forward .\n2. Remember, we refer to each value that a particular categorical feature can take as the levels of the categorical\nfeature.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":804,"page_label":"750","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"750 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\nTable A.1\nA dataset showing the positions and monthly training expenses of a school basketball team.\nTRAINING\nID P OSITION EXPENSES\n1 center 56.75\n2 guard 1,800.11\n3 guard 1,341.03\n4 forward 749.50\n5 guard 1,150.00\n6 forward 928.30\n7 center 250.90\n8 guard 806.15\n9 guard 1,209.02\n10 forward 405.72TRAINING\nID P OSITION EXPENSES\n11 center 550.00\n12 center 223.89\n13 center 103.23\n14 forward 758.22\n15 forward 430.79\n16 forward 675.11\n17 guard 1,657.20\n18 guard 1,405.18\n19 guard 760.51\n20 forward 985.41\nTable A.2\nA frequency table for the P OSITION feature from the school basketball team dataset in Table A.1[750].\nLevel Count Proportion\nguard 8 40%\nforward 7 35%\ncenter 5 25%\nA.3 Populations and Samples\nThroughout the discussion in the previous sections about central tendency and variation, we con-\nsistently used the word sample to refer to the set of values in an ABT for a particular feature. In\nstatistics it is very important to understand the difference between a population and a sample. The\nterm population is used in statistics to represent all possible measurements or outcomes that are of\ninterest to us in a particular study or piece of analysis. The term sample refers to the subset of the\npopulation that is selected for analysis.\nFor example, consider Table A.3[751], which shows a set of results for polls run shortly before the\n2012 United States presidential election, in which Mitt Romney and Barack Obama were the front-\nrunners.3In the ﬁrst poll in the table, from Pew Research, we can see that a sample of just 2,709\nlikely voters4was used. This poll put Obama ahead of Romney in the race to the White House. In this\nexample the actual population of interest was the voting population of the United States, which was\napproximately 240,926,957people. It would be almost impossible to ask the full voting population\n3. This data is taken from the collection at Real Clear Politics: www.realclearpolitics.com/epolls/2012/president/\nus/general election romney vsobama-1171.html.\n4. Likely voters are the subset of registered voters who have been identiﬁed as most likely to actually vote in an\nelection.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":805,"page_label":"751","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A.3 Populations and Samples 751\nTable A.3\nPoll results from the run-up to the 2012 U.S. presidential election.\nMargin Sample\nPoll Obama Romney Other Date of Error Size\nPew Research 50 47 3 04-Nov˘2.2 2,709\nABC News/Wash Post 50 47 3 04-Nov˘2.5 2,345\nCNN/Opinion Research 49 49 2 04-Nov˘3.5 963\nPew Research 50 47 3 03-Nov˘2.2 2,709\nABC News/Wash Post 49 48 3 03-Nov˘2.5 2,069\nABC News/Wash Post 49 49 2 30-Oct˘3.0 1,288\ntheir voting intentions before an actual election—after all, that is what the actual election is for—so\npolling companies take a sample.\nWhile the sample of 2,709voters out of a population of 240,926,957might appear quite small, we\ncan also see from the table that the margin of error for the poll is given as ˘2.2%. The margin of\nerror takes into account the fact that this is just a sample from a much larger population.5All the\nother polls in the table were conducted with similar-sized samples. You should notice, however, that,\nin general, the larger the sample, the smaller the margin of error. This reﬂects the fact that if we use\na bigger sample, we can be more conﬁdent in our approximations of the characteristics of the full\npopulation.\nIn choosing a sample, it is important that it be representative of the population. In this example\nthe sample should represent the voting population—for example, there should be a representative\nproportion of males compared to females and of different age categories within the sample. If a\nsample is not representative, we say that the sample is biased . Using a simple random sample is\nthe most straightforward way of avoiding biased samples. In a simple random sample, each item\nin the population is equally likely to make it into the sample. Other, more sophisticated sampling\nmethods can be used to ensure that a sample maintains relationships that exist in a population. We\ndiscuss sampling methods in more detail in Section 3.6.3[91].\nIn the context of a predictive analytics scenario, the sample is the set of values that occur in an\nABT. The population is the set of all the values that could possibly occur. For example, in an ABT\nfor a motor insurance claims fraud prediction problem, we may include details of 500claims that\nhave happened in the past. This would be our sample. The population would be all the claims that\nhave ever happened.\nUp to this point we have outlined descriptive statistics that we can use to describe the values in a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":805,"page_label":"751","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"methods can be used to ensure that a sample maintains relationships that exist in a population. We\ndiscuss sampling methods in more detail in Section 3.6.3[91].\nIn the context of a predictive analytics scenario, the sample is the set of values that occur in an\nABT. The population is the set of all the values that could possibly occur. For example, in an ABT\nfor a motor insurance claims fraud prediction problem, we may include details of 500claims that\nhave happened in the past. This would be our sample. The population would be all the claims that\nhave ever happened.\nUp to this point we have outlined descriptive statistics that we can use to describe the values in a\nsample. How do we relate these values to the actual underlying population? Statistics that describe\nthe population are referred to as population parameters . In general we use the sample statistics,\nwhich we have already calculated, as estimates for the population parameters. The population mean\nof a feature is usually denoted by µ, and in general, given a sufﬁciently large sample, we use the\nsample mean aas a point estimate of µ. The population variance of a feature is usually denoted\nbyσ2. In general, given a sufﬁciently large sample, we use the sample variance, varpaq, as a point\nestimate of σ2. This process is known as statistical inference .\n5. This size of margin of error is common for these types of election polls.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":806,"page_label":"752","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"752 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\nCareful readers will have noticed that in the equation for variance given in Equation (A.3)[747], we\ndivided the sum of the differences between the values of the feature aandanot by n, the number of\nvalues for ain the ABT, but by n´1. We divide by n´1so that the sample variance is an unbiased\nestimate of the population variance. We say that the estimate is unbiased if its variance, on average,\nequals that of the population variance. If we divided by n, we would have a biased estimator that\non average underestimates the variance. It is in small differences like this that we see the impact of\nworking on samples rather than populations.\nA.4 Data Visualization\nWhen performing data exploration, data visualization can help enormously. In this section we\ndescribe three important data visualization techniques that can be used to visualize the values in a\nsingle feature: the bar plot , the histogram , and the box plot . For the examples throughout this\nsection, we will use the dataset in Table A.1[750], which lists the position that each player on a school\nbasketball team plays at and the average training expenses they accrue each month.\nA.4.1 Bar Plots\nThe simplest form of data visualization we can use for data exploration is the bar plot. A bar plot\nincludes a vertical bar for each level of a categorical feature. The height of each bar indicates the\nfrequency of the associated level (readers will most likely already be familiar with the bar plot). In\na slight variation of the bar plot, we can show densities rather than frequencies by dividing each\nfrequency by the total number of values in the dataset. This makes bar plots comparable across\ndatasets or samples of different sizes and is referred to as a probability distribution , because the\ndensities actually tell us the probability that we would pick each level if we were to select one\ninstance at random from the dataset.\nAnother simple variant of the basic bar plot orders the bars in descending order.6Typically we\nuse bar plots to discover the most frequent levels for a feature, and this ordering makes this more\napparent. Figure A.5[753]shows example bar plots of all three types for the P OSITION feature from\nthe dataset in Table A.1[750]. We can see that guard is the most frequent level.\nA.4.2 Histograms\nFigure A.6[753]is a bar plot of the T RAINING EXPENSES feature from Table A.1[750]. The ﬁgure","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":806,"page_label":"752","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"datasets or samples of different sizes and is referred to as a probability distribution , because the\ndensities actually tell us the probability that we would pick each level if we were to select one\ninstance at random from the dataset.\nAnother simple variant of the basic bar plot orders the bars in descending order.6Typically we\nuse bar plots to discover the most frequent levels for a feature, and this ordering makes this more\napparent. Figure A.5[753]shows example bar plots of all three types for the P OSITION feature from\nthe dataset in Table A.1[750]. We can see that guard is the most frequent level.\nA.4.2 Histograms\nFigure A.6[753]is a bar plot of the T RAINING EXPENSES feature from Table A.1[750]. The ﬁgure\nillustrates why a bar plot is not an appropriate graphic to use to visualize a continuous feature: as is\ngenerally the case with a continuous feature, there are as many distinct values as there are instances\nin the dataset, and therefore there are as many bars in the histogram as there are instances, each bar\nhaving a height of 1.0.\nThe way to solve this problem is to visualize intervals rather than speciﬁc values, and this is what\nahistogram does. Figure A.7(a)[754]shows the frequency histogram for the T RAINING EXPENSES\nfeature when we deﬁne ten 200-unit intervals spanning the range that this feature can take (the fre-\nquencies come from Table A.4(a)[755]). In this histogram the width of each bar indicates the extent\nof the interval the bar represents, and the height of each bar is based on the number of instances\nin the dataset that have a value inside the interval. This type of histogram is often referred to as a\nfrequency histogram . Generally, there is not an optimal set of intervals for a given feature. For\n6. These charts are often referred to as Pareto charts , especially when they also include a line indicating the\ncumulative total frequency or density.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":807,"page_label":"753","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A.4 Data Visualization 753\nguard center forward\nPositionFrequency\n0 2 4 6 8\n(a)\nguard center forward\nPositionDensity\n0.0 0.1 0.2 0.3 0.4 (b)\nguard forward center\nPositionDensity\n0.0 0.1 0.2 0.3 0.4 (c)\nFigure A.5\nExample bar plots for the P OSITION feature in Table A.1[750]: (a) frequency bar plot, (b) density bar\nplot, and (c) order density bar plot.\nFeature V alueFrequency\n0 500 1000 1500 20000 2\nFigure A.6\nBar plot of the continuous T RAINING EXPENSES feature from Table A.1[750].\nexample, we could have used four 500-unit intervals to generate the histogram instead—see Figure\nA.7(b)[754], based on frequencies from Table A.4(b)[755]—or, indeed, any other set of intervals.\nWe can convert a histogram to a probability distribution by dividing the count for each interval by\nthe total number of observations in the dataset multiplied by the width of the interval. As a result,\nthe area of each bar (the bar height times the bar width) gives the probability for the feature taking a\nvalue in the interval represented by that bar. The resulting histogram is called a density histogram\nbecause the height of each bar represents how densely the instances in the dataset that fall within the\ninterval are packed into the area of the bar.\nFigure A.7(c)[754]illustrates the density histogram of the T RAINING EXPENSES feature using ten\n200-unit intervals, and Figure A.7(d)[754]illustrates the density histogram using four 500-unit inter-\nvals. Notice that the vertical axes in these histograms are labeled density , rather than frequency.\nTable A.4(a)[755]shows the density and probability calculations for the T RAINING EXPENSES feature","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":808,"page_label":"754","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"754 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\nFeature ValueFrequency\n0 500 1000 1500 20000 2 4 6 8\n(a)\nFeature ValueFrequency\n0 500 1000 1500 20000 2 4 6 8 (b)\nFeature V alueDensity\n0 500 1000 1500 20000.0000 0.0005 0.0010\n(c)\nFeature V alueDensity\n0 500 1000 1500 20000.0000 0.0005 0.0010 (d)\nFigure A.7\n(a) and (b) frequency histograms and (c) and (d) density histograms for the continuous T RAINING\nEXPENSES feature from Table A.1[750], illustrating how using intervals overcomes the problem seen\nin Figure A.6[753]and the effect of varying interval sizes.\nwhen we use ten 200-unit intervals, and Table A.4(b)[755]shows the same calculations when we use\nfour500-unit intervals.7Recall that we compute the density for each interval by dividing the number\n7. When deﬁning intervals, a square bracket, rors, indicates that the boundary value is included in the interval,\nand a parenthesis, porq, indicates that it is excluded from the interval.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":809,"page_label":"755","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"A.4 Data Visualization 755\nTable A.4\nThe density calculation for the T RAINING EXPENSES feature from Table A.1[750]using (a) ten 200-\nunit intervals and (b) four 500-unit intervals.\n(a) 200-unit intervals\nInterval Count Density Prob\nr0,200q 2 0.00050 0 .1\nr200,400q 2 0.00050 0 .1\nr400,600q 3 0.00075 0.15\nr600,800q 4 0.00100 0 .2\nr800,1000q 3 0.00075 0.15\nr1000,1200q 1 0.00025 0.05\nr1200,1400q 2 0.00050 0 .1\nr1400,1600q 1 0.00025 0.05\nr1600,1800q 1 0.00025 0.05\nr1800,2000q 1 0.00025 0.02(b) 500-unit intervals\nInterval Count Density Prob\nr0,500q 6 0.0006 0.3\nr500,1000q 8 0.0008 0.4\nr1000,1500q 4 0.0004 0.2\nr1500,2000q 2 0.0002 0.1\nof observations in the interval by the width of the interval multiplied by the total number of observa-\ntions. Notice that the sum of the probabilities (the bar areas in the histograms) in both of these tables\nis1.0, which is what we would expect with a probability distribution—all probability distributions\nsum to 1.0.\nA.4.3 Box Plots\nThe last data visualization technique we will discuss for visualizing the values of a single feature\nis the box plot .8A box plot is a visual representation of the ﬁve key descriptive statistics for a\ncontinuous feature: minimum, 1stquartile, median, 3rdquartile, and maximum. Figure A.8(a)[756]\nshows the structure of a box plot. In a box plot the vertical axis shows the range of values that a\nfeature can take. The extent of the rectangular box in the middle of the plot is determined by the 3rd\nquartile at the top and the 1stquartile at the bottom. The height of this rectangle, then, also shows\nthe inter-quartile range. The strong black line across the middle of the rectangle shows the median.\nThe whiskers that emerge from the top and bottom of the main rectangle in a box plot are designed\nto show the range of the data. The top whisker extends to whichever is lower of the maximum value\nof the feature or the upper quartile plus 1.5times the IQR. Similarly, the bottom whisker extends to\nwhichever is higher of the minimum value of the feature or the lower quartile minus 1.5times the\nIQR. Values that fall outside the whiskers are referred to as outliers and are shown as small circles.\nFigure A.8(b)[756]shows a box plot for the T RAINING EXPENSES feature from the dataset in Table\nA.1[750]. From this plot we can get a concise, but detailed, description of the feature and notice the\ninclusion of an outlier value. In comparison with a box plot, an individual histogram provides more","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":809,"page_label":"755","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"to show the range of the data. The top whisker extends to whichever is lower of the maximum value\nof the feature or the upper quartile plus 1.5times the IQR. Similarly, the bottom whisker extends to\nwhichever is higher of the minimum value of the feature or the lower quartile minus 1.5times the\nIQR. Values that fall outside the whiskers are referred to as outliers and are shown as small circles.\nFigure A.8(b)[756]shows a box plot for the T RAINING EXPENSES feature from the dataset in Table\nA.1[750]. From this plot we can get a concise, but detailed, description of the feature and notice the\ninclusion of an outlier value. In comparison with a box plot, an individual histogram provides more\ninformation; for example, histograms show the distribution of the values of a feature. Box plots,\n8. Box plots are one of the collection of visual data exploration techniques ﬁrst presented in Tukey’s inﬂuential\n1977 book Exploratory Data Analysis (Tukey, 1977).","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":810,"page_label":"756","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"756 Appendix A Descriptive Statistics and Data Visualization for Machine Learning\nFEATURE VALUES \nValues displayed \nfor a single \nfeature \nMEDIAN The median \nvalue for \nthe feature 3\nrd QUAR TILE \nThe value for \nthe 3rd quartile \nof the feature values \n1\nst QUAR TILE \nThe value for \nthe 1st \nquartile of the feature values OUTLIERS \nValues that fall \noutside quartile ± 1.5*IQR MAX Max value \nbelow 3\nrd Q  \n+ 1.5*IQR \nMIN \nMin value \nabove 1st Q  \n- 1.5*IQR 10 50 \n30 \n20 40 \n0 \n(a) The structure of a box plot\n0 500 1000 2000Training Expenses (b) Box plot example\nFigure A.8\n(a) The structure of a box plot; and (b) a box plot for the T RAINING EXPENSES feature from the\nbasketball team dataset in Table A.1[750].\nhowever, can be placed side by side, and in Section 3.5.1.2[74]we see that the ability to place multiple\nbox plots side by side is the main advantage box plots have over histograms.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":811,"page_label":"757","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"B Introduction to Probability for Machine Learning\nIn this appendix we introduce the fundamental concepts of probability theory that are used in\nprobability-based machine learning algorithms. Speciﬁcally, we present the basics of calculating\nprobabilities based on relative frequencies , calculating conditional probabilities , the probability\nproduct rule , the probability chain rule , and the Theorem of Total Probability .\nB.1 Probability Basics\nProbability is the branch of mathematics that deals with measuring the likelihood (or uncertainty)\naround events. The roots of probability are in gambling, where, understandably, gamblers wanted\nto be able to predict future events based on their likelihood. There are two ways of computing\nthe likelihood of a future event: (1) use the relative frequency of the event in the past, or (2) use\nasubjective estimate (ideally from an expert!). In the predictive analytics context, the standard\napproach is to use relative frequency, and we focus on this approach in this chapter.\nProbability has a longer history, and broader applicability, than predictive analytics. Consequently,\nthe standard language of probability has developed some esoteric terminology, including terms such\nassample space ,experiment ,outcome ,event , and random variable . So we will begin by ﬁrst\nexplaining this terminology and aligning it with the more familiar terminology of predictive analytics.\nIn probability a domain of interest is represented by a set of random variables . For example,\nif we want to model the behavior of a die using probability, we would begin by creating a random\nvariable, let us call it X, that has a domain equal to the set of possible outcomes when we roll the die,\nnamely, the set t\n,\n,\n,\n,\n,\nu. Extending this example, if we wanted to study the behavior of\ntwo dice, we would create two random variables, we might call them Dice 1andDice 2, each having\nthe domaint\n,\n,\n,\n,\n,\nu. In this extended context, an experiment involves rolling the two\ndice, and the sample space deﬁnes the set of all possible outcomes for this experiment (see Figure\nFigure B.1\nThesample space for the domain of two dice.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":812,"page_label":"758","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"758 Appendix B Introduction to Probability for Machine Learning\nB.1[757]). An event is then an experiment whose outcome ﬁxes the values of the random variables.\nFor example, an event in this domain would be represented as Dice 1“\n,Dice 2“\n .\nTable B.1[758]lists a small dataset of instances from the sample space shown in Figure B.1[757]. We\nwill use this example dataset to illustrate how to map the terminology of probability into the language\nof predictive analytics:\n‚The set of random variables in a domain maps to the set of features in a dataset (both descriptive\nand target). D ICE1 and D ICE2 are the equivalent of random variables.\n‚Thesample space for a domain is the set of all possible combinations of assignments of values to\nfeatures.\n‚Anexperiment whose outcome has been already been recorded is a row in the dataset. Each row\nin Table B.1[757]records the outcome of a previous experiment.\n‚Anexperiment whose outcome we do not yet know but would like to predict is the prediction task\nfor which we are building a model.\n‚Anevent is any subset of an experiment. An event may describe an assignment of values to all the\nfeatures in the domain (e.g., a full row in the dataset) or an assignment to one or more features in\nthe domain. D ICE1 =\n is an example of an event. D ICE1 =\n , DICE2 =\n is also an event.\nSo that we are consistent with the terminology throughout this book, in the rest of this chapter, we\nuse the predictive analytics terms ( feature ,dataset ,prediction , and event ) rather than the traditional\nterms from probability.\nA feature can take one or more values from a domain, and we can ﬁnd out the likelihood of\na feature taking any particular value using a probability function ,Ppq. A probability function\nis a function that takes an event (an assignment of values to features) as a parameter and returns\nthe likelihood of that event. For example, PpDICE1“\nqwill return the likelihood of the event\nDICE1“\n , and PpDICE1“\n,DICE2“\nqwill return the likelihood of the event where\nDICE1“\n and D ICE2“\n . If we are deﬁning the probability function for a categorical feature,\nthen the function is known as a probability mass function because it can be understood as returning\na discrete probability mass for each level in the domain of the feature. The probability mass is\nsimply the probability of an event. Conversely, if the feature we are dealing with is a continuous","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":812,"page_label":"758","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"is a function that takes an event (an assignment of values to features) as a parameter and returns\nthe likelihood of that event. For example, PpDICE1“\nqwill return the likelihood of the event\nDICE1“\n , and PpDICE1“\n,DICE2“\nqwill return the likelihood of the event where\nDICE1“\n and D ICE2“\n . If we are deﬁning the probability function for a categorical feature,\nthen the function is known as a probability mass function because it can be understood as returning\na discrete probability mass for each level in the domain of the feature. The probability mass is\nsimply the probability of an event. Conversely, if the feature we are dealing with is a continuous\nfeature, the probability function is known as a probability density function . For this introduction,\nwe focus on categorical features and probability mass functions.\nProbability mass functions have two properties: (1) they always return a value between 0.0and\n1.0; and (2) the sum of the probabilities over the set of events covering all the possible assignments\nof values to features must equal 1.0. Formally these properties are deﬁned as follows:\n0ďPpf“levelqď1\nTable B.1\nA dataset of instances from the sample space in Figure B.1[757].\nID D ICE1 D ICE2\n1\n2\n3\n4\n5","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":813,"page_label":"759","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"B.1 Probability Basics 759\nÿ\nlPlevelspfqPpf“lq“1.0\nwhere levelspfqreturns the set of levels in the domain of the feature f.\nProbability functions are the basic building blocks of probability theory, and they are very easy to\ncreate from a dataset. The value returned by a probability function for an event is simply the relative\nfrequency of that event in the dataset. The relative frequency of an event is calculated as how often\nthe event happened divided by how often it could have happened. For example, the relative frequency\nof the event D ICE1“\n is simply the count of all the rows in the dataset where D ICE1 has a value\nof\n divided by the number of rows in the dataset. Based on Table B.1[758], the probability of the\nevent D ICE1“\n is1\nPpDICE1“\nq“|td1,d4u|\n|td1,d2,d3,d4,d5u|“2\n5“0.4\nSo far we have focused on calculating the probability of an individual event. In a predictive an-\nalytics task, we will often be interested in calculating the probability of more than one event. For\nexample, we might want to know the probability of the target feature taking a particular value and\none of the descriptive features taking a particular value at the same time. Technically, if an event in-\nvolves more than one feature, it can be considered to be composed of several simple events. In these\ncases the probability calculated is known as a joint probability . The probability of a joint event is\nsimply the relative frequency of the joint event within the dataset. In terms of rows in a dataset, this\ncomputation is simply the number of rows where the set of assignments listed in the joint event holds\ndivided by the total number of rows in the dataset. For example, the probability of the joint event2\nDICE1“\n,DICE2“\n would be calculated as\nPpDICE1“\n,DICE2“\nq“|td3u|\n|td1,d2,d3,d4,d5u|“1\n5“0.2\nThe type of probabilities we have calculated so far are known as prior probabilities oruncon-\nditional probabilities . Often, however, we want to know the probability of an event in the context\nwhere one or more other events are known to have happened. This type of probability, where we\ntake one or more events to already hold, is known as a posterior probability , because it is calculated\nafter other events have happened. It is also commonly known as a conditional probability , because\nthe probability calculated is valid conditional on the given events (or evidence).When we want to\nexpress this type of probability, formally we use a vertical bar, |, to separate the events we want the","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":813,"page_label":"759","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"PpDICE1“\n,DICE2“\nq“|td3u|\n|td1,d2,d3,d4,d5u|“1\n5“0.2\nThe type of probabilities we have calculated so far are known as prior probabilities oruncon-\nditional probabilities . Often, however, we want to know the probability of an event in the context\nwhere one or more other events are known to have happened. This type of probability, where we\ntake one or more events to already hold, is known as a posterior probability , because it is calculated\nafter other events have happened. It is also commonly known as a conditional probability , because\nthe probability calculated is valid conditional on the given events (or evidence).When we want to\nexpress this type of probability, formally we use a vertical bar, |, to separate the events we want the\nprobability for (listed on the left-hand side of the bar) from the events that we know have already\nhappened. The vertical bar symbol can be read as given that . So the probability of D ICE1“\ngiven that DICE2“\n would be written as\nPpDICE1“\n|DICE2“\nq\nTheconditional probability for an event given that we know another event is true is calculated\nby dividing the number of rows in the dataset where both events are true by the number of rows in\nthe dataset where just the given event is true. For example, the conditional probability for the event\n1. In the notation used in this book, d1refers to the instance in a dataset with an ID of 1, and so on.\n2. When listing a joint event, we use a comma ,to denote logical and.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":814,"page_label":"760","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"760 Appendix B Introduction to Probability for Machine Learning\nTable B.2\nA simple dataset for M ENINGITIS with three common symptoms of the disease listed as descriptive\nfeatures: H EADACHE , FEVER , and V OMITING .\nID H EADACHE FEVER VOMITING MENINGITIS\n11 true true false false\n37 false true false false\n42 true false true false\n49 true false true false\n54 false true false true\n57 true false true false\n73 true false true false\n75 true false true true\n89 false true false false\n92 true false true true\nDICE1“\n given that DICE2“\n would be calculated as\nPpDICE1“\n|DICE2“\nq“|td3u|\n|td2,d3u|“1\n2“0.5\nWe now understand the theory of how to calculate a simple unconditional probability, a joint prob-\nability, and a conditional probability using a dataset. Now is a good point to ground this knowledge\nin a more interesting example focused on predictive data analytics. We will use the dataset in Table\nB.2[760]for this.3The target being predicted in this dataset is whether or not a patient is suffering\nfrom meningitis, and the descriptive features are common symptoms associated with meningitis.\nA quick comment on our notation. Throughout this chapter, named features will be denoted by the\nuppercase initial letters of their names—for example, a feature named M ENINGITIS will be denoted\nbyM. Also, where a named feature is binary, we will use the lowercase initial letter of the feature\nname to denote the event where the feature is true and the lowercase initial letter preceded by the ␣\nsymbol to denote the event where it is false. So, mwill denote the event M ENINGITIS“true and\n␣mwill denote M ENINGITIS“false . Given the dataset in Table B.2[760], the probability of a patient\nhaving a headache is\nPphq“|td11,d42,d49,d57,d73,d75,d92u|\n|td11,d37,d42,d49,d54,d57,d73,d75,d89,d92u|“7\n10“0.7 (B.1)\nthe probability of a patient having a headache and meningitis is\nPpm,hq“|td75,d92u|\n|td11,d37,d42,d49,d54,d57,d73,d75,d89,d92u|“2\n10“0.2 (B.2)\n3. This data has been artiﬁcially generated for this example.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":815,"page_label":"761","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"B.2 Probability Distributions and Summing Out 761\nand the probability of a patient having meningitis given that we know that the patient has a headache\nis\nPpm|hq“|td75,d92u|\n|td11,d42,d49,d57,d73,d75,d92u|“2\n7“0.2857 (B.3)\nB.2 Probability Distributions and Summing Out\nSometimes it is useful to talk about the probabilities for all the possible assignments to a feature. To\ndo this we use the concept of a probability distribution . A probability distribution is a data structure\nthat describes the probability of a feature taking a value for all the possible values the feature can take.\nThe probability distribution for a categorical feature is a vector that lists the probabilities associated\nwith each of the values in the domain of the feature. A vector is an ordered list, so the mechanism\nfor matching a probability in the vector with a particular value in the domain is just to look up the\nposition of the probability within the vector. We use bold notation Ppqto distinguish between a\nprobability distribution and a probability function Ppq. For example, the probability distribution for\nthe binary feature M ENINGITIS from Table B.2[760], with a probability of 0.3of being trueand using\nthe convention of the ﬁrst element in the vector being the probability for a true value, would be\nwritten as PpMq“⟨0.3,0.7⟩.\nThe concept of a probability distribution also applies to joint probabilities, which gives us the\nconcept of a joint probability distribution . A joint probability distribution is a multidimensional\nmatrix where each cell in the matrix lists the probability for one of the events in the sample space\ndeﬁned by the combination of feature values. The dimensions of the matrix are dependent on the\nnumber of features and the number of values in the domains of the features. The joint probability\ndistribution for the four binary features from Table B.2[760](HEADACHE , FEVER , VOMITING , and\nMENINGITIS ) would be written as\nPpH,F,V,Mq“»\n———————————–Pph,f,v,mq, Pp␣h,f,v,mq\nPph,f,v,␣mq, Pp␣h,f,v,␣mq\nPph,f,␣v,mq, Pp␣h,f,␣v,mq\nPph,f,␣v,␣mq, Pp␣h,f,␣v,␣mq\nPph,␣f,v,mq, Pp␣h,␣f,v,mq\nPph,␣f,v,␣mq, Pp␣h,␣f,v,␣mq\nPph,␣f,␣v,mq, Pp␣h,␣f,␣v,mq\nPph,␣f,␣v,␣mq,Pp␣h,␣f,␣v,␣mqﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nRemember that the sum of all the elements in a probability distribution must be 1.0. Consequently,\nthe sum of all the cells in a joint probability distribution must be 1.0. Afull joint probability distri-","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":815,"page_label":"761","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"number of features and the number of values in the domains of the features. The joint probability\ndistribution for the four binary features from Table B.2[760](HEADACHE , FEVER , VOMITING , and\nMENINGITIS ) would be written as\nPpH,F,V,Mq“»\n———————————–Pph,f,v,mq, Pp␣h,f,v,mq\nPph,f,v,␣mq, Pp␣h,f,v,␣mq\nPph,f,␣v,mq, Pp␣h,f,␣v,mq\nPph,f,␣v,␣mq, Pp␣h,f,␣v,␣mq\nPph,␣f,v,mq, Pp␣h,␣f,v,mq\nPph,␣f,v,␣mq, Pp␣h,␣f,v,␣mq\nPph,␣f,␣v,mq, Pp␣h,␣f,␣v,mq\nPph,␣f,␣v,␣mq,Pp␣h,␣f,␣v,␣mqﬁ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬂ\nRemember that the sum of all the elements in a probability distribution must be 1.0. Consequently,\nthe sum of all the cells in a joint probability distribution must be 1.0. Afull joint probability distri-\nbution is simply a joint probability distribution over all the features in a domain. Given a full joint\nprobability distribution, we can compute the probability of any event in a domain by summing over\nthe cells in the distribution where that event is true. For example, imagine we want to compute the\nprobability of Pphqin the domain speciﬁed by the joint probability distribution PpH,F,V,Mq. To do\nthis we simply sum the values in the cells containing h, in other words, the cells in the ﬁrst column of\nthe distribution. Calculating probabilities in this way is known as summing out ormarginalization .4\n4. Summing out is sometimes referred to as marginalization because statisticians used to carry out these calcu-\nlations in the margins of the probability tables they were working with!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":816,"page_label":"762","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"762 Appendix B Introduction to Probability for Machine Learning\nWe can also use summing out to compute conditional probabilities from a joint probability distribu-\ntion. For example, imagine we wish to calculate the probability of hgiven fwhen we don’t care what\nvalues VorMtake. In this context, VandMare examples of hidden features . A hidden feature is a\nfeature whose value is not speciﬁed as part of the evidence. We can calculate Pph,V“?,M“?|fq\nfrom PpH,F,V,Mqby summing the values in all the cells where handfare the case (the top four\ncells in the ﬁrst column).\nThe process of summing out is a key concept in probability-based prediction. In order to make a\nprediction, a model must compute the probability for a target event in the context where some other\nevents are known (the evidence) and where there are potentially one or more hidden features. As\nwe have seen, using a joint probability distribution, a model can carry out this calculation by simply\nconditioning on the evidence features and summing out the hidden features. Unfortunately, the size\nof a joint probability distribution grows exponentially as the number of features and the number of\nvalues in the domains of the features grow. Consequently, they are difﬁcult to generate because of the\ncurse of dimensionality: computing the probability for each cell in a joint probability table requires a\nset of instances and, because the number of cells grows exponentially as features and feature values\nare added, so does the size of the dataset required to generate the joint probability distribution. As a\nresult, for any domain of reasonable complexity, it is not tractable to deﬁne the full joint probability\ndistribution, and therefore probability-based prediction models build more compact representations\nof full joint probability distributions instead.\nB.3 Some Useful Probability Rules\nSeveral important rules in probability theory allow us to compute new probabilities in terms of pre-\nviously computed probabilities. Note that throughout the rest of the chapter, we use uppercase letters\nto denote generic events where an unspeciﬁed feature (or set of features) is assigned a value (or set\nof values). Typically we will use letters from the end of the alphabet (e.g., X,Y,Z) for this pur-\npose. Also, we will use subscripts on uppercase letters to iterate over events. So,ř\niPpXiqshould be\ninterpreted as summing over all the possible combinations of value assignments to the features in X.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":816,"page_label":"762","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"of full joint probability distributions instead.\nB.3 Some Useful Probability Rules\nSeveral important rules in probability theory allow us to compute new probabilities in terms of pre-\nviously computed probabilities. Note that throughout the rest of the chapter, we use uppercase letters\nto denote generic events where an unspeciﬁed feature (or set of features) is assigned a value (or set\nof values). Typically we will use letters from the end of the alphabet (e.g., X,Y,Z) for this pur-\npose. Also, we will use subscripts on uppercase letters to iterate over events. So,ř\niPpXiqshould be\ninterpreted as summing over all the possible combinations of value assignments to the features in X.\nThe ﬁrst rule we will introduce deﬁnes conditional probability in terms of joint probability:\nPpX|Yq“PpX,Yq\nPpYq(B.4)\nWe have already calculated the conditional probability of the event mgiven hdirectly from the\ndataset in Table B.2[760]asPpm|hq“0.2857 (see Equation (B.3)[761]). We will now recalculate this\nprobability using our rule-based deﬁnition of conditional probability. From our previous calculations,\nwe already know that Pphq“0.7(see Equation (B.1)[760]) and Ppm,hq“0.2(see Equation (B.2)[760]).\nSo our calculation for Ppm|hqis\nPpm|hq“Ppm,hq\nPphq“0.2\n0.7“0.2857\nUsing Equation (B.4)[762], we can provide a second deﬁnition for the probability of a joint event,\nwhich is known as the product rule :\nPpX,Yq“PpX|YqˆPpYq (B.5)","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":817,"page_label":"763","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"B.4 Summary 763\nWe can demonstrate the product rule by recalculating the probability Ppm,hqusing previously com-\nputed probabilities:\nPpm,hq“Ppm|hqˆPphq“0.2857ˆ0.7“0.2\nAgain, the result of the calculation matches the probability computed directly from the dataset (see\nEquation (B.2)[760]).\nThere are a few points worth noting about the product rule. First, it deﬁnes the probability of a\njoint event PpX,Yqin terms of a conditional (or posterior) probability PpX|Yqmultiplied by an\nunconditional (or prior) probability PpYq. Second, the order of the events in the product rule is not\nimportant, and we can condition the calculation on any of the events listed in the and(in logic, the\nandoperation is symmetric):\nPpX,Yq“PpX|YqPpYq“PpY|XqPpXq\nWe can also extend the product rule to deﬁne the joint probability of more than two events. When\nwe generalize the rule in this way, it is known as the probability chain rule :\nPpA,B,C,..., Zq“PpZqˆPpY|ZqˆPpX|Y,Zqˆ¨¨¨ˆ PpA|B,..., X,Y,Zq (B.6)\nAs with the simple two-event version, the order of events in the chain rule is not important.\nFinally, the Theorem of Total Probability deﬁnes the unconditional probability for any event X\nas\nPpXq“ÿ\niPpX|YiqPpYiq (B.7)\nwhere each Yiis one of a set of events Y1toYkthat cover all the possible outcomes in a domain\nand have no overlap between them. Because an event deﬁnes a partition of a dataset (the rows from\nthe dataset that match the event), then each Yideﬁnes a set of rows from a dataset, and the set of\ndata partitions deﬁned by Y1toYkmust cover the full dataset and not overlap with each other. The\nTheorem of Total Probability is a formal speciﬁcation of the summing out process we introduced\nearlier in Section B.2[761].\nTo illustrate how the Theorem of Total Probability can be used to calculate probabilities, we will\ncompute Pphqby summing out M(note: earlier, in Equation (B.1)[760], we computed Pphq“0.7):\nPphq“pPph|mqˆPpmqq`pPph|␣mqˆPp␣mqq\n“p0.6666ˆ0.3q`p0.7143ˆ0.7q“0.7\nWe can, if we wish, sum out more than one feature. For example, we could compute Pphqby\nsumming out all the other features in the dataset:\nPphq“ÿ\niPlevelpMqÿ\njPlevelpFqÿ\nkPlevelpVqPph|Mi,Fj,VkqˆPpMi,Fj,Vkq\nWe will, however, leave this calculation to the interested reader (the result should still be 0.7).\nB.4 Summary\nProbability theory underpins a great deal of machine learning. This section has provided an overview\nof the aspects of probability that readers need to understand in order to follow the other sections in","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":817,"page_label":"763","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"compute Pphqby summing out M(note: earlier, in Equation (B.1)[760], we computed Pphq“0.7):\nPphq“pPph|mqˆPpmqq`pPph|␣mqˆPp␣mqq\n“p0.6666ˆ0.3q`p0.7143ˆ0.7q“0.7\nWe can, if we wish, sum out more than one feature. For example, we could compute Pphqby\nsumming out all the other features in the dataset:\nPphq“ÿ\niPlevelpMqÿ\njPlevelpFqÿ\nkPlevelpVqPph|Mi,Fj,VkqˆPpMi,Fj,Vkq\nWe will, however, leave this calculation to the interested reader (the result should still be 0.7).\nB.4 Summary\nProbability theory underpins a great deal of machine learning. This section has provided an overview\nof the aspects of probability that readers need to understand in order to follow the other sections in\nthis book. One thing to note is that many of the rules and techniques we presented were different ways","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":818,"page_label":"764","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"764 Appendix B Introduction to Probability for Machine Learning\nof achieving the same thing—for example, we can calculate Pphqby simple counting, by summing\nout from a full joint probability distribution, or by using the Theorem of Total Probability. This is\nan aspect of probability theory with which beginners sometimes struggle. The important thing to\nremember, though, is that the different approaches exist because in different scenarios it will often\nbe easier to apply one approach over the others. Just like the proverbial cat, there is more than one\nway to skin a probability problem!","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":819,"page_label":"765","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"C Differentiation Techniques for Machine Learning\nIn this appendix we present the basic differentiation techniques that are required to understand how\nlinear regression can be used to build predictive analytics models. In particular we explain what a\nderivative is, how to calculate derivatives for continuous functions, the chain rule for differentiation,\nand what a partial derivative is.\n0 10 20 30 40 500 20 40 60 80\nTimeSpeed\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n(a)\n0 10 20 30 40 50−40 −20 010\nTimeAcceler ation/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF\n/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF/uni25CF (b)\nFigure C.1\n(a) The speed of a car during a journey along a minor road before joining a highway and ﬁnally\ncoming to a sudden halt; and (b) the acceleration, the derivative of speed with respect to time, for\nthis journey.\nTo begin, imagine a car journey where we start out driving on a minor road at about 30mph and\nthen move onto a highway, where we drive at about 80mph before noticing an accident and braking\nsuddenly. Figure C.1(a)[765]shows a proﬁle of the speed during this journey measured at different\npoints in time. Figure C.1(b)[765]shows a proﬁle of the acceleration during this journey. We can see\nthat when the car is driving at a constant speed, on the minor road or the highway, acceleration is zero\nas the speed is not changing. In contrast, acceleration has modest positive values when we are taking\noff initially and slightly larger positive values when we increase speed on reaching the highway. The\nsudden braking at the end of the journey results in large negative values that slowly taper off to match\nthe speed proﬁle in Figure C.1(a)[765].","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":819,"page_label":"765","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"then move onto a highway, where we drive at about 80mph before noticing an accident and braking\nsuddenly. Figure C.1(a)[765]shows a proﬁle of the speed during this journey measured at different\npoints in time. Figure C.1(b)[765]shows a proﬁle of the acceleration during this journey. We can see\nthat when the car is driving at a constant speed, on the minor road or the highway, acceleration is zero\nas the speed is not changing. In contrast, acceleration has modest positive values when we are taking\noff initially and slightly larger positive values when we increase speed on reaching the highway. The\nsudden braking at the end of the journey results in large negative values that slowly taper off to match\nthe speed proﬁle in Figure C.1(a)[765].\nAcceleration is a measure of the rate of change of speed over time. We can say more formally that\nacceleration is, in fact, the derivative of speed with respect to time. Differentiation is the set of\ntechniques from calculus (the branch of mathematics that deals with how things change) that allows\nus to calculate derivatives . In an example like the car journey just described, where we have a set\nof discrete measurements, calculating the derivative is simply a matter of determining the difference\nbetween subsequent pairs of measurements. For example, the derivative of speed with respect to","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":820,"page_label":"766","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"766 Appendix C Differentiation Techniques for Machine Learning\ntime at time index 21is the speed at time index 21minus the speed at time index 20, which is\n44.28´51.42“7.14. These values are marked in Figure C.1[765]. All the values of acceleration have\nbeen calculated in this way.\nC.1 Derivatives of Continuous Functions\nWhile it is interesting to see how derivatives can be calculated for discrete examples, it is much more\ncommon that we need to calculate the derivative of a continuous function. A continuous function,\nfpxq, generates an output for every value of a variable xbased on some expression involving x. For\nexample:\nfpxq “ 2x`3\nfpxq “ x2\nfpxq “ 3x3`2x2´x´2\nare continuous functions with a single variable x. Graphs of these functions are shown in Figure\nC.2[767]. Each graph also shows the derivative of the function. We will return to these shortly.\nThe function fpxq“2x`3is known as a linear function because the output is a combination of\nonly additions and multiplications1involving x. The other two functions are known as polynomial\nfunctions as they include addition, multiplication, and raising to exponents. Of those, fpxq “ x2\nis an example of a second order polynomial function , also known as a quadratic function , as its\nhighest exponent is 2, and fpxq“3x3`2x2´x´2is athird order polynomial function , also\nknown as a cubic function , as its highest exponent is 3.\nLooking ﬁrst at Figure C.2(a)[767], the function here is very simple, fpxq“2x`3, which results\nin a straight diagonal line. A straight diagonal line gives us a constant rate of change (in this case an\nincrease of 2in the value of the function for every change of 1inx), so the derivative of this function\nwith respect to xis just a constant. This is represented by the horizontal dashed line.\nWe can intuitively see from Figure C.2(b)[767]forfpxq“x2that the rate of change of the value of\nthis function is likely to be high at the steep edges of the curve and low at the bottom (imagine a ball\nrolling around inside this shape!). This intuition is mirrored in the derivative of the function with\nrespect to x. We can see that on the left-hand side of the graph (for large negative values of x), the\nrate of change has a high negative value, while on the right-hand side of the graph (for large positive\nvalues of x), the rate of change has a large positive value. In the middle of the graph, at the bottom","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":820,"page_label":"766","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"with respect to xis just a constant. This is represented by the horizontal dashed line.\nWe can intuitively see from Figure C.2(b)[767]forfpxq“x2that the rate of change of the value of\nthis function is likely to be high at the steep edges of the curve and low at the bottom (imagine a ball\nrolling around inside this shape!). This intuition is mirrored in the derivative of the function with\nrespect to x. We can see that on the left-hand side of the graph (for large negative values of x), the\nrate of change has a high negative value, while on the right-hand side of the graph (for large positive\nvalues of x), the rate of change has a large positive value. In the middle of the graph, at the bottom\nof the curve, the rate of change is zero. It should be no surprise to learn that the derivative of the\nfunction with respect to xalso gives us the slope of the function at that value of x. The shape of the\nderivative in Figure C.2(c)[767]can be understood similarly.\nTo actually calculate the derivative, referred to asd\ndxfpxq, of a simple continuous function, fpxq,\nwe use a small number of differentiation rules:\n1. Note that subtraction is viewed as addition of negative numbers, and division is seen as multiplication by\nreciprocals, so both are also allowed.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":821,"page_label":"767","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"C.1 Derivatives of Continuous Functions 767\n−10 −5 0 5 10−10 0 10 20\nx\n(a)fpxq“2x`3\n−10 −5 0 5 10−20020406080100\nx (b)fpxq“x2\n−10 −5 0 5 10−3000 −1000 0100020003000\nx\n(c)fpxq“3x3`2x2´x´2\n−10 −5 0 5 10−20000 20000 60000 100000\nx (d)fpxq“p x2`1q2\nFigure C.2\nExamples of continuous functions (shown as solid lines) and their derivatives (shown as dashed\nlines).\n1.d\ndxα“ 0 (whereαis any constant)\n2.d\ndxαxn“αˆnˆxn´1\n3.d\ndxa`b“d\ndxa`d\ndxb(where aandbare expressions that\nmay or may not contain x)\n4.d\ndxαˆc“αˆd\ndxc(whereαis any constant and cis an\nexpression containing x)\nApplying these rules to the ﬁrst of our previous examples, fpxq“2x`3(Figure C.2(a)[767]), we\nﬁrst apply Rule 3 to split this function into two parts, 2xand3, and then apply differentiation rules\nto each. By Rule 2 we can differentiate 2xto2(remember that xis really x1). The 3is a constant, so\nby Rule 1 differentiates to zero. The derivative of the function, then, isd\ndxfpxq“2.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":822,"page_label":"768","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"768 Appendix C Differentiation Techniques for Machine Learning\nFor the last function, fpxq“3x3`2x2´x´2(Figure C.2(c)[767]), we ﬁrst apply Rule 3 to divide\nthis into four parts: 3x3,2x2,x, and 2. Applying Rule 2 to each of the ﬁrst three parts gives 9x2,4x,\nand´1. The ﬁnal part, 2, is a constant and so differentiates to zero. The derivative of this function\nthen isd\ndxfpxq“9x2`4x´1.\nWe can see from these examples that calculating derivatives of simple functions is a matter of,\nfairly mechanically, applying these four simple rules. Calculating the derivatives of the other two\nfunctions are left as exercises for the reader. Some of the functions that we will encounter later on\nin this chapter will be a little more complex, and we need two more differentiation rules to handle\nthese.\nC.2 The Chain Rule\nThe function fpxq“p x2`1q2(shown in Figure C.2(d)[767]) cannot be differentiated using the rules\njust described because it is a composite function —it is a function of a function . We can rewrite fpxq\nasfpxq“pgpxqq2where gpxq“ x2`1. The differentiation chain rule allows us to differentiate\nfunctions of this kind.2The chain rule is\nd\ndxfpgpxqq“d\nd gpxqfpgpxqqˆd\ndxgpxq (C.1)\nThe differentiation is performed in two steps. First, treating gpxqas a unit, we differentiate fpgpxqq\nwith respect to gpxq, and then we differentiate gpxqwith respect to x, in both cases using the differ-\nentiation rules from the previous section. The derivative of fpgpxqqwith respect to xis the product\nof these two pieces.\nApplying this to the example fpxq“p x2`1q2we get\nd\ndxpx2`1q2“d\ndpx2`1qpx2`1q2ˆd\ndxpx2`1q\n“`\n2ˆpx2`1q˘\nˆp2xq\n“4x3`4x\nFigure C.2(d)[767]shows this example function and its derivative calculated using the chain rule.\nC.3 Partial Derivatives\nSome functions are not deﬁned in terms of just one variable. For example, fpx,yq“x2´y2`2x`\n4y´xy`2is a function deﬁned in terms of two variables, xandy. Rather than deﬁning a curve\n(as was the case for all the previous examples), this function deﬁnes a surface, as shown in Figure\nC.3(a)[769]. Using partial derivatives offers us an easy way to calculate the derivative of a function\nlike this. A partial derivative (denoted by the symbol B) of a function of more than one variable is its\nderivative with respect to one of those variables with the other variables held constant.\nFor the example function fpx,yq“x2´y2`2x`4y´xy`2, we get two partial derivatives:\nB\nBxpx2´y2`2x`4y´xy`2q “ 2x`2´y","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":822,"page_label":"768","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"C.3 Partial Derivatives\nSome functions are not deﬁned in terms of just one variable. For example, fpx,yq“x2´y2`2x`\n4y´xy`2is a function deﬁned in terms of two variables, xandy. Rather than deﬁning a curve\n(as was the case for all the previous examples), this function deﬁnes a surface, as shown in Figure\nC.3(a)[769]. Using partial derivatives offers us an easy way to calculate the derivative of a function\nlike this. A partial derivative (denoted by the symbol B) of a function of more than one variable is its\nderivative with respect to one of those variables with the other variables held constant.\nFor the example function fpx,yq“x2´y2`2x`4y´xy`2, we get two partial derivatives:\nB\nBxpx2´y2`2x`4y´xy`2q “ 2x`2´y\n2. This is not to be confused with the probability chain rule discussed in Section B.3[762]. These are two com-\npletely different operations.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":823,"page_label":"769","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"C.3 Partial Derivatives 769\nwhere the terms y2and4yare treated as constants as they do not include x, and\nB\nBypx2´y2`2x`4y´xy`2q“´ 2y`4´x\nwhere the terms x2and2xare treated as constants as they do not include y. Figures C.3(b)[769]and\nC.3(c)[769]show these partial derivatives.\n(a)fpx,yq“x2´y2`2x`4y´xy`2\n(b)B\nBxfpx,yq“2x`2´y\n (c)B\nByfpx,yq“´ 2y`4´x\nFigure C.3\n(a) A continuous function in two variables, xandy; (b) the partial derivative of this function with\nrespect to x; and (c) the partial derivative of this function with respect to y.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":825,"page_label":"771","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"D Introduction to Linear Algebra\nIn this appendix we introduce some of the fundamental operations in linear algebra . In particular,\nwe introduce the vector and matrix operations that we use in the book.\nD.1 Basic Types\nThere are a number of fundamental mathematical types that are the building blocks of linear algebra.\nThese include\n‚Ascalar is a single number.\n‚Amatrix is a 2-dimensional ( nˆm) array of numbers.\nC“»\n———–c1,1c1,2... c1,m\nc2,1c2,2... c2,m\n... ... ... ...\ncn,1c1,2... cn,mﬁ\nﬃﬃﬃﬂ\nEach element in a matrix is identiﬁed by two indices, the row index and then the column index.\nFor example,\nCr2,2s“c2,2\n‚Avector is an array of numbers, organized in a speciﬁc order. A vector can be either a column\nvector or a row vector. For example, ais a column vector, and bis a row vector.\na“»\n———–a1\na2\n...\nanﬁ\nﬃﬃﬃﬂ\nb“”\nb1b2... bmı\nEach element in a vector is identiﬁed by a single index. For example:\nar2s“a2\nbr2s“b2\nVectors are often treated as special cases of matrices. For example, a column vector can be thought\nof as a matrix with just one column, and a row vector can be thought of as a matrix with a single\nrow.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":826,"page_label":"772","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"772 Appendix D Introduction to Linear Algebra\nD.2 Transpose\nA basic operation on vectors and matrices to the transpose . The transpose of a vector converts a\ncolumn vector to a row vector, and vice versa. If ais a vector, then we write a⊺for the transpose of\na. For example:\na“»\n———–a1\na2\n...\nanﬁ\nﬃﬃﬃﬂ\na⊺“”\na1a2... anı\nThe transpose of a matrix ﬂips the matrix on its main diagonal (the main diagonal of a matrix\ncontain all the elements whose indices are equal, e.g., c1,1,c2,2,and so on). To create the transpose of\na matrix, take the ﬁrst row of the matrix and write it as the ﬁrst column; then write the second row of\nthe matrix and write it as the second column; and so on. For example:\nC“»\n—–c1,1c1,2c1,3c1,4\nc2,1c2,2c2,3c2,4\nc3,1c3,2c3,3c3,4ﬁ\nﬃﬂ\nC⊺“»\n———–c1,1c2,1c3,1\nc1,2c2,2c3,2\nc1,3c2,3c3,3\nc1,4c2,4c3,4ﬁ\nﬃﬃﬃﬂ\nD.3 Multiplication\nIn general, there is no special symbol used to denote a matrix product. Instead, we write the matrix\nproduct by writing the names of the two matrices side by side. For example, DEis the way we write\nthe product for two matrices DandE, although sometimes a dotmay be inserted between the two\nmatrices (a¨is frequently used to highlight that one or both of the matrices is a vector):\nDE“D¨E\nIn order to multiply one matrix by another, the number of columns in the matrix on the left of the\nproduct must equal the number of rows in the matrix on the right. If this condition does not hold,\nthen the product of the two matrices is not deﬁned. For example, if Dis a2ˆ3matrix (i.e., a matrix\nwith 2 rows and 3 columns) and Eis a3ˆ3matrix, then the product of these two matrices DEis\ndeﬁned, because the number of columns in D(3) equals the number of rows in E(3). However, the\nmatrix product EDis not deﬁned because the number of columns in E(3) is not equal to the number\nof rows in D(2). The product ED⊺is however deﬁned because D⊺is a3ˆ2matrix and the number\nof columns is E(3) equals the number of rows in D⊺(3).\nThe result of multiplying two matrices is another matrix whose dimensions are equal to the number\nof rows in the left matrix and the number of columns in the right matrix. For example, multiplying a","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":827,"page_label":"773","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"D.3 Multiplication 773\n2ˆ3matrix by a 3ˆ3matrix results in a 2ˆ3. Each value in the resulting matrix is calculated as\nfollows, where iiterates over the columns in the ﬁrst matrix ( D) and the rows in the second matrix\n(E)\nDE r,c“ÿ\niDrr,isˆEri,cs\nFor example:\nD“«\nd1,1d1,2d1,3\nd2,1d2,2d2,3ﬀ\nE“»\n—–e1,1e1,2e1,3\ne2,1e2,2e2,3\ne3,1e3,2e3,3ﬁ\nﬃﬂ\nDE“«\npd1,1e1,1q`pd1,2e2,1q`pd1,3e3,1q pd1,1e1,2q`pd1,2e2,2q`pd1,3e3,2q pd1,1e1,3q`pd1,2e2,3q`pd1,3e3,3q\npd2,1e1,1q`pd2,2e2,1q`pd2,3e3,1q pd2,1e1,2q`pd2,2e2,2q`pd2,3e3,2q pd2,1e1,3q`pd2,2e2,3q`pd2,3e3,3qﬀ\nThe product of two vectors of the same dimensions is known as the dot product . For example,\ngiven two row vectors FandG, both of which have dimensions 1ˆ3\nF“”\nf1f2f3ı\nG“”\ng1g2g3ı\nThedot product ofFandG, written F¨G(and thus named) is equivalent to the matrix product\nFG⊺\nF“”\nf1f2f3ı\nG⊺“»\n—–g1\ng2\ng3ﬁ\nﬃﬂ\nF¨G“pf1g1q`p f2g2q`p f3g3q\nFollowing the rules of standard matrix multiplication, the result of multiplying a 1ˆ3matrix by a\n3ˆ1matrix is a 1ˆ1matrix (a matrix with a single value, i.e., a scalar).\nFrequently, in discussing deep learning we use an elementwise product of two matrices, known as\ntheHadamard product . The symbolddenotes the Hadamard product, and the Hadamard product\nof two matrices DandEis written DdE. The Hadamard product assumes that both matrices have\nthe same dimensions, and it produces a matrix with the same dimensions as the two inputs. Each\nvalue in the resulting matrix is the product of the corresponding cells in the two input matrices:\nDE r,c“Drr,csˆErr,cs","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":828,"page_label":"774","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"774 Appendix D Introduction to Linear Algebra\nFor example\nD“«\nd1,1d1,2\nd2,1d2,2ﬀ\nE“«\ne1,1e1,2\ne2,1e2,2ﬀ\nDdE“«\npd1,1e1,1q pd1,2e1,2q\npd2,1e2,1q pd2,2e2,2qﬀ\nD.4 Summary\nLinear algebra is an important topic in machine learning. What we have presented here is a very\nshort introduction to some of the most basic operations. This introduction is focused primarily on\nsupporting your understanding of the content in this book, in particular the chapter on deep learning,\nrather than on providing a comprehensive introduction to linear algebra. There are many excellent\nbooks on linear algebra; one of the standard textbooks on the topic is (Strang, 2016). Another\nrelevant textbook by the same author is (Strang, 2019), which speciﬁcally focuses on introducing\nlinear algebra from the perspective of understanding deep learning.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":829,"page_label":"775","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography\nAlimoglu, Fevzi, and Ethem Alpaydin. 1996. Methods of combining multiple classiﬁers based on dif-\nferent representations for pen-based handwritten digit recognition. In Proceedings of the ﬁfth Turkish\nartiﬁcial intelligence and artiﬁcial neural networks symposium (TAINN’96) .\nAndoni, Alexandr, and Piotr Indyk. 2006. Near-optimal hashing algorithms for approximate nearest\nneighbor in high dimensions. In Proceedings of the 47th annual IEEE symposium on foundations of\ncomputer science (FOCS’06) , 459–468. IEEE.\nAnguita, Davide, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-Ortiz. 2013. A\npublic domain dataset for human activity recognition using smartphones. In Proceedings of the 21st\ninternational European symposium on artiﬁcial neural networks, computational intelligence, and\nmachine learning (ESANN’13) , 437–442.\nAnscombe, Francis J. 1973. Graphs in statistical analysis. American Statistician 27 (1): 17–21.\nAnton, H., and C. Rorres. 2010. Elementary linear algebra: Applications version . Wiley.\nhttp://books.google.ie/books?id=1PJ-WHepeBsC.\nAshenfelter, Orley. 2008. Predicting the quality and prices of bordeaux wine. The Economic Journal\n118 (529): 174–184. doi:10.1111/j.1468-0297.2008.02148.x.\nAshmore, Malcolm. 1993. The theatre of the blind: Starring a promethean prankster, a phoney phe-\nnomenon, a prism, a pocket, and a piece of wood. Social Studies of Science 23 (1): 67–106.\nAsimov, Isaac. 1950. I, robot . Gnome Press.\nAyres, Ian. 2008. Super crunchers: Why thinking-by-numbers is the new way to be smart . Bantam.\nBache, K., and M. Lichman. 2013. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml.\nBall, N. M., J. Loveday, M. Fukugita, O. Nakamura, S. Okamura, J. Brinkmann, and R. J. Brun-\nner. 2004. Galaxy types in the sloan digital sky survey using supervised artiﬁcial neural net-\nworks. Monthly Notices of the Royal Astronomical Society 348 (3): 1038–1046. doi:10.1111/j.1365-\n2966.2004.07429.x. http://mnras.oxfordjournals.org/content/348/3/1038.abstract.\nBanerji, Manda, Ofer Lahav, Chris J. Lintott, Filipe B. Abdalla, Kevin Schawinski, Steven P. Bam-\nford, Dan Andreescu, Phil Murray, M. Jordan Raddick, Anze Slosar, Alex Szalay, Daniel Thomas,\nand Jan Vandenberg. 2010. Galaxy zoo: Reproducing galaxy morphologies via machine learning.\nMonthly Notices of the Royal Astronomical Society 406 (1): 342–353.\nBarber, David. 2012. Bayesian reasoning and machine learning . Cambridge University Press.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":830,"page_label":"776","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"776 Bibliography\nBatista, Gustavo E. A. P. A., and Maria Carolina Monard. 2003. An analysis of four missing data\ntreatment methods for supervised learning. Applied Artiﬁcial Intelligence 17 (5-6): 519–533.\nBayes, Thomas, and Richard Price. 1763. An essay towards solving a problem in the doctrine of\nchances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter to John Canton,\nAMFRS. Philosophical Transactions (1683-1775) .\nBejar, J., U. Cort ´es, and M. Poch. 1991. LINNEO+: A classiﬁcation methodology for ill-\nstructured domains, Research report RT-93-10-R, Dept. Llenguatges i Sistemes Informatics, Uni-\nversitat Polit `ecnica de Catalunya.\nBellman, R. E. 1957a. Dynamic programming . Princeton University Press.\nBellman, R. E. 1957b. A markov decision process. Journal of Mathematical Mechanics 6: 679–684.\nBentley, Jon Louis. 1975. Multidimensional binary search trees used for associative searching. Com-\nmunications of the ACM 18 (9): 509–517. doi:10.1145/361002.361007.\nBerk, Richard A., and Justin Bleich. 2013. Statistical procedures for forecasting criminal behavior.\nCriminology & Public Policy 12 (3): 513–544.\nBernstein, Peter L. 1996. Against the gods: The remarkable story of risk . Wiley.\nBerry, Michael J. A, and Gordon S. Linoff. 2004. Data mining techniques: for marketing, sales, and\ncustomer relationship management . Wiley.\nBertin, Jacques. 2010. Semiology of graphics: Diagrams, networks, maps . ESRI Press.\nBertsekas, Dimitri. 2017. Dynamic programming and optimal control . Athena Scientiﬁc.\nBishop, Christopher M. 1996. Neural networks for pattern recognition . Oxford University Press.\nBishop, C. M. 2006. Pattern recognition and machine learning . Springer.\nBlondlot, Ren ´e. 1903. Sur une nouvelle action produite par les rayons n et sur plusieurs fait relatifs `a\nces radiations. Comptes Rendus de l’Acad ´emie des Sciences de Paris 137: 166–169.\nBostrom, Nick. 2003. Ethical issues in advanced artiﬁcial intelligence. In Science ﬁction and philos-\nophy: From time travel to superintelligence , 277–284. Wiley-Blackwell.\nBray, Freddie, Jacques Ferlay, Isabelle Soerjomataram, Rebecca L. Siegel, Lindsey A. Torre, and\nAhmedin Jemal. 2018. Global cancer statistics 2018: Globocan estimates of incidence and mortality\nworldwide for 36 cancers in 185 countries. CA: A Cancer Journal for Clinicians 68 (6): 394–424.\ndoi:10.3322/caac.21492.\nBreiman, Leo. 1993. Classiﬁcation and regression trees . CRC Press.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":830,"page_label":"776","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Blondlot, Ren ´e. 1903. Sur une nouvelle action produite par les rayons n et sur plusieurs fait relatifs `a\nces radiations. Comptes Rendus de l’Acad ´emie des Sciences de Paris 137: 166–169.\nBostrom, Nick. 2003. Ethical issues in advanced artiﬁcial intelligence. In Science ﬁction and philos-\nophy: From time travel to superintelligence , 277–284. Wiley-Blackwell.\nBray, Freddie, Jacques Ferlay, Isabelle Soerjomataram, Rebecca L. Siegel, Lindsey A. Torre, and\nAhmedin Jemal. 2018. Global cancer statistics 2018: Globocan estimates of incidence and mortality\nworldwide for 36 cancers in 185 countries. CA: A Cancer Journal for Clinicians 68 (6): 394–424.\ndoi:10.3322/caac.21492.\nBreiman, Leo. 1993. Classiﬁcation and regression trees . CRC Press.\nBreiman, Leo. 1996. Bagging predictors. Machine Learning 24 (2): 123–140.\nBreiman, Leo. 2001. Random forests. Machine Learning 45 (1): 5–32.\nBrockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. 2016. OpenAI Gym. arXiv:1606.01540 .\nBrown, Noam, and Tuomas Sandholm. 2017. Libratus: The superhuman AI for no-limit poker. In\nProceedings of the twenty-sixth international joint conference on artiﬁcial intelligence, IJCAI 2017,\nMelbourne, Australia, August 19–25, 2017 , 5226–5228. doi:10.24963/ijcai.2017/772.\nBurges, Christopher J. C. 1998. A tutorial on support vector machines for pattern recognition. Data\nMining and Knowledge Discovery 2 (2): 121–167.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":831,"page_label":"777","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography 777\nCaruana, Rich, and Alexandru Niculescu-Mizil. 2006. An empirical comparison of supervised learn-\ning algorithms. In Proceedings of the 23rd international conference on machine learning , 161–168.\nACM.\nCaruana, Rich, Nikos Karampatziakis, and Ainur Yessenalina. 2008. An empirical evaluation of\nsupervised learning in high dimensions. In Proceedings of the 25th international conference on ma-\nchine learning , 96–103. ACM.\nCasscells, Ward, Arno Schoenberger, and Thomas B. Graboys. 1978. Interpretation by physicians of\nclinical laboratory results. New England Journal of Medicine 299 (18): 999–1001.\nChandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. ACM\ncomputing surveys (CSUR) 41 (3): 15.\nChang, Winston. 2012. R graphics cookbook: Practical recipes for visualizing data . O’Reilly Media.\nChapelle, Olivier, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised learning\n(Chapelle, O. et al., eds.; 2006) [book reviews]. IEEE Transactions on Neural Networks 20 (3):\n542–542.\nChapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer,\nand Rudiger Wirth. 2000. CRISP-DM 1.0 Step-by-step data mining guide, Technical report, CRISP-\nDM consortium.\nCharniak, Eugene. 2019. Introduction to deep learning . MIT Press.\nChen, Tianqi, and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings\nof the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , 785–\n794. ACM.\nCleary, Duncan, and Revenue Irish Tax. 2011. Predictive analytics in the public sector: Using data\nmining to assist better target selection for audit. In The proceedings of the 11th European conference\non egovernment: Faculty of administration, University of Ljubljana, Ljubljana, Slovenia, 16–17 June\n2011 , 168. Academic Conferences Limited.\nCohen, Jacob. 1960. A coefﬁcient of agreement for nominal scales. Educational and Psychological\nMeasurement 20 (1): 34–46.\nCooper, Gregory F., and Edward Herskovits. 1992. A Bayesian method for the induction of proba-\nbilistic networks from data. Machine Learning 9 (4): 309–347.\nCover, T. M., and J. A. Thomas. 1991. Elements of information theory . Wiley.\nCrawford, Kate. 2017. The trouble with bias. Conference on Neural Information Processing Systems,\ninvited speaker.\nCristianini, Nello, and John Shawe-Taylor. 2000. An introduction to support vector machines and\nother kernel-based learning methods . Cambridge University Press.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":831,"page_label":"777","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"2011 , 168. Academic Conferences Limited.\nCohen, Jacob. 1960. A coefﬁcient of agreement for nominal scales. Educational and Psychological\nMeasurement 20 (1): 34–46.\nCooper, Gregory F., and Edward Herskovits. 1992. A Bayesian method for the induction of proba-\nbilistic networks from data. Machine Learning 9 (4): 309–347.\nCover, T. M., and J. A. Thomas. 1991. Elements of information theory . Wiley.\nCrawford, Kate. 2017. The trouble with bias. Conference on Neural Information Processing Systems,\ninvited speaker.\nCristianini, Nello, and John Shawe-Taylor. 2000. An introduction to support vector machines and\nother kernel-based learning methods . Cambridge University Press.\nCunningham, Padraig. 2009. A taxonomy of similarity mechanisms for case-based reasoning. IEEE\nTransactions on Knowledge and Data Engineering 21 (11): 1532–1543.\nCybenko, George. 1988. Continuous valued neural networks with two hidden layers are sufﬁcient,\nTechnical report, Department of Computer Science, Tufts University.\nCybenko, George. 1989. Approximation by superpositions of sigmoids. Mathematics of Control,\nSignals and Systems 2: 303–314.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":832,"page_label":"778","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"778 Bibliography\nDaelemans, W., and A. van den Bosch. 2005. Memory-based language processing .Studies in natural\nlanguage processing . Cambridge University Press.\nDalgaard, Peter. 2008. Introductory statistics with R . Springer.\nDavenport, Thomas H. 2006. Competing on analytics. Harvard Business Review 84 (1): 98–107.\nhttp://hbr.harvardbusiness.org/2006/01/competing-on-analytics/ar/1.\nDavenport, Thomas H., and Jinho Kim. 2013. Keeping up with the quants: Your guide to understand-\ning and using analytics . Harvard Business Press Books.\nDavies, E. R. 2005. Machine vision: Theory, algorithms, practicalities , 3rd ed. Elsevier.\nDe Bruyne, Katrien, Bram Slabbinck, Willem Waegeman, Paul Vauterin, Bernard De Baets, and\nPeter Vandamme. 2011. Bacterial species identiﬁcation from maldi-tof mass spectra through data\nanalysis and machine learning. Systematic and Applied Microbiology 34 (1): 20–29.\nDemsar, Janez. 2006. Statistical comparisons of classiﬁers over multiple data sets. Journal of Ma-\nchine Learning Research 7: 1–30.\nDietterich, Thomas G. 2000. Ensemble methods in machine learning. In International workshop on\nmultiple classiﬁer systems , 1–15. Springer.\nDoctorow, Corey. 2010. Little brother . Macmillan.\nDrucker, Harris. 1997. Improving regressors using boosting techniques. In International conference\non machine learning ICML , V ol. 97, 107–115.\nDua, Dheeru, and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/\nml.\nDunn, Joseph C. 1974. Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics\n4 (1): 95–104.\nEco, Umberto. 1999. Kant and the platypus . Vintage U.K. Random House.\nElman, Jeffrey L. 1990. Finding structure in time. Cognitive Science 14 (2): 179–211.\nEsposito, Floriana, Donato Malerba, and Giovanni Semeraro. 1997. A comparative analysis of meth-\nods for pruning decision trees. IEEE Transactions on Pattern Analysis and Machine Intelligence 19\n(5): 476–491.\nEster, Martin, Hans-Peter Kriegel, J ¨org Sander, and Xiaowei Xu. 1996. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In Proceedings of the second international\nconference on knowledge discovery and data mining KDD , V ol. 96, 226–231.\nFanaee-T, Hadi, and Goao Gama. 2014. Event labeling combining ensemble detectors and back-\nground knowledge. Progress in Artiﬁcal Intelligence 2 (2-3): 113–127.\nFawcett, Tom. 2006. An introduction to ROC analysis. Pattern Recognition Letters 27 (8): 861–874.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":832,"page_label":"778","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Esposito, Floriana, Donato Malerba, and Giovanni Semeraro. 1997. A comparative analysis of meth-\nods for pruning decision trees. IEEE Transactions on Pattern Analysis and Machine Intelligence 19\n(5): 476–491.\nEster, Martin, Hans-Peter Kriegel, J ¨org Sander, and Xiaowei Xu. 1996. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In Proceedings of the second international\nconference on knowledge discovery and data mining KDD , V ol. 96, 226–231.\nFanaee-T, Hadi, and Goao Gama. 2014. Event labeling combining ensemble detectors and back-\nground knowledge. Progress in Artiﬁcal Intelligence 2 (2-3): 113–127.\nFawcett, Tom. 2006. An introduction to ROC analysis. Pattern Recognition Letters 27 (8): 861–874.\nFrank, Eibe. 2000. Pruning decision trees and lists. PhD dissertation, Department of Computer Sci-\nence, University of Waikato.\nFranklin, Janet. 2009. Mapping species distributions: Spatial inference and prediction (ecology,\nbiodiversity and conservation) . Cambridge University Press.\nFranklin, Janet, Paul McCullough, and Curtis Gray. 2000. Terrain variables used for predictive map-\nping of vegetation communities in Southern California. In Terrain analysis: Principles and applica-\ntions , eds. John P. Wilson and John C. Gallant. Wiley.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":833,"page_label":"779","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography 779\nFreund, Yoav, and Robert E. Schapire. 1995. A desicion-theoretic generalization of on-line learning\nand an application to boosting. In Computational Learning Theory , 23–37. Springer.\nFriedman, J., J. Bently, and R. Finkel. 1977. An algorithm for ﬁnding the best matches in logarithmic\nexpected time. ACM Transactions on Mathematical Software 3 (3): 209–226.\nFriedman, J., T. Hastie, and R. Tibshirani. 2001. The elements of statistical learning , V ol. 1. Springer.\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. Additive logistic regression: A sta-\ntistical view of boosting. The Annals of Statistics 28 (2): 337–407.\nFriedman, Jerome H. 2001. Greedy function approximation: A gradient boosting machine. Annals of\nStatistics 21 (5): 1189–1232.\nFry, Ben. 2007. Visualizing data: Exploring and explaining data with the processing environment .\nO’Reilly Media.\nG¨adenfors, Peter. 2004. Conceptual spaces: The geometry of thought . MIT Press.\nGal, Yarin, and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recur-\nrent neural networks. In Advances in neural information processing systems 29: Annual conference\non neural information processing systems 2016, December 5–10, 2016, Barcelona, Spain , 1019–\n1027.\nGleick, James. 2011. The information: A history, a theory, a ﬂood . HarperCollins UK.\nGlorot, Xavier, and Yoshua Bengio. 2010. Understanding the difﬁculty of training deep feedforward\nneural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence\nand statistics (AISTATS) , 249–256. JMLR.\nGlorot, Xavier, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiﬁer neural networks. In\nProceedings of the fourteenth international conference on artiﬁcial intelligence and statistics (AIS-\nTATS) , 315–323. JMLR.\nGoldberg, Yoav. 2017. Neural network methods for natural language processing .Synthesis lectures\non human language technology . Morgan and Claypool.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT Press.\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural in-\nformation processing systems 27: Annual conference on neural information processing systems 2014,\nDecember 8–13, 2014, Montreal, Quebec, Canada , 2672–2680.\nGross, Philip, Albert Boulanger, Marta Arias, David L. Waltz, Philip M. Long, Charles Lawson,","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":833,"page_label":"779","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"TATS) , 315–323. JMLR.\nGoldberg, Yoav. 2017. Neural network methods for natural language processing .Synthesis lectures\non human language technology . Morgan and Claypool.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT Press.\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural in-\nformation processing systems 27: Annual conference on neural information processing systems 2014,\nDecember 8–13, 2014, Montreal, Quebec, Canada , 2672–2680.\nGross, Philip, Albert Boulanger, Marta Arias, David L. Waltz, Philip M. Long, Charles Lawson,\nRoger Anderson, Matthew Koenig, Mark Mastrocinque, William Fairechio, et al.. 2006. Predicting\nelectricity distribution feeder failures using machine learning susceptibility analysis. In Proceedings\nof the twenty-ﬁrst national conference on artiﬁcial intelligence (AAAI’06) , 1705–1711. AAAI Press.\nGuisan, Antoine, and Niklaus E. Zimmermann. 2000. Predictive habitat distribution models in ecol-\nogy. Ecological Modelling 135 (2): 147–186.\nGuo, Yanming, Yu Liu, Ard Oerlemans, Songyang Lao, Song Wu, and Michael S. Lew. 2016. Deep\nlearning for visual understanding: A review. Neurocomputing 187: 27–48.\nGwiazda, J., E. Ong, R. Held, and F. Thorn. 2000. Vision: Myopia and ambient night-time lighting.\nNature 404 (6774): 144–144. http://dx.doi.org/10.1038/35004663.\nHan, Jiawei, Jian Pei, and Micheline Kamber. 2011. Data mining: concepts and techniques . Elsevier.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":834,"page_label":"780","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"780 Bibliography\nHand, David J., and Christoforos Anagnostopoulos. 2013. When is the area under the receiver op-\nerating characteristic curve an appropriate measure of classiﬁer performance? Pattern Recognition\nLetters 34 (5): 492–495.\nHart, P. 1968. The condensed nearest neighbor rule. IEEE Transactions on Information Theory 14\n(3): 515–516.\nHastie, T., R. Tibshirani, and J. Friedman. 2001. The elements of statistical learning . Springer.\nHastie, T., R. Tibshirani, and J. Friedman. 2009. The elements of statistical learning . Springer.\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectiﬁers: Sur-\npassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE interna-\ntional conference on computer vision , 1026–1034.\nHebb, Donald O. 1949. The organization of behavior; A neuropsychological theory . Wiley.\nHecht-Nielsen, Robert. 1987. Kolmogorov’s mapping neural network existence theorem. In Proceed-\nings of the IEEE ﬁrst international conference on neural networks , V ol. 3, 11–13.\nHerculano-Houzel, Suzana. 2009. The human brain in numbers: A linearly scaled-up primate brain.\nFrontiers in Human Neuroscience 3: 31. doi:10.3389/neuro.09.031.2009.\nHinton, G. E. 2005. What kind of graphical model is the brain? In Proceedings of the 19th interna-\ntional joint conference on artiﬁcial intelligence (IJCAI-05) . IJCAI.\nHinton, G. E., S. Osindero, and Y . W. Teh. 2006. A fast learning algorithm for deep belief nets.\nNeural Computing 18: 1527–1554.\nHirschowitz, Anton. 2001. Closing the CRM loop: The 21st century marketer’s challenge: Trans-\nforming customer insight into customer value. Journal of Targeting, Measurement and Analysis for\nMarketing 10 (2): 168–178.\nHochreiter, Sepp, and J ¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9\n(8): 1735–1780.\nHornik, Kur, Maxwell Stinchcombe, and Halber White. 1989. Multilayer feedforward networks are\nuniversal approximators. Neural Networks 2: 359–366.\nHoward, R. 1960. Dynamic programming and Markov processes. MIT Press.\nHubble, E. 1936. The realm of the nebulæ . Yale University Press.\nHubel, D. H., and T. N. Wiesel. 1962. Receptive ﬁelds, binocular interation and functional architec-\nture in the cat’s visual cortex. Journal of Physiology 160: 106–154.\nHunter, Elizabeth, Brian Mac Namee, and John Kelleher. 2018. An open-data-driven agent-based\nmodel to simulate infectious disease outbreaks. PloS One 13 (12): 0208775.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":834,"page_label":"780","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Hochreiter, Sepp, and J ¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9\n(8): 1735–1780.\nHornik, Kur, Maxwell Stinchcombe, and Halber White. 1989. Multilayer feedforward networks are\nuniversal approximators. Neural Networks 2: 359–366.\nHoward, R. 1960. Dynamic programming and Markov processes. MIT Press.\nHubble, E. 1936. The realm of the nebulæ . Yale University Press.\nHubel, D. H., and T. N. Wiesel. 1962. Receptive ﬁelds, binocular interation and functional architec-\nture in the cat’s visual cortex. Journal of Physiology 160: 106–154.\nHunter, Elizabeth, Brian Mac Namee, and John Kelleher. 2018. An open-data-driven agent-based\nmodel to simulate infectious disease outbreaks. PloS One 13 (12): 0208775.\nIoffe, Sergey, and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In Proceedings of the thirty-second international conference on\nmachine learning , 448–456. JMLR.\nJapkowicz, Nathalie, and Mohak Shah. 2011. Evaluating learning algorithms: A classiﬁcation per-\nspective . Cambridge University Press.\nJaynes, Edwin T. 2003. Probability theory: The logic of science . Cambridge University Press.\nJurafsky, Daniel, and James H. Martin. 2008. Speech and language processing: An introduction to\nnatural language processing, computational linguistics, and speech recognition , 2nd ed. Prentice\nHall.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":835,"page_label":"781","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography 781\nKansagara, Devan, Honora Englander, Amanda Salanitro, David Kagen, Cecelia Theobald, Michele\nFreeman, and Sunil Kripalani. 2011. Risk prediction models for hospital readmission: A systematic\nreview. JAMA 306 (15): 1688–1698.\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding groups in data: An introduction to cluster\nanalysis . Wiley.\nKelleher, John D. 2016. Fundamentals of machine learning for neural machine translation. In Trans-\nlating europe forum 2016: Focus on translation technologies . European Commission Directorate-\nGeneral for Translation.\nKelleher, John D. 2019. Deep learning . Essential Knowledge Series. MIT Press.\nKelleher, John D., and Simon Dobnik. 2017. What is not where: The challenge of integrating spatial\nrepresentations into deep learning architectures. In Proceedings of the conference on logic and ma-\nchine learning in natural language (LaML 17) . V ol. 1 of Clasp papers in computational linguistics ,\n41–52.\nKeri, Jonah. 2007. Baseball between the numbers: Why everything you know about the game is\nwrong. Basic Books.\nKingma, Diederik P., and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 .\nKlotz, Irving M. 1980. The n-ray affair. Scientiﬁc American 242 (5): 122–131.\nKlubi ˇcka, Filip, Giancarlo D. Salton, and John D. Kelleher. 2018. Is it worth it? Budget-related\nevaluation metrics for model selection. In Proceedings of the eleventh international conference on\nlanguage resources and evaluation (LREC 2018) .\nKohavi, Ron. 1996. Scaling up the accuracy of Naive-Bayes classiﬁers: A decision-tree hybrid. In\nProceedings of the twenty-ﬁfth ACM SIGKDD international conference on knowledge discovery and\ndata mining KDD , 202–207.\nKollar, Daphne, and Nir Friedman. 2009. Probabilistic graphical models: Principles and techniques .\nMIT Press.\nKolmogorov, Andrei Nikolaevich. 1963. On the representation of continuous functions of several\nvariables by superpositions of continuous functions of one variable and addition. American Mathe-\nmatical Society Translations .\nKrizhevsky, A., I. Sutskever, and G. E. Hinton. 2012. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information processing systems , 1097–1105.\nKuncheva, Ludmila I. 2004. Combining pattern classiﬁers: Methods and algorithms. Wiley.\nKutner, Michael, Christopher Nachtsheim, John Neter, and William Li. 2004. Applied linear statisti-\ncal models . McGraw-Hill.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":835,"page_label":"781","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Kollar, Daphne, and Nir Friedman. 2009. Probabilistic graphical models: Principles and techniques .\nMIT Press.\nKolmogorov, Andrei Nikolaevich. 1963. On the representation of continuous functions of several\nvariables by superpositions of continuous functions of one variable and addition. American Mathe-\nmatical Society Translations .\nKrizhevsky, A., I. Sutskever, and G. E. Hinton. 2012. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information processing systems , 1097–1105.\nKuncheva, Ludmila I. 2004. Combining pattern classiﬁers: Methods and algorithms. Wiley.\nKutner, Michael, Christopher Nachtsheim, John Neter, and William Li. 2004. Applied linear statisti-\ncal models . McGraw-Hill.\nLe Cun, Yann, L ´eon Bottou, Yoshua Bengio, and Haffner Patrick. 1998. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE 86 (11): 2278–2324.\nLehmann, Thomas Martin, Mark Oliver G ¨uld, Daniel Keysers, Henning Schubert, Michael Kohnen,\nand Berthold B. Wein. 2003. Determining the view of chest radiographs. Journal of Digital Imaging\n16 (3): 280–291.\nLeshno, Moshe, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. 1993. Multilayer feedfor-\nward networks with a nonpolynomial activation function can approximate any function. Neural Net-\nworks 6: 861–867.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":836,"page_label":"782","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"782 Bibliography\nLevitt, Steven D., and Stephen J. Dubner. 2005. Freakonomics: A rogue economist explores the\nhidden side of everything . Penguin.\nLewis, Michael. 2004. Moneyball: The art of winning an unfair game . Norton.\nLi, Deren, Shuliang Wang, and Deyi Li. 2015. Spatial data mining . Springer.\nLintott, C. J., K. Schawinski, A. Slosar, K. Land, S. Bamford, D. Thomas, M. J. Raddick, R. C.\nNichol, A. Szalay, D. Andreescu, P. Murray, and J. Vandenberg. 2008. Galaxy Zoo: Morphologies\nderived from visual inspection of galaxies from the Sloan Digital Sky Survey. Monthly Notices of the\nRoyal Astronomical Society 389: 1179–1189. doi:10.1111/j.1365-2966.2008.13689.x.\nLintott, C., K. Schawinski, S. Bamford, A. Slosar, K. Land, D. Thomas, E. Edmondson, K. Masters,\nR. C. Nichol, M. J. Raddick, A. Szalay, D. Andreescu, P. Murray, and J. Vandenberg. 2011. Galaxy\nZoo 1: Data release of morphological classiﬁcations for nearly 900 000 galaxies. Monthly Notices of\nthe Royal Astronomical Society 410: 166–178. doi:10.1111/j.1365-2966.2010.17432.x.\nLoh, Wei-Yin. 2011. Classiﬁcation and regression trees. Wiley Interdisciplinary Reviews: Data Min-\ning and Knowledge Discovery 1 (1): 14–23.\nMaas, Andrew L., Awni Y . Hannun, and Andrew Y . Ng. 2013. Rectiﬁer nonlinearities improve neural\nnetwork acoustic models. In Proceedings of thirtieth international conference on machine learning\n(ICML 13) , V ol. 30. JMLR.\nMac Namee, B., P. Cunningham, S. Byrne, and O. I. Corrigan. 2002. The problem of bias in training\ndata in regression problems in medical decision support. Artiﬁcial Intelligence in Medicine 24 (1):\n51–70.\nMac Namee, Brian. 2009. Agent based modeling in computer graphics and games. In Encyclopedia\nof complexity andsystems science , ed. R. A. Meyers. Dublin Institute of Technology.\nMacKay, David J. C. 2003. Information theory, inference and learning algorithms . Cambridge Uni-\nversity Press.\nMahalunkar, Abhijit, and John D Kelleher. 2018. Using regular languages to explore the represen-\ntational capacity of recurrent neural architectures. In International conference on artiﬁcial neural\nnetworks , 189–198. Springer.\nMakhoul, John, Amro El-Jaroudi, and Richard Schwartz. 1989. Formation of disconnected decision\nregions with a single hidden layer. In Proceedings of the international joint conference on neural\nnetworks , V ol. 1, 455–460. IEEE.\nMangasarian, Olvi L., and William H. Wolberg. 1990. Cancer diagnosis via linear programming.\nSIAM News 23 (5): 1–18.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":836,"page_label":"782","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"MacKay, David J. C. 2003. Information theory, inference and learning algorithms . Cambridge Uni-\nversity Press.\nMahalunkar, Abhijit, and John D Kelleher. 2018. Using regular languages to explore the represen-\ntational capacity of recurrent neural architectures. In International conference on artiﬁcial neural\nnetworks , 189–198. Springer.\nMakhoul, John, Amro El-Jaroudi, and Richard Schwartz. 1989. Formation of disconnected decision\nregions with a single hidden layer. In Proceedings of the international joint conference on neural\nnetworks , V ol. 1, 455–460. IEEE.\nMangasarian, Olvi L., and William H. Wolberg. 1990. Cancer diagnosis via linear programming.\nSIAM News 23 (5): 1–18.\nMarsland, Stephen. 2011. Machine learning: An algorithmic perspective . CRC Press.\nMcCandlish, Sam, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. 2018. An empirical model\nof large-batch training. CoRR abs/1812.06162. http://arxiv.org/abs/1812.06162.\nMcCulloch, Warren S., and Walter Pitts. 1943. A logical calculus of the ideas immanent in the\nnervous system. Bulletin of Mathematical Biophysics 5: 115–133.\nMcGrayne, Sharon Bertsch. 2011. The theory that would not die: How Bayes’ rule cracked the\nenigma code, hunted down Russian submarines, and emerged triumphant from two centuries of con-\ntroversy . Yale University Press.\nMichie, D. 1961. Trial and error. In Science survey, part 2 , eds. S. A. Barnett and A. McLaren,\n129–145. Penguin.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":837,"page_label":"783","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography 783\nMichie, D. 1963. Experiments on the mechanisation of game learning. Computer Journal 1: 232–\n263.\nMingers, John. 1987. Expert systems - rule induction with statistical data. Journal of the Operational\nResearch Society 38: 39–47.\nMingers, John. 1989. An empirical comparison of selection measures for decision-tree induction.\nMachine Learning 3 (4): 319–342.\nMinsky, Marvin, and Seymour Papert. 1969. Perceptrons . MIT Press.\nMishne, Gilad, and Natalie S. Glance. 2006. Predicting movie sales from blogger sentiment. In AAAI\nspring symposium: Computational approaches to analyzing weblogs , 155–158.\nMitchell, T. 1997. Machine learning . McGraw Hill.\nMitchell, Tom M., Svetlana V . Shinkareva, Andrew Carlson, Kai-Min Chang, Vicente L. Malave,\nRobert A. Mason, and Marcel A. Just. 2008. Predicting human brain activity associated with the\nmeanings of nouns. Science 320 (5880): 1191–1195. doi:10.1126/science.1152876.\nMnih, V olodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602 .\nMontgomery, Douglas C. 2004. Introduction to statistical quality control . Wiley.\nMontgomery, Douglas C. 2012. Design and analysis of experiments . Wiley.\nMontgomery, Douglas C., and George C. Runger. 2010. Applied statistics and probability for engi-\nneers . Wiley.\nMont ´ufar, Guido. 2014. Universal approximation depth and errors of narrow belief networks with\ndiscrete units. Neural Computation 26 (7): 1386–1407.\nMoon, Todd K. 1996. The expectation-maximization algorithm. IEEE Signal Processing Magazine\n13 (6): 47–60.\nMurphy, Kevin P. 2012. Machine learning: A probabilistic perspective . MIT Press.\nNeapolitan, Richard E. 2004. Learning Bayesian networks . Pearson Prentice Hall.\nNg, Andrew Y ., Michael I. Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an\nalgorithm. In Advances in neural information processing systems , 849–856.\nOECD. 2013. The OECD privacy framework . Organisation for Economic Co-operation and Devel-\nopment.\nOsowski, Stainslaw, Linh Tran Hoai, and T. Markiewicz. 2004. Support vector machine-based expert\nsystem for reliable heartbeat recognition. IEEE Transactions on Biomedical Engineering 51 (4):\n582–589. doi:10.1109/TBME.2004.824138.\nPalaniappan, Sellappan, and Raﬁah Awang. 2008. Intelligent heart disease prediction system using","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":837,"page_label":"783","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Neapolitan, Richard E. 2004. Learning Bayesian networks . Pearson Prentice Hall.\nNg, Andrew Y ., Michael I. Jordan, and Yair Weiss. 2002. On spectral clustering: Analysis and an\nalgorithm. In Advances in neural information processing systems , 849–856.\nOECD. 2013. The OECD privacy framework . Organisation for Economic Co-operation and Devel-\nopment.\nOsowski, Stainslaw, Linh Tran Hoai, and T. Markiewicz. 2004. Support vector machine-based expert\nsystem for reliable heartbeat recognition. IEEE Transactions on Biomedical Engineering 51 (4):\n582–589. doi:10.1109/TBME.2004.824138.\nPalaniappan, Sellappan, and Raﬁah Awang. 2008. Intelligent heart disease prediction system using\ndata mining techniques. International Journal of Computer Science and Network Security 8 (8):\n343–350.\nPearl, Judea. 1988. Probabilistic reasoning in intelligent systems: Networks of plausible inference .\nMorgan Kaufmann.\nPearl, Judea. 2000. Causality: Models, reasoning and inference , V ol. 29. Cambridge University\nPress.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":838,"page_label":"784","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"784 Bibliography\nQuinlan, J. Ross. 1986. Induction of decision trees. Machine Learning 1 (1): 81–106.\nQuinlan, J. Ross. 1987. Simplifying decision trees. International Journal of Man-Machine Studies\n27 (3): 221–234.\nQuinlan, J. Ross. 1993. C4.5: Programs for machine learning . Morgan Kaufmann.\nQuinn, Graham E., Chai H. Shin, Maureen G. Maguire, and Richard A. Stone. 1999. Myopia and\nambient lighting at night. Nature 399 (6732): 113–114. http://dx.doi.org/10.1038/20094.\nReagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. Deep\nlearning for computer architects . Morgan and Claypool.\nReed, Russell D., and Robert J. Marks. 1999. Neural smithing: Supervised learning in feedforward\nartiﬁcial networks . MIT Press.\nRice, John A. 2006. Mathematical statistics and data analysis . Cengage Learning.\nRichter, Michael M., and Rosina O. Weber. 2013. Case-based reasoning: A textbook . Springer.\nRosenblatt, Frank. 1958. The perceptron: A probabilistic model for information storage and organi-\nzation in the brain. Psychological Review 65 (6): 386–408.\nRousseeuw, Peter J. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster\nanalysis. Journal of computational and applied mathematics 20: 53–65.\nRubin, Daniel J. 2015. Hospital readmission of patients with diabetes. Current Diabetes Reports 15\n(4): 17.\nSamet, Hanan. 1990. The design and analysis of spatial data structures , V ol. 199. Addison-Wesley.\nSarle, Warren S. 1983. Cubic clustering criterion, Technical Report SAS Technical Report A-108,\nSAS Institute.\nScarne, John. 1986. Scarne’s new complete guide to gambling . Simon & Schuster.\nSchapire, Robert E. 1990. The strength of weak learnability. Machine Learning 5 (2): 197–227.\nSchapire, Robert E. 1999. A brief introduction to boosting. In Proceedings of the sixteenth interna-\ntional joint conference on artiﬁcial intelligence (IJCAI-99) , V ol. 2, 1401–1406.\nSchwartz, Paul M. 2010. Data protection law and the ethical use of analytics, Technical report, The\nCentre for Information Policy Leadership (at Hunton & Williams LLP).\nScrucca, Luca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. 2016. Mclust 5: Clustering,\nclassiﬁcation and density estimation using gaussian ﬁnite mixture models. The R Journal 8 (1): 289.\nSegata, N., E. Blanzieri, S. J. Delany, and Padraig Cunningham. 2009. Noise reduction for instance-\nbased learning with a local maximal margin approach. Journal of Intelligent Information Systems 35:","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":838,"page_label":"784","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"tional joint conference on artiﬁcial intelligence (IJCAI-99) , V ol. 2, 1401–1406.\nSchwartz, Paul M. 2010. Data protection law and the ethical use of analytics, Technical report, The\nCentre for Information Policy Leadership (at Hunton & Williams LLP).\nScrucca, Luca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. 2016. Mclust 5: Clustering,\nclassiﬁcation and density estimation using gaussian ﬁnite mixture models. The R Journal 8 (1): 289.\nSegata, N., E. Blanzieri, S. J. Delany, and Padraig Cunningham. 2009. Noise reduction for instance-\nbased learning with a local maximal margin approach. Journal of Intelligent Information Systems 35:\n301–331.\nSejnowski, Terrence J. 2018. The deep learning revolution . MIT Press.\nShannon, Claude E., and Warren Weaver. 1949. The mathematical theory of communication . Univer-\nsity of Illinois Press.\nSiddiqi, Naeem. 2005. Credit risk scorecards: Developing and implementing intelligent credit scor-\ning. Wiley.\nSiegel, Eric. 2013. Predictive analytics: The power to predict who will click, buy, lie, or die , 1st ed.\nWiley.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":839,"page_label":"785","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Bibliography 785\nSilver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander\nDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2017. Mastering the\ngame of go without human knowledge. Nature 550 (7676): 354.\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si-\nmonyan, and Demis Hassabis. 2018. A general reinforcement learning algorithm that masters chess,\nshogi, and go through self-play. Science 362 (6419): 1140–1144. doi:10.1126/science.aar6404.\nSilver, Nate. 2012. The signal and the noise: Why so many predictions fail — but some don’t . Penguin\nPress.\nSing, Tobias, Oliver Sander, Niko Beerenwinkel, and Thomas Lengauer. 2005. ROCR: Visualizing\nclassiﬁer performance in R. Bioinformatics 21 (20): 3940–3941.\nSmyth, B., and M. Keane. 1995. Remembering to forget: A competence preserving case deletion\npolicy for case-based reasoning systems. In The fourteenth international joint conference on artiﬁcial\nintelligence (IJCAI-95) , ed. C. Mellish, 337–382. ACM.\nSpercher, David A. 1965. On the structure of continuous functions of several variables. Transactions\nof teh American Mathematical Society 115 (3): 340–355.\nSrivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n2014. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learn-\ning Research 15 (1): 1929–1958.\nStewart, James. 2012. Calculus , 7th ed. Cengage Learning.\nStoughton, Chris, Robert H. Lupton, Mariangela Bernardi, Michael R. Blanton, Scott Burles, Fran-\ncisco J. Castander, A. J. Connolly, Daniel J. Eisenstein, Joshua A. Frieman, G. S. Hennessy, et\nal.. 2002. Sloan digital sky survey: Early data release. The Astronomical Journal 123 (1): 485.\nhttp://stacks.iop.org/1538-3881/123/i=1/a=485.\nStrang, Gilbert. 2016. Introduction to linear algebra , 5th ed. Wellesley-Cambridge Press.\nStrang, Gilbert. 2019. Linear algebra and learning from data . Wellesley-Cambridge Press.\nSutton, Richard S., and Andrew G. Barto. 1998. Reinforcement learning: An introduction . MIT\nPress.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement learning: An introduction . MIT","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":839,"page_label":"785","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Stoughton, Chris, Robert H. Lupton, Mariangela Bernardi, Michael R. Blanton, Scott Burles, Fran-\ncisco J. Castander, A. J. Connolly, Daniel J. Eisenstein, Joshua A. Frieman, G. S. Hennessy, et\nal.. 2002. Sloan digital sky survey: Early data release. The Astronomical Journal 123 (1): 485.\nhttp://stacks.iop.org/1538-3881/123/i=1/a=485.\nStrang, Gilbert. 2016. Introduction to linear algebra , 5th ed. Wellesley-Cambridge Press.\nStrang, Gilbert. 2019. Linear algebra and learning from data . Wellesley-Cambridge Press.\nSutton, Richard S., and Andrew G. Barto. 1998. Reinforcement learning: An introduction . MIT\nPress.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement learning: An introduction . MIT\nPress.\nSvolba, Gerhard. 2007. Data preparation for analytics using SAS . SAS Institute.\nSvolba, Gerhard. 2012. Data quality for analytics using SAS . SAS Institute.\nTaleb, Nassim Nicholas. 2008. The black swan: The impact of the highly improbable . Penguin.\nTempel, E., E. Saar, L. J. Liivam ¨agi, A. Tamm, J. Einasto, M. Einasto, and V . M ¨uller. 2011. Galaxy\nmorphology, luminosity, and environment in the SDSS DR7. A&A 529: 53. doi:10.1051/0004-\n6361/201016196.\nTene, Omer, and Jules Polonetsky. 2013. Big data for all: Privacy and user control in the age of\nanalytics. Northwestern Journal of Technology and Intellectual Property 11 (5): 239–247.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":840,"page_label":"786","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"786 Bibliography\nTesauro, Gerald. 1994. TD-gammon, a self-teaching backgammon program, achieves master-level\nplay. Neural Computation 6 (2): 215–219.\nTijms, Henk. 2012. Understanding probability . Cambridge University Press.\nTrask, Andrew. 2019. Grokking deep learning . Manning.\nTsanas, Athanasios, and Angeliki Xifara. 2012. Accurate quantitative estimation of energy perfor-\nmance of residential buildings using statistical machine learning tools. Energy and Buildings 49:\n560–567.\nTsoumakas, Grigorios, Min-Ling Zhang, and Zhi-Hua Zhou. 2012. Introduction to the special issue\non learning from multi-label data. Machine Learning 88 (1-2): 1–4.\nT¨ufekci, Pinar. 2014. Prediction of full load electrical power output of a base load operated combined\ncycle power plant using machine learning methods. International Journal of Electrical Power &\nEnergy Systems 60: 126–140.\nTufte, Edward R. 2001. The visual display of quantitative information . Graphics Press.\nTukey, John W. 1977. Exploratory data analysis . Addison-Wesley.\nVapnik, Vladimir. 2000. The nature of statistical learning theory . Springer.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural informa-\ntion processing systems , 5998–6008.\nWiddows, Dominic. 2004. Geometry and meaning . CSLI Publications.\nWirth, R ¨udiger, and Jochen Hipp. 2000. CRISP-DM: Towards a standard process model for data\nmining. In Proceedings of the 4th international conference on the practical applications of knowledge\ndiscovery and data mining , 29–39. Citeseer.\nWolpert David, H. 1996. The lack of a priori distinctions between learning algorithms. Neural Com-\nputation 8 (7): 1341–1390.\nWood, Robert W. 1904. The n-rays. Nature 70: 530–531.\nWooldridge, Michael. 2009. An introduction to multiagent systems . Wiley.\nWooldridge, Michael, and Nicholas R. Jennings. 1995. Intelligent agents: Theory and practice. The\nKnowledge Engineering Review 10 (2): 115–152.\nWoolery, L., J. Grzymala-Busse, S. Summers, and A. Budihardjo. 1991. The use of machine learning\nprogram LERS-LB 2.5 in knowledge acquisition for expert system development in nursing. Comput-\ners in Nursing 9: 227–234.\nZadnik, Karla, Lisa A. Jones, Brett C. Irvin, Robert N. Kleinstein, Ruth E. Manny, Julie A. Shin,\nand Donald O. Mutti. 2000. Vision: Myopia and ambient night-time lighting. Nature 404 (6774):\n143–144. http://dx.doi.org/10.1038/35004661.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":840,"page_label":"786","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Wood, Robert W. 1904. The n-rays. Nature 70: 530–531.\nWooldridge, Michael. 2009. An introduction to multiagent systems . Wiley.\nWooldridge, Michael, and Nicholas R. Jennings. 1995. Intelligent agents: Theory and practice. The\nKnowledge Engineering Review 10 (2): 115–152.\nWoolery, L., J. Grzymala-Busse, S. Summers, and A. Budihardjo. 1991. The use of machine learning\nprogram LERS-LB 2.5 in knowledge acquisition for expert system development in nursing. Comput-\ners in Nursing 9: 227–234.\nZadnik, Karla, Lisa A. Jones, Brett C. Irvin, Robert N. Kleinstein, Ruth E. Manny, Julie A. Shin,\nand Donald O. Mutti. 2000. Vision: Myopia and ambient night-time lighting. Nature 404 (6774):\n143–144. http://dx.doi.org/10.1038/35004661.\nZhang, Nevin Lianwen, and David Poole. 1994. A simple approach to bayesian network computa-\ntions. In Proceedings of the tenth biennial Canadian artiﬁcial intelligence conference , 171–178.\nZhou, Zhi-Hua. 2012. Ensemble methods: Foundations and algorithms . CRC Press.","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":841,"page_label":"787","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index\n68´95´99.7rule, 62, 71\n˚, xx\nχ2pruning, 155\nχ2statistic, 583\nϵ-greedy action selection policy, 656, 658, 674\nϵ0bootstrap, 546\nabsence-presence, 214\nabsolute rarity, 720\nABT, seeanalytics base table\naction, 643, 676\naction-value behavior network, 672\naction-value function, 642, 643, 651, 676\naction-value table, 654\naction-value target network, 672\nactivation function, 386\nAdaBoost, 171\nAdaboost.R2, 161\nAdam, 523\nadditive models, 165\nafﬁne function, 385\nagent, seeintelligent agent\nagglomerative hierarchical clustering, 618, 618,\n629, 635\naggregate features, 35\nAHC, seeagglomerative hierarchical clustering\nAlphaGo, 677\nAlphaZero, 677\nanalytics base table, 17, 23, 28, 28, 45, 49, 50,\n52, 53, 94, 97, 600, 625, 688\nanalytics solution, 23, 24\nANOV A test, 86, 95\nAnscombe’s quartet, 84anti-discrimination legislation, 40\napproximate methods, 668, 676\narea under the curve, 561, 590\narithmetic mean, 550, 551, 577, 591, 745, 745\nartiﬁcial intelligence, 304, 677\nartiﬁcial neural network, seeneural network\nartiﬁcial neuron, 383\nastronomy, 703\nAUC, seearea under the curve\naugment data, 599, 629\nauto-encoder, 624, 624, 629, 733\naverage class accuracy, 550, 551, 554, 577, 586,\n591, 698\naverage linkage, 619, 635\nbackpropagation of error, 387, 403, 404, 624,\n731\nbackpropagation through time, 502, 518\nbackward sequential selection, 229\nbag-of-words, 223, 236\nbagging, 159, 159, 171, 179, 733, 735\nbalanced sample, 693\nbar plot, 54, 745, 752\nbasis functions, 351, 365, 368\nbatch, 327, 417\nbatch gradient descent, 327, 416, 417\nbatch learning, 417\nbatch normalization, 523\nbatch size, 417\nbatches, 391, 417\nBayes’ Theorem, 243, 245, 248, 731\nBayes, Thomas, 248\nBayesian information criterion, 292","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":842,"page_label":"788","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"788 Index\nBayesian MAP prediction model, 254\nBayesian network, 243, 265, 285, 304, 732,\n733, 735\nBayesian optimal classiﬁer, 254\nBayesian score, 293\nbehavior policy, 657, 659, 664\nBellman Equations, 653\nBellman optimality equation, 653\nbias, 91, 385, 751\nbias term, 480, 493\nBIC, seeBayesian information criterion\nBienaym ´e formula, 455\nbimodal distribution, 60\nbinary data, 34\nbinary logarithm, 124\nbinary tree, 196\nbinning, 89, 102, 146, 280\nbinomial distribution, 178\nbins, 89\nbits, 126\nblack box model, 739\nBlackjack, 645\nblame assignment, 404, 405, 413\nBlondlot, Ren ´e, 533\nBoltzmann action selection, 658\nboosting, 159, 159, 171, 178, 179, 733, 735\nbootstrap aggregating, seebagging\nbootstrap samples, 159\nbootstrapping, 546, 655, 662, 676\nbottleneck layer, 624, 628\nbox plot, 54, 451, 745, 752, 755\nbranches, 121\nbreast cancer, 109\nbrute-force search, 318\nburn-in time, 300\nbusiness problem, 23\nBusiness Understanding, 16, 19, 24, 28, 46,\n685, 704, 730\nC4.5, 169\ncalculus, 765\ncapacity for model retraining, 739\nCardano, Gerolamo, 243\ncardinality, 54, 54, 693\nCART, 169\ncase-based reasoning, 234categorical cross entropy, 378\ncategorical data, 34\ncausal graphs, 293\nCBR, seecase-based reasoning\ncell, 508\ncentral tendency, 54, 550, 745\ncentroid linkage, 619\nchain rule (differentiation), 312, 325, 345, 381,\n410, 413, 435, 731, 765, 768\nchain rule (probability), 245, 251, 257, 291,\n757, 763, 768\nchannel, 492\nChebyshev distance, 186\nchessboard distance, 186\nchild node, 286\nchurn prediction, 584, 685\ncitizen science, 708\nclamp transformation, 70, 715\nclasses, 551\nclassiﬁcation accuracy, 539, 545, 550\ncluster centroids, 600\nclustering, 597, 599, 603, 629\nclustering evaluation internal criterion, 608\nclusters, 599\nCNN, seeconvolutional neural network\nco-absence, 212, 213\nco-presence, 212, 213\nCohen’s kappa, 726\ncollection limitation principle, 41\ncomparative experiments, 583\ncomplete case analysis, 69, 712\ncomplete linkage, 619\ncomponent, 274\ncomposite function, 768\ncomposition, 401\nconcept drift, 232, 578, 579, 657, 727\ncondensed nearest neighbor, 233\nconditional independence, 256, 256, 261, 302\nconditional maximum entropy model, 357\nconditional probability, 245, 246, 251, 757, 759,\n759, 762\nconditional probability table, 286\nconditionally independent, 285, 288\nconﬁdence factor, 161, 178\nconfounding feature, 85\nconfusion matrix, 537, 553, 572, 591\nconnectionism, 382","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":843,"page_label":"789","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index 789\nconsistent model, 6, 7, 121, 141\nconstrained quadratic optimization problem,\n363\nconstraints, 363\ncontinuous data, 34\ncontinuous function, 766\ncontrol group, 583\nconvergence, 323\nconvergence criterion, 418\nconvex surface, 318\nconvolutional auto-encoders, 630\nconvolutional neural network, 434, 477, 485,\n673, 674\nconvolving a function, 485\ncoordinate system, 183\ncorrelation, 81, 82, 94, 103, 223\ncorrelation matrix, 83\nCorruption Perception Index, 237, 294\ncosine, 216\ncosine similarity, 216, 223, 231, 237\ncost function, 409\ncovariance, 81, 218\ncovariance matrix, 83, 219\nCPI, seeCorruption Perception Index\nCPT, seeconditional probability table\ncredit scoring, 538, 553, 563\nCRISP-DM, seeCross Industry Standard\nProcess for Data Mining\ncritical value pruning, 155\nCRM, seecustomer relationship management\nCross Industry Standard Process for Data\nMining, 16, 22, 28, 46, 53, 94, 534, 600,\n730\ncross-correlation, 485\ncross-entropy, 434, 463, 465\ncross-sell model, 572\ncrowdsourcing, 708\ncubic clustering criterion, 609\ncubic function, 766\ncumulative gain, 567, 567, 594, 700\ncumulative gain chart, 569, 570, 592\ncumulative lift, 567, 570, 700\ncumulative lift chart, 570\ncumulative lift curve, 570\ncumulative reward, 640, 643, 676\ncurse of dimensionality, 225, 232, 255, 262,\n740, 762customer churn, 48\ncustomer relationship management, 572\ncustomer segmentation, 599\ncut a hierarchical agglomeration, 622\nd-separation, 289\ndata, 3\ndata analytics, 3\ndata availability, 33\ndata exploration, 34, 53, 94\ndata fragmentation, 255, 262\ndata management tools, 42\ndata manipulation, 42\ndata manipulation tools, 42\ndata mining, 16\nData Preparation, 17, 19, 28, 46, 53, 87, 94, 95,\n535, 691, 713, 730\ndata preprocessing, 421\ndata protection legislation, 40\ndata quality issues, 53, 63, 94\ndata quality plan, 64, 94\ndata quality report, 53, 54, 94, 98, 105, 110,\n614, 693, 710\ndata subject, 40\nData Understanding, 17, 19, 28, 46, 53, 94, 688,\n707, 730\ndata visualization, 99, 752\ndata-driven decisions, 19\ndatabase management systems, 42\ndataset, 5, 758\nDBScan, 630\nde Fermat, Pierre, 243\nde-noising auto-encoders, 630\ndeciles, 567, 582\ndecision boundary, 189, 338, 359, 396, 736\ndecision stumps, 165\ndecision surface, 341\ndecision tree, 117, 121, 121, 169, 541, 554, 556,\n732, 733, 735, 736\ndecisions, 3\ndecoder, 624\ndeep learning, xvi, 19, 381, 396, 599, 624, 629,\n637, 676, 773\ndeep Q learning, 677\ndeep Q network, 637, 664, 671, 673, 676\ndegrees of freedom, 272","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":844,"page_label":"790","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"790 Index\ndelta value, 323\ndendrogram, 622\ndensity, 752, 753\ndensity curve, 61\ndensity histogram, 753\nDeployment, 17, 20, 702, 727, 730\ndepth of a ﬁlter, 492\ndepth of a neural network, 389\nderivative, 765\nderived features, 34, 41, 45\ndescriptive features, 5, 19, 23, 28, 598, 688\ndiabetes, 51\ndiagnosis, 4\ndifferentiation, 370, 765\ndirected acyclic graph, 499\ndirected cyclic graph, 499\ndiscontinuous function, 341\ndiscount rate, 642\ndiscounted return, 642\ndiscriminative model, 733\ndisease diagnosis, 538\ndistance matrix, 620, 621\ndistance measure, 599, 601, 618\ndistance metric, 184, 231\ndistance weighted k nearest neighbor, 194\ndistributions, 102\ndocument classiﬁcation, 4, 223\ndomain, 34, 757\ndomain concept, 23, 30, 45, 688, 689, 707\ndomain representation, 732\ndomain subconcept, 32\ndosage prediction, 3\ndot product, 216, 320, 342, 385, 773\nDQN, seedeep Q network\nDropMask, 530\ndropout, 434, 472, 473, 507\nDunn index, 609\ndying ReLU, 444\ndynamic programming, 653, 677\neager learner, 232\nearly stopping, 418, 432, 434, 472, 472, 477\nearly stopping criteria, 152, 155\necological modeling, 135\nedges, 286\nEEG, seeelectroencephalography pattern\nrecognitionelectroencephalography pattern recognition, 353\neligibility traces, 655\nemail classiﬁcation, 536\nembedding, 624, 626\nencoder, 624\nenrich data, 599, 629\nensemble, seemodel ensemble\nentropy, 43, 117, 120, 125, 149, 172, 173, 611,\n731\nenvironment, 643, 676\nepisode, 639\nepoch, 416, 418\nequal-frequency binning, 89, 91, 102, 280, 294,\n307\nequal-width binning, 89, 90, 102, 280\nequation of a line, 313, 385\nequivariant, 483\nergodic Markov chain, 299\nerror function, 312, 315, 315, 367, 409\nerror rate, 155\nerror surface, 311, 317\nerror-based learning, 19, 311, 599\nethics, 47\nETL, seeextract-transform-load\nEuclidean coordinate space, 185\nEuclidean distance, 185, 200, 231, 237, 577,\n601, 602, 620, 631, 632, 636, 731\nEuclidean norm, 364\nEuler’s number, 342, 580\nEvaluation, 17, 19, 534, 698, 725, 730\nevent, 246, 757, 758, 758\nevolutionary reinforcement learning, 641\nexpectation, 652\nexpectation maximization algorithm, 600, 629\nexpected return, 642, 643, 676\nexpected reward, 651\nexperience replay, 671\nexperiment, 246, 757, 758\nexperimental design, 586\nexploding gradients, 448, 452, 507\nexploitation, 655, 656\nexploration, 655, 656\nexponential distribution, 60, 72, 270, 274\nexternal criteria, 611\nextract-transform-load, 42, 702\nF measure, seeF1measure","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":845,"page_label":"791","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index 791\nF score, seeF1measure\nF1measure, 548, 549, 549–551, 611\nF1score, seeF1measure\nfactorization, 256, 302\nfactors, 258\nfalse alarms, 538\nfalse negative, 537, 556\nfalse negative rate, 548\nfalse positive, 254, 537, 556\nfalse positive rate, 548\nfat tails, 272\nfeature, 45, 758\nfeature map, 485\nfeature selection, 94, 181, 227, 227, 232, 614,\n722, 740\nfeature space, 181, 183, 184, 231, 599\nfeature subset space, 228\nfeedforward network, 389\nﬁlter dimension, 486\nﬁlters, 227, 482\nﬁt, 315, 367\nﬂag features, 35\nFN,seefalse negative\nFNR, seefalse negative rate\nfolds, 543\nforget gate, 508\nforward reasoning, 248\nforward sequential selection, 229\nFP,seefalse positive\nFPR, seefalse positive rate\nfraud detection, 262, 538\nfrequency counts, 749\nfrequency histogram, 752\nfrequency table, 749\nfull joint probability distribution, 284, 302, 761\nfully connected network, 389\nfully observable environment, 640, 640, 673\nGaeltacht, 655\ngain, 567, 569\ngain chart, 567, 570\ngalaxy morphology, 704\nGalaxy Zoo, 708\ngamma function, 271\nGapminder, 237\ngates, 508Gauss, Carl Friedrich, 317\nGauss-Jordan elimination, 220\nGaussian distribution, 61\nGaussian mixture model, 629\nGaussian radial basis kernel, 366\ngeneralization, 11, 14, 536\nGeneralized Bayes’ Theorem, 251\nGenerative Adversarial Networks, 523\ngenerative model, 733\nGibbs sampling, 298\nGini coefﬁcient, 294, 563, 586, 590\nGini index, 145, 169, 174, 563\nglobal minimum, 318, 319\nGlorot initialization, 458\ngoal, 639, 643, 676\nGoldilocks model, 14\nGPUs, 394\ngradient, 321, 368\ngradient boosting, xvi, 159, 164\ngradient descent, 168, 272, 274, 275, 319, 321,\n368, 403, 541, 655\ngraphical models, 304, 742\ngreedy action selection policy, 641, 656, 680\ngreedy local search problem, 228\ngrid world, 659\nground truth, 607\ngroup think, 158\nguided search, 274, 319, 321\nHadamard product, xxvii, 475, 773\nhamming distance, 240\nHand, David, 586\nharmonic mean, 550, 551, 552, 554, 577, 586,\n698\nHe initialization, 461\nheating load prediction, 371\nHebb’s Postulate, 404\nheterogeneity, 126\nhidden features, 762\nhidden layers, 389\nhistogram, 54, 745, 752, 752\nhistory, 639\nhits, 538\nhold-out sampling, 541, 547\nhold-out test set, 533, 535, 540, 579, 719\nHuman Activity Recognition Using\nSmartphones Dataset, 636","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":846,"page_label":"792","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"792 Index\nhypercube, 224\nhyperplane, 197, 197, 362\nhypersphere, 201, 217\nID3, seeIterative Dichotomizer 3\nidentity criterion, 184, 211\nidentity function, 624\nidentity matrix, 219\nill-posed problem, 7, 10, 19, 21, 22, 145\nimbalanced data, 193, 550, 693\nimputation, 69, 296\nincremental shrinkage, 168\nindependence, 256, 302\nindependent features, 83\nindex, 211, 212\ninductive bias, 11, 11, 19, 22, 123, 141, 328,\n357, 362, 729, 736\ninductive learning, 11, 729\ninformation, 120\ninformation gain, 117, 120, 129, 130, 133, 135,\n172, 174, 227, 614, 717\ninformation gain ratio, 142, 174\ninformation theory, 117, 126\ninformation-based learning, 19, 117\ninput gate, 508, 511\ninsights, 3\ninstance, 5, 28\nintegration, 276\nintelligent agent, 638, 639, 643, 676, 677\ninter-annotator agreement, 726\ninter-cluster distance, 608\ninter-quartile range, 70, 749, 755\ninteracting features, 227\ninteraction effect, 169\ninteraction term, 355\ninterior nodes, 121\ninterpolate, 748\ninterpretability of models, 739\ninterval data, 34\ninterval size, 276\nintra-cluster distance, 608\ninvalid data, 63, 94\ninvalid outliers, 65, 68, 715\ninvariant distribution, 299\ninverse covariance matrix, 219, 242\ninverse reasoning, 248inverted dropout, 474, 475, 530\nIQR, seeinter-quartile range\nirregular cardinality, 63, 65, 94\nirrelevant features, 227\niteration, 416\nIterative Dichotomizer 3, 11, 117, 133, 133,\n169, 173, 176, 541, 731\nJ48, 169\nJaccard index, 215, 231\nJaccard similarity measure, 635\njackkniﬁng, 545\njoint probability, 246, 251, 759\njoint probability distribution, 247, 761\nk nearest neighbor, 181, 192, 231, 536, 554,\n575, 719, 731, 733, 735\nk-d tree, 181, 196, 212, 232, 241\nk-fold cross validation, 543, 611\nk-means clustering, 597, 600, 601, 624, 629,\n631, 636, 740\nk-means++, 605, 607, 624\nk-medoids clustering, 601\nk-NN, seek nearest neighbor\nK-S chart, seeKolmogorov-Smirnov chart\nK-S statistic, seeKolmogorov-Smirnov statistic\nK2 score, 293\nKaiming initialization, 461\nkernel function, 366, 373\nkernel trick, 366, 373\nknowledge elicitation, 31\nKolmogorov-Smirnov chart, 563\nKolmogorov-Smirnov statistic, 563, 583\nKolmogorov-Smirnov test, 272\nKronecker delta, 192, 195\nL2loss, 168\nlabeled dataset, 9\nLagrange multipliers, 363\nLaplace smoothing, 267, 308\nlazy learner, 232\nleaf nodes, 121\nleaky ReLU, 445\nlearning rate, 168, 323, 332, 379, 422, 654\nlearning rate decay, 334\nleast squares optimization, 318","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":847,"page_label":"793","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index 793\nleave-one-out cross validation, 545\nleft skew, 59\nlevels, 34\nlift, 567, 569, 700\nlift chart, 570\nlight tails, 272\nlinear, 386, 625\nlinear activations, 674\nlinear algebra, xvi, 771\nlinear annealing, 674\nlinear function, 766\nlinear kernel, 366\nlinear relationship, 312, 351, 368\nlinear separator, 339\nlinearly separable, 338, 354, 365, 396\nlinkage method, 618\nlocal models, 189\nlocal receptive ﬁeld, 479\nlocality sensitive hashing, 233\nlocation parameter, 271\nlocation-scale family of distributions, 271\nlogarithm, 124\nlogistic function, 342, 386, 461\nlogistic regression, 311, 338, 342, 368, 556,\n719, 732, 733, 735, 736\nlogistic unit, 386\nlogit, xxvii, 464\nLogitBoost, 171\nlong short-term memory, 508\nlong tails, 59\nlongevity, 33\nloss, 168, 625\nloss function, 168, 315, 409, 670\nloss given default, 554\nlower quartile, 749, 755\nLSTM, seelong short-term memory\nLU decomposition, 220\nlucky split, 543, 586\nLunar Lander, 668\nmachine learning, 4, 5\nmachine learning algorithm, 6, 19\nMAE, seemean absolute error\nMahalanobis distance, 217, 223, 231, 242\nManhattan distance, 185, 185, 231, 237, 577\nMAP, seemaximum a posteriorimapping features, 36, 65\nmargin, 361\nmargin extents, 361, 364\nmargin of error, 751\nmarginalization, 761\nMarkov assumption, 644\nMarkov blanket, 288\nMarkov chain, 298\nMarkov chain Monte Carlo, 298, 733\nMarkov decision process, 638, 643, 645\nMarkov process, 644, 679, 681\nmatrix, 771\nmatrix product, 385\nmax, 489\nmax pooling, 490\nMaxEnt model, 357\nmaximum a posteriori, 254, 261, 556\nmaximum entropy model, 357\nmaximum likelihood, 301\nMCMC, seeMarkov chain Monte Carlo\nMDP, seeMarkov decision process\nmean, 54, 69, 745, 746\nmean absolute error, 577, 578\nmean imputation, 374\nmean squared error, 575\nmean squared error loss, 625\nmeasures of similarity, 181\nmedian, 54, 69, 550, 745, 746, 749, 755\nmetric, 184, 211\nmini-batch gradient descent, 417, 671\nmini-batches, 417\nminimum description length principle, 292\nMinkowski distance, 185, 186\nmisclassiﬁcation rate, 179, 533, 536, 539, 540,\n549, 551\nmisses, 538\nmissing indicator feature, 69\nmissing values, 63, 64, 94, 231, 693\nmixing time, 300\nmixture of Gaussians distribution, 270, 274\nMNIST, 477\nmode, 54, 69, 746, 749\nmode imputation, 374\nmodel ensemble, xvi, 158, 170, 178, 476, 733\nmodel parameters, 314\nmodel residuals, 164\nmodel-based clustering, 629","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":848,"page_label":"794","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"794 Index\nmodel-based reinforcement learning, 643\nmodel-free reinforcement learning, 657, 676\nModeling, 17, 19, 20, 87, 697, 719, 730\nMonte Carlo methods, 298, 677\nMSE, seemean squared error\nmulti-armed bandit problem, 656\nmulti-feature, 319\nmulti-label classiﬁcation, 742\nmulti-layer perceptron, 673\nmultimodal distribution, 60, 274\nmultinomial logistic regression, 357, 376\nmultinomial model, 311, 369, 572\nmultivariable, 319\nmultivariable linear regression, 319, 319, 575\nmultivariable linear regression with gradient\ndescent, 11, 311, 731\nN rays, 533\nnaive Bayes model, 243, 261, 284, 308, 309,\n556, 731, 732, 735, 736\nnaive neural Q-learning, 671\nnatural language processing, 234\nnatural logarithm, 580\nnearest neighbor, 303, 732, 736\nnearest neighbor algorithm, 181, 187, 231\nnegative level, 537\nnegatively covariant, 74\nnetwork freezing, 671\nneural network, 369, 381, 383, 599, 624, 629,\n669, 735\nnext-best-offer model, 37\nNo Free Lunch Theorem, 13, 736\nnodes, 286\nnoise, 7, 66, 69, 191\nnoise dampening mechanism, 157\nnon-linear model, 311\nnon-linear relationship, 368\nnon-negativity criterion, 184, 211\nnon-parametric model, 732\nnormal distribution, 59, 61, 71, 78, 269, 270,\n557\nnormalization, 87, 181, 206, 231, 329, 332, 346,\n421\nnormalization constant, 251\nnormalized mutual information, 611\nnull hypothesis, 333numeric data, 34\nobservation period, 37, 689\nOccam’s razor, 123, 292\noff-policy reinforcement learning, 659, 676\non-line gradient descent, 415\non-policy reinforcement learning, 664, 676\none-class classiﬁcation, 235\none-hot encoding, 463\none-row-per-subject, 29\none-versus-all model, 357, 357, 358, 367, 369\nongoing model validation, 579, 702\nOpenAI, 668\nOpenAI Gym, 668\noptimal control domain, 653\nordinal data, 34\nother features, 36\nout-of-time sampling, 546, 586\noutcome, 757, 758\noutcome period, 37, 689\noutlier detection, 235\noutliers, 63, 65, 87, 91, 94, 696, 745, 755\noutput gate, 508, 512\nover-sampling, 93\noverﬁtting, 14, 153, 157, 193, 256, 265, 432,\n434, 472, 541\noverlap metric, 240\np-value, 333\npadding, 487\npaperclip maximizer, 677\nparadox of the false positive, 254\nparameterized model, 311, 312, 314\nparametric model, 732\nParametric ReLU, 445\nparent node, 286\nPareto charts, 752\nparsimony, 646\npartial derivative, 312, 318, 381, 765, 768\npartially observable environments, 640\nPascal, Blaise, 243\npatience, 473\nPDF, seeprobability density function\nPearson correlation, 82, 223\nPearson, Karl, 82\npeeking, 536","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":849,"page_label":"795","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index 795\npercentiles, 54, 91, 567, 748\nperceptron, 396\nperceptron learning rule, 342\nperformance measure, 535, 540\npersonal data, 40\nplacebo, 584\npolicy, 641, 643, 676\npolicy gradient, 641\npolicy-based reinforcement learning, 643\npolynomial functions, 766\npolynomial kernel, 366\npolynomial relationship, 352\npopulation, 750\npopulation mean, 61\npopulation parameters, 751\npopulation standard deviation, 61\npositive level, 537, 538\npositively covariant, 74\npost-pruning, 155, 698\nposterior probability, 759\nposterior probability distribution, 250\npre-pruning, 155, 169\nprecision, 548, 549, 572\nprediction, 4, 758\nprediction model, 3, 19\nprediction score, 556, 574\nprediction speed, 738\nprediction subject, 23, 29, 689, 707\npredictive data analytics, 3, 3, 21\npredictive features, 227\npreference bias, 11\npresence-absence, 214\nprice prediction, 3\nprior probability, 251, 759\nprobability density function, 61, 246, 269, 758\nprobability distribution, 59, 94, 247, 752, 761\nprobability function, 246, 758\nprobability mass, 266, 758\nprobability mass function, 246, 758\nprobability theory, 243, 757\nprobability-based learning, 19, 243\nproduct rule, 245, 249, 757, 762\nproﬁt matrix, 553, 592\npropensity modeling, 4, 36, 689\nproportions, 749\nproxy features, 33, 36\npruning, 117, 170pruning dataset, 155\npurpose speciﬁcation principle, 41\nQ-learning, 637, 657, 657, 676, 680, 741\nquadratic function, 352, 766\nR, 222, 276\nR2, 578, 586\nr-trees, 233\nrandom action selection policy, 656\nrandom forest, 159, 159, 171, 175\nrandom sampling, 92\nrandom sampling with replacement, 93\nrandom sampling without replacement, 94\nrandom variable, 246, 652, 757, 758\nrange, 747\nrange normalization, 87, 87, 101, 206, 322, 340,\n342, 374, 375, 421, 422\nrank and prune, 227, 614\nrate parameter, 274\nratio features, 35\nraw features, 34, 41, 45\nrecall, 548, 549, 551, 572\nreceiver operating characteristic curve, 558, 589\nreceiver operating characteristic index, 558,\n561, 586, 593\nreceiver operating characteristic space, 559\nrectiﬁed linear activation function, 386, 625,\n626, 674\nrectiﬁed linear unit, 386\nrectiﬁer, 386\nrecurrent neural network, 434, 499\nreduced error pruning, 155, 174, 698\nredundant features, 227\nregression task, 149\nregression tree, 149, 165\nregularization, 477\nreinforcement learning, xvi, 5, 20, 637, 638,\n674, 740\nrelative frequency, 245, 757, 759\nrelative rarity, 720\nReLU, seerectiﬁed linear unit\nreplay memory, 671\nreplicated training set, 160\nrepresentation learning, 401, 599, 624, 629","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":849,"page_label":"795","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"rate parameter, 274\nratio features, 35\nraw features, 34, 41, 45\nrecall, 548, 549, 551, 572\nreceiver operating characteristic curve, 558, 589\nreceiver operating characteristic index, 558,\n561, 586, 593\nreceiver operating characteristic space, 559\nrectiﬁed linear activation function, 386, 625,\n626, 674\nrectiﬁed linear unit, 386\nrectiﬁer, 386\nrecurrent neural network, 434, 499\nreduced error pruning, 155, 174, 698\nredundant features, 227\nregression task, 149\nregression tree, 149, 165\nregularization, 477\nreinforcement learning, xvi, 5, 20, 637, 638,\n674, 740\nrelative frequency, 245, 757, 759\nrelative rarity, 720\nReLU, seerectiﬁed linear unit\nreplay memory, 671\nreplicated training set, 160\nrepresentation learning, 401, 599, 624, 629\nrepresentational capacity, 396","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":850,"page_label":"796","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"796 Index\nresidual, 315\nrestriction bias, 11, 357\nreturn, 640\nreward, 639, 643, 676\nreward hypothesis, 640\nright skew, 59\nrisk assessment, 3\nRMSE, seeroot mean squared error\nRNN, seerecurrent neural network\nROC curve, seereceiver operating characteristic\ncurve\nROC index, seereceiver operating characteristic\nindex\nROC space, seereceiver operating characteristic\nspace\nroot mean squared error, 577, 578\nroot node, 121\nRussel-Rao index, 214, 231\nS-I-R models, 644\nsabremetrics, 183\nsample, 541, 745, 750, 751\nsample covariance, 81\nsample mean, 745\nsample space, 246, 757, 758\nsampling, 87, 91\nsampling bias, 12\nsampling density, 224\nsampling method, 541, 546\nsampling variance, 153\nsampling with replacement, 159\nsampling without replacement, 159\nSARSA, 664, 664, 666, 676\nsaturated, 437, 447\nscalar, 771\nscale parameter, 271\nscatter plot, 73, 183\nscatter plot matrix, 74, 84, 103\nSDSS, seeSloan Digital Sky Survey\nsecond mode, 749\nsecond order polynomial function, 352, 766\nseeds, 600\nselection bias, 12\nsemi-supervised learning, 5, 742\nsensitivity, 548, 559\nseparating hyperplane, 362sequential gradient descent, 415\nShannon, Claude, 731\nsigmoid, 625, 626\nsilhouette, 609, 610\nsilhouette method, 612\nsilhouette plot, 610\nsilhouette width, 609\nsimilarity index, 212, 231\nsimilarity measure, 181\nsimilarity-based learning, 19, 181\nsimple linear regression, 314, 732, 735\nsimple multivariable linear regression, 367\nsimple random sample, 751\nsingle linkage, 619, 620, 635\nsituational ﬂuency, 24, 48, 686, 706\nskew, 59\nSloan Digital Sky Survey, 703\nslope of a line, 313, 766\nsmall multiples, 75, 77\nsmoothing, 243, 265, 266, 282\nsocial science, 293\nsoft margin, 366\nsoftmax function, 463, 658\nsoftmax output layer, 434, 463, 463, 495\nSokal-Michener index, 214, 231\nspam ﬁltering, 262\nsparse, 443\nsparse data, 215, 223, 225, 237, 262\nspeciﬁcity, 548, 559\nspectral clustering, 629\nSPLOM, seescatter plot matrix\nsquared error, 378\nstability index, 580, 590, 591, 727\nstacked bar plot, 77\nstale model, 578, 580, 583, 702\nstandard deviation, 54, 748\nstandard error, 333\nstandard normal distribution, 62\nstandard scores, 87, 717\nstandardization, 87, 101, 421, 450, 455\nstate, 640, 643, 676\nstate generation function, 640, 673\nstationarity assumption, 234\nstationary distribution, 299\nstatistical inference, 751\nstatistical signiﬁcance, 585\nstatistical signiﬁcance test, 333","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":851,"page_label":"797","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"Index 797\nstatistics, 369\nstep-wise sequential search, 722, 723\nstochastic, 415\nstochastic gradient descent, 327, 415\nstratiﬁcation feature, 93\nstratiﬁed sampling, 93, 710\nstride, 486\nstudent- tdistribution, 271, 272\nstunted trees, 701\nsub-sampling layers, 489\nsubagging, 159\nsubjective estimate, 757\nsubset generation, 228\nsubset selection, 228\nsubspace sampling, 159\nsum of squared errors, 315, 367, 409, 411, 424,\n426, 433, 441, 575, 578, 731\nsummary statistics, 96\nsumming out, 247, 761–763\nsupervised learning, 5, 21, 597, 598, 674\nsupport vector machine, 311, 332, 361, 369,\n373, 719, 732, 733, 735\nsupport vectors, 362\nSVM, seesupport vector machine\nsymmetry criterion, 184, 211\nt-test, 333\ntanh, 387, 461\nTanimoto similarity, 223\ntarget feature, 5, 19, 28, 598\ntarget hypersphere, 201\ntarget level imbalance, 719\ntarget network freezing, 672\ntarget policy, 657, 664, 680\ntaxi-cab distance, 185\nTD(0), 655\nTD-Gammon, 677\ntemporal-difference learning, 637, 638, 654,\n654, 676\ntermination condition, 229\ntest set, 535, 541\ntest statistic, 333\ntext analytics, 262\ntextual data, 34\nTheorem of Total Probability, 245, 249–251,\n757, 763, 764thinning, 299\nthird order polynomial function, 766\nThree Laws of Robotics, 677\ntiming, 33\nTN,seetrue negative\nTNR, seetrue negative rate\ntolerance, 323\ntop sampling, 92\ntotal sum of squares, 578\nTP,seetrue positive\nTPR, seetrue positive rate\ntraining instance, 6\ntraining set, 6, 541, 719\nTransformer, 523\nTransparency International, 237\ntranspose, 772\ntrapezoidal method, 562, 593\ntreatment group, 583\ntree pruning, 154, 169\ntriangular inequality criterion, 184, 211\ntrue negative, 537\ntrue negative rate, 548, 558\ntrue positive, 537\ntrue positive rate, 548, 549, 558\nTwentyTwos, 645, 647, 680\ntwo-stage model, 722, 725\ntype I errors, 538\ntype II errors, 538\nunbiased estimate, 752\nunconditional probability, 759\nunder-sampled training set, 720\nunder-sampling, 93, 720\nunderﬁtting, 14, 193\nuniform distribution, 59\nunimodal distribution, 59, 272\nunit hypercube, 224\nunits, 386\nuniversal approximation theorem, 400\nuniversal approximators, 400\nunstable gradients, 449\nunsupervised learning, xvi, 5, 20, 597, 598, 628,\n674, 740\nupper quartile, 749, 755\nupsell model, 213, 572\nuse limitation principle, 41","type":"Document"}
{"id":null,"metadata":{"producer":"pdfTeX-1.40.18","creator":"LaTeX with hyperref package","creationdate":"2020-06-30T23:12:20+01:00","author":"John D. Kelleher;Brian Mac Namee;Aoife D'Arcy;","ebx_publisher":"MIT Press","keywords":"","moddate":"2020-09-03T04:31:31+05:30","ptex.fullbanner":"This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3","subject":"","title":"Fundamentals of Machine Learning for Predictive Data Analytics","trapped":"/False","source":"c:\\Users\\wwwab\\Development\\ministry_rag_q-a\\knowledge_base\\raw_data\\documents\\Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","total_pages":853,"page":852,"page_label":"798","file_name":"Fundamentals_of_Machine_Learning_for_Predictive_Data_Analytics_2020.pdf","file_type":".pdf"},"page_content":"798 Index\nvalid data, 63, 94\nvalid outliers, 65, 68, 715\nvalid pixels, 487\nvalidation dataset, 155, 473, 541, 541, 719\nvalue function, 642\nvalue-based reinforcement learning, 676\nvanishing gradients, 387, 403, 435, 451, 507\nvariable elimination, 298\nvariable selection, 227\nvariance, 149, 206, 220, 747, 747, 748, 752\nvariation, 54, 745, 746\nvariational auto-encoders, 630\nvariational RNN, 507\nvector, 216, 771\nviolin plot, 451\nV oronoi region, 189\nV oronoi tessellation, 189, 231\nweak learners, 165\nweight sharing, 483\nweight space, 317, 321, 338, 369\nweight update rule, 327, 403\nweighted dataset, 160\nweighted k nearest neighbor, 194, 209, 237, 238\nweighted sum, 524\nweighted variance, 149\nweights, 314\nWestern Electric rules, 579\nwhiskers, 755\nWilcoxon-Mann-Whitney statistic, 563\nWisconsin breast cancer dataset, 109, 377\nwrapper-based feature selection, 228, 541, 722\nXavier initialization, 458, 458, 459, 461\nXGBoost, 171\nz-score, 87\nz-transform, 87","type":"Document"}
